{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to KrakenHashes","text":"Distributed Password Cracking System for Security Professionals"},{"location":"#what-is-krakenhashes","title":"What is KrakenHashes?","text":"<p>KrakenHashes is a powerful distributed password cracking platform that coordinates GPU and CPU resources across multiple agents to perform high-speed hash cracking. Built for security professionals, penetration testers, and red teams, it provides a secure web interface for managing complex password auditing operations.</p> <ul> <li> <p> High Performance</p> <p>Leverage distributed GPU/CPU resources across multiple agents for maximum cracking speed</p> </li> <li> <p> Enterprise Security</p> <p>JWT authentication, MFA support, role-based access control, and encrypted communications</p> </li> <li> <p> Modern Web Interface</p> <p>Intuitive React-based UI for job management, real-time monitoring, and result analysis</p> </li> <li> <p> Scalable Architecture</p> <p>Add agents dynamically, schedule resources, and manage workloads efficiently</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get KrakenHashes running in under 5 minutes with Docker!</p> <p>Requirements: Docker Engine 19.03.0+ and Docker Compose v2.0+</p> <pre><code># Download and run\nmkdir krakenhashes &amp;&amp; cd krakenhashes\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/docker-compose.yml\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/.env.example\ncp .env.example .env # Edit .env to set passwords\n</code></pre> <p>Once the .env has been edited you can run the following <pre><code>docker compose up -d  # Note: use 'docker compose' not 'docker-compose'\n</code></pre></p> <p>Get Started  Installation Guide</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#intelligent-job-management","title":"Intelligent Job Management","text":"<ul> <li>Preset job templates and workflows for common attack patterns</li> <li>Priority-based scheduling with adaptive load balancing</li> <li>Automatic job chunking for optimal distribution</li> <li>Real-time progress tracking and ETA calculations</li> </ul>"},{"location":"#comprehensive-hash-support","title":"Comprehensive Hash Support","text":"<ul> <li>Support for 300+ hash types via Hashcat</li> <li>Manual hash type selection with detailed metadata</li> <li>Bulk hash import and management</li> <li>Client-based organization for engagements</li> </ul>"},{"location":"#resource-management","title":"Resource Management","text":"<ul> <li>Centralized wordlist and rule file management</li> <li>Automatic file synchronization to agents</li> <li>Binary version management for Hashcat</li> <li>Efficient storage with deduplication</li> </ul>"},{"location":"#real-time-monitoring","title":"Real-Time Monitoring","text":"<ul> <li>Live job progress visualization</li> <li>Agent health and performance metrics</li> <li>GPU/CPU temperature and usage tracking</li> <li>WebSocket-based real-time updates</li> </ul>"},{"location":"#multi-user-collaboration","title":"Multi-User Collaboration","text":"<ul> <li>Role-based access control (Admin/User)</li> <li>Client and engagement management</li> <li>Audit logging for compliance</li> <li>Data retention policies</li> </ul>"},{"location":"#enterprise-security","title":"Enterprise Security","text":"<ul> <li>Multi-factor authentication (TOTP, Email, Backup codes)</li> <li>TLS/SSL support with multiple certificate options</li> <li>API key authentication for agents</li> <li>JWT-based session management with refresh tokens</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li> <p>Penetration Testing</p> <p>Coordinate password audits across multiple client engagements with proper data isolation</p> </li> <li> <p>Security Assessments</p> <p>Validate password policies by testing organizational hash dumps against common patterns</p> </li> <li> <p>Incident Response</p> <p>Quickly crack passwords during forensic investigations and evidence recovery</p> </li> <li> <p>Security Research</p> <p>Analyze hash algorithm vulnerabilities and benchmark cracking performance</p> </li> </ul>"},{"location":"#system-components","title":"System Components","text":"<pre><code>graph LR\n    A[Web Interface] --&gt;|HTTPS| B[Backend API]\n    B --&gt;|WebSocket| C[Agent Pool]\n    B --&gt;|SQL| D[(PostgreSQL)]\n    C --&gt;|Executes| E[Hashcat]\n    B --&gt;|Stores| F[File Storage]</code></pre> <ul> <li>Backend Service - Go-based API server with job scheduling and resource management</li> <li>Web Interface - React frontend with Material-UI for intuitive user experience  </li> <li>Agent System - Distributed agents that execute Hashcat with hardware optimization</li> <li>PostgreSQL Database - Reliable storage for jobs, results, and system configuration</li> </ul>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li> <p> Getting Started</p> <p>New to KrakenHashes? Start here for installation and your first crack</p> </li> <li> <p> User Guide</p> <p>Learn how to create jobs, manage hashlists, and analyze results</p> </li> <li> <p> Admin Guide</p> <p>System configuration, user management, and operational procedures</p> </li> <li> <p> Deployment</p> <p>Production deployment, Docker setup, and update procedures</p> </li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Alpha Software</p> <p>KrakenHashes is currently in alpha development (v0.1.0). While core functionality is implemented, expect:</p> <ul> <li>Breaking changes between versions</li> <li>Incomplete features and documentation</li> <li>No migration path for data until v1.0</li> <li>Active development with frequent updates</li> </ul> <p>See our GitHub repository for the latest development status and roadmap.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li> Documentation - You're here! Browse the guides</li> <li> GitHub Issues - Report bugs and request features</li> <li> Discord Community - Join our Discord server</li> </ul>"},{"location":"#license","title":"License","text":"<p>KrakenHashes is open source software licensed under the GNU Affero General Public License v3.0.</p>"},{"location":"admin-guide/","title":"Administrator Guide","text":"<p>Comprehensive guide for KrakenHashes system administrators.</p>"},{"location":"admin-guide/#in-this-section","title":"In This Section","text":"<ul> <li> <p> System Setup</p> <p>Configure KrakenHashes for your environment</p> </li> <li> <p> Resource Management</p> <p>Manage binaries, wordlists, rules, and storage</p> </li> <li> <p> Operations</p> <p>User management, monitoring, and maintenance</p> </li> <li> <p> Advanced Features</p> <p>Presets, chunking, and performance optimization</p> </li> <li> <p> Security Guide</p> <p>Security considerations, data retention, and best practices</p> </li> </ul>"},{"location":"admin-guide/#first-time-setup-sequence","title":"First-Time Setup Sequence","text":"<p>When setting up KrakenHashes for the first time, follow this sequence:</p> <ol> <li>Upload Hashcat Binary (Required First)</li> <li>Navigate to Admin \u2192 Binary Management</li> <li>Upload a compressed hashcat binary (.7z, .zip, .tar.gz)</li> <li>Wait for verification to complete</li> <li> <p>This triggers creation of system preset jobs including the potfile job</p> </li> <li> <p>Verify Potfile Initialization</p> </li> <li>Check Resources \u2192 Wordlists for \"Pot-file\" entry</li> <li>Check Admin \u2192 Preset Jobs for \"Potfile Run\" job</li> <li> <p>Both should exist after binary upload</p> </li> <li> <p>Continue with Standard Setup</p> </li> <li>Upload wordlists</li> <li>Configure agents</li> <li>Create hashlists</li> </ol>"},{"location":"admin-guide/#quick-links","title":"Quick Links","text":""},{"location":"admin-guide/#initial-setup","title":"Initial Setup","text":"<ol> <li>System Configuration</li> <li>SSL/TLS Setup</li> <li>Email Configuration</li> <li>Authentication Settings</li> </ol>"},{"location":"admin-guide/#daily-operations","title":"Daily Operations","text":"<ul> <li>User Management</li> <li>Agent Management</li> <li>Job Execution Settings</li> <li>System Monitoring</li> <li>Potfile Management</li> <li>Data Retention</li> <li>Backup Procedures</li> </ul>"},{"location":"admin-guide/#optimization","title":"Optimization","text":"<ul> <li>Performance Tuning</li> <li>Job Chunking</li> <li>Storage Management</li> </ul>"},{"location":"admin-guide/#administrative-tasks","title":"Administrative Tasks","text":""},{"location":"admin-guide/#system-maintenance","title":"System Maintenance","text":"<ul> <li> Regular updates and patches</li> <li> Database maintenance and optimization</li> <li> File system cleanup and organization</li> <li> Performance monitoring and tuning</li> </ul>"},{"location":"admin-guide/#security-management","title":"Security Management","text":"<ul> <li> User access control and auditing</li> <li> Security policy enforcement</li> <li> Certificate management and renewal</li> <li> Incident response procedures</li> <li> Secure data deletion and retention</li> <li> Security Best Practices</li> </ul>"},{"location":"admin-guide/#resource-management","title":"Resource Management","text":"<ul> <li>:material-gpu: Agent capacity planning</li> <li> Storage allocation and cleanup</li> <li> Wordlist and rule curation</li> <li> Binary version management</li> </ul>"},{"location":"admin-guide/#best-practices","title":"Best Practices","text":"<p>Security First</p> <ul> <li>Enable MFA for all administrative accounts</li> <li>Regularly rotate API keys and passwords</li> <li>Monitor system logs for suspicious activity</li> <li>Keep all components updated</li> </ul> <p>Performance</p> <ul> <li>Schedule intensive jobs during off-peak hours</li> <li>Monitor agent resource utilization</li> <li>Implement data retention policies</li> <li>Optimize database indexes regularly</li> </ul>"},{"location":"admin-guide/#need-help","title":"Need Help?","text":"<ul> <li> Check specific guides in this section</li> <li>:material-discord: Join our Discord #admin channel</li> <li> Contact support for enterprise assistance</li> </ul>"},{"location":"admin-guide/security/","title":"Security Guide","text":"<p>This guide covers security considerations and best practices for KrakenHashes deployment and operation.</p>"},{"location":"admin-guide/security/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Security</li> <li>Data Retention Security</li> <li>Authentication &amp; Access Control</li> <li>Network Security</li> <li>Agent Security</li> <li>Database Security</li> <li>File System Security</li> <li>Audit &amp; Compliance</li> </ol>"},{"location":"admin-guide/security/#data-security","title":"Data Security","text":""},{"location":"admin-guide/security/#password-hash-protection","title":"Password Hash Protection","text":"<p>KrakenHashes handles sensitive password hash data. Follow these practices:</p> <ul> <li>Access Control: Limit access to hashlists based on roles and teams</li> <li>Client Isolation: Hashlists are associated with specific clients for data segregation</li> <li>Secure Storage: Hash files stored with restricted permissions in <code>/var/lib/krakenhashes/hashlists/</code></li> <li>No Plaintext Logging: System never logs recovered passwords or sensitive hash values</li> </ul>"},{"location":"admin-guide/security/#cracked-password-handling","title":"Cracked Password Handling","text":"<ul> <li>Passwords stored only in database after successful crack</li> <li>No automatic export of cracked passwords</li> <li>Access to cracked passwords requires authentication</li> <li>Potfile managed separately with controlled access</li> </ul> <p>Potfile Security</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) contains plaintext passwords from ALL cracked hashes and is NOT affected by the retention system. This means: - Passwords remain in the potfile even after hashlists are deleted - The potfile grows indefinitely unless manually managed - May conflict with data protection compliance requirements - Must be secured with strict file permissions and access controls</p>"},{"location":"admin-guide/security/#data-retention-security","title":"Data Retention Security","text":""},{"location":"admin-guide/security/#secure-data-deletion","title":"Secure Data Deletion","text":"<p>The retention system implements multiple layers of secure deletion to prevent data recovery:</p>"},{"location":"admin-guide/security/#1-file-system-security","title":"1. File System Security","text":"<p>When files are deleted due to retention policies:</p> <ul> <li>Random Overwrite: Files are overwritten with random data before deletion</li> <li>Multiple Passes: Sensitive data overwritten in 4KB chunks</li> <li>Immediate Removal: Files removed from filesystem after overwrite</li> <li>No Recovery: Prevents standard file recovery tools from retrieving data</li> </ul> <p>Implementation: <pre><code>// Files are overwritten with random data\nrandomData := make([]byte, 4096)\nfor written &lt; fileInfo.Size() {\n    rand.Read(randomData)\n    file.Write(randomData)\n}\nos.Remove(filePath)\n</code></pre></p>"},{"location":"admin-guide/security/#2-database-security","title":"2. Database Security","text":"<p>Database records are protected against recovery:</p> <ul> <li>Transaction Safety: All deletions occur within database transactions</li> <li>CASCADE Deletion: Related records automatically removed via foreign key constraints</li> <li>Orphan Cleanup: Hashes not linked to any hashlist are deleted</li> <li>VACUUM Operations: PostgreSQL VACUUM ANALYZE prevents recovery from:</li> <li>Dead tuples in heap files</li> <li>Write-Ahead Log (WAL) entries</li> <li>Transaction logs</li> </ul> <p>Affected tables during retention purge: - <code>hashlists</code> (primary target) - <code>hashlist_hashes</code> (associations) - <code>hashes</code> (orphaned entries) - <code>agent_hashlists</code> (distribution records) - <code>job_executions</code> (job history) - <code>job_tasks</code> (task assignments)</p>"},{"location":"admin-guide/security/#3-agent-side-cleanup","title":"3. Agent-Side Cleanup","text":"<p>Agents automatically clean temporary files:</p> <ul> <li>3-Day Retention: Temporary files removed after 3 days</li> <li>Automatic Process: Runs every 6 hours</li> <li>Protected Resources: Base files (binaries, wordlists, rules) never auto-deleted</li> <li>Storage Management: Prevents disk space exhaustion on compute nodes</li> </ul>"},{"location":"admin-guide/security/#retention-policy-configuration","title":"Retention Policy Configuration","text":"<ul> <li>System Default: Configurable default retention period for all data</li> <li>Client-Specific: Per-client retention overrides for compliance</li> <li>Audit Trail: All deletions logged with timestamp and affected records</li> <li>Manual Override: Administrators can trigger immediate purge if needed</li> </ul>"},{"location":"admin-guide/security/#potfile-exclusion-from-retention","title":"Potfile Exclusion from Retention","text":"<p>Critical Security Gap</p> <p>The potfile is NOT managed by the retention system and requires separate security procedures:</p> <p>Security Implications: - Plaintext passwords persist indefinitely in the potfile - Deleted hashlist passwords remain recoverable from potfile - No automatic cleanup when clients/hashlists are deleted - Potential compliance violation for GDPR/data protection laws</p> <p>Required Actions: 1. Implement manual potfile cleanup procedures 2. Create audit trail for potfile modifications 3. Consider encrypting the potfile at rest 4. Restrict potfile access to minimum required personnel 5. Document potfile retention policy separately for compliance</p>"},{"location":"admin-guide/security/#authentication-access-control","title":"Authentication &amp; Access Control","text":""},{"location":"admin-guide/security/#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>KrakenHashes supports multiple MFA methods:</p> <ul> <li>TOTP: Time-based One-Time Passwords via authenticator apps (Bitwarden, Google Authenticator, Authy)</li> <li>Email: Verification codes sent to registered email</li> <li>Passkey: WebAuthn/FIDO2 credentials using security keys or platform authenticators</li> <li>Backup Codes: Recovery codes for emergency access</li> </ul>"},{"location":"admin-guide/security/#passkey-security","title":"Passkey Security","text":"<p>Passkeys provide the highest level of MFA security:</p> <ul> <li>Phishing Resistant: Credentials bound to specific domain (RP ID)</li> <li>Clone Detection: Sign count validation detects duplicated authenticators</li> <li>No Shared Secrets: Public key cryptography instead of shared secrets</li> <li>User Presence: Requires user interaction (touch, biometric)</li> </ul>"},{"location":"admin-guide/security/#password-security","title":"Password Security","text":"<ul> <li>Bcrypt Hashing: All passwords hashed with bcrypt</li> <li>Configurable Requirements: Min length, complexity rules</li> <li>Password History: Prevents password reuse</li> <li>Account Lockout: Automatic lockout after failed attempts</li> </ul>"},{"location":"admin-guide/security/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>System roles with increasing privileges:</p> <ol> <li>User: Standard access to assigned resources</li> <li>Admin: Full system administration</li> <li>Agent: Agent-specific operations only</li> <li>System: Internal system operations</li> </ol>"},{"location":"admin-guide/security/#jwt-token-security","title":"JWT Token Security","text":"<ul> <li>Configurable Session Duration: Default 60-minute session timeout (configurable by admin)</li> <li>Sliding Window Sessions: Sessions automatically extend on user activity after \u2153 of session time, eliminating unexpected logouts during active use</li> <li>Grace Period: 5-minute grace period when tokens refresh, allowing concurrent requests from multiple browser tabs to complete</li> <li>Secure Storage: Tokens never logged or stored in plaintext</li> <li>Revocation Support: Immediate token invalidation</li> </ul>"},{"location":"admin-guide/security/#session-token-linking","title":"Session-Token Linking","text":"<p>KrakenHashes enforces true session termination through a database-backed token-session relationship:</p> <ul> <li>Bound Sessions: Each active session is linked to its JWT token via foreign key constraint</li> <li>CASCADE Delete: Terminating a session automatically revokes the associated JWT token from the database</li> <li>True Logout: Session termination immediately invalidates authentication - users cannot continue accessing resources</li> <li>No Orphaned Tokens: Database constraints prevent tokens from persisting after session removal</li> <li>Security Enforcement: Authentication middleware validates both token existence and session validity</li> </ul> <p>This architecture ensures that when a user or administrator terminates a session through the UI, the action has immediate effect. The JWT token is removed from the database, causing the authentication middleware to reject subsequent requests with that token.</p>"},{"location":"admin-guide/security/#network-security","title":"Network Security","text":""},{"location":"admin-guide/security/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<p>Multiple TLS modes supported:</p> <ol> <li>Self-Signed Certificates</li> <li>Automatic generation with proper extensions</li> <li>Browser-compatible certificates</li> <li> <p>Full certificate chain support</p> </li> <li> <p>Provided Certificates</p> </li> <li>Custom certificate installation</li> <li>Certificate validation</li> <li> <p>Chain verification</p> </li> <li> <p>Let's Encrypt</p> </li> <li>Automatic renewal via ACME</li> <li>Production-ready certificates</li> </ol>"},{"location":"admin-guide/security/#api-security","title":"API Security","text":"<ul> <li>Rate Limiting: Prevents abuse and DoS attacks</li> <li>CORS Configuration: Controlled cross-origin access</li> <li>Request Validation: Input sanitization and validation</li> <li>API Key Authentication: Secure agent authentication</li> </ul>"},{"location":"admin-guide/security/#agent-security","title":"Agent Security","text":""},{"location":"admin-guide/security/#registration-security","title":"Registration Security","text":"<ul> <li>Claim Codes: One-time or continuous registration codes</li> <li>API Key Generation: Unique keys per agent</li> <li>Certificate Exchange: TLS certificate verification</li> <li>Voucher Expiration: Time-limited registration windows</li> </ul>"},{"location":"admin-guide/security/#communication-security","title":"Communication Security","text":"<ul> <li>WebSocket over TLS: Encrypted agent communication</li> <li>Heartbeat Monitoring: Detect disconnected agents</li> <li>Message Authentication: Signed messages prevent tampering</li> <li>Command Authorization: Agents verify command sources</li> </ul>"},{"location":"admin-guide/security/#file-synchronization","title":"File Synchronization","text":"<ul> <li>Checksum Verification: MD5/SHA validation of transferred files</li> <li>Partial Downloads: Resume support for large files</li> <li>Access Control: Agents access only assigned files</li> <li>Cleanup Policy: Automatic removal of unused files</li> </ul>"},{"location":"admin-guide/security/#database-security","title":"Database Security","text":""},{"location":"admin-guide/security/#connection-security","title":"Connection Security","text":"<ul> <li>TLS Connections: Encrypted database connections</li> <li>Connection Pooling: Limited concurrent connections</li> <li>Prepared Statements: SQL injection prevention</li> <li>Transaction Isolation: ACID compliance</li> </ul>"},{"location":"admin-guide/security/#data-protection","title":"Data Protection","text":"<ul> <li>No Soft Deletes: Hard deletion with CASCADE</li> <li>Audit Tables: Separate audit trail for critical operations</li> <li>UUID Primary Keys: Prevent sequential ID attacks</li> <li>JSONB Validation: Schema validation for JSON fields</li> </ul>"},{"location":"admin-guide/security/#backup-security","title":"Backup Security","text":"<ul> <li>Encrypted Backups: Optional backup encryption</li> <li>Offsite Storage: Remote backup recommendations</li> <li>Point-in-Time Recovery: Transaction log backups</li> <li>Test Restorations: Regular recovery testing</li> </ul>"},{"location":"admin-guide/security/#file-system-security","title":"File System Security","text":""},{"location":"admin-guide/security/#directory-permissions","title":"Directory Permissions","text":"<p>Recommended permissions:</p> <pre><code>/var/lib/krakenhashes/\n\u251c\u2500\u2500 binaries/     (755, krakenhashes:krakenhashes)\n\u251c\u2500\u2500 wordlists/    (755, krakenhashes:krakenhashes)\n\u251c\u2500\u2500 rules/        (755, krakenhashes:krakenhashes)\n\u2514\u2500\u2500 hashlists/    (750, krakenhashes:krakenhashes)  # Restricted\n</code></pre>"},{"location":"admin-guide/security/#path-sanitization","title":"Path Sanitization","text":"<ul> <li>Directory Traversal Prevention: Path validation</li> <li>Symlink Protection: Restricted symlink following</li> <li>Temporary File Security: Secure temp file creation</li> <li>Upload Validation: File type and size limits</li> </ul>"},{"location":"admin-guide/security/#audit-compliance","title":"Audit &amp; Compliance","text":""},{"location":"admin-guide/security/#audit-logging","title":"Audit Logging","text":"<p>Comprehensive audit trail for:</p> <ul> <li>User Actions: Login, logout, configuration changes</li> <li>Data Access: Hashlist views, downloads</li> <li>Administrative Actions: User management, system configuration</li> <li>Security Events: Failed logins, MFA failures, suspicious activity</li> <li>Retention Operations: All deletion operations with details</li> </ul>"},{"location":"admin-guide/security/#compliance-features","title":"Compliance Features","text":"<ul> <li>Data Retention Policies: Configurable per regulations</li> <li>Secure Deletion: Meets data destruction requirements</li> <li>Access Logs: Complete access trail for auditing</li> <li>Export Controls: Restricted data export capabilities</li> </ul>"},{"location":"admin-guide/security/#security-monitoring","title":"Security Monitoring","text":"<p>Monitor these key metrics:</p> <ol> <li>Failed Login Attempts: Potential brute force attacks</li> <li>API Rate Limit Hits: Possible abuse</li> <li>Agent Disconnections: Network or security issues</li> <li>Retention Purge Logs: Verify proper data deletion</li> <li>Database VACUUM Status: Ensure WAL cleanup</li> </ol>"},{"location":"admin-guide/security/#log-retention","title":"Log Retention","text":"<ul> <li>Security Logs: Retain for compliance period</li> <li>Audit Logs: Permanent retention recommended</li> <li>System Logs: Rotate based on size/age</li> <li>Backup Logs: Match backup retention period</li> </ul>"},{"location":"admin-guide/security/#security-best-practices","title":"Security Best Practices","text":""},{"location":"admin-guide/security/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Weekly</li> <li>Review security event logs</li> <li>Check failed login attempts</li> <li> <p>Verify agent connectivity</p> </li> <li> <p>Monthly</p> </li> <li>Audit user access and roles</li> <li>Review retention policy compliance</li> <li> <p>Test backup restoration</p> </li> <li> <p>Quarterly</p> </li> <li>Update TLS certificates</li> <li>Review and update passwords</li> <li>Security assessment</li> </ol>"},{"location":"admin-guide/security/#emergency-procedures","title":"Emergency Procedures","text":"<ol> <li>Suspected Breach</li> <li>Disable affected accounts</li> <li>Revoke all JWT tokens</li> <li>Review audit logs</li> <li> <p>Change system passwords</p> </li> <li> <p>Data Leak</p> </li> <li>Identify affected hashlists</li> <li>Trigger immediate retention purge</li> <li>Notify affected clients</li> <li> <p>Document incident</p> </li> <li> <p>Agent Compromise</p> </li> <li>Revoke agent API key</li> <li>Remove agent from system</li> <li>Audit agent's job history</li> <li>Regenerate claim codes</li> </ol>"},{"location":"admin-guide/security/#conclusion","title":"Conclusion","text":"<p>Security in KrakenHashes is multi-layered, from secure data deletion in the retention system to comprehensive authentication and audit trails. Regular monitoring and adherence to these security practices ensures data protection and compliance with security requirements.</p>"},{"location":"admin-guide/advanced/chunking/","title":"KrakenHashes Chunking System","text":""},{"location":"admin-guide/advanced/chunking/#overview","title":"Overview","text":"<p>KrakenHashes uses an intelligent chunking system to distribute password cracking workloads across multiple agents. This document explains how chunks are created, distributed, and tracked for different attack types.</p>"},{"location":"admin-guide/advanced/chunking/#what-is-chunking","title":"What is Chunking?","text":"<p>Chunking divides large password cracking jobs into smaller, manageable pieces that can be: - Distributed across multiple agents for parallel processing - Completed within a reasonable time frame (default: 20 minutes) - Resumed if interrupted or failed - Tracked for accurate progress reporting</p>"},{"location":"admin-guide/advanced/chunking/#how-chunking-works","title":"How Chunking Works","text":""},{"location":"admin-guide/advanced/chunking/#basic-chunking-no-rules","title":"Basic Chunking (No Rules)","text":"<p>For simple dictionary attacks without rules: 1. The system calculates the total keyspace (number of password candidates) 2. Based on agent benchmark speeds, it determines optimal chunk sizes 3. Each chunk processes a portion of the wordlist using hashcat's <code>--skip</code> and <code>--limit</code> parameters</p> <p>Example:  - Wordlist: 1,000,000 passwords - Agent speed: 1,000,000 H/s - Target chunk time: 1,200 seconds (20 minutes) - Chunk size: 1,200,000,000 candidates - Result: Single chunk processes entire wordlist</p>"},{"location":"admin-guide/advanced/chunking/#enhanced-chunking-with-rules","title":"Enhanced Chunking with Rules","text":"<p>When rules are applied, the effective keyspace multiplies:</p> <p>Effective Keyspace = Wordlist Size \u00d7 Number of Rules</p> <p>For example: - Wordlist: 1,000,000 passwords - Rules: 1,000 rules - Effective keyspace: 1,000,000,000 candidates</p>"},{"location":"admin-guide/advanced/chunking/#rule-splitting","title":"Rule Splitting","text":"<p>When a job with rules would take significantly longer than the target chunk time, KrakenHashes can split the rules:</p> <ol> <li>Detection: If estimated time &gt; 2\u00d7 target chunk time</li> <li>Splitting: Divides rules into smaller files</li> <li>Distribution: Each agent receives full wordlist + partial rules</li> <li>Progress: Tracks completion across all rule chunks</li> </ol> <p>Example: - Wordlist: 1,000,000 passwords - Rules: 10,000 rules - Agent speed: 1,000,000 H/s - Without splitting: 10,000 seconds (2.8 hours) per chunk - With splitting into 10 chunks: 1,000 rules each, ~1,000 seconds per chunk</p>"},{"location":"admin-guide/advanced/chunking/#combination-attacks","title":"Combination Attacks","text":"<p>For combination attacks (-a 1), the effective keyspace is:</p> <p>Effective Keyspace = Wordlist1 Size \u00d7 Wordlist2 Size</p> <p>The system tracks progress through the virtual keyspace while hashcat processes the first wordlist sequentially.</p>"},{"location":"admin-guide/advanced/chunking/#attack-mode-support","title":"Attack Mode Support","text":"Attack Mode Description Chunking Method 0 (Straight) Dictionary Wordlist position + optional rule splitting 1 (Combination) Two wordlists Virtual keyspace tracking 3 (Brute-force) Mask attack Mask position chunking 6 (Hybrid W+M) Wordlist + Mask Wordlist position chunking 7 (Hybrid M+W) Mask + Wordlist Mask position chunking 9 (Association) Per-hash rules Rule splitting when applicable"},{"location":"admin-guide/advanced/chunking/#progress-tracking","title":"Progress Tracking","text":""},{"location":"admin-guide/advanced/chunking/#standard-progress","title":"Standard Progress","text":"<ul> <li>Shows candidates tested vs total keyspace</li> <li>Updates in real-time via WebSocket</li> <li>Accurate percentage completion</li> </ul>"},{"location":"admin-guide/advanced/chunking/#with-rule-multiplication","title":"With Rule Multiplication","text":"<ul> <li>Display format: \"X / Y (\u00d7Z)\" where Z is the multiplication factor</li> <li>Accounts for all rules across all chunks</li> <li>Aggregates progress from distributed rule chunks</li> </ul>"},{"location":"admin-guide/advanced/chunking/#progress-bar-visualization","title":"Progress Bar Visualization","text":"<p>The progress bar always shows: - Green: Completed keyspace - Gray: Remaining keyspace - Percentage: Based on effective keyspace</p>"},{"location":"admin-guide/advanced/chunking/#configuration","title":"Configuration","text":"<p>Administrators can tune chunking behavior via system settings:</p> Setting Default Description <code>default_chunk_duration</code> 1200s Target time per chunk (20 minutes) <code>chunk_fluctuation_percentage</code> 20% Threshold for merging final chunks <code>rule_split_enabled</code> true Enable automatic rule splitting <code>rule_split_threshold</code> 2.0 Time multiplier to trigger splitting <code>rule_split_min_rules</code> 100 Minimum rules before considering split"},{"location":"admin-guide/advanced/chunking/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/advanced/chunking/#for-users","title":"For Users","text":"<ol> <li>Large Rule Files: Will automatically split for better distribution</li> <li>Multiple Rule Files: Multiplication is handled automatically</li> <li>Progress Monitoring: Check effective keyspace in job details</li> <li>Benchmarks: Ensure agents have current benchmarks for accurate chunking</li> </ol>"},{"location":"admin-guide/advanced/chunking/#for-administrators","title":"For Administrators","text":"<ol> <li>Chunk Duration: Balance between progress granularity and overhead</li> <li>Rule Splitting: Monitor temp directory space for large rule files</li> <li>Benchmarks: Configure benchmark validity period appropriately</li> <li>Resource Usage: Rule splitting creates temporary files</li> </ol>"},{"location":"admin-guide/advanced/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/chunking/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check if effective keyspace is much larger than expected</li> <li>Verify agent benchmarks are current</li> <li>Consider enabling rule splitting if disabled</li> </ul>"},{"location":"admin-guide/advanced/chunking/#uneven-distribution","title":"Uneven Distribution","text":"<ul> <li>Some chunks may be larger due to:</li> <li>Fluctuation threshold preventing small final chunks</li> <li>Rule count not evenly divisible</li> <li>Different agent speeds</li> </ul>"},{"location":"admin-guide/advanced/chunking/#rule-splitting-not-occurring","title":"Rule Splitting Not Occurring","text":"<p>Verify: - <code>rule_split_enabled</code> is true - Rule file has &gt; <code>rule_split_min_rules</code> rules - Estimated time exceeds threshold</p>"},{"location":"admin-guide/advanced/chunking/#technical-details","title":"Technical Details","text":""},{"location":"admin-guide/advanced/chunking/#keyspace-calculation","title":"Keyspace Calculation","text":"<pre><code>Attack Mode 0 (Dictionary):\n- Without rules: wordlist_size\n- With rules: wordlist_size \u00d7 total_rule_count\n\nAttack Mode 1 (Combination):\n- Always: wordlist1_size \u00d7 wordlist2_size\n\nAttack Mode 3 (Brute-force):\n- Calculated from mask: charset_size^length\n\nAttack Mode 6/7 (Hybrid):\n- Wordlist_size \u00d7 mask_keyspace\n</code></pre>"},{"location":"admin-guide/advanced/chunking/#chunk-assignment","title":"Chunk Assignment","text":"<ol> <li>Agent requests work</li> <li>System calculates optimal chunk size based on:</li> <li>Agent's benchmark speed</li> <li>Target chunk duration</li> <li>Remaining keyspace</li> <li>Chunk boundaries determined:</li> <li>Start position (skip)</li> <li>Chunk size (limit)</li> <li>Agent receives chunk assignment</li> <li>Progress tracked and aggregated</li> </ol>"},{"location":"admin-guide/advanced/chunking/#rule-chunk-files","title":"Rule Chunk Files","text":"<p>When rule splitting is active: - Temporary files created in configured directory - Named: <code>job_[ID]_chunk_[N].rule</code> - Automatically cleaned up after job completion - Synced to agents like normal rule files</p>"},{"location":"admin-guide/advanced/chunking/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Pre-calculation of optimal chunk distribution</li> <li>Dynamic chunk resizing based on actual speed</li> <li>Rule deduplication before splitting</li> <li>Compression for rule chunk transfers</li> </ul>"},{"location":"admin-guide/advanced/job-priority/","title":"Job Priority and Interruption System","text":""},{"location":"admin-guide/advanced/job-priority/#overview","title":"Overview","text":"<p>KrakenHashes implements a sophisticated priority system that ensures critical password auditing tasks receive the resources they need. The system supports priority-based scheduling, automatic job interruption, and intelligent resource allocation to optimize your password cracking operations.</p>"},{"location":"admin-guide/advanced/job-priority/#priority-system-fundamentals","title":"Priority System Fundamentals","text":""},{"location":"admin-guide/advanced/job-priority/#priority-scale","title":"Priority Scale","text":"<p>Jobs in KrakenHashes use a priority scale from 0 to 100:</p> <ul> <li>Critical Priority (90-100): Emergency response, security incidents</li> <li>High Priority (70-89): Time-sensitive audits, compliance deadlines</li> <li>Normal Priority (40-69): Standard security assessments</li> <li>Low Priority (10-39): Background processing, research tasks</li> <li>Minimal Priority (0-9): Non-urgent, opportunistic processing</li> </ul>"},{"location":"admin-guide/advanced/job-priority/#how-priority-affects-job-execution","title":"How Priority Affects Job Execution","text":"<ol> <li>Job Selection Order: Higher priority jobs are assigned to agents first</li> <li>Resource Allocation: High priority jobs can use more agents simultaneously</li> <li>Queue Position: Within the same priority level, jobs follow FIFO (First-In-First-Out)</li> <li>Interruption Rights: Jobs with high priority override can interrupt lower priority running jobs</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#high-priority-override-feature","title":"High Priority Override Feature","text":""},{"location":"admin-guide/advanced/job-priority/#what-is-high-priority-override","title":"What is High Priority Override?","text":"<p>The high priority override feature allows critical jobs to interrupt lower priority jobs that are currently running. This ensures that urgent tasks don't have to wait for long-running, low-priority jobs to complete.</p>"},{"location":"admin-guide/advanced/job-priority/#when-to-enable-high-priority-override","title":"When to Enable High Priority Override","text":"<p>Enable this feature for jobs that: - Respond to active security incidents - Have strict compliance deadlines - Require immediate results for business-critical decisions - Support time-sensitive investigations</p>"},{"location":"admin-guide/advanced/job-priority/#how-it-works","title":"How It Works","text":"<ol> <li>Trigger Condition: Interruption only occurs when:</li> <li>No agents are available for assignment</li> <li>A high-priority job with override enabled is waiting</li> <li> <p>Lower priority jobs are currently running</p> </li> <li> <p>Interruption Process:</p> </li> <li>System identifies the lowest priority running job</li> <li>Sends stop command to agents working on that job</li> <li>Moves interrupted job to \"pending\" status (not paused)</li> <li> <p>Assigns freed agents to the high-priority job</p> </li> <li> <p>Automatic Resumption:</p> </li> <li>Interrupted jobs automatically resume when agents become available</li> <li>Jobs maintain their progress and continue from where they stopped</li> <li>No manual intervention required</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#configuration","title":"Configuration","text":"<p>To enable high priority override for a preset job:</p> <ol> <li>Navigate to Jobs \u2192 Preset Jobs \u2192 [Job Name]</li> <li>In the Advanced Settings section, toggle \"Allow High Priority Override\"</li> <li>Set an appropriate priority level (typically 70+)</li> <li>Save the preset job</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#job-interruption-behavior","title":"Job Interruption Behavior","text":""},{"location":"admin-guide/advanced/job-priority/#status-transitions","title":"Status Transitions","text":"<p>When a job is interrupted: - Before: Status = \"running\" - During Interruption: Status changes to \"pending\" - After Resumption: Status returns to \"running\"</p>"},{"location":"admin-guide/advanced/job-priority/#what-happens-to-interrupted-jobs","title":"What Happens to Interrupted Jobs?","text":"<ol> <li>Progress Preserved: All completed work is saved</li> <li>Automatic Queue Return: Job returns to pending queue with same priority</li> <li>Smart Resumption: Job continues from last checkpoint, no work repeated</li> <li>Agent Cleanup: Agents properly release resources and become available</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#system-wide-interruption-control","title":"System-Wide Interruption Control","text":"<p>Administrators can enable or disable job interruption globally:</p> <ol> <li>Navigate to Admin Panel \u2192 System Settings</li> <li>Find \"Job Interruption Enabled\" setting</li> <li>Toggle to enable/disable interruption system-wide</li> </ol> <p>When disabled: - No jobs will be interrupted regardless of priority - High priority jobs wait in queue normally - System operates in strict FIFO mode within priority levels</p>"},{"location":"admin-guide/advanced/job-priority/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/advanced/job-priority/#setting-appropriate-priorities","title":"Setting Appropriate Priorities","text":""},{"location":"admin-guide/advanced/job-priority/#security-incident-response-priority-95-100","title":"Security Incident Response (Priority: 95-100)","text":"<pre><code>Priority: 100\nAllow High Priority Override: Yes\nMax Agents: Unlimited\nReason: Immediate threat mitigation required\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#compliance-audit-due-today-priority-80-90","title":"Compliance Audit - Due Today (Priority: 80-90)","text":"<pre><code>Priority: 85\nAllow High Priority Override: Yes\nMax Agents: 10\nReason: Regulatory deadline approaching\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#weekly-security-assessment-priority-50-60","title":"Weekly Security Assessment (Priority: 50-60)","text":"<pre><code>Priority: 55\nAllow High Priority Override: No\nMax Agents: 5\nReason: Routine scheduled assessment\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#research-project-priority-10-20","title":"Research Project (Priority: 10-20)","text":"<pre><code>Priority: 15\nAllow High Priority Override: No\nMax Agents: 2\nReason: Long-term analysis, no deadline\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#priority-strategy-guidelines","title":"Priority Strategy Guidelines","text":"<ol> <li>Reserve High Priorities: Don't use high priorities for routine tasks</li> <li>Consider Business Impact: Align priority with actual business urgency</li> <li>Plan for Interruptions: Design workflows assuming possible interruptions</li> <li>Monitor Resource Usage: Track how priority affects overall throughput</li> <li>Document Priority Decisions: Maintain a priority assignment guide for your team</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#avoiding-priority-inflation","title":"Avoiding Priority Inflation","text":"<p>To prevent \"priority creep\" where all jobs become high priority:</p> <ol> <li>Establish Clear Criteria: Document what qualifies for each priority level</li> <li>Regular Review: Audit priority assignments monthly</li> <li>Default to Normal: Start with priority 50 unless justified otherwise</li> <li>Limit Override Usage: Only enable override for truly critical jobs</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#priority-in-workflows","title":"Priority in Workflows","text":""},{"location":"admin-guide/advanced/job-priority/#workflow-priority-inheritance","title":"Workflow Priority Inheritance","text":"<p>When jobs are created from workflows: 1. Each preset job maintains its configured priority 2. Jobs execute in priority order within the workflow 3. Higher priority jobs from other workflows can interleave</p>"},{"location":"admin-guide/advanced/job-priority/#example-workflow-priority-design","title":"Example Workflow Priority Design","text":"<pre><code>Emergency Response Workflow:\n\u251c\u2500\u2500 Quick Dictionary (Priority: 95)\n\u251c\u2500\u2500 Common Patterns (Priority: 90)\n\u251c\u2500\u2500 Extended Dictionary (Priority: 85)\n\u2514\u2500\u2500 Brute Force Backup (Priority: 80)\n\nStandard Audit Workflow:\n\u251c\u2500\u2500 Leaked Passwords (Priority: 60)\n\u251c\u2500\u2500 Company Variations (Priority: 55)\n\u251c\u2500\u2500 Rule-Based Attack (Priority: 50)\n\u2514\u2500\u2500 Comprehensive Check (Priority: 45)\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#monitoring-priority-impact","title":"Monitoring Priority Impact","text":""},{"location":"admin-guide/advanced/job-priority/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ol> <li>Interruption Frequency: How often jobs are interrupted</li> <li>Wait Time by Priority: Average wait time per priority level</li> <li>Completion Time Impact: Effect of interruptions on job completion</li> <li>Resource Utilization: Agent usage across priority levels</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#identifying-issues","title":"Identifying Issues","text":"<p>Watch for these warning signs: - Frequent interruptions of the same job - Low priority jobs never completing - All jobs set to high priority - Agents constantly switching between jobs</p>"},{"location":"admin-guide/advanced/job-priority/#advanced-scenarios","title":"Advanced Scenarios","text":""},{"location":"admin-guide/advanced/job-priority/#multi-tenant-environments","title":"Multi-Tenant Environments","text":"<p>For systems serving multiple teams or clients:</p> <ol> <li>Priority Ranges per Tenant: Assign priority bands to each tenant</li> <li>Fair Resource Sharing: Implement quotas alongside priorities</li> <li>Override Restrictions: Limit override capability to specific roles</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#scheduled-priority-changes","title":"Scheduled Priority Changes","text":"<p>For jobs that change priority over time:</p> <ol> <li>Escalation: Increase priority as deadlines approach</li> <li>De-escalation: Reduce priority after peak hours</li> <li>Time-Based Rules: Automate priority adjustments based on schedule</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#emergency-override-procedures","title":"Emergency Override Procedures","text":"<p>For critical incidents requiring immediate resources:</p> <ol> <li>Emergency Priority (100): Reserved for security incidents</li> <li>Administrative Override: Allow admins to force interrupt any job</li> <li>Audit Trail: Log all emergency overrides for review</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/job-priority/#job-not-interrupting-lower-priority-work","title":"Job Not Interrupting Lower Priority Work","text":"<p>Check: 1. Is \"Job Interruption Enabled\" in system settings? 2. Does the job have \"Allow High Priority Override\" enabled? 3. Are there actually lower priority jobs running? 4. Do the running jobs allow interruption?</p>"},{"location":"admin-guide/advanced/job-priority/#interrupted-job-not-resuming","title":"Interrupted Job Not Resuming","text":"<p>Verify: 1. Job status is \"pending\" not \"failed\" 2. Agents are available and online 3. No higher priority jobs in queue 4. Job hasn't exceeded retry limits</p>"},{"location":"admin-guide/advanced/job-priority/#excessive-interruptions","title":"Excessive Interruptions","text":"<p>Solutions: 1. Review and adjust priority assignments 2. Increase agent capacity 3. Implement scheduling to reduce contention 4. Consider priority bands to limit interruption cascades</p>"},{"location":"admin-guide/advanced/job-priority/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/advanced/job-priority/#impact-on-system-performance","title":"Impact on System Performance","text":"<ul> <li>Minimal Overhead: Priority checks are lightweight</li> <li>Interruption Cost: ~5-10 seconds to stop and reassign</li> <li>Progress Tracking: Checkpoint frequency affects resumption granularity</li> </ul>"},{"location":"admin-guide/advanced/job-priority/#optimizing-for-priority-systems","title":"Optimizing for Priority Systems","text":"<ol> <li>Appropriate Chunk Sizes: Smaller chunks (5-10 minutes) for better interruption response</li> <li>Checkpoint Frequency: Balance between progress saving and performance</li> <li>Agent Pool Size: More agents reduce need for interruptions</li> <li>Priority Distribution: Spread priorities to reduce conflicts</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"admin-guide/advanced/job-priority/#agent-scheduling","title":"Agent Scheduling","text":"<p>Priority system works with agent scheduling: - Scheduled agents only available during defined hours - Priority determines job selection within available windows - Interruptions respect scheduling boundaries</p>"},{"location":"admin-guide/advanced/job-priority/#max-agents-limits","title":"Max Agents Limits","text":"<p>Priority interacts with max agent settings: - High priority jobs reach max agents first - Lower priority jobs use remaining capacity - Override can free agents even from max-agent-limited jobs</p>"},{"location":"admin-guide/advanced/job-priority/#resource-management","title":"Resource Management","text":"<p>Priority affects resource allocation: - File sync prioritizes high-priority job requirements - Binary selection considers job priority - Wordlist/rule loading optimized for high-priority jobs</p>"},{"location":"admin-guide/advanced/job-priority/#summary","title":"Summary","text":"<p>The KrakenHashes priority and interruption system provides powerful tools for managing competing password auditing demands. By understanding and properly configuring priorities, you can ensure critical tasks complete quickly while maintaining efficient resource utilization across all jobs.</p> <p>Key takeaways: - Use priority levels that reflect actual business urgency - Enable high priority override only for critical jobs - Monitor interruption patterns to optimize settings - Design workflows with priority strategies in mind - Maintain clear documentation of priority policies</p>"},{"location":"admin-guide/advanced/performance/","title":"Performance Tuning Guide","text":"<p>This guide provides comprehensive performance optimization strategies for KrakenHashes deployments. All recommendations are based on the actual implementation details and system architecture.</p>"},{"location":"admin-guide/advanced/performance/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Database Performance</li> <li>Job Execution Optimization</li> <li>Agent Performance</li> <li>File I/O Optimization</li> <li>Network and WebSocket Tuning</li> <li>Storage Performance</li> <li>Monitoring and Benchmarking</li> <li>System Settings Reference</li> </ol>"},{"location":"admin-guide/advanced/performance/#database-performance","title":"Database Performance","text":""},{"location":"admin-guide/advanced/performance/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<p>The system uses PostgreSQL with connection pooling. Current default settings:</p> <pre><code>// From backend/internal/db/db.go\ndb.SetMaxOpenConns(25)      // Maximum number of open connections\ndb.SetMaxIdleConns(5)       // Maximum number of idle connections\ndb.SetConnMaxLifetime(5 * time.Minute)  // Connection lifetime\n</code></pre> <p>Optimization recommendations:</p> <ol> <li> <p>For small deployments (1-10 agents): <pre><code># Keep defaults\nMAX_OPEN_CONNS=25\nMAX_IDLE_CONNS=5\n</code></pre></p> </li> <li> <p>For medium deployments (10-50 agents): <pre><code># Increase connection pool\nMAX_OPEN_CONNS=50\nMAX_IDLE_CONNS=10\n</code></pre></p> </li> <li> <p>For large deployments (50+ agents): <pre><code># Use external connection pooler (PgBouncer)\nMAX_OPEN_CONNS=100\nMAX_IDLE_CONNS=20\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#index-optimization","title":"Index Optimization","text":"<p>The system uses extensive indexing. Key performance indexes:</p> <pre><code>-- Agent performance queries\nidx_agents_status\nidx_agents_last_heartbeat\nidx_agent_performance_metrics_device_lookup (composite)\n\n-- Job execution queries\nidx_job_tasks_job_chunk (composite)\nidx_job_tasks_status\nidx_job_executions_status\n\n-- Hash lookups\nidx_hashes_hash_value (unique)\nidx_hashlists_status\n</code></pre> <p>Monitoring index usage:</p> <pre><code>-- Check index usage statistics\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Find missing indexes\nSELECT \n    schemaname,\n    tablename,\n    attname,\n    n_distinct,\n    most_common_vals\nFROM pg_stats\nWHERE schemaname = 'public'\nAND n_distinct &gt; 100\nAND tablename NOT IN (\n    SELECT tablename \n    FROM pg_indexes \n    WHERE schemaname = 'public'\n);\n</code></pre>"},{"location":"admin-guide/advanced/performance/#query-optimization","title":"Query Optimization","text":"<ol> <li> <p>Batch Processing Configuration: <pre><code># Environment variable\nexport KH_HASHLIST_BATCH_SIZE=1000  # Default: 1000\n\n# Recommendations:\n# - Small hashlists (&lt;100K): 500\n# - Medium hashlists (100K-1M): 1000\n# - Large hashlists (&gt;1M): 2000-5000\n</code></pre></p> </li> <li> <p>Pagination Optimization:</p> </li> <li>Use cursor-based pagination for large result sets</li> <li>Limit page sizes to 100-500 items</li> <li>Add appropriate indexes for ORDER BY columns</li> </ol>"},{"location":"admin-guide/advanced/performance/#job-execution-optimization","title":"Job Execution Optimization","text":""},{"location":"admin-guide/advanced/performance/#chunking-system-configuration","title":"Chunking System Configuration","text":"<p>The dynamic chunking system optimizes workload distribution based on agent performance.</p> <p>Key system settings:</p> <pre><code>-- Configure chunk behavior\nUPDATE system_settings SET value = '20' \nWHERE key = 'chunk_fluctuation_percentage';  -- Default: 20%\n\n-- Benchmark cache duration\nUPDATE system_settings SET value = '168' \nWHERE key = 'benchmark_cache_duration_hours';  -- Default: 168 (7 days)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#chunk-size-calculation","title":"Chunk Size Calculation","text":"<p>The system calculates chunks based on: 1. Agent benchmark speeds 2. Desired chunk duration 3. Attack mode modifiers 4. Available keyspace</p> <p>Optimization strategies:</p> <ol> <li> <p>For GPU clusters with similar performance: <pre><code>-- Larger chunks, less overhead\nUPDATE job_executions \nSET chunk_duration = 3600  -- 1 hour chunks\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>For mixed hardware environments: <pre><code>-- Smaller chunks for better distribution\nUPDATE job_executions \nSET chunk_duration = 900  -- 15 minute chunks\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>For time-sensitive jobs: <pre><code>-- Very small chunks for quick feedback\nUPDATE job_executions \nSET chunk_duration = 300  -- 5 minute chunks\nWHERE id = ?;\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#job-priority-and-scheduling","title":"Job Priority and Scheduling","text":"<p>The scheduler processes jobs based on priority and available agents:</p> <ol> <li>Priority levels:</li> <li>Critical: Process immediately</li> <li>High: Process within minutes</li> <li>Normal: Standard queue processing</li> <li> <p>Low: Process when resources available</p> </li> <li> <p>Scheduling optimization: <pre><code>-- Configure scheduler check interval (updated in v1.1+)\nUPDATE system_settings\nSET value = '3'  -- 3 seconds for faster response\nWHERE key = 'scheduler_check_interval_seconds';\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#parallel-scheduling-performance-improvements","title":"Parallel Scheduling Performance Improvements","text":"<p>Version 1.1+ introduced dramatic performance improvements through parallel execution:</p>"},{"location":"admin-guide/advanced/performance/#parallel-benchmarking-system","title":"Parallel Benchmarking System","text":"<p>The benchmark system now executes all benchmark requests simultaneously using goroutines:</p> <p>Performance Impact: - Sequential (old): 15 agents \u00d7 30s = 450 seconds - Parallel (new): 15 agents in ~12 seconds - Improvement: 96% reduction (37.5x faster)</p> <p>How it works: 1. Identifies all agents needing benchmarks 2. Sends all benchmark requests in parallel via goroutines 3. Waits for completion using database polling 4. Proceeds with task assignment once benchmarks complete</p> <p>Configuration: <pre><code>-- Benchmark cache duration\nUPDATE system_settings\nSET value = '168'  -- 7 days (default)\nWHERE key = 'benchmark_cache_duration_hours';\n\n-- Speedtest timeout (applies to each benchmark)\nUPDATE system_settings\nSET value = '180'  -- 3 minutes\nWHERE key = 'speedtest_timeout_seconds';\n</code></pre></p> <p>Benefits: - Eliminates benchmark bottleneck for large agent deployments - Scales linearly with agent count - No configuration changes required - Automatic with v1.1+ upgrade</p>"},{"location":"admin-guide/advanced/performance/#parallel-task-assignment-system","title":"Parallel Task Assignment System","text":"<p>The task assignment system now uses two-phase parallel execution:</p> <p>Performance Impact: - Sequential (old): 15 agents \u00d7 30s file sync = 450 seconds - Parallel (new): All agents in ~20 seconds - Improvement: 95% reduction (22.5x faster)</p> <p>How it works: 1. Phase 1 (Sequential - ~35ms): Calculate all chunk ranges upfront    - Prevents keyspace overlaps    - Determines rule split chunks    - Plans all assignments 2. Phase 2 (Parallel - ~20s): Execute all operations via goroutines    - Create rule chunk files    - Sync hashlists to agents    - Sync files (30s blocking window, but all agents in parallel)    - Create task database records    - Send WebSocket assignments</p> <p>Benefits: - Eliminates file sync bottleneck - All agents receive work simultaneously - Maintains sequential planning for correctness - Automatic with v1.1+ upgrade</p>"},{"location":"admin-guide/advanced/performance/#combined-impact","title":"Combined Impact","text":"<p>For a typical deployment with 15 agents:</p> Operation Sequential Parallel Improvement Benchmarking (15 agents) 450s 12s 96% faster Task Assignment (15 agents) 450s 20s 95% faster Total Scheduling Cycle 900s 32s 96% faster <p>Scaling characteristics: - Sequential system: Time increases linearly with agent count - Parallel system: Time remains constant (limited by slowest operation) - Benefits increase with larger deployments</p>"},{"location":"admin-guide/advanced/performance/#scheduler-interval-reduction","title":"Scheduler Interval Reduction","text":"<p>With parallel execution eliminating bottlenecks, the scheduler interval was reduced:</p> <ul> <li>Old: 30 seconds between checks</li> <li>New: 3 seconds between checks</li> <li>Result: Jobs start 10x faster after being queued</li> </ul> <p>When to adjust: <pre><code>-- For very large deployments (100+ agents), consider increasing slightly\nUPDATE system_settings\nSET value = '5'  -- 5 seconds\nWHERE key = 'scheduler_check_interval_seconds';\n\n-- For small deployments (&lt;10 agents), can reduce further\nUPDATE system_settings\nSET value = '1'  -- 1 second for maximum responsiveness\nWHERE key = 'scheduler_check_interval_seconds';\n</code></pre></p>"},{"location":"admin-guide/advanced/performance/#agent-performance","title":"Agent Performance","text":""},{"location":"admin-guide/advanced/performance/#hardware-detection-and-benchmarking","title":"Hardware Detection and Benchmarking","text":"<p>The system automatically detects GPU capabilities and runs benchmarks.</p> <p>Benchmark optimization:</p> <pre><code>-- Configure speedtest timeout\nUPDATE system_settings \nSET value = '180'  -- 3 minutes\nWHERE key = 'speedtest_timeout_seconds';\n\n-- For faster benchmarks (less accurate)\nUPDATE system_settings \nSET value = '60'  -- 1 minute\nWHERE key = 'speedtest_timeout_seconds';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#agent-configuration","title":"Agent Configuration","text":"<ol> <li> <p>GPU-specific optimizations: <pre><code># Agent config.yaml\nextra_parameters: \"--optimize-kernel-workload --force\"\n\n# For NVIDIA GPUs\nextra_parameters: \"-O -w 4\"\n\n# For AMD GPUs\nextra_parameters: \"-O -w 3\"\n</code></pre></p> </li> <li> <p>Memory management: <pre><code># Limit GPU memory usage\nextra_parameters: \"--gpu-memory-fraction=0.8\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#workload-distribution","title":"Workload Distribution","text":"<p>The system supports multiple distribution strategies:</p> <ol> <li>Round-robin: Equal distribution</li> <li>Performance-based: More work to faster agents</li> <li>Priority-based: Specific agents for specific jobs</li> </ol>"},{"location":"admin-guide/advanced/performance/#file-io-optimization","title":"File I/O Optimization","text":""},{"location":"admin-guide/advanced/performance/#hash-list-processing","title":"Hash List Processing","text":"<p>The system uses buffered I/O and batch processing for efficient file handling.</p> <p>Current implementation: - Buffered reading with <code>bufio.Scanner</code> - Configurable batch sizes - Streaming processing (no full file load)</p> <p>Optimization tips:</p> <ol> <li> <p>For NVMe storage: <pre><code>export KH_HASHLIST_BATCH_SIZE=5000  # Larger batches\n</code></pre></p> </li> <li> <p>For network storage: <pre><code>export KH_HASHLIST_BATCH_SIZE=500   # Smaller batches\n</code></pre></p> </li> <li> <p>File upload limits: <pre><code>export KH_MAX_UPLOAD_SIZE_MB=32     # Default: 32MB\n# Increase for trusted environments\nexport KH_MAX_UPLOAD_SIZE_MB=256    # 256MB\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#file-synchronization","title":"File Synchronization","text":"<p>The agent file sync system uses: - Chunk-based transfers - Resume capability - Integrity verification</p> <p>Optimization:</p> <ol> <li>LAN deployments:</li> <li>Increase chunk sizes</li> <li> <p>Disable compression</p> </li> <li> <p>WAN deployments:</p> </li> <li>Enable compression</li> <li>Smaller chunk sizes</li> <li>More aggressive retry policies</li> </ol>"},{"location":"admin-guide/advanced/performance/#file-hash-caching","title":"File Hash Caching","text":"<p>The directory monitor uses an in-memory file hash cache to dramatically reduce disk I/O when monitoring wordlist and rule directories.</p> <p>How It Works:</p> <ol> <li>ModTime+Size Validation: Before calculating MD5, the cache checks if file modification time and size have changed</li> <li>Cache Hit: If unchanged, returns cached hash (no disk read)</li> <li>Cache Miss: Calculates MD5, updates cache for future requests</li> <li>Background Population: Cache is populated asynchronously at startup</li> </ol> <p>Performance Impact:</p> Metric Before After Disk I/O (steady state) ~500MB/s constant Near zero MD5 calculations Every file, every 30s Only changed files SSD wear High Negligible <p>Key Benefits:</p> <ul> <li>Automatic: No configuration required</li> <li>Memory efficient: ~100 bytes per cached file</li> <li>Thread-safe: RWMutex pattern for concurrent access</li> <li>Self-healing: Automatically recalculates when files change</li> </ul> <p>Potfile Sync Optimization:</p> <p>During heavy crack ingestion (thousands of passwords per minute), the potfile changes frequently. A 5-minute hash history window prevents agents from repeatedly re-downloading the potfile:</p> <ul> <li>Agent's potfile hash is checked against recent valid hashes</li> <li>If hash is within the 5-minute window, sync is skipped</li> <li>After ingestion stops, agents sync to the latest version</li> </ul> <p>For technical details, see the File Hash Cache Architecture documentation.</p>"},{"location":"admin-guide/advanced/performance/#network-and-websocket-tuning","title":"Network and WebSocket Tuning","text":""},{"location":"admin-guide/advanced/performance/#websocket-configuration","title":"WebSocket Configuration","text":"<p>The system uses WebSocket for real-time agent communication.</p> <p>Key optimizations:</p> <ol> <li>Message processing:</li> <li>Asynchronous handlers for non-blocking operation</li> <li>Goroutine-based processing for heavy operations</li> <li> <p>30-second timeout for async operations</p> </li> <li> <p>Heartbeat optimization: <pre><code>// Agent sends heartbeat every 30 seconds\n// Server expects heartbeat within 90 seconds\n</code></pre></p> </li> <li> <p>Connection management: <pre><code># Nginx configuration for WebSocket\nproxy_read_timeout 3600s;\nproxy_send_timeout 3600s;\nproxy_connect_timeout 60s;\n\n# Buffer sizes\nproxy_buffer_size 64k;\nproxy_buffers 8 32k;\nproxy_busy_buffers_size 128k;\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#tlsssl-performance","title":"TLS/SSL Performance","text":"<p>The system supports multiple TLS modes with configurable parameters:</p> <pre><code># Certificate configuration\nexport KH_CERT_KEY_SIZE=2048        # Faster handshakes\n# or\nexport KH_CERT_KEY_SIZE=4096        # Better security\n\n# For high-traffic deployments\nexport KH_TLS_SESSION_CACHE=on\nexport KH_TLS_SESSION_TIMEOUT=300\n</code></pre>"},{"location":"admin-guide/advanced/performance/#storage-performance","title":"Storage Performance","text":""},{"location":"admin-guide/advanced/performance/#directory-structure-optimization","title":"Directory Structure Optimization","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/      # Hashcat binaries (SSD recommended)\n\u251c\u2500\u2500 wordlists/     # Large wordlists (HDD acceptable)\n\u251c\u2500\u2500 rules/         # Rule files (SSD preferred)\n\u251c\u2500\u2500 hashlists/     # User hashlists (SSD recommended)\n\u2514\u2500\u2500 temp/          # Temporary files (RAM disk optimal)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#storage-recommendations","title":"Storage Recommendations","text":"<ol> <li>SSD for critical paths:</li> <li>Database files</li> <li>Hashcat binaries</li> <li>Active hashlists</li> <li> <p>Temporary processing</p> </li> <li> <p>HDD acceptable for:</p> </li> <li>Large wordlist storage</li> <li>Archived hashlists</li> <li> <p>Backup data</p> </li> <li> <p>RAM disk for temporary files: <pre><code># Create RAM disk for temp files\nsudo mkdir -p /mnt/ramdisk\nsudo mount -t tmpfs -o size=2G tmpfs /mnt/ramdisk\n\n# Link to KrakenHashes temp\nln -s /mnt/ramdisk /data/krakenhashes/temp\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#monitoring-and-benchmarking","title":"Monitoring and Benchmarking","text":""},{"location":"admin-guide/advanced/performance/#metrics-collection-and-retention","title":"Metrics Collection and Retention","text":"<p>The system includes automatic metrics aggregation:</p> <pre><code>-- Configure retention\nUPDATE system_settings \nSET value = '30'  -- Keep realtime data for 30 days\nWHERE key = 'metrics_retention_days';\n\n-- Enable/disable aggregation\nUPDATE system_settings \nSET value = 'true' \nWHERE key = 'enable_aggregation';\n</code></pre> <p>Aggregation levels: - Realtime \u2192 Daily (after 24 hours) - Daily \u2192 Weekly (after 7 days) - Cleanup runs daily at 2 AM</p>"},{"location":"admin-guide/advanced/performance/#performance-monitoring-queries","title":"Performance Monitoring Queries","text":"<pre><code>-- Agent performance overview\nSELECT \n    a.name,\n    a.status,\n    COUNT(DISTINCT jt.id) as active_tasks,\n    AVG(apm.hashes_per_second) as avg_speed,\n    MAX(apm.temperature) as max_temp\nFROM agents a\nLEFT JOIN job_tasks jt ON a.id = jt.agent_id AND jt.status = 'in_progress'\nLEFT JOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.created_at &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.id, a.name, a.status;\n\n-- Job execution performance\nSELECT \n    je.id,\n    je.status,\n    je.created_at,\n    je.completed_at,\n    je.progress,\n    COUNT(jt.id) as total_chunks,\n    COUNT(CASE WHEN jt.status = 'completed' THEN 1 END) as completed_chunks\nFROM job_executions je\nLEFT JOIN job_tasks jt ON je.id = jt.job_execution_id\nGROUP BY je.id\nORDER BY je.created_at DESC;\n</code></pre>"},{"location":"admin-guide/advanced/performance/#benchmarking-best-practices","title":"Benchmarking Best Practices","text":"<ol> <li>Initial benchmarking:</li> <li>Run comprehensive benchmarks on agent registration</li> <li>Test all hash types your organization uses</li> <li> <p>Store results for 7 days (default)</p> </li> <li> <p>Periodic re-benchmarking:</p> </li> <li>After driver updates</li> <li>After hardware changes</li> <li> <p>Monthly for consistency</p> </li> <li> <p>Benchmark commands: <pre><code># Force re-benchmark for specific agent\ncurl -X POST https://api.krakenhashes.com/agents/{id}/benchmark \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#system-settings-reference","title":"System Settings Reference","text":""},{"location":"admin-guide/advanced/performance/#performance-related-settings","title":"Performance-Related Settings","text":"Setting Key Default Description Optimization Range <code>chunk_fluctuation_percentage</code> 20 Threshold for merging small chunks 10-30% <code>benchmark_cache_duration_hours</code> 168 How long to cache benchmark results 24-720 hours <code>metrics_retention_days</code> 30 Realtime metrics retention 7-90 days <code>enable_aggregation</code> true Enable metrics aggregation true/false <code>speedtest_timeout_seconds</code> 180 Benchmark timeout 60-600 seconds <code>scheduler_check_interval_seconds</code> 30 Job scheduler interval 10-60 seconds"},{"location":"admin-guide/advanced/performance/#environment-variables","title":"Environment Variables","text":"Variable Default Description Optimization Tips <code>KH_HASHLIST_BATCH_SIZE</code> 1000 Database batch insert size 500-5000 based on hardware <code>KH_MAX_UPLOAD_SIZE_MB</code> 32 Maximum file upload size 32-1024 based on trust <code>DATABASE_MAX_OPEN_CONNS</code> 25 Max database connections 25-100 based on load <code>DATABASE_MAX_IDLE_CONNS</code> 5 Max idle connections 20% of max open"},{"location":"admin-guide/advanced/performance/#performance-troubleshooting","title":"Performance Troubleshooting","text":""},{"location":"admin-guide/advanced/performance/#common-bottlenecks","title":"Common Bottlenecks","text":"<ol> <li>Database connection exhaustion:</li> <li>Symptom: \"too many connections\" errors</li> <li> <p>Solution: Increase connection pool or use PgBouncer</p> </li> <li> <p>Slow hash imports:</p> </li> <li>Symptom: Hashlist processing takes hours</li> <li> <p>Solution: Increase batch size, use SSD storage</p> </li> <li> <p>Agent communication delays:</p> </li> <li>Symptom: Delayed job updates</li> <li> <p>Solution: Check network latency, adjust timeouts</p> </li> <li> <p>Memory exhaustion:</p> </li> <li>Symptom: OOM errors during processing</li> <li>Solution: Reduce batch sizes, add swap space</li> </ol>"},{"location":"admin-guide/advanced/performance/#performance-checklist","title":"Performance Checklist","text":"<ul> <li> Database indexes are being used (check pg_stat_user_indexes)</li> <li> Connection pool sized appropriately for agent count</li> <li> Batch sizes optimized for hardware</li> <li> Metrics retention configured</li> <li> Storage using appropriate media (SSD/HDD)</li> <li> Network timeouts adjusted for environment</li> <li> Benchmark cache duration set appropriately</li> <li> Chunk sizes appropriate for job types</li> </ul>"},{"location":"admin-guide/advanced/performance/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"admin-guide/advanced/performance/#small-deployment-1-10-agents","title":"Small Deployment (1-10 agents)","text":"<pre><code># Keep most defaults\nexport KH_HASHLIST_BATCH_SIZE=1000\nexport DATABASE_MAX_OPEN_CONNS=25\n# Use default chunk fluctuation (20%)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#medium-deployment-10-50-agents","title":"Medium Deployment (10-50 agents)","text":"<pre><code>export KH_HASHLIST_BATCH_SIZE=2000\nexport DATABASE_MAX_OPEN_CONNS=50\nexport DATABASE_MAX_IDLE_CONNS=10\n# Adjust chunk fluctuation to 15%\nUPDATE system_settings SET value = '15' WHERE key = 'chunk_fluctuation_percentage';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#large-deployment-50-agents","title":"Large Deployment (50+ agents)","text":"<pre><code>export KH_HASHLIST_BATCH_SIZE=5000\nexport DATABASE_MAX_OPEN_CONNS=100\nexport DATABASE_MAX_IDLE_CONNS=20\n# Use PgBouncer for connection pooling\n# Adjust chunk fluctuation to 10%\nUPDATE system_settings SET value = '10' WHERE key = 'chunk_fluctuation_percentage';\n# Reduce metrics retention\nUPDATE system_settings SET value = '14' WHERE key = 'metrics_retention_days';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#next-steps","title":"Next Steps","text":"<ol> <li>Review current system metrics</li> <li>Identify bottlenecks using monitoring queries</li> <li>Apply appropriate optimizations</li> <li>Monitor impact and adjust</li> <li>Document environment-specific settings</li> </ol> <p>For additional support, consult the System Overview documentation or contact the development team.</p>"},{"location":"admin-guide/advanced/presets/","title":"Preset Jobs and Job Workflows","text":""},{"location":"admin-guide/advanced/presets/#overview","title":"Overview","text":"<p>KrakenHashes provides two powerful features for standardizing and automating password cracking strategies:</p> <ul> <li>Preset Jobs: Pre-configured job templates that define specific attack strategies</li> <li>Job Workflows: Ordered sequences of preset jobs that execute systematically</li> </ul> <p>These features allow administrators to create reusable attack strategies that can be consistently applied across different hashlists, ensuring thorough and efficient password recovery.</p>"},{"location":"admin-guide/advanced/presets/#preset-jobs","title":"Preset Jobs","text":""},{"location":"admin-guide/advanced/presets/#what-are-preset-jobs","title":"What are Preset Jobs?","text":"<p>Preset jobs are templates that encapsulate all the parameters needed for a specific password cracking attack. Think of them as recipes that define: - Which wordlists to use - Which rules to apply - What attack mode to employ - How much priority the job should have - Various execution parameters</p>"},{"location":"admin-guide/advanced/presets/#creating-a-preset-job","title":"Creating a Preset Job","text":"<p>To create a new preset job:</p> <ol> <li>Navigate to Admin &gt; Preset Jobs</li> <li>Click the \"Create Preset Job\" button</li> </ol> <p> <ol> <li>Fill in the preset job details:</li> </ol>"},{"location":"admin-guide/advanced/presets/#basic-information","title":"Basic Information","text":"<ul> <li>Name: A unique, descriptive name (e.g., \"Common Passwords with Rules\")</li> <li>Attack Mode: Select the hashcat attack mode (see Attack Modes section below)</li> <li>Priority: Set execution priority (0-1000, higher = more important)</li> <li>Binary Version: Select the hashcat binary version to use</li> </ul> <p>"},{"location":"admin-guide/advanced/presets/#attack-specific-configuration","title":"Attack-Specific Configuration","text":"<p>Based on the selected attack mode, different fields will appear:</p> <ul> <li>Wordlists: Select one or more wordlists (depending on attack mode)</li> <li>Rules: Select rule files to apply transformations</li> <li>Mask: Define patterns for brute force attacks (e.g., <code>?d?d?d?d</code> for 4 digits)</li> </ul> <p>"},{"location":"admin-guide/advanced/presets/#advanced-options","title":"Advanced Options","text":"<ul> <li>Chunk Size: Time allocation per work unit (default: 900 seconds)</li> <li>Small Job: Check if this is a quick-running job</li> <li>Allow High Priority Override: Enable this job to interrupt lower priority running jobs (see details below)</li> <li>Status Updates: Enable real-time progress reporting</li> </ul>"},{"location":"admin-guide/advanced/presets/#high-priority-override-feature","title":"High Priority Override Feature","text":"<p>The Allow High Priority Override option gives a preset job special privileges in the scheduling system:</p>"},{"location":"admin-guide/advanced/presets/#what-it-does","title":"What It Does","text":"<p>When enabled, this preset job can interrupt lower priority jobs that are currently running, but only when: 1. No agents are available for assignment 2. This job has a higher priority than running jobs 3. The system-wide \"Job Interruption Enabled\" setting is active</p>"},{"location":"admin-guide/advanced/presets/#when-to-use-it","title":"When to Use It","text":"<p>Enable high priority override for jobs that: - Respond to security incidents or breaches - Have strict compliance or legal deadlines - Support time-sensitive investigations - Require immediate results for critical business decisions</p>"},{"location":"admin-guide/advanced/presets/#how-interruption-works","title":"How Interruption Works","text":"<ol> <li>Automatic Process: The system automatically identifies the lowest priority running job</li> <li>Graceful Interruption: Agents receive stop commands and save their progress</li> <li>Status Change: Interrupted jobs change from \"running\" to \"pending\" status</li> <li>Automatic Resumption: Interrupted jobs resume automatically when agents become available</li> <li>No Work Lost: All completed work is preserved and jobs continue from their last checkpoint</li> </ol>"},{"location":"admin-guide/advanced/presets/#best-practices","title":"Best Practices","text":"<ul> <li>Use Sparingly: Reserve this feature for truly critical jobs</li> <li>Set Appropriate Priority: Jobs with override should have priority 70+ to justify interruption</li> <li>Monitor Impact: Track how often jobs are interrupted to ensure system efficiency</li> <li>Document Usage: Maintain clear policies about when to enable this feature</li> </ul>"},{"location":"admin-guide/advanced/presets/#example-scenarios","title":"Example Scenarios","text":"<p>Emergency Incident Response: - Priority: 95 - Allow High Priority Override: \u2713 Yes - Justification: Active security breach requires immediate password analysis</p> <p>Routine Security Audit: - Priority: 50 - Allow High Priority Override: \u2717 No - Justification: Standard assessment with flexible timeline</p> <p>Compliance Deadline Today: - Priority: 85 - Allow High Priority Override: \u2713 Yes - Justification: Regulatory requirement with penalty for non-compliance</p>"},{"location":"admin-guide/advanced/presets/#attack-modes","title":"Attack Modes","text":"<p>KrakenHashes supports six different attack modes:</p>"},{"location":"admin-guide/advanced/presets/#1-straight-attack-mode-0","title":"1. Straight Attack (Mode 0)","text":"<p>The most common dictionary attack with optional rule transformations. - Requirements: 1 wordlist, 0 or more rules - Example: Using <code>rockyou.txt</code> with <code>best64.rule</code></p>"},{"location":"admin-guide/advanced/presets/#2-combination-attack-mode-1","title":"2. Combination Attack (Mode 1)","text":"<p>Combines words from two different wordlists. - Requirements: Exactly 2 wordlists, no rules - Example: Combining <code>firstnames.txt</code> with <code>years.txt</code> to get \"John2023\"</p> <p>"},{"location":"admin-guide/advanced/presets/#3-brute-force-attack-mode-3","title":"3. Brute Force Attack (Mode 3)","text":"<p>Generates passwords based on mask patterns. - Requirements: Mask pattern only, no wordlists - Common Masks:   - <code>?d?d?d?d</code> - 4 digits (0000-9999)   - <code>?l?l?l?l?l?l</code> - 6 lowercase letters   - <code>?u?l?l?l?d?d</code> - Capital + 3 lowercase + 2 digits</p> <p>"},{"location":"admin-guide/advanced/presets/#increment-mode-optional","title":"Increment Mode (Optional)","text":"<p>Enable increment mode for brute force attacks to automatically test mask patterns of increasing lengths:</p> <ul> <li>Increment Mode: Toggle to enable (<code>increment</code> or <code>increment_inverse</code>)</li> <li>Increment Min: Minimum mask length to start with (default: 1)</li> <li>Increment Max: Maximum mask length to end with (default: mask length)</li> </ul> <p>Example: Mask <code>?d?d?d?d</code> with min=2, max=4 tests:</p> <ol> <li><code>?d?d</code> (00-99)</li> <li><code>?d?d?d</code> (000-999)</li> <li><code>?d?d?d?d</code> (0000-9999)</li> </ol> <p>Increment Inverse: Starts from longest mask and works down to shortest. Useful when longer passwords are more likely.</p> <p>When to Use Increment Mode</p> <ul> <li>PIN Discovery: Testing 4-8 digit PINs with a single preset job</li> <li>Length Exploration: When password length policies are unknown</li> <li>Exhaustive Searches: Covering all lengths up to a maximum systematically</li> </ul> <p>Technical Details</p> <p>KrakenHashes decomposes increment mode jobs into discrete \"layers\" (one per mask length) for efficient distributed processing. Each layer is scheduled and tracked independently, allowing multiple agents to work on different layers in parallel. See Increment Mode Architecture for implementation details.</p>"},{"location":"admin-guide/advanced/presets/#4-hybrid-wordlist-mask-mode-6","title":"4. Hybrid Wordlist + Mask (Mode 6)","text":"<p>Appends mask-generated characters to dictionary words. - Requirements: 1 wordlist and mask pattern - Example: <code>passwords.txt</code> + <code>?d?d?d</code> = \"password123\"</p>"},{"location":"admin-guide/advanced/presets/#5-hybrid-mask-wordlist-mode-7","title":"5. Hybrid Mask + Wordlist (Mode 7)","text":"<p>Prepends mask-generated characters to dictionary words. - Requirements: 1 wordlist and mask pattern - Example: <code>?d?d?d</code> + <code>passwords.txt</code> = \"123password\"</p>"},{"location":"admin-guide/advanced/presets/#6-association-attack-mode-9","title":"6. Association Attack (Mode 9)","text":"<p>Currently not implemented</p>"},{"location":"admin-guide/advanced/presets/#managing-preset-jobs","title":"Managing Preset Jobs","text":""},{"location":"admin-guide/advanced/presets/#viewing-preset-jobs","title":"Viewing Preset Jobs","text":"<p>The preset jobs list shows: - Name and attack mode - Wordlist and rule counts - Priority level - Binary version - Action buttons (Edit/Delete)</p> <p> Preset Job Management interface showing three configured preset jobs with their attack modes, priorities, agent limits, and resource assignments</p>"},{"location":"admin-guide/advanced/presets/#editing-preset-jobs","title":"Editing Preset Jobs","text":"<ol> <li>Click the Edit button on any preset job</li> <li>Modify the desired fields</li> <li>Click Update Preset Job</li> </ol>"},{"location":"admin-guide/advanced/presets/#deleting-preset-jobs","title":"Deleting Preset Jobs","text":"<ul> <li>Click the Delete button</li> <li>Confirm the deletion</li> <li>Note: You cannot delete preset jobs that are used in workflows</li> </ul>"},{"location":"admin-guide/advanced/presets/#job-workflows","title":"Job Workflows","text":""},{"location":"admin-guide/advanced/presets/#what-are-job-workflows","title":"What are Job Workflows?","text":"<p>Job workflows are ordered sequences of preset jobs that execute one after another. They allow you to: - Create comprehensive attack strategies - Ensure consistent methodology - Prioritize efficient attacks first - Automate complex multi-stage attacks</p>"},{"location":"admin-guide/advanced/presets/#creating-a-job-workflow","title":"Creating a Job Workflow","text":"<ol> <li>Navigate to Admin &gt; Job Workflows</li> <li>Click \"Create Job Workflow\"</li> </ol> <p> Job Workflows interface displaying a workflow with multiple jobs, priority indicators, and interruption capabilities</p> <ol> <li>Enter a workflow name (e.g., \"Standard Password Audit\")</li> <li>Add preset jobs to the workflow:</li> <li>Type in the search box to find preset jobs</li> <li>Click on a preset job to add it as a step</li> <li>Added jobs appear in the steps list below</li> </ol> <p> <ol> <li>Arrange the execution order:</li> <li>Steps are automatically sorted by priority (highest first)</li> <li> <p>Within the same priority, they execute in the order added</p> </li> <li> <p>Click Create Job Workflow</p> </li> </ol>"},{"location":"admin-guide/advanced/presets/#understanding-workflow-execution","title":"Understanding Workflow Execution","text":"<p>When a workflow runs: 1. All preset jobs are queued in priority order 2. Higher priority jobs execute first 3. Jobs with the same priority run in sequence 4. Each job completes before the next begins</p>"},{"location":"admin-guide/advanced/presets/#managing-workflows","title":"Managing Workflows","text":""},{"location":"admin-guide/advanced/presets/#viewing-workflows","title":"Viewing Workflows","text":"<p>The workflow list displays: - Workflow name - Number of steps - Creation date - Action buttons</p> <p>"},{"location":"admin-guide/advanced/presets/#workflow-details","title":"Workflow Details","text":"<p>Click on a workflow name to see: - Complete list of preset jobs in order - Priority of each step - Wordlists and rules for each step</p> <p>"},{"location":"admin-guide/advanced/presets/#editing-workflows","title":"Editing Workflows","text":"<ol> <li>Click Edit on any workflow</li> <li>Add or remove preset jobs</li> <li>Reorder steps as needed</li> <li>Click Update Job Workflow</li> </ol> <p>Note: When updating a workflow, all existing steps are replaced with the new configuration.</p>"},{"location":"admin-guide/advanced/presets/#best-practices_1","title":"Best Practices","text":""},{"location":"admin-guide/advanced/presets/#preset-job-design","title":"Preset Job Design","text":"<ol> <li>Start Simple: Begin with common, fast attacks</li> <li>Straight attack with common passwords</li> <li> <p>Small, targeted wordlists with effective rules</p> </li> <li> <p>Progressive Complexity: Order jobs from fast/likely to slow/exhaustive</p> </li> <li>Priority 100: Common passwords</li> <li>Priority 80: Leaked password lists</li> <li>Priority 60: Hybrid attacks</li> <li>Priority 40: Targeted brute force</li> <li> <p>Priority 20: Exhaustive searches</p> </li> <li> <p>Naming Conventions: Use descriptive names that indicate:</p> </li> <li>Attack type</li> <li>Target pattern</li> <li>Approximate runtime</li> </ol>"},{"location":"admin-guide/advanced/presets/#workflow-strategy","title":"Workflow Strategy","text":"<ol> <li>Standard Workflows: Create templates for common scenarios</li> <li>\"Quick Audit\" - Fast, high-probability attacks</li> <li>\"Comprehensive Audit\" - Thorough multi-day approach</li> <li> <p>\"Compliance Check\" - Specific policy violations</p> </li> <li> <p>Resource Management:</p> </li> <li>Group small jobs together</li> <li>Save intensive attacks for last</li> <li> <p>Use chunk sizes appropriate to job complexity</p> </li> <li> <p>Monitoring: Enable status updates for long-running jobs</p> </li> </ol>"},{"location":"admin-guide/advanced/presets/#examples","title":"Examples","text":""},{"location":"admin-guide/advanced/presets/#example-1-quick-security-audit-workflow","title":"Example 1: Quick Security Audit Workflow","text":"<p>Create these preset jobs: 1. \"Top 1000 Passwords\" (Priority: 100)    - Mode: Straight    - Wordlist: top1000.txt    - No rules</p> <ol> <li>\"Common with Basic Rules\" (Priority: 90)</li> <li>Mode: Straight</li> <li>Wordlist: common-passwords.txt</li> <li> <p>Rules: basic.rule</p> </li> <li> <p>\"4-6 Digit PINs\" (Priority: 80)</p> </li> <li>Mode: Brute Force</li> <li>Mask: <code>?d?d?d?d?d?d</code></li> <li>Increment Mode: enabled</li> <li>Increment Min: 4</li> <li>Increment Max: 6</li> </ol>"},{"location":"admin-guide/advanced/presets/#example-2-targeted-corporate-audit","title":"Example 2: Targeted Corporate Audit","text":"<ol> <li>\"Corporate Dictionary\" (Priority: 100)</li> <li>Mode: Straight</li> <li>Wordlist: corporate-terms.txt</li> <li> <p>Rules: best64.rule</p> </li> <li> <p>\"Names + Years\" (Priority: 90)</p> </li> <li>Mode: Combination</li> <li>Wordlist 1: employee-names.txt</li> <li> <p>Wordlist 2: years-2020-2024.txt</p> </li> <li> <p>\"Corporate + Numbers\" (Priority: 80)</p> </li> <li>Mode: Hybrid (Wordlist + Mask)</li> <li>Wordlist: corporate-terms.txt</li> <li>Mask: ?d?d?d</li> </ol>"},{"location":"admin-guide/advanced/presets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/presets/#common-issues","title":"Common Issues","text":"<ol> <li>\"Wordlist not found\": Ensure wordlists are uploaded before creating preset jobs</li> <li>\"Invalid mask pattern\": Check mask syntax (?d=digit, ?l=lowercase, ?u=uppercase, ?s=special)</li> <li>\"Priority exceeds maximum\": Check system settings for max priority value</li> </ol>"},{"location":"admin-guide/advanced/presets/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use smaller, targeted wordlists for initial attempts</li> <li>Apply rules selectively - more isn't always better</li> <li>Set appropriate chunk sizes (larger for simple attacks, smaller for complex)</li> <li>Monitor system resources when running multiple workflows</li> </ul>"},{"location":"admin-guide/advanced/presets/#future-enhancements","title":"Future Enhancements","text":"<p>The preset jobs and workflows system is designed to integrate with: - Automated job distribution to agents - Real-time progress monitoring - Success rate analytics - Dynamic workflow optimization</p> <p>As KrakenHashes evolves, these features will provide the foundation for intelligent, adaptive password auditing strategies.</p>"},{"location":"admin-guide/operations/agents/","title":"Agent Management Guide","text":"<p>This guide covers the comprehensive management of KrakenHashes agents, including registration, monitoring, scheduling, and troubleshooting.</p>"},{"location":"admin-guide/operations/agents/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Understanding Agents</li> <li>Agent Registration Process</li> <li>Managing Agent Connections</li> <li>Monitoring Agent Health and Performance</li> <li>Agent Scheduling and Availability</li> <li>Hardware Capabilities and Benchmarks</li> <li>Troubleshooting Agent Issues</li> </ol>"},{"location":"admin-guide/operations/agents/#understanding-agents","title":"Understanding Agents","text":""},{"location":"admin-guide/operations/agents/#what-are-agents","title":"What are Agents?","text":"<p>Agents are distributed compute nodes that execute password cracking tasks using hashcat. They connect to the KrakenHashes backend server via WebSocket and receive job assignments based on their availability and capabilities.</p>"},{"location":"admin-guide/operations/agents/#agent-architecture","title":"Agent Architecture","text":"<p>Each agent consists of: - Hardware Detection: Automatic detection of GPUs (NVIDIA, AMD, Intel) and CPUs - Performance Monitoring: Real-time tracking of resource utilization - File Synchronization: Automatic download of wordlists, rules, and hashlists - Job Execution: Running hashcat with specified attack parameters - Result Reporting: Real-time crack updates back to the server</p>"},{"location":"admin-guide/operations/agents/#agent-states","title":"Agent States","text":"<p>Agents can be in one of the following states:</p> <ul> <li><code>pending</code>: Initial registration state, awaiting activation</li> <li><code>active</code>: Connected and ready to receive jobs</li> <li><code>inactive</code>: Disconnected but previously active</li> <li><code>error</code>: Experiencing issues preventing normal operation</li> <li><code>disabled</code>: Administratively disabled</li> </ul>"},{"location":"admin-guide/operations/agents/#agent-registration-process","title":"Agent Registration Process","text":""},{"location":"admin-guide/operations/agents/#overview","title":"Overview","text":"<p>Agent registration uses a claim code (voucher) system to ensure only authorized agents can join the system.</p>"},{"location":"admin-guide/operations/agents/#creating-claim-codes","title":"Creating Claim Codes","text":"<ol> <li>Navigate to Admin Panel</li> <li> <p>Go to Settings \u2192 Agent Management \u2192 Claim Codes</p> </li> <li> <p>Generate New Claim Code <pre><code>Type: Single-use or Continuous\nOwner: Select user who will own the agents\n</code></pre></p> </li> <li> <p>Claim Code Types</p> </li> <li>Single-use: Can only be used once to register one agent</li> <li>Continuous: Can be used multiple times (useful for auto-scaling)</li> </ol>"},{"location":"admin-guide/operations/agents/#agent-registration-steps","title":"Agent Registration Steps","text":"<ol> <li> <p>Agent Installation <pre><code># On the agent machine\n./krakenhashes-agent install\n</code></pre></p> </li> <li> <p>Initial Registration <pre><code># Using the claim code\n./krakenhashes-agent --host your-server:31337 --claim XXXX-XXXX-XXXX\n</code></pre></p> </li> <li> <p>Certificate Download</p> </li> <li>Agent automatically downloads TLS certificates</li> <li> <p>Stores credentials in <code>~/.krakenhashes/agent/</code></p> </li> <li> <p>Connection Establishment</p> </li> <li>Agent connects via WebSocket using API key authentication</li> <li>Sends hardware information and capabilities</li> </ol>"},{"location":"admin-guide/operations/agents/#registration-security","title":"Registration Security","text":"<ul> <li>Claim codes are normalized (uppercase, no hyphens)</li> <li>API keys are generated using cryptographically secure random bytes</li> <li>TLS certificates ensure encrypted communication</li> <li>Agent ID and API key must match for authentication</li> </ul> <p> Agent Management interface showing active claim vouchers with different types and currently registered agents with their hardware configurations</p> <p> Detailed agent view displaying system information, hardware configuration, and GPU device management with enable/disable controls</p>"},{"location":"admin-guide/operations/agents/#managing-agent-connections","title":"Managing Agent Connections","text":""},{"location":"admin-guide/operations/agents/#websocket-communication","title":"WebSocket Communication","text":"<p>Agents maintain persistent WebSocket connections for: - Real-time job assignments - Status updates - Crack result reporting - Heartbeat monitoring</p>"},{"location":"admin-guide/operations/agents/#connection-parameters","title":"Connection Parameters","text":"<pre><code># Environment variables for connection tuning\nKH_WRITE_WAIT: \"10s\"      # Write timeout\nKH_PONG_WAIT: \"60s\"       # Time to wait for pong response\nKH_PING_PERIOD: \"54s\"     # Ping interval (must be &lt; pong wait)\n</code></pre>"},{"location":"admin-guide/operations/agents/#monitoring-connected-agents","title":"Monitoring Connected Agents","text":"<ol> <li>Dashboard View</li> <li>Real-time agent status on main dashboard</li> <li>Shows connected/disconnected agents</li> <li> <p>Current task assignments</p> </li> <li> <p>Agent List Page</p> </li> <li>Detailed view of all agents</li> <li>Filter by status, owner, or team</li> <li>Last heartbeat timestamps</li> </ol>"},{"location":"admin-guide/operations/agents/#managing-agent-settings","title":"Managing Agent Settings","text":"<pre><code>// PUT /api/agents/{id}\n{\n  \"isEnabled\": true,\n  \"ownerId\": \"user-uuid\",\n  \"extraParameters\": \"--custom-charset1=?l?u?d\",\n  \"binaryVersionId\": 3,\n  \"binaryOverride\": true\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#agent-binary-version-override","title":"Agent Binary Version Override","text":"<p>Users can configure their agents to use a specific hashcat binary version instead of the job-level or system default binary.</p>"},{"location":"admin-guide/operations/agents/#configuring-binary-override","title":"Configuring Binary Override","text":"<ol> <li>Via Agent Details Page</li> <li>Navigate to your agent's detail page</li> <li>Scroll to the \"Binary Version Override\" section</li> <li>Enable \"Override Binary\" toggle</li> <li>Select desired binary version from dropdown</li> <li> <p>Click \"Save\"</p> </li> <li> <p>Via API <pre><code>// PUT /api/agents/{id}\n{\n  \"binaryVersionId\": 3,\n  \"binaryOverride\": true\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/agents/#binary-selection-hierarchy","title":"Binary Selection Hierarchy","text":"<p>When an agent needs to execute a job or benchmark, the system uses this priority order:</p> <ol> <li>Agent Override (highest priority) - Binary specified in agent settings</li> <li>Job Binary - Binary specified for the specific job execution</li> <li>System Default (lowest priority) - Active default binary for the system</li> </ol>"},{"location":"admin-guide/operations/agents/#use-cases","title":"Use Cases","text":"<ul> <li>Testing New Versions: Test new hashcat releases on specific agents before wider deployment</li> <li>Compatibility Issues: Work around driver or hardware compatibility problems</li> <li>Performance Optimization: Use specific binary versions that perform better on certain hardware</li> <li>Gradual Rollouts: Migrate agents to new versions incrementally</li> </ul>"},{"location":"admin-guide/operations/agents/#hashcat-version-compatibility-note","title":"Hashcat Version Compatibility Note","text":"<p>\u26a0\ufe0f Important: Hashcat 7.x may detect devices but fail to recognize them as usable compute devices depending on GPU driver versions. If your agent shows devices in the hardware detection but they are not available for job execution, it is recommended to use Hashcat 6.x binaries (such as 6.2.6 or 6.2.5) as they have better compatibility with older driver versions.</p>"},{"location":"admin-guide/operations/agents/#important-notes","title":"Important Notes","text":"<ul> <li>Agent binary override affects device detection, benchmarks, and job execution</li> <li>The preferred binary is automatically downloaded to the agent if not present</li> <li>If the preferred binary becomes unavailable, the system falls back to the next priority level</li> </ul>"},{"location":"admin-guide/operations/agents/#disablingenabling-agents","title":"Disabling/Enabling Agents","text":"<ul> <li>Disabled agents remain connected but don't receive jobs</li> <li>Useful for maintenance or troubleshooting</li> <li>Preserves agent configuration and history</li> </ul>"},{"location":"admin-guide/operations/agents/#monitoring-agent-health-and-performance","title":"Monitoring Agent Health and Performance","text":""},{"location":"admin-guide/operations/agents/#real-time-metrics","title":"Real-time Metrics","text":"<p>Agents report metrics every 30 seconds:</p> <pre><code>{\n  \"cpu_usage\": 45.2,\n  \"memory_usage\": 62.8,\n  \"gpu_utilization\": 98.5,\n  \"gpu_temp\": 72.0,\n  \"gpu_metrics\": {\n    \"device_0\": {\n      \"temperature\": 72,\n      \"utilization\": 98.5,\n      \"memory_used\": 8192,\n      \"fan_speed\": 85\n    }\n  }\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#performance-monitoring-dashboard","title":"Performance Monitoring Dashboard","text":"<ol> <li>Agent Detail Page</li> <li>Historical performance graphs</li> <li>Temperature trends</li> <li>Utilization patterns</li> <li> <p>Hash rate performance</p> </li> <li> <p>Metrics Time Ranges</p> </li> <li>1 hour (default)</li> <li>24 hours</li> <li>7 days</li> <li>30 days</li> </ol>"},{"location":"admin-guide/operations/agents/#device-management","title":"Device Management","text":"<p>Each agent can have multiple devices (GPUs):</p> <pre><code>// GET /api/agents/{id}/devices\n[\n  {\n    \"id\": 1,\n    \"device_index\": 0,\n    \"device_type\": \"GPU\",\n    \"device_name\": \"NVIDIA GeForce RTX 4090\",\n    \"is_enabled\": true,\n    \"capabilities\": {\n      \"compute_capability\": \"8.9\",\n      \"memory\": 24576\n    }\n  }\n]\n</code></pre>"},{"location":"admin-guide/operations/agents/#enablingdisabling-devices","title":"Enabling/Disabling Devices","text":"<pre><code>// PUT /api/agents/{id}/devices/{deviceId}\n{\n  \"enabled\": false  // Disable specific GPU\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#runtime-selection","title":"Runtime Selection","text":"<p>Each physical GPU device supports multiple compute backends (runtimes). Administrators can select which runtime to use per device.</p> <p>Available Runtimes: - CUDA: NVIDIA GPUs (optimal performance) - HIP: AMD GPUs (modern Radeon cards) - OpenCL: Universal (all vendors)</p> <p>Update Runtime: <pre><code>// PATCH /api/agents/{id}/devices/{deviceId}/runtime\n{\n  \"runtime\": \"HIP\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Device runtime updated successfully\"\n}\n</code></pre></p> <p>Device Structure with Runtime Options: <pre><code>{\n  \"id\": 1,\n  \"device_id\": 0,  // Physical device index\n  \"device_type\": \"GPU\",\n  \"device_name\": \"AMD Radeon RX 7700S\",\n  \"enabled\": true,\n  \"selected_runtime\": \"HIP\",\n  \"runtime_options\": [\n    {\n      \"backend\": \"HIP\",\n      \"device_id\": 1,      // Hashcat device ID\n      \"processors\": 16,\n      \"clock\": 2208,\n      \"memory_total\": 8176,\n      \"memory_free\": 8064,\n      \"pci_address\": \"03:00.0\"\n    },\n    {\n      \"backend\": \"OpenCL\",\n      \"device_id\": 3,      // Hashcat device ID\n      \"processors\": 16,\n      \"clock\": 2208,\n      \"memory_total\": 8176,\n      \"memory_free\": 8064,\n      \"pci_address\": \"03:00.0\"\n    }\n  ]\n}\n</code></pre></p> <p>Important Notes: - Changes take effect immediately for new jobs - Each physical GPU can only run under one runtime at a time - Runtime selection affects benchmark and job execution performance</p>"},{"location":"admin-guide/operations/agents/#agent-scheduling-and-availability","title":"Agent Scheduling and Availability","text":""},{"location":"admin-guide/operations/agents/#scheduling-overview","title":"Scheduling Overview","text":"<p>Agents support weekly scheduling to optimize resource usage and costs.</p>"},{"location":"admin-guide/operations/agents/#configuring-agent-schedules","title":"Configuring Agent Schedules","text":"<ol> <li> <p>Enable Scheduling <pre><code>// PUT /api/agents/{id}/scheduling-enabled\n{\n  \"enabled\": true,\n  \"timezone\": \"America/New_York\"\n}\n</code></pre></p> </li> <li> <p>Set Daily Schedules <pre><code>// POST /api/agents/{id}/schedules\n{\n  \"dayOfWeek\": 1,  // Monday (0=Sunday, 6=Saturday)\n  \"startTimeUTC\": \"22:00\",\n  \"endTimeUTC\": \"06:00\",\n  \"timezone\": \"America/New_York\",\n  \"isActive\": true\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/agents/#schedule-features","title":"Schedule Features","text":"<ul> <li>UTC Storage: All times stored in UTC for consistency</li> <li>Timezone Display: Shown in user's local timezone</li> <li>Overnight Support: Schedules can span midnight</li> <li>Bulk Updates: Update entire week at once</li> </ul>"},{"location":"admin-guide/operations/agents/#schedule-validation","title":"Schedule Validation","text":"<ul> <li>Start and end times must be different</li> <li>Day of week must be 0-6</li> <li>Times in HH:MM format</li> <li>Automatic handling of daylight saving time</li> </ul>"},{"location":"admin-guide/operations/agents/#availability-considerations","title":"Availability Considerations","text":"<p>When scheduling is enabled: - Agents only receive jobs during scheduled hours - Running jobs continue to completion - Agents remain connected outside schedule - Heartbeat monitoring continues</p>"},{"location":"admin-guide/operations/agents/#hardware-capabilities-and-benchmarks","title":"Hardware Capabilities and Benchmarks","text":""},{"location":"admin-guide/operations/agents/#hardware-detection","title":"Hardware Detection","text":"<p>Agents automatically detect:</p> <pre><code>{\n  \"hardware\": {\n    \"cpus\": [\n      {\n        \"model\": \"AMD Ryzen 9 7950X\",\n        \"cores\": 16,\n        \"threads\": 32,\n        \"frequency\": 4.5\n      }\n    ],\n    \"gpus\": [\n      {\n        \"vendor\": \"NVIDIA\",\n        \"model\": \"GeForce RTX 4090\",\n        \"memory\": 24576,\n        \"driver\": \"545.29.06\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#benchmark-system","title":"Benchmark System","text":"<p>Agents can run benchmarks for different hash types:</p> <pre><code>-- Benchmark results stored per agent\nagent_benchmarks (\n  agent_id,\n  attack_mode,     -- 0=dictionary, 3=bruteforce, etc.\n  hash_type,       -- 0=MD5, 1000=NTLM, etc.\n  speed,           -- Hashes per second\n  created_at\n)\n</code></pre>"},{"location":"admin-guide/operations/agents/#performance-metrics","title":"Performance Metrics","text":"<p>Key metrics tracked: - Hash Rate: Speed for each hash type - GPU Temperature: Thermal monitoring - GPU Utilization: Processing efficiency - Memory Usage: VRAM consumption - Power Consumption: Wattage tracking</p>"},{"location":"admin-guide/operations/agents/#consecutive-failure-tracking","title":"Consecutive Failure Tracking","text":"<p>Agents track consecutive task failures: - Increments on task failure - Resets on successful completion - Can trigger automatic disabling - Helps identify problematic agents</p>"},{"location":"admin-guide/operations/agents/#agent-disconnection-and-task-recovery","title":"Agent Disconnection and Task Recovery","text":""},{"location":"admin-guide/operations/agents/#understanding-disconnection-scenarios","title":"Understanding Disconnection Scenarios","text":"<p>KrakenHashes handles three distinct agent disconnection scenarios, each with specific recovery mechanisms:</p>"},{"location":"admin-guide/operations/agents/#1-backend-restart-planned-or-unplanned","title":"1. Backend Restart (Planned or Unplanned)","text":"<p>When the backend server restarts while agents have running tasks:</p> <p>What Happens: - Tasks transition to <code>reconnect_pending</code> state - Agents continue processing and cache crack results locally - Agent attempts reconnection with exponential backoff - Upon reconnection, agent reports current task status</p> <p>Recovery Process: 1. Agent reconnects and sends <code>current_task_status</code> message 2. Backend validates the task belongs to this agent 3. Task transitions from <code>reconnect_pending</code> back to <code>running</code> 4. Cached crack results are processed 5. Job continues without losing progress</p> <p>Grace Period: Configured via Admin Panel \u2192 Settings \u2192 Job Execution \u2192 Reconnect Grace Period (default: 5 minutes)</p>"},{"location":"admin-guide/operations/agents/#2-graceful-agent-shutdown","title":"2. Graceful Agent Shutdown","text":"<p>When an agent is properly stopped (SIGTERM, Ctrl+C, or service stop):</p> <p>What Happens: - Agent sends <code>agent_shutdown</code> notification to backend - Backend immediately marks task as <code>pending</code> for reassignment - No retry count increment (not a failure) - Task becomes available for other agents</p> <p>Recovery Process: - Task is immediately available for reassignment - No grace period applies - Original agent can claim new tasks upon restart</p>"},{"location":"admin-guide/operations/agents/#3-agent-crash-or-network-failure","title":"3. Agent Crash or Network Failure","text":"<p>When an agent disconnects unexpectedly (crash, network loss, power failure):</p> <p>What Happens: - Backend detects lost WebSocket connection - Task enters <code>reconnect_pending</code> state - Grace period timer starts</p> <p>Recovery Process:</p> <p>If agent reconnects within grace period: - Agent reports it has no running task - Backend marks task as <code>pending</code> for reassignment - Agent becomes available for new tasks</p> <p>If grace period expires: - Task automatically transitions to <code>pending</code> - Available for any agent to claim - Retry count may increment based on configuration</p>"},{"location":"admin-guide/operations/agents/#task-state-transitions","title":"Task State Transitions","text":"<pre><code>running \u2192 reconnect_pending \u2192 running (agent reconnects with task)\nrunning \u2192 reconnect_pending \u2192 pending (agent reconnects without task)\nrunning \u2192 reconnect_pending \u2192 pending (grace period expires)\nrunning \u2192 pending (graceful shutdown)\n</code></pre>"},{"location":"admin-guide/operations/agents/#monitoring-disconnection-events","title":"Monitoring Disconnection Events","text":""},{"location":"admin-guide/operations/agents/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<ul> <li>Reconnection frequency: High frequency indicates network issues</li> <li>Grace period utilization: Tasks recovering vs. timing out</li> <li>Task reassignment rate: How often tasks move between agents</li> <li>Cached data volume: Amount of data agents cache during disconnections</li> </ul>"},{"location":"admin-guide/operations/agents/#log-indicators","title":"Log Indicators","text":"<p>Backend logs to monitor: <pre><code>INFO: Agent X: Task status - HasTask: true, TaskID: xxx\nINFO: Task can be recovered [task_id=xxx, status=reconnect_pending]\nINFO: Successfully recovered task\n</code></pre></p> <p>Agent logs to monitor: <pre><code>INFO: Sending current task status to backend\nINFO: Successfully sent current task status - HasTask: true\nINFO: Connection state: disconnected\nINFO: Reconnection attempt X - Waiting Ys before retry\n</code></pre></p>"},{"location":"admin-guide/operations/agents/#configuring-recovery-behavior","title":"Configuring Recovery Behavior","text":""},{"location":"admin-guide/operations/agents/#reconnect-grace-period","title":"Reconnect Grace Period","text":"<p>Adjust based on your environment: - Stable networks: 3-5 minutes - Cloud environments: 5-10 minutes - Unreliable networks: 10-15 minutes - Maintenance windows: 15-30 minutes</p>"},{"location":"admin-guide/operations/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Set grace period based on recovery time: Consider how long it takes to:</li> <li>Restart backend services</li> <li>Complete rolling updates</li> <li> <p>Recover from network outages</p> </li> <li> <p>Monitor grace period effectiveness:</p> </li> <li>Track successful recoveries vs. timeouts</li> <li>Adjust if seeing excessive reassignments</li> <li> <p>Consider agent network stability</p> </li> <li> <p>Plan maintenance windows:</p> </li> <li>Increase grace period before maintenance</li> <li>Notify users of extended recovery time</li> <li> <p>Monitor agent reconnections post-maintenance</p> </li> <li> <p>Handle chronic disconnections:</p> </li> <li>Identify agents with frequent disconnections</li> <li>Check network path and stability</li> <li>Consider dedicated network routes for critical agents</li> </ol>"},{"location":"admin-guide/operations/agents/#troubleshooting-recovery-issues","title":"Troubleshooting Recovery Issues","text":""},{"location":"admin-guide/operations/agents/#agent-not-recovering-task-after-reconnection","title":"Agent Not Recovering Task After Reconnection","text":"<p>Symptoms: Agent reconnects but task is reassigned</p> <p>Common Causes: - Task ID mismatch between agent and backend - Database constraint violations (check backend logs) - Agent started fresh without cached state</p> <p>Solutions: 1. Check agent has persistent storage for state 2. Verify task ID in agent and backend logs match 3. Review backend logs for \"Failed to recover task\" errors 4. Ensure database migrations are up to date</p>"},{"location":"admin-guide/operations/agents/#tasks-stuck-in-reconnect_pending","title":"Tasks Stuck in Reconnect_Pending","text":"<p>Symptoms: Tasks remain in reconnect_pending after grace period</p> <p>Common Causes: - Backend service not running task cleanup - Database lock preventing state transition - Incorrect grace period configuration</p> <p>Solutions: 1. Verify job cleanup service is running 2. Check database for locks on job_tasks table 3. Manually transition stuck tasks if needed:    <pre><code>UPDATE job_tasks \nSET status = 'pending', updated_at = NOW() \nWHERE status = 'reconnect_pending' \nAND updated_at &lt; NOW() - INTERVAL '10 minutes';\n</code></pre></p>"},{"location":"admin-guide/operations/agents/#excessive-task-reassignments","title":"Excessive Task Reassignments","text":"<p>Symptoms: Tasks frequently moving between agents</p> <p>Common Causes: - Grace period too short - Network instability - Agents crashing frequently</p> <p>Solutions: 1. Increase reconnect grace period 2. Investigate network stability 3. Check agent system resources and logs 4. Consider agent health checks</p>"},{"location":"admin-guide/operations/agents/#troubleshooting-agent-issues","title":"Troubleshooting Agent Issues","text":""},{"location":"admin-guide/operations/agents/#common-connection-issues","title":"Common Connection Issues","text":"<ol> <li>Agent Won't Connect</li> <li>Check TLS certificates are valid</li> <li>Verify API key hasn't expired</li> <li>Ensure firewall allows WebSocket (port 8443)</li> <li> <p>Check agent logs for detailed errors</p> </li> <li> <p>Frequent Disconnections</p> </li> <li>Review ping/pong timeout settings</li> <li>Check network stability</li> <li>Monitor agent system resources</li> <li>Verify no proxy interference</li> </ol>"},{"location":"admin-guide/operations/agents/#authentication-problems","title":"Authentication Problems","text":"<ol> <li> <p>Invalid API Key <pre><code># Regenerate API key\n./krakenhashes-agent reregister --claim-code NEW-CODE\n</code></pre></p> </li> <li> <p>Certificate Issues</p> </li> <li>Check certificate expiration</li> <li>Verify CA certificate is trusted</li> <li>Ensure certificate matches server hostname</li> </ol>"},{"location":"admin-guide/operations/agents/#performance-issues","title":"Performance Issues","text":"<ol> <li>Low Hash Rates</li> <li>Check GPU driver versions</li> <li>Monitor thermal throttling</li> <li>Verify power settings</li> <li> <p>Review extra parameters</p> </li> <li> <p>High Failure Rate</p> </li> <li>Check hashcat binary compatibility</li> <li>Verify file synchronization</li> <li>Review job parameters</li> <li>Monitor system stability</li> </ol>"},{"location":"admin-guide/operations/agents/#debugging-tools","title":"Debugging Tools","text":"<ol> <li> <p>Agent Logs <pre><code># View agent logs\ntail -f ~/.krakenhashes/agent/logs/agent.log\n</code></pre></p> </li> <li> <p>Server-side Monitoring <pre><code>-- Check agent errors\nSELECT id, name, last_error, consecutive_failures\nFROM agents\nWHERE last_error IS NOT NULL\nORDER BY updated_at DESC;\n</code></pre></p> </li> <li> <p>WebSocket Messages</p> </li> <li>Enable debug logging for detailed messages</li> <li>Monitor heartbeat intervals</li> <li>Check message acknowledgments</li> </ol>"},{"location":"admin-guide/operations/agents/#recovery-procedures","title":"Recovery Procedures","text":"<ol> <li> <p>Reset Agent State <pre><code>-- Clear error state\nUPDATE agents \nSET status = 'inactive', \n    last_error = NULL,\n    consecutive_failures = 0\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>Force Reconnection</p> </li> <li>Restart agent service</li> <li>Clear local cache</li> <li> <p>Verify network connectivity</p> </li> <li> <p>Complete Re-registration</p> </li> <li>Generate new claim code</li> <li>Remove agent from database</li> <li>Perform fresh registration</li> </ol>"},{"location":"admin-guide/operations/agents/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Set Up Alerts</li> <li>Agent offline &gt; 5 minutes</li> <li>Consecutive failures &gt; 3</li> <li>Temperature &gt; 85\u00b0C</li> <li> <p>Low hash rates</p> </li> <li> <p>Regular Maintenance</p> </li> <li>Update agent software</li> <li>Clean GPU fans</li> <li>Check thermal paste</li> <li> <p>Update drivers</p> </li> <li> <p>Capacity Planning</p> </li> <li>Monitor job queue depth</li> <li>Track agent utilization</li> <li>Plan for peak loads</li> <li>Consider scheduling optimization</li> </ol>"},{"location":"admin-guide/operations/agents/#advanced-topics","title":"Advanced Topics","text":""},{"location":"admin-guide/operations/agents/#agent-clustering","title":"Agent Clustering","text":"<ul> <li>Group agents by capability</li> <li>Assign specialized workloads</li> <li>Balance load across regions</li> <li>Implement failover strategies</li> </ul>"},{"location":"admin-guide/operations/agents/#security-hardening","title":"Security Hardening","text":"<ul> <li>Rotate API keys periodically</li> <li>Implement IP whitelisting</li> <li>Use dedicated agent VLANs</li> <li>Monitor for anomalous behavior</li> </ul>"},{"location":"admin-guide/operations/agents/#integration-points","title":"Integration Points","text":"<ul> <li>Export metrics to monitoring systems</li> <li>Webhook notifications for events</li> <li>API automation for scaling</li> <li>Custom scheduling algorithms</li> </ul>"},{"location":"admin-guide/operations/agents/#conclusion","title":"Conclusion","text":"<p>Effective agent management is crucial for maintaining a high-performance distributed cracking system. Regular monitoring, proper scheduling, and proactive troubleshooting ensure optimal resource utilization and job completion rates.</p> <p>For additional support or advanced configurations, consult the system administrator documentation or contact the development team.</p>"},{"location":"admin-guide/operations/backup/","title":"Backup and Restore Procedures","text":"<p>This guide provides comprehensive backup and restore procedures for KrakenHashes, covering database backups, file system backups, Docker volumes, and automated backup strategies.</p>"},{"location":"admin-guide/operations/backup/#overview","title":"Overview","text":"<p>KrakenHashes stores critical data in multiple locations that require regular backups:</p> <ol> <li>PostgreSQL Database - Contains all system metadata, user accounts, job configurations, and hash crack results</li> <li>File Storage - Binary files, wordlists, rules, and uploaded hashlists</li> <li>Docker Volumes - Persistent storage for containerized deployments</li> <li>Configuration Files - TLS certificates and system configuration</li> </ol>"},{"location":"admin-guide/operations/backup/#data-locations","title":"Data Locations","text":""},{"location":"admin-guide/operations/backup/#database","title":"Database","text":"<ul> <li>Container: <code>krakenhashes-postgres</code></li> <li>Volume: <code>krakenhashes_postgres_data</code></li> <li>Critical Tables:</li> <li><code>users</code> - User accounts and authentication</li> <li><code>agents</code> - Registered compute agents</li> <li><code>hashlists</code> - Hash collections metadata</li> <li><code>hashes</code> - Individual hash records and crack status</li> <li><code>clients</code> - Customer/engagement tracking</li> <li><code>job_executions</code> - Job execution history</li> <li><code>wordlists</code>, <code>rules</code> - Attack resource metadata</li> <li><code>auth_tokens</code> - Authentication tokens</li> <li><code>vouchers</code> - Agent registration codes</li> </ul>"},{"location":"admin-guide/operations/backup/#file-storage","title":"File Storage","text":"<p>Default location: <code>/var/lib/krakenhashes/</code> (configurable via <code>KH_DATA_DIR</code>)</p> <pre><code>/var/lib/krakenhashes/\n\u251c\u2500\u2500 binaries/          # Hashcat and other tool binaries\n\u251c\u2500\u2500 wordlists/         # Wordlist files\n\u2502   \u251c\u2500\u2500 general/\n\u2502   \u251c\u2500\u2500 specialized/\n\u2502   \u251c\u2500\u2500 targeted/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 rules/             # Rule files\n\u2502   \u251c\u2500\u2500 hashcat/\n\u2502   \u251c\u2500\u2500 john/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 hashlists/         # Uploaded hash files\n\u2514\u2500\u2500 hashlist_uploads/  # Temporary upload directory\n</code></pre>"},{"location":"admin-guide/operations/backup/#docker-volumes","title":"Docker Volumes","text":"<ul> <li><code>krakenhashes_postgres_data</code> - PostgreSQL data</li> <li><code>krakenhashes_app_data</code> - Application data files</li> </ul>"},{"location":"admin-guide/operations/backup/#backup-strategies","title":"Backup Strategies","text":""},{"location":"admin-guide/operations/backup/#1-postgresql-database-backup","title":"1. PostgreSQL Database Backup","text":""},{"location":"admin-guide/operations/backup/#manual-backup","title":"Manual Backup","text":"<pre><code># Create backup directory\nmkdir -p /backup/krakenhashes/postgres/$(date +%Y%m%d)\n\n# Backup using pg_dump (recommended for portability)\ndocker exec krakenhashes-postgres pg_dump \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --verbose \\\n  --format=custom \\\n  --file=/tmp/krakenhashes_$(date +%Y%m%d_%H%M%S).dump\n\n# Copy backup from container\ndocker cp krakenhashes-postgres:/tmp/krakenhashes_$(date +%Y%m%d_%H%M%S).dump \\\n  /backup/krakenhashes/postgres/$(date +%Y%m%d)/\n\n# Alternative: Direct backup with compression\ndocker exec krakenhashes-postgres pg_dump \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  | gzip &gt; /backup/krakenhashes/postgres/$(date +%Y%m%d)/krakenhashes_$(date +%Y%m%d_%H%M%S).sql.gz\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-database-backup-script","title":"Automated Database Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-db-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Database Backup Script\n# Run via cron for automated backups\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/postgres\"\nRETENTION_DAYS=30\nDB_CONTAINER=\"krakenhashes-postgres\"\nDB_NAME=\"krakenhashes\"\nDB_USER=\"krakenhashes\"\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Check if container is running\nif ! docker ps | grep -q \"${DB_CONTAINER}\"; then\n    log \"ERROR: Container ${DB_CONTAINER} is not running\"\n    exit 1\nfi\n\nlog \"Starting database backup...\"\n\n# Perform backup\nBACKUP_FILE=\"${BACKUP_DIR}/${DATE_DIR}/krakenhashes_${TIMESTAMP}.dump\"\nif docker exec \"${DB_CONTAINER}\" pg_dump \\\n    -U \"${DB_USER}\" \\\n    -d \"${DB_NAME}\" \\\n    --verbose \\\n    --format=custom \\\n    --file=\"/tmp/backup_${TIMESTAMP}.dump\"; then\n\n    # Copy backup from container\n    docker cp \"${DB_CONTAINER}:/tmp/backup_${TIMESTAMP}.dump\" \"${BACKUP_FILE}\"\n\n    # Clean up temp file in container\n    docker exec \"${DB_CONTAINER}\" rm \"/tmp/backup_${TIMESTAMP}.dump\"\n\n    # Compress backup\n    gzip \"${BACKUP_FILE}\"\n\n    log \"Database backup completed: ${BACKUP_FILE}.gz\"\n\n    # Calculate backup size\n    SIZE=$(du -h \"${BACKUP_FILE}.gz\" | cut -f1)\n    log \"Backup size: ${SIZE}\"\nelse\n    log \"ERROR: Database backup failed\"\n    exit 1\nfi\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f -name \"*.dump.gz\" -mtime +${RETENTION_DAYS} -delete\n\nlog \"Backup process completed\"\n</code></pre> <p>Make it executable: <pre><code>chmod +x /usr/local/bin/krakenhashes-db-backup.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#2-file-system-backup","title":"2. File System Backup","text":""},{"location":"admin-guide/operations/backup/#manual-backup_1","title":"Manual Backup","text":"<pre><code># Create backup directory\nmkdir -p /backup/krakenhashes/files/$(date +%Y%m%d)\n\n# Backup data directory with compression\ntar -czf /backup/krakenhashes/files/$(date +%Y%m%d)/krakenhashes_data_$(date +%Y%m%d_%H%M%S).tar.gz \\\n  -C /var/lib/krakenhashes \\\n  binaries wordlists rules hashlists hashlist_uploads\n\n# Backup with progress indicator\ntar -czf /backup/krakenhashes/files/$(date +%Y%m%d)/krakenhashes_data_$(date +%Y%m%d_%H%M%S).tar.gz \\\n  --checkpoint=1000 \\\n  --checkpoint-action=echo=\"Processed %{r}T files\" \\\n  -C /var/lib/krakenhashes \\\n  binaries wordlists rules hashlists hashlist_uploads\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-file-backup-script","title":"Automated File Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-file-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes File System Backup Script\n# Backs up all data files including binaries, wordlists, rules, and hashlists\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/files\"\nDATA_DIR=\"${KH_DATA_DIR:-/var/lib/krakenhashes}\"\nRETENTION_DAYS=30\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Check if data directory exists\nif [ ! -d \"${DATA_DIR}\" ]; then\n    log \"ERROR: Data directory ${DATA_DIR} does not exist\"\n    exit 1\nfi\n\nlog \"Starting file system backup...\"\nlog \"Source directory: ${DATA_DIR}\"\n\n# Calculate total size\nTOTAL_SIZE=$(du -sh \"${DATA_DIR}\" | cut -f1)\nlog \"Total data size: ${TOTAL_SIZE}\"\n\n# Create backup\nBACKUP_FILE=\"${BACKUP_DIR}/${DATE_DIR}/krakenhashes_data_${TIMESTAMP}.tar.gz\"\n\n# Backup with progress\ntar -czf \"${BACKUP_FILE}\" \\\n    --checkpoint=1000 \\\n    --checkpoint-action=echo=\"[$(date '+%Y-%m-%d %H:%M:%S')] Processed %{r}T files\" \\\n    -C \"${DATA_DIR}\" \\\n    binaries wordlists rules hashlists hashlist_uploads 2&gt;&amp;1 | while read line; do\n        log \"$line\"\n    done\n\nif [ ${PIPESTATUS[0]} -eq 0 ]; then\n    log \"File backup completed: ${BACKUP_FILE}\"\n\n    # Calculate backup size\n    SIZE=$(du -h \"${BACKUP_FILE}\" | cut -f1)\n    log \"Backup size: ${SIZE}\"\n\n    # Create checksum\n    sha256sum \"${BACKUP_FILE}\" &gt; \"${BACKUP_FILE}.sha256\"\n    log \"Checksum created: ${BACKUP_FILE}.sha256\"\nelse\n    log \"ERROR: File backup failed\"\n    exit 1\nfi\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f \\( -name \"*.tar.gz\" -o -name \"*.sha256\" \\) -mtime +${RETENTION_DAYS} -delete\n\nlog \"File backup process completed\"\n</code></pre> <p>Make it executable: <pre><code>chmod +x /usr/local/bin/krakenhashes-file-backup.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#3-docker-volume-backup","title":"3. Docker Volume Backup","text":""},{"location":"admin-guide/operations/backup/#manual-volume-backup","title":"Manual Volume Backup","text":"<pre><code># Stop services to ensure consistency\ndocker-compose down\n\n# Backup PostgreSQL volume\ndocker run --rm \\\n  -v krakenhashes_postgres_data:/source:ro \\\n  -v /backup/krakenhashes/volumes:/backup \\\n  alpine tar -czf /backup/postgres_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /source .\n\n# Backup application data volume\ndocker run --rm \\\n  -v krakenhashes_app_data:/source:ro \\\n  -v /backup/krakenhashes/volumes:/backup \\\n  alpine tar -czf /backup/app_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /source .\n\n# Restart services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-volume-backup-script","title":"Automated Volume Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-volume-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Docker Volume Backup Script\n# Backs up Docker volumes with minimal downtime\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/volumes\"\nRETENTION_DAYS=30\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\nCOMPOSE_FILE=\"/path/to/krakenhashes/docker-compose.yml\"\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Function to backup a volume\nbackup_volume() {\n    local volume_name=$1\n    local backup_name=$2\n\n    log \"Backing up volume: ${volume_name}\"\n\n    docker run --rm \\\n        -v \"${volume_name}:/source:ro\" \\\n        -v \"${BACKUP_DIR}/${DATE_DIR}:/backup\" \\\n        alpine tar -czf \"/backup/${backup_name}_${TIMESTAMP}.tar.gz\" -C /source .\n\n    if [ $? -eq 0 ]; then\n        log \"Volume backup completed: ${backup_name}_${TIMESTAMP}.tar.gz\"\n\n        # Create checksum\n        cd \"${BACKUP_DIR}/${DATE_DIR}\"\n        sha256sum \"${backup_name}_${TIMESTAMP}.tar.gz\" &gt; \"${backup_name}_${TIMESTAMP}.tar.gz.sha256\"\n    else\n        log \"ERROR: Failed to backup volume ${volume_name}\"\n        return 1\n    fi\n}\n\nlog \"Starting Docker volume backup...\"\n\n# For live backups without downtime (PostgreSQL only)\nif docker ps | grep -q \"krakenhashes-postgres\"; then\n    log \"Creating PostgreSQL checkpoint for consistent backup...\"\n    docker exec krakenhashes-postgres psql -U krakenhashes -c \"CHECKPOINT;\"\nfi\n\n# Backup volumes\nbackup_volume \"krakenhashes_postgres_data\" \"postgres_data\"\nbackup_volume \"krakenhashes_app_data\" \"app_data\"\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f \\( -name \"*.tar.gz\" -o -name \"*.sha256\" \\) -mtime +${RETENTION_DAYS} -delete\n\nlog \"Volume backup process completed\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#4-configuration-backup","title":"4. Configuration Backup","text":"<pre><code>#!/bin/bash\n\n# Backup configuration files\nBACKUP_DIR=\"/backup/krakenhashes/config/$(date +%Y%m%d)\"\nmkdir -p \"${BACKUP_DIR}\"\n\n# Backup TLS certificates\ntar -czf \"${BACKUP_DIR}/certs_$(date +%Y%m%d_%H%M%S).tar.gz\" \\\n  -C /etc/krakenhashes certs\n\n# Backup environment files\ncp /path/to/krakenhashes/.env \"${BACKUP_DIR}/env_$(date +%Y%m%d_%H%M%S)\"\n\n# Backup docker-compose configuration\ncp /path/to/krakenhashes/docker-compose.yml \"${BACKUP_DIR}/\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-backup-schedule","title":"Automated Backup Schedule","text":"<p>Add to crontab (<code>crontab -e</code>):</p> <pre><code># KrakenHashes Automated Backups\n# Database backup - every 6 hours\n0 */6 * * * /usr/local/bin/krakenhashes-db-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# File system backup - daily at 2 AM\n0 2 * * * /usr/local/bin/krakenhashes-file-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# Volume backup - daily at 3 AM\n0 3 * * * /usr/local/bin/krakenhashes-volume-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# Configuration backup - weekly on Sunday at 4 AM\n0 4 * * 0 /usr/local/bin/krakenhashes-config-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n</code></pre>"},{"location":"admin-guide/operations/backup/#restore-procedures","title":"Restore Procedures","text":""},{"location":"admin-guide/operations/backup/#1-database-restore","title":"1. Database Restore","text":""},{"location":"admin-guide/operations/backup/#from-pg_dump-backup","title":"From pg_dump backup","text":"<pre><code># Stop the application\ndocker-compose stop krakenhashes\n\n# Restore database\ndocker exec -i krakenhashes-postgres pg_restore \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --clean \\\n  --if-exists \\\n  --verbose \\\n  &lt; /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.dump\n\n# Or from compressed SQL\ngunzip -c /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.sql.gz | \\\n  docker exec -i krakenhashes-postgres psql -U krakenhashes -d krakenhashes\n\n# Restart application\ndocker-compose start krakenhashes\n</code></pre>"},{"location":"admin-guide/operations/backup/#full-database-recreation","title":"Full database recreation","text":"<pre><code># Stop all services\ndocker-compose down\n\n# Remove old database volume\ndocker volume rm krakenhashes_postgres_data\n\n# Recreate and start database\ndocker-compose up -d postgres\n\n# Wait for database to be ready\nsleep 10\n\n# Create database and user\ndocker exec krakenhashes-postgres psql -U postgres -c \"CREATE DATABASE krakenhashes;\"\ndocker exec krakenhashes-postgres psql -U postgres -c \"CREATE USER krakenhashes WITH PASSWORD 'your-password';\"\ndocker exec krakenhashes-postgres psql -U postgres -c \"GRANT ALL PRIVILEGES ON DATABASE krakenhashes TO krakenhashes;\"\n\n# Restore from backup\ndocker exec -i krakenhashes-postgres pg_restore \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --verbose \\\n  &lt; /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.dump\n\n# Start all services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#2-file-system-restore","title":"2. File System Restore","text":"<pre><code># Create data directory if it doesn't exist\nmkdir -p /var/lib/krakenhashes\n\n# Extract backup\ntar -xzf /backup/krakenhashes/files/20240101/krakenhashes_data_20240101_120000.tar.gz \\\n  -C /var/lib/krakenhashes\n\n# Verify checksum\ncd /backup/krakenhashes/files/20240101\nsha256sum -c krakenhashes_data_20240101_120000.tar.gz.sha256\n\n# Fix permissions\nchown -R 1000:1000 /var/lib/krakenhashes\nchmod -R 750 /var/lib/krakenhashes\n</code></pre>"},{"location":"admin-guide/operations/backup/#3-docker-volume-restore","title":"3. Docker Volume Restore","text":"<pre><code># Stop services\ndocker-compose down\n\n# Remove existing volumes\ndocker volume rm krakenhashes_postgres_data krakenhashes_app_data\n\n# Recreate volumes\ndocker volume create krakenhashes_postgres_data\ndocker volume create krakenhashes_app_data\n\n# Restore PostgreSQL volume\ndocker run --rm \\\n  -v krakenhashes_postgres_data:/target \\\n  -v /backup/krakenhashes/volumes/20240101:/backup:ro \\\n  alpine tar -xzf /backup/postgres_data_20240101_120000.tar.gz -C /target\n\n# Restore application data volume\ndocker run --rm \\\n  -v krakenhashes_app_data:/target \\\n  -v /backup/krakenhashes/volumes/20240101:/backup:ro \\\n  alpine tar -xzf /backup/app_data_20240101_120000.tar.gz -C /target\n\n# Start services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#backup-verification","title":"Backup Verification","text":""},{"location":"admin-guide/operations/backup/#automated-verification-script","title":"Automated Verification Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-verify-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Backup Verification Script\n# Verifies backup integrity and tests restore procedures\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes\"\nTEST_RESTORE_DIR=\"/tmp/krakenhashes-restore-test\"\nVERIFICATION_LOG=\"/var/log/krakenhashes-backup-verification.log\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" | tee -a \"${VERIFICATION_LOG}\"\n}\n\n# Function to verify file integrity\nverify_file() {\n    local file=$1\n    local checksum_file=\"${file}.sha256\"\n\n    if [ -f \"${checksum_file}\" ]; then\n        if sha256sum -c \"${checksum_file}\" &gt; /dev/null 2&gt;&amp;1; then\n            log \"\u2713 Checksum verified: $(basename ${file})\"\n            return 0\n        else\n            log \"\u2717 Checksum failed: $(basename ${file})\"\n            return 1\n        fi\n    else\n        log \"\u26a0 No checksum file for: $(basename ${file})\"\n        return 2\n    fi\n}\n\n# Function to test database restore\ntest_db_restore() {\n    local backup_file=$1\n\n    log \"Testing database restore from: $(basename ${backup_file})\"\n\n    # Create test database\n    docker exec krakenhashes-postgres psql -U postgres -c \"CREATE DATABASE krakenhashes_test;\"\n\n    # Attempt restore\n    if gunzip -c \"${backup_file}\" | docker exec -i krakenhashes-postgres psql -U postgres -d krakenhashes_test &gt; /dev/null 2&gt;&amp;1; then\n        # Verify some data\n        USERS=$(docker exec krakenhashes-postgres psql -U postgres -d krakenhashes_test -t -c \"SELECT COUNT(*) FROM users;\")\n        log \"\u2713 Database restore successful. Found ${USERS// /} users.\"\n\n        # Clean up\n        docker exec krakenhashes-postgres psql -U postgres -c \"DROP DATABASE krakenhashes_test;\"\n        return 0\n    else\n        log \"\u2717 Database restore failed\"\n        docker exec krakenhashes-postgres psql -U postgres -c \"DROP DATABASE IF EXISTS krakenhashes_test;\"\n        return 1\n    fi\n}\n\n# Main verification process\nlog \"Starting backup verification...\"\n\n# Find latest backups\nLATEST_DB_BACKUP=$(find \"${BACKUP_DIR}/postgres\" -name \"*.dump.gz\" -type f -mtime -1 | sort -r | head -1)\nLATEST_FILE_BACKUP=$(find \"${BACKUP_DIR}/files\" -name \"*.tar.gz\" -type f -mtime -1 | sort -r | head -1)\nLATEST_VOLUME_BACKUP=$(find \"${BACKUP_DIR}/volumes\" -name \"postgres_data_*.tar.gz\" -type f -mtime -1 | sort -r | head -1)\n\n# Verify database backup\nif [ -n \"${LATEST_DB_BACKUP}\" ]; then\n    verify_file \"${LATEST_DB_BACKUP}\"\n    test_db_restore \"${LATEST_DB_BACKUP}\"\nelse\n    log \"\u26a0 No recent database backup found\"\nfi\n\n# Verify file backup\nif [ -n \"${LATEST_FILE_BACKUP}\" ]; then\n    verify_file \"${LATEST_FILE_BACKUP}\"\n\n    # Test extraction\n    mkdir -p \"${TEST_RESTORE_DIR}\"\n    if tar -tzf \"${LATEST_FILE_BACKUP}\" &gt; /dev/null 2&gt;&amp;1; then\n        log \"\u2713 File backup archive is valid\"\n    else\n        log \"\u2717 File backup archive is corrupted\"\n    fi\n    rm -rf \"${TEST_RESTORE_DIR}\"\nelse\n    log \"\u26a0 No recent file backup found\"\nfi\n\n# Verify volume backup\nif [ -n \"${LATEST_VOLUME_BACKUP}\" ]; then\n    verify_file \"${LATEST_VOLUME_BACKUP}\"\nelse\n    log \"\u26a0 No recent volume backup found\"\nfi\n\nlog \"Backup verification completed\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":""},{"location":"admin-guide/operations/backup/#recovery-time-objectives-rto","title":"Recovery Time Objectives (RTO)","text":"<ul> <li>Database: 30 minutes</li> <li>File System: 1 hour</li> <li>Full System: 2 hours</li> </ul>"},{"location":"admin-guide/operations/backup/#recovery-point-objectives-rpo","title":"Recovery Point Objectives (RPO)","text":"<ul> <li>Database: 6 hours (based on backup frequency)</li> <li>File System: 24 hours</li> <li>Configuration: 7 days</li> </ul>"},{"location":"admin-guide/operations/backup/#recovery-priority","title":"Recovery Priority","text":"<ol> <li>PostgreSQL Database - Contains all system state</li> <li>Configuration Files - Required for system operation</li> <li>Hashlists - User-uploaded hash files</li> <li>Wordlists/Rules - Can be re-downloaded if needed</li> <li>Binaries - Can be re-downloaded from version tracking</li> </ol>"},{"location":"admin-guide/operations/backup/#emergency-recovery-checklist","title":"Emergency Recovery Checklist","text":"<ol> <li>Assess Damage</li> <li> Identify failed components</li> <li> Determine data loss extent</li> <li> <p> Document incident timeline</p> </li> <li> <p>Prepare Recovery Environment</p> </li> <li> Provision new hardware/VMs</li> <li> Install Docker and dependencies</li> <li> <p> Restore network configuration</p> </li> <li> <p>Restore Core Services</p> </li> <li> Restore PostgreSQL database</li> <li> Restore configuration files</li> <li> <p> Verify TLS certificates</p> </li> <li> <p>Restore Data</p> </li> <li> Restore file system data</li> <li> Verify hashlist integrity</li> <li> <p> Restore Docker volumes</p> </li> <li> <p>Validation</p> </li> <li> Test user authentication</li> <li> Verify agent connectivity</li> <li> Check job execution</li> <li> <p> Validate data integrity</p> </li> <li> <p>Communication</p> </li> <li> Notify users of recovery status</li> <li> Document lessons learned</li> <li> Update recovery procedures</li> </ol>"},{"location":"admin-guide/operations/backup/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Testing</li> <li>Test restore procedures monthly</li> <li>Perform full disaster recovery drill quarterly</li> <li> <p>Document test results and issues</p> </li> <li> <p>Off-site Storage</p> </li> <li>Keep backups in multiple locations</li> <li>Use cloud storage for critical backups</li> <li> <p>Maintain 3-2-1 backup strategy (3 copies, 2 different media, 1 off-site)</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor backup job success/failure</li> <li>Alert on backup size anomalies</li> <li> <p>Track backup storage usage</p> </li> <li> <p>Security</p> </li> <li>Encrypt backups at rest</li> <li>Restrict backup access</li> <li> <p>Regularly rotate backup credentials</p> </li> <li> <p>Documentation</p> </li> <li>Keep recovery procedures updated</li> <li>Document system dependencies</li> <li>Maintain contact information for key personnel</li> </ol>"},{"location":"admin-guide/operations/backup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/backup/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Backup Fails with \"Permission Denied\" <pre><code># Fix backup directory permissions\nsudo chown -R $(whoami):$(whoami) /backup/krakenhashes\nsudo chmod -R 750 /backup/krakenhashes\n</code></pre></p> </li> <li> <p>Database Restore Fails <pre><code># Check PostgreSQL logs\ndocker logs krakenhashes-postgres\n\n# Verify database exists\ndocker exec krakenhashes-postgres psql -U postgres -l\n\n# Check user permissions\ndocker exec krakenhashes-postgres psql -U postgres -c \"\\du\"\n</code></pre></p> </li> <li> <p>Insufficient Disk Space <pre><code># Check disk usage\ndf -h /backup\n\n# Clean old backups manually\nfind /backup/krakenhashes -type f -mtime +${DAYS} -delete\n</code></pre></p> </li> <li> <p>Slow Backup Performance <pre><code># Use parallel compression\ntar -cf - -C /var/lib/krakenhashes . | pigz &gt; backup.tar.gz\n\n# Adjust PostgreSQL backup parameters\ndocker exec krakenhashes-postgres psql -U postgres -c \"SET maintenance_work_mem = '1GB';\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/backup/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"admin-guide/operations/backup/#backup-monitoring-script","title":"Backup Monitoring Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-backup-monitor.sh</code>:</p> <pre><code>#!/bin/bash\n\n# Check if backups are current and send alerts\n\nBACKUP_DIR=\"/backup/krakenhashes\"\nMAX_AGE_HOURS=25  # Alert if backup is older than 25 hours\nALERT_EMAIL=\"admin@example.com\"\n\ncheck_backup_age() {\n    local backup_type=$1\n    local pattern=$2\n\n    latest=$(find \"${BACKUP_DIR}/${backup_type}\" -name \"${pattern}\" -type f -mtime -1 | sort -r | head -1)\n\n    if [ -z \"$latest\" ]; then\n        echo \"CRITICAL: No recent ${backup_type} backup found\" | \\\n          mail -s \"KrakenHashes Backup Alert\" \"${ALERT_EMAIL}\"\n    fi\n}\n\n# Check each backup type\ncheck_backup_age \"postgres\" \"*.dump.gz\"\ncheck_backup_age \"files\" \"*.tar.gz\"\ncheck_backup_age \"volumes\" \"postgres_data_*.tar.gz\"\n</code></pre> <p>Add to crontab: <pre><code># Monitor backups daily at 9 AM\n0 9 * * * /usr/local/bin/krakenhashes-backup-monitor.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#summary","title":"Summary","text":"<p>This backup strategy ensures:</p> <ul> <li>Comprehensive Coverage: All critical data is backed up</li> <li>Automation: Reduces human error and ensures consistency</li> <li>Verification: Regular testing of backup integrity</li> <li>Quick Recovery: Clear procedures for various failure scenarios</li> <li>Scalability: Procedures scale with system growth</li> </ul> <p>Regular review and testing of these procedures is essential for maintaining a reliable backup and recovery system.</p>"},{"location":"admin-guide/operations/clients/","title":"Client Management","text":"<p>KrakenHashes allows associating hashlists with specific clients or engagements. This helps organize work and enables tailored settings, such as data retention policies.</p>"},{"location":"admin-guide/operations/clients/#overview","title":"Overview","text":"<ul> <li>Purpose: Clients represent distinct entities (e.g., internal teams, external customers, specific penetration testing engagements) for which hashlists are managed.</li> <li>Association: Hashlists can be optionally linked to a client during upload.</li> <li>Administration: Administrators can create, view, update, and delete client records.</li> </ul> <p> Client Management page showing the data table with columns for Name, Description, Contact, Retention period, Creation date, and available Actions</p>"},{"location":"admin-guide/operations/clients/#managing-clients-via-api","title":"Managing Clients via API","text":"<p>Administrators use the following API endpoints to manage clients:</p> <ul> <li> <p><code>GET /api/admin/clients</code></p> <ul> <li>Description: Lists all existing clients.</li> <li>Response: An array of client objects.</li> </ul> </li> <li> <p><code>POST /api/admin/clients</code></p> <ul> <li>Description: Creates a new client.</li> <li>Request Body (Example): <pre><code>{\n  \"name\": \"Project Hydra\",\n  \"description\": \"Q3 Internal Assessment\",\n  \"contact_info\": \"team-lead@example.com\"\n}\n</code></pre></li> <li><code>name</code> is required and must be unique.</li> <li><code>description</code> and <code>contact_info</code> are optional.</li> <li>Response: The newly created client object.</li> </ul> </li> <li> <p><code>GET /api/admin/clients/{id}</code></p> <ul> <li>Description: Retrieves details for a specific client by its UUID.</li> <li>Response: A single client object.</li> </ul> </li> <li> <p><code>PUT /api/admin/clients/{id}</code></p> <ul> <li>Description: Updates an existing client.</li> <li>Request Body: Same format as POST, but fields are optional (only provided fields are updated).</li> <li>Response: The updated client object.</li> </ul> </li> <li> <p><code>DELETE /api/admin/clients/{id}</code></p> <ul> <li>Description: Deletes a client.</li> <li>Important: Deleting a client typically also involves cleaning up associated resources like hashlists or reassigning them. The exact behavior might depend on implementation details (e.g., whether associated hashlists are deleted or disassociated).</li> <li>Response: Typically a 204 No Content on success.</li> </ul> </li> </ul>"},{"location":"admin-guide/operations/clients/#client-specific-data-retention","title":"Client-Specific Data Retention","text":"<p>Administrators can configure data retention policies specific to each client. This overrides the default system-wide retention setting (see Data Retention).</p> <ul> <li>Purpose: Allows different retention periods for data belonging to different clients or engagements.</li> <li>Configuration: Client retention settings are managed alongside other client details, likely via the <code>PUT /api/admin/clients/{id}</code> endpoint or dedicated sub-endpoints (check API definition for specifics).<ul> <li>Expected Fields (Example): <pre><code>{\n  \"name\": \"Project Hydra\",\n  \"description\": \"Q3 Internal Assessment\",\n  // ... other client fields\n  \"retention_months\": 30,       // Months to retain hashlists for THIS client\n  \"retention_override\": true  // Must be true to use retention_days\n}\n</code></pre></li> </ul> </li> <li>Precedence: Client-specific retention policy always takes precedence over the default policy. </li> </ul>"},{"location":"admin-guide/operations/data-retention/","title":"Data Retention","text":"<p>KrakenHashes includes a comprehensive data retention system that automatically and securely purges old hashlists and associated data based on configurable retention policies.</p>"},{"location":"admin-guide/operations/data-retention/#overview","title":"Overview","text":"<p>The retention system ensures compliance with data retention policies by automatically removing expired data from both the database and filesystem. It includes secure deletion mechanisms to prevent data recovery.</p> <p>Potfile Exclusion</p> <p>The retention system does NOT affect the potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>), which contains plaintext passwords from all cracked hashes. The potfile is considered a permanent system resource and must be manually managed. If you need to remove passwords associated with deleted hashlists from the potfile, you must do so manually or implement a separate cleanup process.</p>"},{"location":"admin-guide/operations/data-retention/#retention-policy-configuration","title":"Retention Policy Configuration","text":""},{"location":"admin-guide/operations/data-retention/#default-retention-policy","title":"Default Retention Policy","text":"<ul> <li>Purpose: Sets the default number of months after which hashlists are automatically deleted</li> <li>Scope: Applies to all hashlists unless overridden by client-specific settings</li> <li>Special Values:<ul> <li><code>0</code> = Keep forever (no automatic deletion)</li> <li><code>NULL</code> = Use system default</li> <li>Any positive integer = Number of months to retain data</li> </ul> </li> </ul>"},{"location":"admin-guide/operations/data-retention/#client-specific-retention","title":"Client-Specific Retention","text":"<p>Each client can have their own retention period that overrides the system default: -   Set during client creation or via the client management interface -   Takes precedence over the default retention setting -   Applies to all hashlists associated with that client</p>"},{"location":"admin-guide/operations/data-retention/#automatic-purge-process","title":"Automatic Purge Process","text":""},{"location":"admin-guide/operations/data-retention/#scheduling","title":"Scheduling","text":"<p>The retention purge runs automatically: -   On Startup: 15 seconds after backend initialization -   Daily: At midnight (server time) -   Processes hashlists in batches of 1000 for scalability</p>"},{"location":"admin-guide/operations/data-retention/#what-gets-deleted","title":"What Gets Deleted","text":"<p>When a hashlist expires based on retention policy:</p> <ol> <li>Database Records:</li> <li>Hashlist record from <code>hashlists</code> table</li> <li>All associations in <code>hashlist_hashes</code> table</li> <li>Orphaned hashes (not associated with any other hashlist)</li> <li> <p>Related records via CASCADE deletion:</p> <ul> <li><code>agent_hashlists</code> entries</li> <li><code>job_executions</code> entries</li> </ul> </li> <li> <p>Filesystem:</p> </li> <li>Hashlist file from <code>/var/lib/krakenhashes/hashlists/</code></li> <li>File is securely overwritten with random data before deletion</li> <li> <p>Prevents recovery via filesystem forensics</p> </li> <li> <p>PostgreSQL Maintenance:</p> </li> <li><code>VACUUM ANALYZE</code> runs on affected tables</li> <li>Reclaims storage space</li> <li>Removes dead tuples to prevent WAL recovery</li> </ol>"},{"location":"admin-guide/operations/data-retention/#what-does-not-get-deleted","title":"What Does NOT Get Deleted","text":"<p>Retained Data</p> <p>The following data is NOT removed by the retention system:</p> <ul> <li>Potfile: Contains plaintext passwords for ALL cracked hashes across all hashlists</li> <li>Clients: Client records remain even when all their hashlists are deleted</li> <li>Users: User accounts are never deleted by retention policies</li> <li>Wordlists: Permanent resources not affected by retention</li> <li>Rules: Permanent resources not affected by retention</li> <li>Binaries: System binaries remain unchanged</li> </ul>"},{"location":"admin-guide/operations/data-retention/#security-features","title":"Security Features","text":"<ul> <li>Secure File Deletion: Files are overwritten with random data before removal</li> <li>Transaction Safety: All database operations occur within a single transaction</li> <li>Audit Logging: Complete audit trail of all deletion operations</li> <li>VACUUM Operations: Prevents data recovery from PostgreSQL internals</li> </ul>"},{"location":"admin-guide/operations/data-retention/#agent-side-cleanup","title":"Agent-Side Cleanup","text":"<p>Agents automatically clean up old files to prevent storage accumulation:</p>"},{"location":"admin-guide/operations/data-retention/#retention-policy","title":"Retention Policy","text":"<ul> <li>3-day retention for temporary files</li> <li>Runs every 6 hours automatically</li> <li>Initial cleanup 1 minute after agent startup</li> </ul>"},{"location":"admin-guide/operations/data-retention/#files-cleaned","title":"Files Cleaned","text":"<ul> <li>Hashlists: Removed after 3 days of inactivity</li> <li>Rule Chunks: Temporary rule segments deleted after 3 days</li> <li>Chunk ID Files: Removed when associated chunks are deleted</li> <li>Preserved Files: Base rules, wordlists, and binaries are never auto-deleted</li> </ul>"},{"location":"admin-guide/operations/data-retention/#configuration-via-api","title":"Configuration via API","text":""},{"location":"admin-guide/operations/data-retention/#view-current-settings","title":"View Current Settings","text":"<p><code>GET /api/admin/settings/retention</code></p> <p>Response: <pre><code>{\n  \"default_retention_months\": 36,\n  \"last_purge_run\": \"2025-09-17T10:35:00.391Z\"\n}\n</code></pre></p>"},{"location":"admin-guide/operations/data-retention/#update-retention-settings","title":"Update Retention Settings","text":"<p><code>PUT /api/admin/settings/retention</code></p> <p>Request: <pre><code>{\n  \"default_retention_months\": 24\n}\n</code></pre></p> <p>Requires administrator privileges.</p>"},{"location":"admin-guide/operations/data-retention/#important-considerations","title":"Important Considerations","text":""},{"location":"admin-guide/operations/data-retention/#data-preservation","title":"Data Preservation","text":"<ul> <li>Clients themselves are never deleted by retention policies</li> <li>Hashes shared across multiple hashlists are only deleted when orphaned</li> <li>User accounts and system settings are unaffected</li> <li>Potfile retains ALL plaintext passwords permanently - must be manually managed</li> </ul>"},{"location":"admin-guide/operations/data-retention/#compliance","title":"Compliance","text":"<ul> <li>Retention policies help meet data protection regulations</li> <li>Audit logs provide evidence of proper data disposal</li> <li>Secure deletion prevents unauthorized data recovery</li> <li>NOTE: Potfile retention may conflict with data protection requirements - consider implementing separate potfile cleanup procedures</li> </ul>"},{"location":"admin-guide/operations/data-retention/#performance-impact","title":"Performance Impact","text":"<ul> <li>VACUUM operations may temporarily impact database performance</li> <li>Purge operations are logged but don't block normal operations</li> <li>Large deletions are handled in batches to minimize impact</li> </ul>"},{"location":"admin-guide/operations/data-retention/#potfile-management","title":"Potfile Management","text":"<p>Security Critical</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) contains plaintext passwords from ALL cracked hashes and is NOT managed by the retention system. This has important implications:</p> <ol> <li>Privacy Risk: Passwords from deleted hashlists remain in the potfile</li> <li>Compliance Issue: May violate data protection regulations requiring complete deletion</li> <li>Manual Management Required: You must implement separate procedures to clean the potfile</li> </ol> <p>Recommended Actions: - Implement a separate potfile cleanup script that removes entries for deleted hashlists - Consider rotating or archiving the potfile periodically - Document potfile management procedures for compliance audits - Restrict access to the potfile to authorized personnel only</p>"},{"location":"admin-guide/operations/data-retention/#monitoring","title":"Monitoring","text":"<p>Check retention activity in the backend logs: <pre><code>docker exec krakenhashes-app tail -f /var/log/krakenhashes/backend/backend.log | grep -i purge\n</code></pre></p> <p>View last purge run time: <pre><code>SELECT value FROM client_settings WHERE key = 'last_purge_run';\n</code></pre></p>"},{"location":"admin-guide/operations/job-settings/","title":"Job Execution Settings","text":""},{"location":"admin-guide/operations/job-settings/#overview","title":"Overview","text":"<p>The Job Execution Settings page allows administrators to configure how KrakenHashes executes and distributes password cracking jobs across agents. These settings control chunking behavior, agent coordination, job control, and rule splitting strategies.</p>"},{"location":"admin-guide/operations/job-settings/#accessing-job-execution-settings","title":"Accessing Job Execution Settings","text":"<ol> <li>Navigate to the Admin Panel</li> <li>Click on Settings in the navigation menu</li> <li>Select Job Execution Settings</li> </ol> <p>The settings are organized into four main categories for easier management.</p>"},{"location":"admin-guide/operations/job-settings/#settings-categories","title":"Settings Categories","text":""},{"location":"admin-guide/operations/job-settings/#job-chunking","title":"Job Chunking","text":"<p>Job chunking divides large password cracking tasks into smaller, manageable pieces that can be distributed across multiple agents. This improves resource utilization and allows for better job scheduling.</p> Setting Description Default Range Notes Default Chunk Duration How long each job chunk should run 15 minutes 1+ minutes Shorter chunks provide more flexibility but increase overhead Chunk Fluctuation Percentage Allowed variance for the final chunk 10% 0-100% Prevents creating very small final chunks"},{"location":"admin-guide/operations/job-settings/#best-practices-for-chunking","title":"Best Practices for Chunking","text":"<ul> <li>Short jobs (&lt; 1 hour): Use 5-10 minute chunks for better distribution</li> <li>Long jobs (&gt; 24 hours): Use 30-60 minute chunks to reduce overhead</li> <li>Mixed agent speeds: Shorter chunks help balance workload</li> </ul>"},{"location":"admin-guide/operations/job-settings/#agent-configuration","title":"Agent Configuration","text":"<p>These settings control how agents behave and interact with the backend server.</p> Setting Description Default Range Notes Hashlist Retention How long agents keep hashlists after job completion 7 days 1+ days Reduces re-download for recurring jobs Max Concurrent Jobs per Agent Maximum jobs an agent can run simultaneously 1 1-10 Higher values for powerful multi-GPU systems Progress Reporting Interval How often agents send progress updates 30 seconds 1+ seconds Lower values increase server load Benchmark Cache Duration How long to cache agent performance benchmarks 30 days 1+ days Reduces benchmark frequency Speedtest Timeout Maximum time to wait for speedtest completion 30 seconds 60-600 seconds Increase for slower systems Reconnect Grace Period Time to wait for agents to reconnect after server restart 5 minutes 1-60 minutes Prevents unnecessary task reassignment"},{"location":"admin-guide/operations/job-settings/#reconnect-grace-period-details","title":"Reconnect Grace Period Details","text":"<p>The Reconnect Grace Period is a critical setting for maintaining job continuity during server maintenance or unexpected restarts:</p> <ul> <li>Purpose: Allows agents with running tasks to reconnect and continue their work without losing progress</li> <li>How it works: </li> <li>When the backend restarts, tasks transition to <code>reconnect_pending</code> state</li> <li>Agents cache crack data locally and continue processing</li> <li>Upon reconnection, agents report their current task status</li> <li>If reconnected within the grace period, tasks resume automatically</li> <li>Recommended values:</li> <li>5 minutes (default): Good for most environments</li> <li>10-15 minutes: For environments with slower network recovery</li> <li>1-3 minutes: For highly available setups with quick recovery</li> </ul>"},{"location":"admin-guide/operations/job-settings/#job-control","title":"Job Control","text":"<p>Control job execution behavior and user interface settings.</p> Setting Description Default Range Notes Allow Job Interruption Higher priority jobs can interrupt running jobs Enabled On/Off Ensures critical jobs run immediately Agent Overflow Allocation Mode How to distribute extra agents when jobs hit max_agents limit <code>fifo</code> <code>fifo</code>, <code>round_robin</code> Controls fairness vs speed tradeoff Real-time Crack Notifications Send notifications when hashes are cracked Enabled On/Off Can increase server load for large jobs Job Refresh Interval How often the UI refreshes job status 5 seconds 1-60 seconds Lower values increase server load Max Chunk Retry Attempts Number of times to retry failed chunks 3 0-10 Set to 0 to disable retries Jobs Per Page Default pagination size for job lists 25 5-100 Adjust based on UI preferences Hashlist Bulk Batch Size Number of hashes processed per batch during import 100,000 1,000-1,000,000 Affects memory usage and import speed"},{"location":"admin-guide/operations/job-settings/#job-interruption-behavior","title":"Job Interruption Behavior","text":"<p>When enabled, the system will: 1. Pause lower priority jobs when higher priority jobs arrive 2. Save the state of interrupted jobs 3. Resume interrupted jobs once higher priority jobs complete 4. Maintain crack progress for all interrupted jobs</p>"},{"location":"admin-guide/operations/job-settings/#agent-overflow-allocation-mode","title":"Agent Overflow Allocation Mode","text":"<p>This setting controls how \"overflow\" agents (agents beyond max_agents limits) are distributed among jobs at the same priority level.</p> <p>Important: This setting only applies to overflow agents when multiple jobs exist at the same priority. Higher priority jobs always override max_agents limits and take ALL available agents.</p> <p>Available Modes:</p>"},{"location":"admin-guide/operations/job-settings/#fifo-mode-default","title":"FIFO Mode (Default)","text":"<p>Behavior: Oldest job gets all overflow agents</p> <pre><code>-- Set FIFO mode\nUPDATE system_settings\nSET value = 'fifo'\nWHERE key = 'agent_overflow_allocation_mode';\n</code></pre> <p>Use Cases: - Default mode: Best for most scenarios - Fairness: Jobs get their turn in creation order - Completion focus: Concentrates resources to finish jobs faster - Simple behavior: Predictable allocation pattern</p> <p>Example: <pre><code>3 jobs at priority 50, each with max_agents = 2\n15 available agents total\n\nJob A (created first):  2 agents (max_agents)\nJob B (created second): 2 agents (max_agents)\nJob C (created third):  2 agents (max_agents)\nOverflow:              9 agents \u2192 ALL go to Job A (oldest)\n\nFinal: Job A = 11 agents, Job B = 2 agents, Job C = 2 agents\n</code></pre></p>"},{"location":"admin-guide/operations/job-settings/#round-robin-mode","title":"Round-Robin Mode","text":"<p>Behavior: Distribute overflow agents evenly across all jobs at same priority</p> <pre><code>-- Set round-robin mode\nUPDATE system_settings\nSET value = 'round_robin'\nWHERE key = 'agent_overflow_allocation_mode';\n</code></pre> <p>Use Cases: - Parallel progress: Want all jobs to progress simultaneously - Testing: Running multiple test jobs at once - Even distribution: Prefer balanced allocation over speed - Multiple clients: Each job from different client, want fairness</p> <p>Example: <pre><code>3 jobs at priority 50, each with max_agents = 2\n15 available agents total\n\nJob A (created first):  2 agents (max_agents)\nJob B (created second): 2 agents (max_agents)\nJob C (created third):  2 agents (max_agents)\nOverflow:              9 agents \u2192 distributed evenly (3 each)\n\nFinal: Job A = 5 agents, Job B = 5 agents, Job C = 5 agents\n</code></pre></p> <p>Allocation Logic: <pre><code>1. Calculate base allocation: 9 overflow / 3 jobs = 3 agents per job\n2. Calculate remainder: 9 % 3 = 0 (no remainder)\n3. Distribute base to all jobs: +3 each\n4. If remainder exists, give to oldest jobs first\n</code></pre></p>"},{"location":"admin-guide/operations/job-settings/#priority-based-behavior","title":"Priority-Based Behavior","text":"<p>Critical: The overflow allocation mode does not affect priority-based allocation:</p> <pre><code>Scenario: 2 jobs, 10 agents available\n\nJob A: Priority 100, max_agents = 3\nJob B: Priority 50,  max_agents = 5\n\nResult (regardless of overflow mode):\n- Job A gets ALL 10 agents (higher priority overrides max_agents)\n- Job B gets 0 agents (lower priority, waits for Job A to finish)\n</code></pre>"},{"location":"admin-guide/operations/job-settings/#configuration-via-sql","title":"Configuration via SQL","text":"<pre><code>-- View current setting\nSELECT key, value, description\nFROM system_settings\nWHERE key = 'agent_overflow_allocation_mode';\n\n-- Change to FIFO mode (default)\nUPDATE system_settings\nSET value = 'fifo'\nWHERE key = 'agent_overflow_allocation_mode';\n\n-- Change to round-robin mode\nUPDATE system_settings\nSET value = 'round_robin'\nWHERE key = 'agent_overflow_allocation_mode';\n</code></pre>"},{"location":"admin-guide/operations/job-settings/#when-to-use-each-mode","title":"When to Use Each Mode","text":"Scenario Recommended Mode Reason Production cracking (default) FIFO Finish jobs faster by concentrating resources Multiple test jobs Round-robin See results from all tests simultaneously Multi-client environment Round-robin Fair distribution across clients Single large job Either No difference (only one job) Time-critical job FIFO Ensures oldest/most important finishes first Parallel research Round-robin Compare multiple approaches simultaneously"},{"location":"admin-guide/operations/job-settings/#hashlist-bulk-batch-size","title":"Hashlist Bulk Batch Size","text":"<p>This setting controls how many hashes are processed in each database batch during hashlist imports and bulk operations. It directly affects memory usage and import performance.</p> <p>How It Works: 1. When importing a large hashlist (e.g., 10 million hashes), the system divides the work into batches 2. Each batch processes up to <code>hashlist_bulk_batch_size</code> hashes at once 3. Larger batches = faster imports but higher memory usage 4. Smaller batches = lower memory usage but slower imports</p> <p>Recommended Values:</p> Environment Batch Size Rationale Low memory (&lt; 4GB RAM) 10,000-25,000 Minimizes memory pressure Standard (4-16GB RAM) 50,000-100,000 Balanced performance High memory (16GB+ RAM) 100,000-500,000 Maximizes import speed Very large hashlists (50M+) 100,000 Prevents memory exhaustion <p>Configuration via SQL:</p> <pre><code>-- View current setting\nSELECT key, value, description\nFROM system_settings\nWHERE key = 'hashlist_bulk_batch_size';\n\n-- Set batch size (example: 50,000 for memory-constrained systems)\nUPDATE system_settings\nSET value = '50000'\nWHERE key = 'hashlist_bulk_batch_size';\n\n-- Set batch size (example: 200,000 for high-memory systems)\nUPDATE system_settings\nSET value = '200000'\nWHERE key = 'hashlist_bulk_batch_size';\n</code></pre> <p>Performance Impact: - Import Time: Doubling batch size typically reduces import time by 20-30% - Memory Usage: Roughly linear with batch size (~10MB per 100,000 hashes) - Database Load: Larger batches create fewer but larger transactions</p>"},{"location":"admin-guide/operations/job-settings/#rule-splitting","title":"Rule Splitting","text":"<p>Rule splitting automatically divides large rule files to improve distribution across agents. This is especially useful for rule files that would otherwise exceed the chunk duration.</p> Setting Description Default Range Notes Enable Rule Splitting Automatically split large rule files Enabled On/Off Improves distribution for large rule sets Rule Split Threshold Split when estimated time exceeds chunk duration by this factor 2.0\u00d7 1.1-10\u00d7 Lower values create more chunks Minimum Rules to Split Don't split files with fewer rules than this 100 10+ Prevents splitting small files Maximum Rule Chunks Maximum chunks to create per rule file 100 2-10000 Limits memory usage Rule Chunk Directory Directory for temporary rule chunks <code>/tmp/rule_chunks</code> Any valid path Must be writable by backend"},{"location":"admin-guide/operations/job-settings/#rule-splitting-algorithm","title":"Rule Splitting Algorithm","text":"<p>The system automatically: 1. Estimates job duration based on hashlist size and rule count 2. Compares estimated duration to chunk duration \u00d7 threshold 3. If exceeding threshold, splits rules into appropriate chunks 4. Distributes chunks across available agents 5. Cleans up temporary chunks after job completion</p>"},{"location":"admin-guide/operations/job-settings/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/operations/job-settings/#network-load","title":"Network Load","text":"<ul> <li>Progress Reporting Interval: Each update creates network traffic</li> <li>Job Refresh Interval: Affects UI responsiveness and server load</li> <li>Calculate: <code>(Number of Agents \u00d7 Active Jobs) / Reporting Interval = Updates per second</code></li> </ul>"},{"location":"admin-guide/operations/job-settings/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>Hashlist Retention: <code>Average Hashlist Size \u00d7 Number of Unique Jobs \u00d7 Retention Days</code></li> <li>Rule Chunks: <code>Original Rule File Size \u00d7 Active Jobs using that rule</code></li> <li>Benchmark Cache: Minimal, typically &lt; 1MB per agent</li> </ul>"},{"location":"admin-guide/operations/job-settings/#optimal-settings-by-environment","title":"Optimal Settings by Environment","text":""},{"location":"admin-guide/operations/job-settings/#small-environment-1-5-agents","title":"Small Environment (1-5 agents)","text":"<ul> <li>Chunk Duration: 10-15 minutes</li> <li>Progress Interval: 30 seconds</li> <li>Max Concurrent Jobs: 1</li> <li>Grace Period: 5 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#medium-environment-5-20-agents","title":"Medium Environment (5-20 agents)","text":"<ul> <li>Chunk Duration: 15-30 minutes</li> <li>Progress Interval: 60 seconds</li> <li>Max Concurrent Jobs: 1-2</li> <li>Grace Period: 10 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#large-environment-20-agents","title":"Large Environment (20+ agents)","text":"<ul> <li>Chunk Duration: 30-60 minutes</li> <li>Progress Interval: 120 seconds</li> <li>Max Concurrent Jobs: 2-3</li> <li>Grace Period: 15 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/job-settings/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/operations/job-settings/#agents-not-receiving-jobs","title":"Agents Not Receiving Jobs","text":"<ul> <li>Check Max Concurrent Jobs per Agent setting</li> <li>Verify agents are not at capacity</li> <li>Review job priority settings</li> </ul>"},{"location":"admin-guide/operations/job-settings/#poor-job-distribution","title":"Poor Job Distribution","text":"<ul> <li>Reduce Default Chunk Duration for better granularity</li> <li>Enable Rule Splitting for large rule files</li> <li>Adjust Chunk Fluctuation Percentage to avoid tiny chunks</li> </ul>"},{"location":"admin-guide/operations/job-settings/#high-server-load","title":"High Server Load","text":"<ul> <li>Increase Progress Reporting Interval</li> <li>Increase Job Refresh Interval</li> <li>Disable Real-time Crack Notifications for large jobs</li> </ul>"},{"location":"admin-guide/operations/job-settings/#lost-progress-after-server-restart","title":"Lost Progress After Server Restart","text":"<ul> <li>Increase Reconnect Grace Period</li> <li>Ensure agents have stable network connections</li> <li>Check agent logs for reconnection issues</li> </ul>"},{"location":"admin-guide/operations/job-settings/#monitoring-settings-impact","title":"Monitoring Settings Impact","text":"<p>Use the following metrics to evaluate settings effectiveness: - Average chunk completion time vs. configured duration - Number of retry attempts per job - Agent utilization percentage - Task reassignment frequency after restarts</p>"},{"location":"admin-guide/operations/job-settings/#related-documentation","title":"Related Documentation","text":"<ul> <li>Agent Management - Managing and monitoring agents</li> <li>Job Chunking - Detailed chunking strategies</li> <li>Performance Tuning - System optimization</li> <li>Rule Management - Managing rule files</li> </ul>"},{"location":"admin-guide/operations/monitoring/","title":"System Monitoring Guide","text":"<p>This guide covers comprehensive monitoring strategies for KrakenHashes, including system health indicators, performance metrics, logging, and alerting configurations.</p>"},{"location":"admin-guide/operations/monitoring/#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Health Indicators</li> <li>Job Monitoring and Statistics</li> <li>Agent Performance Metrics</li> <li>Database Monitoring</li> <li>Log Analysis and Alerting</li> <li>Performance Baselines</li> <li>Monitoring Dashboards and Tools</li> </ol>"},{"location":"admin-guide/operations/monitoring/#system-health-indicators","title":"System Health Indicators","text":""},{"location":"admin-guide/operations/monitoring/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>The system provides a basic health check endpoint for monitoring service availability:</p> <pre><code># Check system health\ncurl https://localhost:31337/api/health\n\n# Expected response\n200 OK\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#service-status-monitoring","title":"Service Status Monitoring","text":"<p>Monitor the following key services:</p> <ol> <li>Backend API Service</li> <li>Port: 31337 (HTTPS), 1337 (HTTP for CA cert)</li> <li>Health endpoint: <code>/api/health</code></li> <li> <p>Version endpoint: <code>/api/version</code></p> </li> <li> <p>PostgreSQL Database</p> </li> <li>Port: 5432</li> <li>Connection pool status</li> <li> <p>Active connections</p> </li> <li> <p>WebSocket Service</p> </li> <li>Agent connections</li> <li>Heartbeat status</li> <li>Connection count</li> </ol>"},{"location":"admin-guide/operations/monitoring/#docker-container-health","title":"Docker Container Health","text":"<p>Monitor container status using Docker commands:</p> <pre><code># Check container status\ndocker-compose ps\n\n# Monitor resource usage\ndocker stats\n\n# Check container logs\ndocker-compose logs -f backend\ndocker-compose logs -f postgres\ndocker-compose logs -f app\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#job-monitoring-and-statistics","title":"Job Monitoring and Statistics","text":""},{"location":"admin-guide/operations/monitoring/#job-execution-metrics","title":"Job Execution Metrics","text":"<p>The system tracks comprehensive job execution metrics:</p> <ol> <li>Job Status Distribution</li> <li>Pending jobs count</li> <li>Running jobs count</li> <li>Completed jobs count</li> <li>Failed jobs count</li> <li> <p>Cancelled jobs count</p> </li> <li> <p>Job Performance Indicators</p> </li> <li>Average job completion time</li> <li>Job success rate</li> <li>Hash cracking rate</li> <li>Keyspace coverage</li> </ol>"},{"location":"admin-guide/operations/monitoring/#job-monitoring-endpoints","title":"Job Monitoring Endpoints","text":"<pre><code># List all jobs with pagination\nGET /api/jobs?page=1&amp;page_size=20\n\n# Get specific job details\nGET /api/jobs/{job_id}\n\n# Get job statistics\nGET /api/jobs/stats\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#job-progress-tracking","title":"Job Progress Tracking","text":"<p>Monitor job progress through these metrics:</p> <ul> <li>Dispatched Percentage: Portion of keyspace distributed to agents</li> <li>Searched Percentage: Portion of keyspace actually processed</li> <li>Overall Progress: Combined metric considering rule splitting</li> <li>Cracked Count: Number of successfully cracked hashes</li> <li>Total Speed: Combined hash rate across all agents</li> </ul>"},{"location":"admin-guide/operations/monitoring/#enhanced-job-details-view","title":"Enhanced Job Details View","text":"<p>The Job Details page now includes a comprehensive completed tasks history that provides administrators with:</p>"},{"location":"admin-guide/operations/monitoring/#completed-tasks-monitoring","title":"Completed Tasks Monitoring","text":"<ul> <li>Historical Performance Data: View hash rates achieved by each agent for completed tasks</li> <li>Task Completion Patterns: Analyze when and how tasks were completed across the job lifecycle</li> <li>Keyspace Analysis: Identify which keyspace ranges were most productive for password cracking</li> <li>Audit Trail: Maintain a complete record of job execution across distributed agents</li> </ul>"},{"location":"admin-guide/operations/monitoring/#available-metrics","title":"Available Metrics","text":"<p>For each completed task, administrators can review: - Agent assignment and performance metrics - Exact keyspace ranges processed - Completion timestamps for timeline analysis - Number of cracks found per task - Average hash rate achieved during execution</p>"},{"location":"admin-guide/operations/monitoring/#use-cases-for-administrators","title":"Use Cases for Administrators","text":"<ul> <li>Performance Optimization: Compare agent efficiency across tasks to identify hardware or configuration issues</li> <li>Resource Planning: Analyze task distribution patterns to optimize future job configurations</li> <li>Troubleshooting: Identify if specific agents or keyspace ranges consistently underperform</li> <li>Compliance Reporting: Maintain detailed execution records for security audits</li> </ul> <p>Data Retention</p> <p>Completed tasks are retained even after job completion, providing long-term visibility into job execution history. This data persists according to your configured retention policies.</p>"},{"location":"admin-guide/operations/monitoring/#database-queries-for-job-monitoring","title":"Database Queries for Job Monitoring","text":"<pre><code>-- Active jobs by status\nSELECT status, COUNT(*) as count \nFROM job_executions \nGROUP BY status;\n\n-- Jobs with high failure rate\nSELECT je.id, je.name, je.error_message,\n       COUNT(jt.id) as total_tasks,\n       SUM(CASE WHEN jt.status = 'failed' THEN 1 ELSE 0 END) as failed_tasks\nFROM job_executions je\nJOIN job_tasks jt ON je.id = jt.job_execution_id\nWHERE je.status = 'failed'\nGROUP BY je.id, je.name, je.error_message\nHAVING SUM(CASE WHEN jt.status = 'failed' THEN 1 ELSE 0 END) &gt; 0;\n\n-- Job performance over time\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    COUNT(*) as jobs_created,\n    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration_seconds\nFROM job_executions\nWHERE completed_at IS NOT NULL\nGROUP BY hour\nORDER BY hour DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#agent-performance-metrics","title":"Agent Performance Metrics","text":""},{"location":"admin-guide/operations/monitoring/#agent-metrics-collection","title":"Agent Metrics Collection","text":"<p>The agent collects and reports the following metrics:</p> <ol> <li>System Metrics</li> <li>CPU usage percentage</li> <li>Memory usage percentage</li> <li>GPU utilization</li> <li>GPU temperature</li> <li> <p>GPU memory usage</p> </li> <li> <p>Performance Metrics</p> </li> <li>Hash rate per device</li> <li>Power usage</li> </ol> <p> Device monitoring dashboard showing real-time temperature, utilization, fan speed, and hash rate metrics across multiple agents</p> <p> Alternative view of the device monitoring interface with detailed performance graphs and timeline data    - Fan speed    - Device temperature</p>"},{"location":"admin-guide/operations/monitoring/#agent-monitoring-endpoints","title":"Agent Monitoring Endpoints","text":"<pre><code># List all agents\nGET /api/admin/agents\n\n# Get agent details with devices\nGET /api/admin/agents/{agent_id}\n\n# Get agent performance metrics\nGET /api/admin/agents/{agent_id}/metrics?timeRange=1h&amp;metrics=temperature,utilization,fanspeed,hashrate\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#agent-health-monitoring","title":"Agent Health Monitoring","text":"<p>Monitor agent health through:</p> <ol> <li>Heartbeat Status</li> <li>Last heartbeat timestamp</li> <li>Connection status (active/inactive)</li> <li> <p>Heartbeat interval (30 seconds default)</p> </li> <li> <p>Error Tracking</p> </li> <li>Last error message</li> <li>Error frequency</li> <li>Recovery status</li> </ol>"},{"location":"admin-guide/operations/monitoring/#job-scheduling-robustness-v121","title":"Job Scheduling Robustness (v1.2.1+)","text":""},{"location":"admin-guide/operations/monitoring/#agent-crash-recovery","title":"Agent Crash Recovery","text":"<p>Starting with version 1.2.1, the system includes automatic recovery from agent crashes with improved keyspace accounting:</p> <p>Problem Solved: When an agent crashes while executing a task, the system now properly returns the unfinished work back to the pool for reassignment.</p> <p>How It Works: 1. Agent crashes with task assigned (e.g., keyspace 0-100,000) 2. Backend detects disconnect and resets task to \"pending\" 3. NEW: <code>dispatched_keyspace</code> is decremented by the task's chunk size 4. Scheduler can now reassign the work to another agent 5. No manual intervention required</p> <p>Monitoring Recovery Operations:</p> <pre><code>-- Check for jobs with recently reset tasks\nSELECT\n    je.id as job_id,\n    je.name as job_name,\n    je.status,\n    je.dispatched_keyspace,\n    je.total_keyspace,\n    COUNT(CASE WHEN jt.status = 'pending'\n               AND jt.keyspace_start IS NOT NULL\n          THEN 1 END) as reset_tasks,\n    COUNT(jt.id) as total_tasks\nFROM job_executions je\nJOIN job_tasks jt ON je.id = jt.job_execution_id\nWHERE je.status IN ('running', 'pending')\nGROUP BY je.id, je.name, je.status, je.dispatched_keyspace, je.total_keyspace\nHAVING COUNT(CASE WHEN jt.status = 'pending'\n                   AND jt.keyspace_start IS NOT NULL\n              THEN 1 END) &gt; 0;\n</code></pre> <p>Expected Results: - Reset tasks should be reassigned within next scheduling cycle - <code>dispatched_keyspace</code> should be less than <code>total_keyspace</code> when reset tasks exist - No jobs should remain stuck with pending work that can't be assigned</p> <p>Validation Query: <pre><code>-- Verify keyspace accounting is correct\nSELECT\n    je.id,\n    je.name,\n    je.total_keyspace,\n    je.dispatched_keyspace,\n    COALESCE(SUM(CASE WHEN jt.status != 'pending'\n                      THEN jt.keyspace_end - jt.keyspace_start\n                      ELSE 0 END), 0) as actual_dispatched,\n    (je.dispatched_keyspace - COALESCE(SUM(CASE WHEN jt.status != 'pending'\n                                                  THEN jt.keyspace_end - jt.keyspace_start\n                                                  ELSE 0 END), 0)) as keyspace_mismatch\nFROM job_executions je\nLEFT JOIN job_tasks jt ON je.id = jt.job_execution_id\nWHERE je.status IN ('running', 'pending')\nGROUP BY je.id, je.name, je.total_keyspace, je.dispatched_keyspace\nHAVING ABS(je.dispatched_keyspace - COALESCE(SUM(CASE WHEN jt.status != 'pending'\n                                                       THEN jt.keyspace_end - jt.keyspace_start\n                                                       ELSE 0 END), 0)) &gt; 1000;\n</code></pre></p> <p>Interpretation: - <code>keyspace_mismatch</code> should be near zero (within 1000 for rounding) - Large mismatches indicate potential accounting issues - Empty results = healthy keyspace tracking</p>"},{"location":"admin-guide/operations/monitoring/#agent-crash-patterns","title":"Agent Crash Patterns","text":"<p>Track agent reliability:</p> <pre><code>-- Agents with frequent task resets (potential stability issues)\nSELECT\n    a.id,\n    a.name,\n    COUNT(CASE WHEN jt.retry_count &gt; 0 THEN 1 END) as reset_count,\n    COUNT(jt.id) as total_tasks,\n    (COUNT(CASE WHEN jt.retry_count &gt; 0 THEN 1 END)::float /\n     NULLIF(COUNT(jt.id), 0) * 100) as reset_percentage\nFROM agents a\nLEFT JOIN job_tasks jt ON a.id = jt.agent_id\nWHERE jt.created_at &gt; NOW() - INTERVAL '7 days'\nGROUP BY a.id, a.name\nHAVING COUNT(CASE WHEN jt.retry_count &gt; 0 THEN 1 END) &gt; 5\nORDER BY reset_percentage DESC;\n</code></pre> <p>Action Items: - Agents with &gt;10% reset rate may have stability issues - Check agent logs for crashes or disconnections - Investigate hardware problems (overheating, power issues) - Consider updating agent software if outdated</p>"},{"location":"admin-guide/operations/monitoring/#database-queries-for-agent-monitoring","title":"Database Queries for Agent Monitoring","text":"<pre><code>-- Agents with stale heartbeats\nSELECT id, name, last_heartbeat, status\nFROM agents\nWHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes'\n  AND status = 'active';\n\n-- Agent performance metrics\nSELECT \n    a.name as agent_name,\n    apm.metric_type,\n    AVG(apm.value) as avg_value,\n    MAX(apm.value) as max_value,\n    MIN(apm.value) as min_value\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.timestamp &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.name, apm.metric_type;\n\n-- GPU device utilization\nSELECT \n    a.name as agent_name,\n    apm.device_name,\n    apm.metric_type,\n    AVG(apm.value) as avg_utilization\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.metric_type = 'utilization'\n  AND apm.timestamp &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.name, apm.device_name, apm.metric_type\nORDER BY avg_utilization DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#database-monitoring","title":"Database Monitoring","text":""},{"location":"admin-guide/operations/monitoring/#connection-pool-monitoring","title":"Connection Pool Monitoring","text":"<p>Monitor database connection health:</p> <pre><code>-- Active connections by state\nSELECT state, COUNT(*) \nFROM pg_stat_activity \nGROUP BY state;\n\n-- Long-running queries\nSELECT \n    pid,\n    now() - pg_stat_activity.query_start AS duration,\n    query,\n    state\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) &gt; interval '5 minutes';\n\n-- Database size growth\nSELECT \n    pg_database.datname,\n    pg_size_pretty(pg_database_size(pg_database.datname)) AS size\nFROM pg_database\nORDER BY pg_database_size(pg_database.datname) DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#table-statistics","title":"Table Statistics","text":"<p>Monitor table growth and performance:</p> <pre><code>-- Table sizes\nSELECT\n    schemaname AS table_schema,\n    tablename AS table_name,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS data_size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Index usage\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#performance-metrics-tables","title":"Performance Metrics Tables","text":"<p>The system maintains dedicated tables for performance tracking:</p> <ol> <li>agent_metrics - Real-time agent system metrics</li> <li>agent_performance_metrics - Detailed performance data with aggregation</li> <li>job_performance_metrics - Job execution performance tracking</li> <li>agent_benchmarks - Hashcat benchmark results per agent</li> </ol>"},{"location":"admin-guide/operations/monitoring/#log-analysis-and-alerting","title":"Log Analysis and Alerting","text":""},{"location":"admin-guide/operations/monitoring/#log-configuration","title":"Log Configuration","text":"<p>Configure logging through environment variables:</p> <pre><code># Enable debug logging\nexport DEBUG=true\n\n# Set log level (DEBUG, INFO, WARNING, ERROR)\nexport LOG_LEVEL=INFO\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#log-locations","title":"Log Locations","text":"<p>When running with Docker, logs are stored in:</p> <pre><code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/\n\u251c\u2500\u2500 backend/      # Backend application logs\n\u251c\u2500\u2500 postgres/     # PostgreSQL logs\n\u2514\u2500\u2500 nginx/        # Nginx/frontend logs\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#log-format","title":"Log Format","text":"<p>The system uses structured logging with the following format: <pre><code>[LEVEL] [TIMESTAMP] [FILE:LINE] [FUNCTION] MESSAGE\n</code></pre></p> <p>Example: <pre><code>[INFO] [2025-08-01 15:04:05.000] [/path/to/file.go:42] [FunctionName] Processing job execution\n</code></pre></p>"},{"location":"admin-guide/operations/monitoring/#key-log-patterns-to-monitor","title":"Key Log Patterns to Monitor","text":"<ol> <li> <p>Error Patterns <pre><code># Find all errors across logs\ngrep -i \"error\" /home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/*/*.log\n\n# Find database connection errors\ngrep -i \"database.*error\\|connection.*failed\" logs/backend/*.log\n\n# Find agent disconnections\ngrep -i \"agent.*disconnect\\|websocket.*close\" logs/backend/*.log\n</code></pre></p> </li> <li> <p>Performance Issues <pre><code># Find slow queries\ngrep -i \"slow query\\|query took\" logs/backend/*.log\n\n# Find memory issues\ngrep -i \"out of memory\\|memory.*limit\" logs/*/*.log\n</code></pre></p> </li> <li> <p>Security Events <pre><code># Find authentication failures\ngrep -i \"auth.*fail\\|login.*fail\\|unauthorized\" logs/backend/*.log\n\n# Find suspicious activity\ngrep -i \"invalid.*token\\|forbidden\\|suspicious\" logs/backend/*.log\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/monitoring/#alert-configuration","title":"Alert Configuration","text":"<p>Set up alerts for critical events:</p> <ol> <li>System Health Alerts</li> <li>Service down (health check fails)</li> <li>Database connection pool exhausted</li> <li> <p>High error rate (&gt;5% of requests)</p> </li> <li> <p>Performance Alerts</p> </li> <li>CPU usage &gt; 90% for 5 minutes</li> <li>Memory usage &gt; 85%</li> <li>Database query time &gt; 5 seconds</li> <li> <p>Job queue backlog &gt; 100 jobs</p> </li> <li> <p>Security Alerts</p> </li> <li>Multiple failed login attempts</li> <li>Unauthorized API access attempts</li> <li>Agent registration anomalies</li> </ol>"},{"location":"admin-guide/operations/monitoring/#performance-baselines","title":"Performance Baselines","text":""},{"location":"admin-guide/operations/monitoring/#establishing-baselines","title":"Establishing Baselines","text":"<p>Monitor and document normal operating parameters:</p> <ol> <li>System Resource Baselines</li> <li>Normal CPU usage: 20-40% (idle), 60-80% (active jobs)</li> <li>Memory usage: 2-4GB (base), +1GB per 1M hashes</li> <li> <p>Database connections: 10-20 (normal load)</p> </li> <li> <p>Job Performance Baselines</p> </li> <li>Job creation rate: 10-50 jobs/hour</li> <li>Average job duration: Varies by attack type</li> <li> <p>Hash processing rate: Device-dependent</p> </li> <li> <p>Agent Performance Baselines</p> </li> <li>Heartbeat interval: 30 seconds</li> <li>Benchmark cache duration: 24 hours</li> <li>GPU utilization: 90-100% during jobs</li> </ol>"},{"location":"admin-guide/operations/monitoring/#benchmark-tracking","title":"Benchmark Tracking","text":"<p>The system automatically tracks agent benchmarks:</p> <pre><code>-- View agent benchmarks\nSELECT \n    a.name as agent_name,\n    ab.attack_mode,\n    ab.hash_type,\n    ab.speed,\n    ab.updated_at\nFROM agents a\nJOIN agent_benchmarks ab ON a.id = ab.agent_id\nORDER BY a.name, ab.attack_mode, ab.hash_type;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#performance-degradation-detection","title":"Performance Degradation Detection","text":"<p>Monitor for performance degradation:</p> <pre><code>-- Compare current vs historical performance\nWITH current_metrics AS (\n    SELECT \n        agent_id,\n        AVG(value) as current_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hash_rate'\n      AND timestamp &gt; NOW() - INTERVAL '1 hour'\n    GROUP BY agent_id\n),\nhistorical_metrics AS (\n    SELECT \n        agent_id,\n        AVG(value) as historical_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hash_rate'\n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '1 day'\n    GROUP BY agent_id\n)\nSELECT \n    a.name,\n    cm.current_avg,\n    hm.historical_avg,\n    ((cm.current_avg - hm.historical_avg) / hm.historical_avg * 100) as percent_change\nFROM agents a\nJOIN current_metrics cm ON a.id = cm.agent_id\nJOIN historical_metrics hm ON a.id = hm.agent_id\nWHERE ABS((cm.current_avg - hm.historical_avg) / hm.historical_avg) &gt; 0.1;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#monitoring-dashboards-and-tools","title":"Monitoring Dashboards and Tools","text":""},{"location":"admin-guide/operations/monitoring/#built-in-monitoring-endpoints","title":"Built-in Monitoring Endpoints","text":"<ol> <li>System Status Dashboard</li> <li>Real-time agent status</li> <li>Active job count</li> <li>System resource usage</li> <li> <p>Recent errors</p> </li> <li> <p>Job Monitoring Dashboard</p> </li> <li>Job queue status</li> <li>Job progress tracking</li> <li>Success/failure rates</li> <li> <p>Performance trends</p> </li> <li> <p>Agent Performance Dashboard</p> </li> <li>Agent availability</li> <li>Device utilization</li> <li>Temperature monitoring</li> <li>Hash rate tracking</li> </ol>"},{"location":"admin-guide/operations/monitoring/#external-monitoring-integration","title":"External Monitoring Integration","text":"<p>The system can be integrated with external monitoring tools:</p> <ol> <li>Prometheus Integration</li> <li>Export metrics via <code>/metrics</code> endpoint (if implemented)</li> <li>Custom metric exporters</li> <li> <p>Alert manager integration</p> </li> <li> <p>Grafana Dashboards</p> </li> <li>PostgreSQL data source</li> <li>Custom dashboard templates</li> <li> <p>Alert visualization</p> </li> <li> <p>Log Aggregation</p> </li> <li>ELK Stack (Elasticsearch, Logstash, Kibana)</li> <li>Fluentd/Fluent Bit</li> <li>Centralized log analysis</li> </ol>"},{"location":"admin-guide/operations/monitoring/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Regular Health Checks</li> <li>Automated health check every 30 seconds</li> <li>Alert on 3 consecutive failures</li> <li> <p>Include dependency checks</p> </li> <li> <p>Capacity Planning</p> </li> <li>Monitor growth trends</li> <li>Plan for peak usage</li> <li> <p>Scale resources proactively</p> </li> <li> <p>Performance Optimization</p> </li> <li>Regular benchmark updates</li> <li>Query optimization based on metrics</li> <li> <p>Resource allocation tuning</p> </li> <li> <p>Security Monitoring</p> </li> <li>Audit log analysis</li> <li>Anomaly detection</li> <li>Access pattern monitoring</li> </ol>"},{"location":"admin-guide/operations/monitoring/#troubleshooting-guide","title":"Troubleshooting Guide","text":"<p>Common issues and monitoring approaches:</p> <ol> <li>High CPU Usage</li> <li>Check active job count</li> <li>Verify agent task distribution</li> <li> <p>Monitor database query performance</p> </li> <li> <p>Memory Leaks</p> </li> <li>Track memory usage over time</li> <li>Identify growing processes</li> <li> <p>Check for unclosed connections</p> </li> <li> <p>Slow Job Processing</p> </li> <li>Verify agent benchmarks</li> <li>Check network latency</li> <li> <p>Monitor file I/O performance</p> </li> <li> <p>Database Performance</p> </li> <li>Analyze slow queries</li> <li>Check index usage</li> <li>Monitor connection pool</li> </ol>"},{"location":"admin-guide/operations/monitoring/#maintenance-and-cleanup","title":"Maintenance and Cleanup","text":""},{"location":"admin-guide/operations/monitoring/#automated-cleanup-services","title":"Automated Cleanup Services","text":"<p>The system includes several cleanup services:</p> <ol> <li>Metrics Cleanup Service</li> <li>Aggregates real-time metrics to daily/weekly</li> <li>Removes old metrics based on retention policy</li> <li> <p>Runs automatically on schedule</p> </li> <li> <p>Agent Cleanup Service</p> </li> <li>Marks stale agents as inactive</li> <li>Cleans up orphaned resources</li> <li> <p>Maintains agent health status</p> </li> <li> <p>Job Cleanup Service</p> </li> <li>Archives completed jobs</li> <li>Removes temporary files</li> <li>Updates job statistics</li> </ol>"},{"location":"admin-guide/operations/monitoring/#manual-maintenance-tasks","title":"Manual Maintenance Tasks","text":"<pre><code># Force cleanup of old metrics\ncurl -X POST https://localhost:31337/api/admin/force-cleanup\n\n# Vacuum database\ndocker exec -it krakenhashes_postgres_1 psql -U postgres -d krakenhashes -c \"VACUUM ANALYZE;\"\n\n# Check database bloat\ndocker exec -it krakenhashes_postgres_1 psql -U postgres -d krakenhashes -c \"\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\"\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#conclusion","title":"Conclusion","text":"<p>Effective monitoring is crucial for maintaining a healthy KrakenHashes deployment. Regular monitoring of system health, job performance, and agent metrics ensures optimal operation and early detection of issues. Implement automated alerting for critical metrics and maintain historical data for trend analysis and capacity planning.</p>"},{"location":"admin-guide/operations/potfile/","title":"Potfile Management","text":"<p>The potfile (short for \"pot file\" from hashcat terminology) is an automated feature in KrakenHashes that accumulates successfully cracked passwords into a specialized wordlist. This dynamic wordlist significantly improves cracking efficiency by trying previously successful passwords against new hashes.</p>"},{"location":"admin-guide/operations/potfile/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>How It Works</li> <li>Configuration Settings</li> <li>File Structure and Location</li> <li>Staging and Processing Mechanism</li> <li>Integration with Jobs</li> <li>Monitoring and Troubleshooting</li> <li>Best Practices</li> </ol>"},{"location":"admin-guide/operations/potfile/#overview","title":"Overview","text":""},{"location":"admin-guide/operations/potfile/#purpose","title":"Purpose","text":"<p>The potfile serves as an organizational memory of all passwords that have been successfully cracked. By maintaining this list, KrakenHashes can:</p> <ul> <li>Accelerate future cracking: Common passwords that worked before are likely to work again</li> <li>Identify password reuse: Quickly detect when the same password is used across different accounts</li> <li>Build organizational intelligence: Accumulate knowledge of password patterns specific to your targets</li> <li>Optimize resource usage: Reduce GPU time by trying known passwords first</li> </ul>"},{"location":"admin-guide/operations/potfile/#key-benefits","title":"Key Benefits","text":"<ol> <li>Automatic Management: No manual intervention required - the system handles everything</li> <li>Real-time Updates: Passwords are staged immediately upon cracking</li> <li>Deduplication: Prevents duplicate entries automatically</li> <li>Distributed Access: All agents receive the updated potfile for use in jobs</li> <li>Performance Optimization: Batch processing minimizes system overhead</li> </ol>"},{"location":"admin-guide/operations/potfile/#how-it-works","title":"How It Works","text":"<p>The potfile system operates through a multi-stage automated process:</p>"},{"location":"admin-guide/operations/potfile/#1-password-crack-detection","title":"1. Password Crack Detection","text":"<p>When an agent successfully cracks a password hash: - The cracked password is sent to the backend server - The backend records the crack in the database - The password is checked against existing potfile entries</p>"},{"location":"admin-guide/operations/potfile/#2-staging-process","title":"2. Staging Process","text":"<p>If the password is new (not already in the potfile): - It's added to the <code>potfile_staging</code> table - The entry includes the plaintext password and original hash value - Multiple passwords can accumulate in staging</p>"},{"location":"admin-guide/operations/potfile/#3-batch-processing","title":"3. Batch Processing","text":"<p>A background worker runs periodically (default: every 60 seconds): - Retrieves all unprocessed entries from staging - Loads the current potfile into memory for deduplication - Filters out any passwords already in the potfile - Appends new unique passwords to the potfile - Deletes processed entries from the staging table</p>"},{"location":"admin-guide/operations/potfile/#4-distribution","title":"4. Distribution","text":"<p>Once updated: - The potfile's MD5 hash is recalculated - Agents are notified of the update - The potfile becomes available for use in cracking jobs</p>"},{"location":"admin-guide/operations/potfile/#configuration-settings","title":"Configuration Settings","text":"<p>The potfile feature is controlled through system settings in the database:</p> Setting Default Description <code>potfile_enabled</code> <code>true</code> Master switch to enable/disable the potfile feature <code>potfile_batch_interval</code> <code>60</code> Seconds between batch processing runs <code>potfile_max_batch_size</code> <code>100000</code> Maximum number of entries to process in a single batch (optimized for high-volume) <code>potfile_wordlist_id</code> (auto) Database ID of the potfile wordlist entry <code>potfile_preset_job_id</code> (auto) Database ID of the associated preset job"},{"location":"admin-guide/operations/potfile/#modifying-settings","title":"Modifying Settings","text":"<p>Settings can be modified through direct database updates:</p> <pre><code>-- Change batch interval to 30 seconds\nUPDATE system_settings \nSET value = '30' \nWHERE key = 'potfile_batch_interval';\n\n-- Adjust max batch size (default is already optimized at 100k for high-volume)\nUPDATE system_settings\nSET value = '100000'\nWHERE key = 'potfile_max_batch_size';\n\n-- Disable potfile feature\nUPDATE system_settings \nSET value = 'false' \nWHERE key = 'potfile_enabled';\n</code></pre> <p>Note: Changes to settings require a server restart to take effect.</p>"},{"location":"admin-guide/operations/potfile/#file-structure-and-location","title":"File Structure and Location","text":""},{"location":"admin-guide/operations/potfile/#file-location","title":"File Location","text":"<pre><code>&lt;data_dir&gt;/wordlists/custom/potfile.txt\n</code></pre> <p>Typically: <pre><code>/data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre></p>"},{"location":"admin-guide/operations/potfile/#file-format","title":"File Format","text":"<ul> <li>Plain text file: One password per line</li> <li>Initial content: Starts with a single blank line (representing an empty password)</li> <li>Encoding: UTF-8</li> <li>Line endings: Unix-style (LF)</li> </ul>"},{"location":"admin-guide/operations/potfile/#example-content","title":"Example Content","text":"<pre><code>(blank line)\npassword123\nAdmin@1234\nWelcome2024!\nCompanyName123\n</code></pre>"},{"location":"admin-guide/operations/potfile/#automatic-creation","title":"Automatic Creation","text":"<ul> <li>Created automatically on first server startup</li> <li>Initialized with a single blank line</li> <li>Registered as a wordlist in the database</li> <li>Associated with a preset job for easy use</li> </ul> <p>Important: The potfile preset job requires at least one hashcat binary version to be uploaded to the system. On fresh installations: - The potfile wordlist is created immediately - The preset job creation is deferred until a binary is available - A background monitor checks every 5 seconds for binary availability - Once a binary is uploaded, the preset job is automatically created</p>"},{"location":"admin-guide/operations/potfile/#performance-optimizations-v121","title":"Performance Optimizations (v1.2.1+)","text":"<p>Starting with version 1.2.1, the potfile system includes significant performance enhancements designed to handle high-volume password cracking operations efficiently.</p>"},{"location":"admin-guide/operations/potfile/#bloom-filter-for-duplicate-detection","title":"Bloom Filter for Duplicate Detection","text":"<p>The system now uses a Bloom filter for O(1) duplicate detection:</p> <p>Benefits: - Near-instant lookups: Check if password exists in potfile without loading entire file - Memory efficient: Probabilistic data structure uses minimal RAM - Auto-reload: Refreshes periodically to stay synchronized with file changes - Reduced disk I/O: Avoids repeated file scanning</p> <p>How It Works: <pre><code>1. Bloom filter initialized on service start\n2. All existing potfile entries loaded into filter\n3. New passwords checked against filter before staging\n4. Filter automatically reloads when potfile changes\n5. False positive rate: &lt; 0.1% (acceptable for deduplication)\n</code></pre></p> <p>Impact on Performance: - Before: Linear scan through entire potfile (O(n)) - After: Constant-time bloom filter check (O(1)) - For 10M password potfile: 1000x faster duplicate detection</p>"},{"location":"admin-guide/operations/potfile/#batch-staging-api","title":"Batch Staging API","text":"<p>The <code>StageBatch()</code> method enables bulk insertion of thousands of passwords in a single operation:</p> <pre><code>// Stage 10,000 passwords at once\nentries := []PotfileStagingEntry{\n    {Password: \"password1\", HashValue: \"hash1\"},\n    {Password: \"password2\", HashValue: \"hash2\"},\n    // ... 9,998 more entries\n}\npotfileService.StageBatch(ctx, entries)\n</code></pre> <p>Advantages: - Single database round trip instead of N individual inserts - Automatic duplicate handling with <code>ON CONFLICT DO NOTHING</code> - Optimized for crack batching system integration - Reduced transaction overhead</p>"},{"location":"admin-guide/operations/potfile/#pre-loaded-settings-n1-query-elimination","title":"Pre-Loaded Settings (N+1 Query Elimination)","text":"<p>Problem Solved: Previously, potfile settings were queried for every single cracked password: <pre><code>1.75M cracks = 1.75M duplicate database queries for settings\n</code></pre></p> <p>Solution: Settings are now loaded once before processing batches: <pre><code>// Load once\npotfileEnabled := getSystemSetting(\"potfile_enabled\")\nclientPotfileEnabled := getClientSetting(hashlist.ClientID)\n\n// Use for all 1.75M cracks without additional queries\nfor _, crack := range crackedHashes {\n    if potfileEnabled &amp;&amp; clientPotfileEnabled {\n        stageBatch = append(stageBatch, crack)\n    }\n}\n</code></pre></p> <p>Impact: - Eliminates millions of redundant queries - Reduces database load by &gt;99% during bulk processing - Faster crack processing times (seconds instead of minutes)</p>"},{"location":"admin-guide/operations/potfile/#mini-batch-transaction-processing","title":"Mini-Batch Transaction Processing","text":"<p>For Large Crack Volumes: The system processes cracks in mini-batches of 20,000 entries:</p> <p>Why Mini-Batching?: 1. Prevents Connection Leaks: Smaller transactions complete faster 2. Reduces Lock Contention: Shorter transaction duration 3. Memory Management: Bounded memory usage per batch 4. Better Failure Isolation: Failed batch doesn't affect entire set</p> <p>Example: <pre><code>1.75M cracked passwords processed as:\n- 88 transactions of 20k entries each\n- vs. 1 giant transaction (prone to timeouts/locks)\n- vs. 1.75M individual transactions (extreme overhead)\n</code></pre></p> <p>Configuration: <pre><code>// Default mini-batch size (optimized for performance)\nconst batchSize = 20000\n\n// Processes 100k \u2192 20k batches \u2192 5 transactions\n// Balances throughput with resource constraints\n</code></pre></p>"},{"location":"admin-guide/operations/potfile/#increased-default-batch-size","title":"Increased Default Batch Size","text":"<p>The <code>potfile_max_batch_size</code> default has been increased: - Old default: 1,000 entries - New default: 100,000 entries</p> <p>Rationale: - Modern hardware can handle larger batches efficiently - Reduced overhead from fewer batch cycles - Better performance during high-volume cracking (e.g., 1M+ cracks) - Still processes in 20k mini-batches internally for safety</p> <p>When to Adjust: - Increase (&gt;100k): Extremely high-volume environments with abundant RAM - Decrease (&lt;100k): Memory-constrained systems or very slow disks - Default (100k): Optimal for 95% of deployments</p>"},{"location":"admin-guide/operations/potfile/#staging-and-processing-mechanism","title":"Staging and Processing Mechanism","text":""},{"location":"admin-guide/operations/potfile/#staging-table-structure","title":"Staging Table Structure","text":"<p>The <code>potfile_staging</code> table temporarily holds passwords before batch processing:</p> Column Type Description <code>id</code> integer Unique identifier <code>password</code> text The plaintext password <code>hash_value</code> text Original hash that was cracked <code>created_at</code> timestamp When the entry was staged <code>processed</code> boolean No longer used (entries are deleted after processing)"},{"location":"admin-guide/operations/potfile/#processing-workflow","title":"Processing Workflow","text":"<pre><code>graph TD\n    A[Password Cracked] --&gt; B{Already in Potfile?}\n    B --&gt;|No| C[Add to Staging Table]\n    B --&gt;|Yes| D[Skip - Duplicate]\n    C --&gt; E[Wait for Batch Timer]\n    E --&gt; F[Background Worker Runs]\n    F --&gt; G[Load Potfile into Memory]\n    G --&gt; H[Filter Duplicates]\n    H --&gt; I[Append to Potfile]\n    I --&gt; J[Delete from Staging]\n    J --&gt; K[Update Metadata]</code></pre>"},{"location":"admin-guide/operations/potfile/#deduplication-logic","title":"Deduplication Logic","text":"<ul> <li>Comparison basis: Plaintext passwords only (not hashes)</li> <li>Case sensitivity: Passwords are case-sensitive</li> <li>Empty passwords: Blank lines are valid passwords</li> <li>Within-batch: Duplicates within the same batch are also filtered</li> </ul>"},{"location":"admin-guide/operations/potfile/#integration-with-crack-batching","title":"Integration with Crack Batching","text":"<p>The potfile system is optimized to work seamlessly with the Crack Batching System:</p> <p>Workflow: 1. Agent batches 10,000 cracked passwords 2. Backend receives <code>CrackBatch</code> message 3. Bulk lookup identifies all hashes (single query) 4. Pre-loaded potfile settings determine staging eligibility 5. <code>StageBatch()</code> inserts qualified passwords (single transaction) 6. Background worker processes staged entries in 20k mini-batches 7. Potfile updated and bloom filter refreshed</p> <p>Performance: - 10k cracks \u2192 potfile in &lt;2 seconds (including bloom filter update) - Zero N+1 query problems - Minimal database overhead - Automatic deduplication at multiple stages</p> <p>See the Crack Batching System documentation for full details on how high-volume cracking integrates with potfile staging.</p>"},{"location":"admin-guide/operations/potfile/#integration-with-jobs","title":"Integration with Jobs","text":""},{"location":"admin-guide/operations/potfile/#automatic-preset-job","title":"Automatic Preset Job","text":"<p>The system automatically creates a preset job for the potfile: - Name: \"Potfile Run\" - Type: Dictionary attack using the potfile as the wordlist - Priority: Can be configured for high-priority execution - Agents: Available to all agents</p> <p>Note: If you don't see the \"Potfile Run\" preset job after installation, ensure you have uploaded at least one hashcat binary through Admin \u2192 Binary Management. The preset job will be created automatically once a binary is available.</p>"},{"location":"admin-guide/operations/potfile/#using-the-potfile-in-jobs","title":"Using the Potfile in Jobs","text":"<ol> <li>Automatic inclusion: The potfile preset job can be included in job workflows</li> <li>Manual selection: Administrators can select the potfile wordlist when creating custom jobs</li> <li>First-pass attack: Often used as the first attack in a workflow due to high success rate</li> </ol>"},{"location":"admin-guide/operations/potfile/#keyspace-calculation","title":"Keyspace Calculation","text":"<ul> <li>The potfile's word count is automatically updated after each batch</li> <li>Keyspace for jobs using the potfile adjusts dynamically</li> <li>Agents receive updated keyspace information</li> </ul>"},{"location":"admin-guide/operations/potfile/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"admin-guide/operations/potfile/#check-potfile-status","title":"Check Potfile Status","text":"<pre><code># View potfile contents\ncat /data/krakenhashes/wordlists/custom/potfile.txt\n\n# Count passwords in potfile\nwc -l /data/krakenhashes/wordlists/custom/potfile.txt\n\n# Check file size\nls -lh /data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre>"},{"location":"admin-guide/operations/potfile/#monitor-staging-table","title":"Monitor Staging Table","text":"<pre><code>-- Count staged entries\nSELECT COUNT(*) FROM potfile_staging;\n\n-- View recent staged passwords\nSELECT password, created_at \nFROM potfile_staging \nORDER BY created_at DESC \nLIMIT 10;\n\n-- Check for processing issues\nSELECT COUNT(*) as stuck_entries \nFROM potfile_staging \nWHERE created_at &lt; NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"admin-guide/operations/potfile/#check-processing-logs","title":"Check Processing Logs","text":"<pre><code># View potfile-related logs\ndocker logs krakenhashes 2&gt;&amp;1 | grep -i potfile\n\n# Check for staging activity\ndocker logs krakenhashes 2&gt;&amp;1 | grep \"staged password\"\n\n# Monitor batch processing\ndocker logs krakenhashes 2&gt;&amp;1 | grep \"Processing.*staged pot-file entries\"\n</code></pre>"},{"location":"admin-guide/operations/potfile/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Issue Cause Solution Passwords not being added Potfile disabled Check <code>potfile_enabled</code> setting Staging table growing Processing stopped Restart server, check logs for errors Duplicate passwords appearing File manually edited Let system manage the file Potfile not updating Batch interval too long Reduce <code>potfile_batch_interval</code> High memory usage Large potfile Consider archiving old entries Potfile preset job missing No binaries uploaded Upload a hashcat binary via Binary Management"},{"location":"admin-guide/operations/potfile/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/operations/potfile/#operational-guidelines","title":"Operational Guidelines","text":"<ol> <li>Let the system manage the potfile</li> <li>Don't manually edit while the server is running</li> <li>Use the staging mechanism for all additions</li> <li> <p>Trust the deduplication logic</p> </li> <li> <p>Monitor staging table size <pre><code>-- Set up an alert if staging exceeds threshold\nSELECT COUNT(*) FROM potfile_staging;\n</code></pre></p> </li> <li> <p>Balance batch processing</p> </li> <li>Shorter intervals: More responsive but higher overhead</li> <li>Longer intervals: More efficient but delayed updates</li> <li> <p>Recommended: 30-60 seconds for most deployments</p> </li> <li> <p>Regular maintenance</p> </li> <li>Monitor potfile size growth</li> <li>Consider rotating very large potfiles (&gt;1GB)</li> <li>Archive historical passwords if needed</li> </ol>"},{"location":"admin-guide/operations/potfile/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory usage: The entire potfile is loaded into memory during processing</li> <li>Disk I/O: Frequent updates can cause disk activity</li> <li>Network transfer: Large potfiles take time to sync to agents</li> <li>Database load: Staging table operations add database activity</li> </ul>"},{"location":"admin-guide/operations/potfile/#security-notes","title":"Security Notes","text":"<p>Important: The potfile contains actual passwords in plaintext</p> <ul> <li>Protect the potfile with appropriate file permissions</li> <li>Ensure backups are encrypted</li> <li>Limit access to the system data directory</li> <li>Consider compliance requirements for password storage</li> <li>Implement audit logging for potfile access</li> </ul> <p>Data Retention Exclusion</p> <p>The potfile is NOT managed by the data retention system. This has critical security and compliance implications:</p> <p>What This Means: - Passwords from deleted hashlists remain in the potfile permanently - The potfile grows indefinitely unless manually managed - Deleting a client or hashlist does NOT remove their passwords from the potfile - This may violate data protection regulations (GDPR, CCPA, etc.) that require complete data deletion</p> <p>Required Manual Management: 1. Implement a cleanup procedure to remove passwords associated with deleted hashlists 2. Document your potfile retention policy separately from the main data retention policy 3. Consider periodic rotation - archive old potfiles and start fresh 4. Audit compliance - ensure your potfile management meets regulatory requirements 5. Track deletions - maintain logs of when and why potfile entries were removed</p> <p>Example Cleanup Approach: <pre><code># Backup current potfile\ncp /data/krakenhashes/wordlists/custom/potfile.txt potfile.backup\n\n# Remove specific patterns (requires careful scripting)\ngrep -v \"pattern_to_remove\" potfile.txt &gt; potfile.tmp\nmv potfile.tmp potfile.txt\n\n# Or rotate periodically\nmv potfile.txt potfile.$(date +%Y%m%d).archive\necho \"\" &gt; potfile.txt  # Start with blank line\n</code></pre></p>"},{"location":"admin-guide/operations/potfile/#backup-and-recovery","title":"Backup and Recovery","text":"<ol> <li>Include in backups: The potfile should be part of regular system backups</li> <li>Database consistency: Back up both the potfile and database together</li> <li>Recovery process: </li> <li>Restore the potfile to its original location</li> <li>Verify the <code>potfile_wordlist_id</code> matches the database</li> <li>Clear any orphaned staging entries</li> <li>Restart the server</li> </ol>"},{"location":"admin-guide/operations/potfile/#integration-tips","title":"Integration Tips","text":"<ul> <li>Workflow optimization: Place potfile attack early in job workflows</li> <li>Custom rules: Combine potfile with rule-based attacks for variations</li> <li>Hybrid attacks: Use potfile + mask attacks for pattern matching</li> <li>Regular updates: Ensure agents sync regularly for latest passwords</li> </ul>"},{"location":"admin-guide/operations/potfile/#advanced-usage","title":"Advanced Usage","text":""},{"location":"admin-guide/operations/potfile/#manual-staging","title":"Manual Staging","text":"<p>If needed, passwords can be manually staged:</p> <pre><code>-- Manually stage a password\nINSERT INTO potfile_staging (password, hash_value, created_at, processed)\nVALUES ('NewPassword123', 'manual_entry', NOW(), false);\n</code></pre>"},{"location":"admin-guide/operations/potfile/#bulk-import","title":"Bulk Import","text":"<p>For importing existing password lists:</p> <pre><code>-- Import from external source (use with caution)\nINSERT INTO potfile_staging (password, hash_value, created_at, processed)\nSELECT DISTINCT password, 'bulk_import', NOW(), false\nFROM external_password_source\nWHERE password NOT IN (\n    SELECT line FROM potfile_lines\n);\n</code></pre>"},{"location":"admin-guide/operations/potfile/#potfile-analysis","title":"Potfile Analysis","text":"<pre><code>-- Most recent additions\nSELECT password, created_at \nFROM potfile_staging \nORDER BY created_at DESC \nLIMIT 20;\n\n-- Processing rate\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    COUNT(*) as passwords_staged\nFROM potfile_staging\nWHERE created_at &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour;\n</code></pre>"},{"location":"admin-guide/operations/potfile/#summary","title":"Summary","text":"<p>The potfile is a powerful feature that automatically builds organizational knowledge of successful passwords. By accumulating cracked passwords and making them available for future jobs, it significantly improves cracking efficiency over time. The automated staging and batch processing system ensures reliable operation with minimal administrative overhead.</p> <p>Key points to remember: - Fully automated operation with configurable parameters - Intelligent deduplication prevents redundant entries - Seamless integration with the job system - Minimal performance impact through batch processing - Critical asset for improving crack rates over time</p> <p>For additional assistance or advanced configuration needs, consult the system logs or contact support.</p>"},{"location":"admin-guide/operations/scheduling/","title":"Agent Scheduling","text":""},{"location":"admin-guide/operations/scheduling/#overview","title":"Overview","text":"<p>The Agent Scheduling feature in KrakenHashes allows administrators to define specific time windows when agents are available for job execution. This feature helps optimize resource usage, manage electricity costs, and ensure agents run during appropriate hours.</p>"},{"location":"admin-guide/operations/scheduling/#key-features","title":"Key Features","text":"<ul> <li>Daily Schedule Configuration: Set different working hours for each day of the week</li> <li>Timezone Support: Schedules are configured in the user's local timezone but stored in UTC</li> <li>Overnight Schedule Support: Schedules can span midnight (e.g., 22:00 - 02:00)</li> <li>Global Enable/Disable: System-wide toggle to enable or disable all scheduling</li> <li>Per-Agent Control: Each agent can have scheduling enabled or disabled independently</li> <li>Schedule Preservation: Schedules are preserved even when disabled</li> </ul>"},{"location":"admin-guide/operations/scheduling/#how-it-works","title":"How It Works","text":""},{"location":"admin-guide/operations/scheduling/#schedule-enforcement","title":"Schedule Enforcement","text":"<p>When scheduling is enabled: 1. The system checks if global scheduling is enabled (admin setting) 2. The system checks if the individual agent has scheduling enabled 3. The system checks if the current UTC time falls within the agent's schedule 4. Only agents that pass all checks are assigned jobs</p>"},{"location":"admin-guide/operations/scheduling/#time-storage-and-display","title":"Time Storage and Display","text":"<ul> <li>Storage: All times are stored in UTC in the database</li> <li>Display: Times are shown in the user's local timezone in the UI</li> <li>Conversion: Automatic conversion happens between local and UTC times</li> </ul>"},{"location":"admin-guide/operations/scheduling/#configuration","title":"Configuration","text":""},{"location":"admin-guide/operations/scheduling/#global-settings","title":"Global Settings","text":"<p>The global scheduling setting can be found in Admin Panel \u2192 System Settings:</p> <pre><code>Enable Agent Scheduling System: [Toggle]\n</code></pre> <p>When disabled: - All agent schedules are ignored - Agents are always available for jobs - Individual agent schedules are preserved but not enforced</p>"},{"location":"admin-guide/operations/scheduling/#per-agent-configuration","title":"Per-Agent Configuration","text":"<p>Individual agent scheduling is configured on the agent details page:</p> <ol> <li>Navigate to Agents \u2192 [Agent Name]</li> <li>Find the Scheduling section</li> <li>Toggle Enable Scheduling to activate scheduling for this agent</li> <li>Click Edit All Schedules to configure daily schedules</li> </ol>"},{"location":"admin-guide/operations/scheduling/#schedule-configuration","title":"Schedule Configuration","text":"<p>When editing schedules:</p> <ol> <li>Add Schedule: Click \"Add Schedule\" for any day to create a time window</li> <li>Set Times: Enter start and end times in 24-hour format (HH:MM)</li> <li>Active Toggle: Enable/disable the schedule for specific days</li> <li>Active (ON): Agent works during the specified hours</li> <li>Active (OFF): Agent does not work at all on this day</li> <li>Copy Schedule: Use the copy icon to apply one day's schedule to all other days</li> <li>Delete Schedule: Remove a schedule for a specific day</li> </ol>"},{"location":"admin-guide/operations/scheduling/#time-input-formats","title":"Time Input Formats","text":"<p>The system accepts various time formats: - <code>9</code> \u2192 <code>09:00:00</code> - <code>17</code> \u2192 <code>17:00:00</code> - <code>9:30</code> \u2192 <code>09:30:00</code> - <code>09:00</code> \u2192 <code>09:00:00</code> - <code>09:00:00</code> \u2192 <code>09:00:00</code></p> <p> Weekly scheduling interface showing daily operating hours configuration with timezone support and per-day activation toggles</p>"},{"location":"admin-guide/operations/scheduling/#examples","title":"Examples","text":""},{"location":"admin-guide/operations/scheduling/#standard-business-hours-9-5-monday-friday","title":"Standard Business Hours (9-5, Monday-Friday)","text":"<pre><code>Monday:    09:00 - 17:00 [Active]\nTuesday:   09:00 - 17:00 [Active]\nWednesday: 09:00 - 17:00 [Active]\nThursday:  09:00 - 17:00 [Active]\nFriday:    09:00 - 17:00 [Active]\nSaturday:  Not scheduled\nSunday:    Not scheduled\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#247-operation-with-weekend-maintenance","title":"24/7 Operation with Weekend Maintenance","text":"<pre><code>Monday:    00:00 - 23:59 [Active]\nTuesday:   00:00 - 23:59 [Active]\nWednesday: 00:00 - 23:59 [Active]\nThursday:  00:00 - 23:59 [Active]\nFriday:    00:00 - 23:59 [Active]\nSaturday:  00:00 - 06:00 [Active]  # Maintenance window 6 AM - Midnight\nSunday:    Not scheduled            # Full day maintenance\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#overnight-processing","title":"Overnight Processing","text":"<pre><code>Monday:    22:00 - 06:00 [Active]  # Runs overnight Mon-Tue\nTuesday:   22:00 - 06:00 [Active]  # Runs overnight Tue-Wed\nWednesday: 22:00 - 06:00 [Active]  # Runs overnight Wed-Thu\nThursday:  22:00 - 06:00 [Active]  # Runs overnight Thu-Fri\nFriday:    22:00 - 06:00 [Active]  # Runs overnight Fri-Sat\nSaturday:  Not scheduled\nSunday:    22:00 - 06:00 [Active]  # Runs overnight Sun-Mon\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#important-behavior-notes","title":"Important Behavior Notes","text":""},{"location":"admin-guide/operations/scheduling/#running-jobs-and-schedule-boundaries","title":"Running Jobs and Schedule Boundaries","text":"<p>The scheduling system only controls when new jobs are assigned, not when running jobs must complete.</p> <p>Key points: - Schedules determine when an agent can receive new jobs - Running jobs will always complete, even if they extend past the scheduled end time - The agent will not accept new jobs outside its schedule, but will finish current work</p>"},{"location":"admin-guide/operations/scheduling/#example-scenario","title":"Example Scenario","text":"<p>If an agent is scheduled to work until 17:00: - At 16:59, the agent receives a job configured for 1-hour chunks - The job will run to completion, potentially until 17:59 or later - No new jobs will be assigned after 17:00 - The agent becomes available for new work at the next scheduled window</p> <p>This design ensures: - No work is lost due to scheduling boundaries - Jobs complete successfully without schedule interruption - Predictable behavior for long-running tasks</p>"},{"location":"admin-guide/operations/scheduling/#job-interruption-and-priority-override","title":"Job Interruption and Priority Override","text":""},{"location":"admin-guide/operations/scheduling/#overview_1","title":"Overview","text":"<p>While the scheduling system doesn't interrupt jobs based on time, KrakenHashes supports priority-based job interruption that works alongside scheduling.</p>"},{"location":"admin-guide/operations/scheduling/#how-job-interruption-works-with-scheduling","title":"How Job Interruption Works with Scheduling","text":"<p>The job interruption system operates independently of agent schedules:</p> <ol> <li>Priority-Based Only: Jobs are interrupted based on priority, not schedule boundaries</li> <li>Schedule Aware: Interrupted jobs can only resume when agents are both:</li> <li>Available (not working on other jobs)</li> <li>Within their scheduled working hours</li> <li>Automatic Management: The system handles all interruption and resumption automatically</li> </ol>"},{"location":"admin-guide/operations/scheduling/#interruption-scenarios","title":"Interruption Scenarios","text":""},{"location":"admin-guide/operations/scheduling/#scenario-1-high-priority-job-during-schedule","title":"Scenario 1: High Priority Job During Schedule","text":"<pre><code>Agent Schedule: 09:00 - 17:00\nCurrent Time: 14:00\nRunning Job: Priority 50 (started at 13:00)\nNew Job: Priority 95 with high priority override\n\nResult: Low priority job interrupted, high priority job takes over\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#scenario-2-interrupted-job-resumes-next-schedule-window","title":"Scenario 2: Interrupted Job Resumes Next Schedule Window","text":"<pre><code>Agent Schedule: 09:00 - 17:00\nJob Interrupted: 16:45 (Priority 50)\nSchedule Ends: 17:00\nNext Day: 09:00 - Job automatically resumes when agent is scheduled again\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#configuration-for-job-interruption","title":"Configuration for Job Interruption","text":""},{"location":"admin-guide/operations/scheduling/#system-wide-setting","title":"System-Wide Setting","text":"<p>Navigate to Admin Panel \u2192 System Settings: <pre><code>Job Interruption Enabled: [Toggle]\n</code></pre></p> <p>When enabled: - Higher priority jobs with override can interrupt lower priority running jobs - Interruption only occurs when no agents are available - Interrupted jobs automatically queue for resumption</p>"},{"location":"admin-guide/operations/scheduling/#per-job-configuration","title":"Per-Job Configuration","text":"<p>In preset job settings: <pre><code>Allow High Priority Override: [Toggle]\nPriority: [0-100]\n</code></pre></p>"},{"location":"admin-guide/operations/scheduling/#best-practices-for-interruption-with-scheduling","title":"Best Practices for Interruption with Scheduling","text":"<ol> <li>Consider Schedule Windows: High-priority jobs should account for agent availability</li> <li>Set Appropriate Chunk Sizes: Smaller chunks (5-10 minutes) allow more responsive interruption</li> <li>Monitor Interruption Patterns: Track if certain schedule windows see excessive interruptions</li> <li>Plan Critical Jobs: Schedule critical jobs during peak agent availability hours</li> </ol>"},{"location":"admin-guide/operations/scheduling/#interaction-between-features","title":"Interaction Between Features","text":"Feature Scheduling Job Interruption Trigger Time-based Priority-based Job Stopping Never stops running jobs Can stop lower priority jobs Resumption N/A - jobs complete Automatic when agents available Configuration Per-agent schedules Per-job override flag System Toggle Global scheduling enable Global interruption enable"},{"location":"admin-guide/operations/scheduling/#common-questions","title":"Common Questions","text":"<p>Q: Can a scheduled agent be interrupted? A: Yes, if a higher priority job with override is waiting and no other agents are available.</p> <p>Q: What happens if an interrupted job's agent goes off-schedule? A: The job remains pending and resumes when any scheduled agent becomes available.</p> <p>Q: Do interrupted jobs lose progress? A: No, all progress is saved and jobs resume from their last checkpoint.</p> <p>Q: Can scheduling prevent interruptions? A: No, but having more agents scheduled reduces the need for interruptions.</p>"},{"location":"admin-guide/operations/scheduling/#schedule-priority","title":"Schedule Priority","text":"<p>The scheduling system follows this priority order:</p> <ol> <li>Global Setting OFF: All schedules ignored, all agents always available</li> <li>Global Setting ON + Agent Scheduling OFF: Agent always available</li> <li>Global Setting ON + Agent Scheduling ON: Agent follows configured schedule</li> </ol>"},{"location":"admin-guide/operations/scheduling/#technical-details","title":"Technical Details","text":""},{"location":"admin-guide/operations/scheduling/#database-schema","title":"Database Schema","text":"<p>Schedules are stored in the <code>agent_schedules</code> table:</p> <pre><code>CREATE TABLE agent_schedules (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER NOT NULL REFERENCES agents(id),\n    day_of_week INTEGER NOT NULL,  -- 0-6 (Sunday-Saturday)\n    start_time TIME NOT NULL,       -- UTC time\n    end_time TIME NOT NULL,         -- UTC time\n    timezone VARCHAR(50) NOT NULL,  -- Original timezone for reference\n    is_active BOOLEAN NOT NULL,\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>GET /api/agents/{id}/schedules</code> - Get agent schedules</li> <li><code>POST /api/agents/{id}/schedules</code> - Update single schedule</li> <li><code>POST /api/agents/{id}/schedules/bulk</code> - Bulk update schedules</li> <li><code>DELETE /api/agents/{id}/schedules/{day}</code> - Delete schedule for a day</li> <li><code>PUT /api/agents/{id}/scheduling-enabled</code> - Toggle scheduling for agent</li> </ul>"},{"location":"admin-guide/operations/scheduling/#job-assignment-integration","title":"Job Assignment Integration","text":"<p>The job assignment service (<code>GetAvailableAgents</code>) checks scheduling:</p> <pre><code>if agent.SchedulingEnabled {\n    schedulingSetting, err := s.systemSettingsRepo.GetSetting(ctx, \"agent_scheduling_enabled\")\n    if err == nil &amp;&amp; schedulingSetting.Value != nil &amp;&amp; *schedulingSetting.Value == \"true\" {\n        isScheduled, err := s.scheduleRepo.IsAgentScheduledNow(ctx, agent.ID)\n        if err != nil || !isScheduled {\n            continue // Skip this agent\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Test Schedules: Always test schedules with non-critical jobs first</li> <li>Timezone Awareness: Be mindful of timezone differences when setting schedules</li> <li>Overlap Planning: Ensure adequate agent coverage during peak hours</li> <li>Maintenance Windows: Schedule maintenance during off-hours</li> <li>Documentation: Document your scheduling strategy for team members</li> </ol>"},{"location":"admin-guide/operations/scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/scheduling/#agent-not-getting-jobs-despite-being-scheduled","title":"Agent Not Getting Jobs Despite Being Scheduled","text":"<ol> <li>Check global scheduling is enabled</li> <li>Verify agent scheduling is enabled</li> <li>Confirm current time falls within schedule</li> <li>Check agent is otherwise eligible (enabled, online, etc.)</li> </ol>"},{"location":"admin-guide/operations/scheduling/#schedule-shows-wrong-times","title":"Schedule Shows Wrong Times","text":"<ol> <li>Verify your browser timezone is correct</li> <li>Check the timezone display in the UI</li> <li>Remember all times are stored in UTC</li> </ol>"},{"location":"admin-guide/operations/scheduling/#overnight-schedules-not-working","title":"Overnight Schedules Not Working","text":"<ol> <li>Ensure end time is properly set for next day</li> <li>Verify the schedule spans midnight correctly</li> <li>Check both days involved in the overnight schedule</li> </ol>"},{"location":"admin-guide/operations/scheduling/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements for the scheduling system:</p> <ul> <li>Holiday calendar integration</li> <li>Schedule templates for common patterns</li> <li>Bulk schedule management across multiple agents</li> <li>Schedule conflict detection and warnings</li> <li>Historical schedule effectiveness reporting</li> </ul>"},{"location":"admin-guide/operations/users/","title":"User Management Guide","text":"<p>This guide covers user administration in KrakenHashes, including user roles, authentication, multi-factor authentication (MFA), and security policies.</p>"},{"location":"admin-guide/operations/users/#table-of-contents","title":"Table of Contents","text":"<ol> <li>User Roles and Permissions</li> <li>Creating and Managing Users</li> <li>Password Policies and Requirements</li> <li>Multi-Factor Authentication Management</li> <li>Session Management</li> <li>User Deactivation and Deletion</li> <li>Audit Logging and User Activity</li> </ol>"},{"location":"admin-guide/operations/users/#user-roles-and-permissions","title":"User Roles and Permissions","text":"<p>KrakenHashes implements a role-based access control (RBAC) system with the following roles:</p>"},{"location":"admin-guide/operations/users/#user-roles","title":"User Roles","text":"<ol> <li>User (default role)</li> <li>View and manage their own profile</li> <li>Submit password cracking jobs</li> <li>View their own job results</li> <li>Manage their own MFA settings</li> <li> <p>Access general system features</p> </li> <li> <p>Admin</p> </li> <li>All user permissions plus:</li> <li>View and manage all users</li> <li>Reset user passwords</li> <li>Enable/disable user accounts</li> <li>Modify system settings</li> <li>View all jobs and results</li> <li>Manage agents and vouchers</li> <li> <p>Access admin dashboard</p> </li> <li> <p>System (special role)</p> </li> <li>Reserved for system operations</li> <li>Cannot be assigned to regular users</li> <li>Cannot be modified or disabled</li> </ol>"},{"location":"admin-guide/operations/users/#permission-matrix","title":"Permission Matrix","text":"Action User Admin System View own profile \u2713 \u2713 \u2713 Edit own profile \u2713 \u2713 \u2713 View all users \u2717 \u2713 \u2713 Manage users \u2717 \u2713 \u2717 System settings \u2717 \u2713 \u2717 View all jobs \u2717 \u2713 \u2713"},{"location":"admin-guide/operations/users/#creating-and-managing-users","title":"Creating and Managing Users","text":""},{"location":"admin-guide/operations/users/#user-creation","title":"User Creation","text":"<p>Currently, users are created through the registration process. Admin creation of users is not directly implemented but can be achieved through the following methods:</p> <ol> <li>Self-Registration (if enabled)</li> <li>Users register themselves through the web interface</li> <li> <p>Email verification may be required</p> </li> <li> <p>Admin-Initiated Registration</p> </li> <li>Admin provides registration link to new users</li> <li>User completes registration process</li> </ol>"},{"location":"admin-guide/operations/users/#user-management-operations","title":"User Management Operations","text":""},{"location":"admin-guide/operations/users/#listing-users","title":"Listing Users","text":"<p>Admins can view all users through the admin dashboard: - Navigate to Admin \u2192 Users - Filter by role (admin, user) - View user details including:   - Username and email   - Role   - Account status (enabled/disabled)   - MFA status   - Last login time</p> <p> User Management page displaying the user accounts table with columns for Username, Email, Role, Status, MFA status, and Last Login information</p>"},{"location":"admin-guide/operations/users/#updating-user-information","title":"Updating User Information","text":"<p>Admins can update: - Username - Email address - User role (user \u2194 admin)</p> <p>Note: System users cannot be modified.</p>"},{"location":"admin-guide/operations/users/#resetting-user-passwords","title":"Resetting User Passwords","text":"<p>Admins can reset user passwords in two ways:</p> <ol> <li>Temporary Password</li> <li>System generates a secure temporary password</li> <li>Password must be changed on next login</li> <li> <p>Share password securely with user</p> </li> <li> <p>Custom Password</p> </li> <li>Admin sets a specific password</li> <li>Must meet password policy requirements</li> <li>User should change on next login</li> </ol>"},{"location":"admin-guide/operations/users/#password-policies-and-requirements","title":"Password Policies and Requirements","text":""},{"location":"admin-guide/operations/users/#default-password-requirements","title":"Default Password Requirements","text":"<p>The system enforces configurable password policies:</p> <ul> <li>Minimum Length: 8 characters (configurable)</li> <li>Character Requirements (configurable):</li> <li>Uppercase letters</li> <li>Lowercase letters</li> <li>Numbers</li> <li>Special characters</li> </ul>"},{"location":"admin-guide/operations/users/#password-policy-configuration","title":"Password Policy Configuration","text":"<p>Admins can configure password policies via: 1. Navigate to Admin \u2192 Settings \u2192 Authentication 2. Adjust password requirements:    <pre><code>{\n  \"minPasswordLength\": 8,\n  \"requireUppercase\": true,\n  \"requireLowercase\": true,\n  \"requireNumbers\": true,\n  \"requireSpecialChars\": true\n}\n</code></pre></p>"},{"location":"admin-guide/operations/users/#password-security-features","title":"Password Security Features","text":"<ul> <li>Passwords are hashed using bcrypt with default cost</li> <li>Password history tracking</li> <li>Last password change timestamp</li> <li>Forced password change capability</li> </ul>"},{"location":"admin-guide/operations/users/#multi-factor-authentication-management","title":"Multi-Factor Authentication Management","text":""},{"location":"admin-guide/operations/users/#mfa-overview","title":"MFA Overview","text":"<p>KrakenHashes supports multiple MFA methods:</p> <ol> <li>Email-based MFA (default)</li> <li>6-digit codes sent via email</li> <li>Configurable code validity period</li> <li> <p>Built-in cooldown between requests</p> </li> <li> <p>Authenticator App (TOTP)</p> </li> <li>Compatible with Google Authenticator, Authy, etc.</li> <li>SHA-512 algorithm</li> <li>30-second time window</li> <li> <p>6-digit codes</p> </li> <li> <p>Backup Codes</p> </li> <li>8 single-use recovery codes (configurable)</li> <li>Generated when MFA is enabled</li> <li>Can be regenerated by user</li> </ol>"},{"location":"admin-guide/operations/users/#global-mfa-settings","title":"Global MFA Settings","text":"<p>Admins can configure MFA requirements:</p> <ol> <li>Mandatory MFA</li> <li>Require all users to enable MFA</li> <li>Configurable allowed methods</li> <li> <p>Email gateway must be configured first</p> </li> <li> <p>MFA Configuration Options <pre><code>{\n  \"requireMfa\": false,\n  \"allowedMfaMethods\": [\"email\", \"authenticator\"],\n  \"emailCodeValidity\": 5,        // minutes\n  \"backupCodesCount\": 8,\n  \"mfaCodeCooldownMinutes\": 1,\n  \"mfaCodeExpiryMinutes\": 5,\n  \"mfaMaxAttempts\": 3\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/users/#user-mfa-management","title":"User MFA Management","text":""},{"location":"admin-guide/operations/users/#enabling-mfa-for-a-user","title":"Enabling MFA for a User","text":"<p>Users can enable MFA through their profile: 1. Navigate to Profile \u2192 Security 2. Choose MFA method 3. Complete verification process 4. Save backup codes</p> <p> User Profile Settings interface displaying account information, password change functionality, and MFA controls that reflect the authentication policies configured by administrators</p>"},{"location":"admin-guide/operations/users/#admin-mfa-operations","title":"Admin MFA Operations","text":"<p>Admins can: - View user MFA status - Disable MFA for a user (if not globally required) - Reset MFA settings in case of lockout</p>"},{"location":"admin-guide/operations/users/#mfa-authentication-flow","title":"MFA Authentication Flow","text":"<ol> <li>User enters username/password</li> <li>System validates credentials</li> <li>If MFA enabled, prompt for code</li> <li>User provides code via preferred method</li> <li>System validates code with retry limits</li> <li>Grant access upon successful verification</li> </ol>"},{"location":"admin-guide/operations/users/#session-management","title":"Session Management","text":""},{"location":"admin-guide/operations/users/#session-security-features","title":"Session Security Features","text":"<ol> <li>JWT-based Authentication</li> <li>Configurable token expiry (default: 7 days)</li> <li>Secure HTTP-only cookies</li> <li> <p>Refresh token support</p> </li> <li> <p>Account Security</p> </li> <li>Failed login attempt tracking</li> <li>Automatic account lockout after threshold</li> <li>Configurable lockout duration</li> <li>Admin unlock capability</li> </ol>"},{"location":"admin-guide/operations/users/#session-configuration","title":"Session Configuration","text":"<pre><code>{\n  \"maxFailedAttempts\": 5,\n  \"lockoutDurationMinutes\": 30,\n  \"jwtExpiryMinutes\": 10080,  // 7 days\n  \"sessionTimeout\": 60         // minutes of inactivity\n}\n</code></pre>"},{"location":"admin-guide/operations/users/#managing-user-sessions","title":"Managing User Sessions","text":""},{"location":"admin-guide/operations/users/#account-lockout","title":"Account Lockout","text":"<p>When a user exceeds failed login attempts: 1. Account is automatically locked 2. User sees lockout message with duration 3. Admin can manually unlock via Admin \u2192 Users \u2192 Unlock</p>"},{"location":"admin-guide/operations/users/#session-monitoring","title":"Session Monitoring","text":"<ul> <li>Last login timestamp tracked</li> <li>Failed login attempts counted</li> <li>Last failed attempt timestamp</li> <li>Account lock status and duration</li> </ul>"},{"location":"admin-guide/operations/users/#user-deactivation-and-deletion","title":"User Deactivation and Deletion","text":""},{"location":"admin-guide/operations/users/#account-deactivation","title":"Account Deactivation","text":"<p>Admins can disable user accounts:</p> <ol> <li>Disable Account</li> <li>User cannot log in</li> <li>Requires disable reason</li> <li>Tracks who disabled and when</li> <li>Account data preserved</li> <li> <p>Can be re-enabled later</p> </li> <li> <p>Enable Account</p> </li> <li>Restores account access</li> <li>Clears disable reason</li> <li>User can log in again</li> </ol>"},{"location":"admin-guide/operations/users/#account-deletion-soft-delete","title":"Account Deletion (Soft Delete)","text":"<p>Admins can delete user accounts using soft delete:</p> <ol> <li>Soft Delete Process</li> <li>Navigate to Admin \u2192 Users</li> <li>Select user to delete</li> <li>Click \"Delete\" button</li> <li>Confirm deletion</li> <li>User's <code>deleted_at</code> timestamp is set</li> <li> <p>User is filtered from all listings</p> </li> <li> <p>Deletion Restrictions</p> </li> <li>Cannot delete your own account</li> <li>Cannot delete system users</li> <li> <p>Deleted users' data is preserved (soft delete)</p> </li> <li> <p>What Happens When Deleted</p> </li> <li>User cannot log in</li> <li>User is hidden from user listings</li> <li>Historical data (jobs, results) remains linked</li> <li>Data preserved for audit and compliance</li> </ol>"},{"location":"admin-guide/operations/users/#important-considerations","title":"Important Considerations","text":"<ul> <li>System users cannot be disabled or deleted</li> <li>Disabled users' data remains intact</li> <li>Deleted users' data remains intact (soft delete)</li> <li>Active sessions are not immediately terminated</li> </ul>"},{"location":"admin-guide/operations/users/#deactivation-process","title":"Deactivation Process","text":"<ol> <li>Navigate to Admin \u2192 Users</li> <li>Select user to disable</li> <li>Click \"Disable Account\"</li> <li>Provide reason for audit trail</li> <li>Confirm action</li> </ol> <p>To re-enable: 1. Find disabled user 2. Click \"Enable Account\" 3. User can now log in</p>"},{"location":"admin-guide/operations/users/#deletion-process","title":"Deletion Process","text":"<ol> <li>Navigate to Admin \u2192 Users</li> <li>Select user to delete</li> <li>Click \"Delete\" button</li> <li>Confirm deletion in dialog</li> <li>User is soft-deleted and hidden from listings</li> </ol>"},{"location":"admin-guide/operations/users/#audit-logging-and-user-activity","title":"Audit Logging and User Activity","text":""},{"location":"admin-guide/operations/users/#user-activity-tracking","title":"User Activity Tracking","text":"<p>The system tracks the following user activities:</p> <ol> <li>Authentication Events</li> <li>Login attempts (successful/failed)</li> <li>MFA verification attempts</li> <li>Password changes</li> <li> <p>Account lockouts</p> </li> <li> <p>Account Modifications</p> </li> <li>Profile updates</li> <li>MFA changes</li> <li>Password resets</li> <li> <p>Role changes</p> </li> <li> <p>Administrative Actions</p> </li> <li>User account modifications</li> <li>Password resets by admin</li> <li>Account enable/disable</li> <li>MFA resets</li> </ol>"},{"location":"admin-guide/operations/users/#audit-information-captured","title":"Audit Information Captured","text":"<p>For each auditable event: - Timestamp - User ID performing action - Target user ID (if applicable) - Action type - Additional context (e.g., disable reason) - IP address (where applicable)</p>"},{"location":"admin-guide/operations/users/#accessing-audit-logs","title":"Accessing Audit Logs","text":"<p>While a dedicated audit log viewer is not yet implemented, audit information is stored in the database:</p> <ul> <li>Login tracking via <code>last_login</code> and <code>failed_login_attempts</code></li> <li>Account changes via <code>disabled_by</code>, <code>disabled_at</code>, <code>disabled_reason</code></li> <li>Password changes via <code>last_password_change</code></li> </ul>"},{"location":"admin-guide/operations/users/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Regular Reviews</li> <li>Review user list for inactive accounts</li> <li>Check for users with excessive privileges</li> <li> <p>Monitor failed login patterns</p> </li> <li> <p>MFA Enforcement</p> </li> <li>Consider mandatory MFA for admin users</li> <li>Regular review of MFA methods in use</li> <li> <p>Ensure email gateway configured for email MFA</p> </li> <li> <p>Password Policies</p> </li> <li>Enforce strong password requirements</li> <li>Consider regular password rotation for admins</li> <li> <p>Monitor for compromised credentials</p> </li> <li> <p>Account Hygiene</p> </li> <li>Disable accounts promptly when users leave</li> <li>Regular review of admin role assignments</li> <li>Document reasons for account actions</li> </ol>"},{"location":"admin-guide/operations/users/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"admin-guide/operations/users/#mfa-issues","title":"MFA Issues","text":"<p>Problem: User locked out of MFA - Solution: Admin can disable MFA for user, user re-enables</p> <p>Problem: Email codes not received - Solution: Check email gateway configuration, verify email address</p> <p>Problem: Authenticator app out of sync - Solution: Verify system time, consider increasing TOTP skew tolerance</p>"},{"location":"admin-guide/operations/users/#account-access-issues","title":"Account Access Issues","text":"<p>Problem: User account locked - Solution: Admin unlocks account or wait for lockout duration</p> <p>Problem: Password reset not working - Solution: Verify password meets policy requirements</p> <p>Problem: Cannot modify system user - Solution: System users are protected and cannot be modified</p>"},{"location":"admin-guide/operations/users/#configuration-issues","title":"Configuration Issues","text":"<p>Problem: Cannot enable global MFA - Solution: Configure email gateway first in Email Settings</p> <p>Problem: Users bypassing MFA - Solution: Check global MFA requirement is enabled</p>"},{"location":"admin-guide/operations/users/#api-endpoints-reference","title":"API Endpoints Reference","text":"<p>For programmatic access, the following admin endpoints are available:</p> <ul> <li><code>GET /api/admin/users</code> - List all users</li> <li><code>GET /api/admin/users/{id}</code> - Get user details</li> <li><code>PUT /api/admin/users/{id}</code> - Update user</li> <li><code>DELETE /api/admin/users/{id}</code> - Soft delete user account</li> <li><code>POST /api/admin/users/{id}/disable</code> - Disable account</li> <li><code>POST /api/admin/users/{id}/enable</code> - Enable account</li> <li><code>POST /api/admin/users/{id}/reset-password</code> - Reset password</li> <li><code>POST /api/admin/users/{id}/disable-mfa</code> - Disable MFA</li> <li><code>POST /api/admin/users/{id}/unlock</code> - Unlock account</li> </ul> <p>All endpoints require admin authentication via JWT token.</p>"},{"location":"admin-guide/resource-management/binaries/","title":"Binary Management","text":"<p>KrakenHashes provides a comprehensive binary management system for hashcat and other password cracking tools. This document explains how to manage binaries, track versions, and ensure secure distribution to agents.</p>"},{"location":"admin-guide/resource-management/binaries/#overview","title":"Overview","text":"<p>The binary management system allows administrators to: - Upload and manage multiple versions of hashcat binaries - Track different binary types and compression formats - Verify binary integrity with MD5 checksums - Automatically extract and prepare binaries for server-side execution - Distribute binaries securely to agents - Maintain an audit trail of all binary operations</p>"},{"location":"admin-guide/resource-management/binaries/#understanding-binary-management","title":"Understanding Binary Management","text":""},{"location":"admin-guide/resource-management/binaries/#architecture","title":"Architecture","text":"<p>The binary management system consists of several components:</p> <ol> <li>Binary Storage: Files are stored in <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Local Extraction: Server-side binaries are extracted to <code>&lt;data_dir&gt;/binaries/local/&lt;version_id&gt;/</code></li> <li>Version Tracking: Database tracks all binary versions with metadata</li> <li>Distribution: Agents download binaries via secure API endpoints</li> <li>Verification: Automatic integrity checking with MD5 hashes</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#binary-types","title":"Binary Types","text":"<p>Currently supported binary types: - <code>hashcat</code> - Hashcat password cracking tool - <code>john</code> - John the Ripper (future support)</p>"},{"location":"admin-guide/resource-management/binaries/#compression-types","title":"Compression Types","text":"<p>Supported compression formats: - <code>7z</code> - 7-Zip archive format - <code>zip</code> - ZIP archive format - <code>tar.gz</code> - Gzip-compressed TAR archive - <code>tar.xz</code> - XZ-compressed TAR archive</p>"},{"location":"admin-guide/resource-management/binaries/#uploading-new-binaries","title":"Uploading New Binaries","text":"<p>KrakenHashes supports two methods for adding binaries: downloading from a URL or direct file upload.</p>"},{"location":"admin-guide/resource-management/binaries/#via-url-download","title":"Via URL Download","text":"<p>To add a new binary version by downloading from a URL, use the admin API endpoint:</p> <pre><code>POST /api/admin/binary\nAuthorization: Bearer &lt;admin_token&gt;\nContent-Type: application/json\n\n{\n  \"binary_type\": \"hashcat\",\n  \"compression_type\": \"7z\",\n  \"source_url\": \"https://github.com/hashcat/hashcat/releases/download/v6.2.6/hashcat-6.2.6.7z\",\n  \"file_name\": \"hashcat-6.2.6.7z\",\n  \"version\": \"6.2.6\",\n  \"description\": \"Official hashcat 6.2.6 release\"\n}\n</code></pre> <p>The system will: 1. Download the binary from the specified URL 2. Calculate and store the MD5 hash 3. Verify the download integrity 4. Extract the binary for server-side use 5. Mark the version as active and verified</p>"},{"location":"admin-guide/resource-management/binaries/#via-direct-upload","title":"Via Direct Upload","text":"<p>For custom-compiled binaries or when URL download isn't available, use the multipart upload endpoint:</p> <pre><code>POST /api/admin/binary/upload\nAuthorization: Bearer &lt;admin_token&gt;\nContent-Type: multipart/form-data\n\nbinary_type: hashcat\ncompression_type: 7z\nversion: 7.1.2+338\ndescription: Custom build with additional patches\nfile: &lt;binary_archive_file&gt;\n</code></pre> <p>Form fields: | Field | Required | Description | |-------|----------|-------------| | <code>file</code> | Yes | The binary archive file | | <code>binary_type</code> | Yes | Type of binary (hashcat, john) | | <code>compression_type</code> | Yes | Archive format (7z, zip, tar.gz, tar.xz) | | <code>version</code> | No | Version string for identification | | <code>description</code> | No | Human-readable description |</p> <p>The system will: 1. Receive and store the uploaded file 2. Calculate and store the MD5 hash 3. Extract the binary for server-side use 4. Mark the version as active and verified</p> <p>Use cases for direct upload: - Custom-compiled hashcat builds with specific optimizations - Pre-release or beta versions not yet on GitHub - Patched versions for specific hardware compatibility - Internal builds with custom modifications</p>"},{"location":"admin-guide/resource-management/binaries/#upload-process","title":"Upload Process","text":"<p>When a binary is uploaded:</p> <ol> <li>Download Phase: The system downloads the binary with retry logic (up to 3 attempts)</li> <li>Verification Phase: MD5 hash is calculated and stored</li> <li>Storage Phase: Binary is saved to <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Extraction Phase: Archive is extracted to <code>&lt;data_dir&gt;/binaries/local/&lt;version_id&gt;/</code></li> <li>Status Update: Version status is set to <code>verified</code></li> </ol>"},{"location":"admin-guide/resource-management/binaries/#version-management-and-tracking","title":"Version Management and Tracking","text":""},{"location":"admin-guide/resource-management/binaries/#database-schema","title":"Database Schema","text":"<p>Binary versions are tracked in the <code>binary_versions</code> table with the following fields:</p> Field Type Nullable Default Description <code>id</code> SERIAL No auto Unique version identifier <code>binary_type</code> ENUM No - Type of binary (hashcat, john) <code>compression_type</code> ENUM No - Compression format <code>source_type</code> VARCHAR(50) No 'url' Source type: 'url' or 'upload' <code>source_url</code> TEXT Yes NULL Original download URL (NULL for uploads) <code>file_name</code> VARCHAR(255) No - Stored filename <code>md5_hash</code> VARCHAR(32) No - MD5 checksum <code>file_size</code> BIGINT No - File size in bytes <code>version</code> VARCHAR(100) Yes NULL Version string (e.g., \"6.2.6\", \"7.1.2+338\") <code>description</code> TEXT Yes NULL Human-readable description <code>created_at</code> TIMESTAMP Yes now() Creation timestamp <code>created_by</code> UUID No - User who added the version <code>is_active</code> BOOLEAN Yes true Whether version is active <code>is_default</code> BOOLEAN Yes false Whether this is the default version <code>last_verified_at</code> TIMESTAMP Yes NULL Last verification time <code>verification_status</code> VARCHAR(50) Yes 'pending' Status: pending, verified, failed, deleted"},{"location":"admin-guide/resource-management/binaries/#verification-status","title":"Verification Status","text":"<p>Binary versions can have the following statuses: - <code>pending</code> - Initial state, download/verification in progress - <code>verified</code> - Successfully downloaded and verified - <code>failed</code> - Download or verification failed - <code>deleted</code> - Binary has been deleted</p>"},{"location":"admin-guide/resource-management/binaries/#listing-versions","title":"Listing Versions","text":"<p>To list all binary versions:</p> <pre><code>GET /api/admin/binary?type=hashcat&amp;active=true\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>Query parameters: - <code>type</code> - Filter by binary type - <code>active</code> - Filter by active status (true/false) - <code>status</code> - Filter by verification status</p>"},{"location":"admin-guide/resource-management/binaries/#getting-latest-version","title":"Getting Latest Version","text":"<p>Agents can retrieve the latest active version:</p> <pre><code>GET /api/binary/latest?type=hashcat\nX-API-Key: &lt;agent_api_key&gt;\n</code></pre>"},{"location":"admin-guide/resource-management/binaries/#platform-specific-considerations","title":"Platform-Specific Considerations","text":""},{"location":"admin-guide/resource-management/binaries/#linux","title":"Linux","text":"<p>The system automatically handles Linux-specific binary names: - Checks for both <code>hashcat</code> and <code>hashcat.bin</code> - Sets executable permissions (0750) on extracted binaries</p>"},{"location":"admin-guide/resource-management/binaries/#windows","title":"Windows","text":"<ul> <li>Looks for <code>hashcat.exe</code> in extracted archives</li> <li>Handles Windows-specific path separators</li> </ul>"},{"location":"admin-guide/resource-management/binaries/#archive-extraction","title":"Archive Extraction","text":"<p>The extraction process intelligently handles common archive structures: - Single directory archives: Contents are moved to the target directory - Multi-file archives: All files are extracted as-is - Nested structures: Properly flattened during extraction</p>"},{"location":"admin-guide/resource-management/binaries/#binary-synchronization-to-agents","title":"Binary Synchronization to Agents","text":""},{"location":"admin-guide/resource-management/binaries/#agent-download-process","title":"Agent Download Process","text":"<p>Agents download binaries through the following process:</p> <ol> <li>Version Check: Agent queries for the latest active version</li> <li>Download Request: Agent requests binary download by version ID</li> <li>Authentication: API key authentication is required</li> <li>Streaming Download: Binary is streamed to the agent</li> <li>Local Verification: Agent verifies MD5 hash after download</li> </ol> <p>Note: Agents can be configured with a preferred binary version override. When an agent has a binary override set, it will download and use that specific version instead of the latest active version. This allows for per-agent binary selection based on hardware compatibility, testing requirements, or performance optimization.</p>"},{"location":"admin-guide/resource-management/binaries/#api-endpoints-for-agents","title":"API Endpoints for Agents","text":"<pre><code># Get latest version metadata\nGET /api/binary/latest?type=hashcat\nX-API-Key: &lt;agent_api_key&gt;\n\n# Download specific version\nGET /api/binary/download/{version_id}\nX-API-Key: &lt;agent_api_key&gt;\n</code></pre>"},{"location":"admin-guide/resource-management/binaries/#synchronization-protocol","title":"Synchronization Protocol","text":"<p>The agent file sync system (<code>agent/internal/sync/sync.go</code>) handles: - Concurrent downloads with configurable limits - Retry logic for failed downloads - Local caching to avoid re-downloads - Integrity verification with MD5 hashes</p>"},{"location":"admin-guide/resource-management/binaries/#per-agent-binary-overrides","title":"Per-Agent Binary Overrides","text":"<p>Users can configure individual agents to use specific binary versions, overriding system defaults.</p>"},{"location":"admin-guide/resource-management/binaries/#configuration","title":"Configuration","text":"<p>Agent binary overrides are set via the Agent Details page or API:</p> <pre><code>// PUT /api/agents/{id}\n{\n  \"binaryVersionId\": 3,\n  \"binaryOverride\": true\n}\n</code></pre>"},{"location":"admin-guide/resource-management/binaries/#selection-priority","title":"Selection Priority","text":"<p>When determining which binary to use for an agent:</p> <ol> <li>Agent Override: If enabled, the agent uses its configured binary version</li> <li>Job Binary: For specific job executions, use the job's binary version</li> <li>System Default: Use the active default binary</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#impact-on-agent-operations","title":"Impact on Agent Operations","text":"<p>Agent binary overrides affect: - Device Detection: The agent uses the preferred binary to detect GPU/CPU capabilities - Benchmarks: Performance benchmarks run with the preferred binary for accurate metrics - Job Execution: Tasks execute using the agent's preferred binary (unless job specifies otherwise)</p>"},{"location":"admin-guide/resource-management/binaries/#version-compatibility","title":"Version Compatibility","text":"<p>\u26a0\ufe0f Hashcat 7.x Compatibility Note: Hashcat version 7.x may detect GPU devices but fail to recognize them as usable for job execution, particularly with older GPU driver versions. If you experience device detection issues where devices appear in hardware detection but are not available for jobs:</p> <ul> <li>Use Hashcat 6.x binaries (e.g., 6.2.6, 6.2.5) which have better driver compatibility</li> <li>Configure affected agents to use 6.x binaries via the binary override feature</li> <li>Update GPU drivers to the latest version before trying 7.x binaries</li> </ul>"},{"location":"admin-guide/resource-management/binaries/#automatic-synchronization","title":"Automatic Synchronization","text":"<p>When an agent binary override is set or changed: 1. Backend sends a <code>config_update</code> WebSocket message with the preferred binary version 2. Agent receives the preference and updates its configuration 3. If the binary isn't already downloaded, the file sync system downloads it 4. Device detection runs with the preferred binary after download completes</p>"},{"location":"admin-guide/resource-management/binaries/#updating-and-replacing-binaries","title":"Updating and Replacing Binaries","text":""},{"location":"admin-guide/resource-management/binaries/#adding-a-new-version","title":"Adding a New Version","text":"<p>To add a new version of hashcat:</p> <ol> <li>Upload the new version via the admin API</li> <li>The system automatically downloads and verifies it</li> <li>Previous versions remain available but can be deactivated</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#deactivating-old-versions","title":"Deactivating Old Versions","text":"<pre><code>DELETE /api/admin/binary/{version_id}\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>This will: - Mark the version as inactive (<code>is_active = false</code>) - Set verification status to <code>deleted</code> - Remove the binary file from disk - Preserve the database record for audit purposes</p>"},{"location":"admin-guide/resource-management/binaries/#version-verification","title":"Version Verification","text":"<p>To manually verify a binary's integrity:</p> <pre><code>POST /api/admin/binary/{version_id}/verify\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>This will: - Check if the file exists on disk - Recalculate the MD5 hash - Compare with stored hash - Update verification status and timestamp</p>"},{"location":"admin-guide/resource-management/binaries/#best-practices-and-security","title":"Best Practices and Security","text":""},{"location":"admin-guide/resource-management/binaries/#security-considerations","title":"Security Considerations","text":"<ol> <li>Source URLs: Only download binaries from trusted sources</li> <li>Official hashcat releases: https://github.com/hashcat/hashcat/releases</li> <li> <p>Verify SSL certificates for download sources</p> </li> <li> <p>Hash Verification: Always verify MD5 hashes after download</p> </li> <li>The system automatically calculates and stores hashes</li> <li> <p>Manual verification can be triggered via API</p> </li> <li> <p>Access Control: Binary management requires admin privileges</p> </li> <li>Only administrators can add/remove binaries</li> <li> <p>Agents have read-only access for downloads</p> </li> <li> <p>File Permissions: Extracted binaries have restricted permissions (0750)</p> </li> <li>Only the application user can execute binaries</li> <li>Group members have read access</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#operational-best-practices","title":"Operational Best Practices","text":"<ol> <li>Version Testing: Test new binary versions before deployment</li> <li>Upload to a test environment first</li> <li>Verify extraction and execution work correctly</li> <li> <p>Check compatibility with existing jobs</p> </li> <li> <p>Retention Policy: Maintain a reasonable number of versions</p> </li> <li>Keep at least 2-3 recent versions for rollback</li> <li>Delete very old versions to save storage space</li> <li> <p>Archive important versions externally</p> </li> <li> <p>Monitoring: Regular verification of binary integrity</p> </li> <li>Schedule periodic verification checks</li> <li>Monitor download failures in logs</li> <li> <p>Track agent synchronization success rates</p> </li> <li> <p>Documentation: Document version changes</p> </li> <li>Note any breaking changes between versions</li> <li>Track performance improvements</li> <li>Document known issues with specific versions</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#storage-management","title":"Storage Management","text":"<ol> <li>Disk Space: Monitor available disk space</li> <li>Binary archives can be large (100MB+)</li> <li>Extracted binaries double the storage requirement</li> <li> <p>Plan for growth with multiple versions</p> </li> <li> <p>Cleanup: Regular cleanup of old versions</p> </li> <li>Delete inactive versions after confirming they're not needed</li> <li>Remove failed download attempts</li> <li>Clean up orphaned extraction directories</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/resource-management/binaries/#common-issues","title":"Common Issues","text":"<ol> <li>Download Failures</li> <li>Check network connectivity to source URL</li> <li>Verify SSL/TLS certificates are valid</li> <li>Check firewall rules for outbound HTTPS</li> <li> <p>Review logs for specific error messages</p> </li> <li> <p>Extraction Failures</p> </li> <li>Ensure required tools are installed (7z, unzip, tar)</li> <li>Check disk space for extraction</li> <li>Verify archive isn't corrupted</li> <li> <p>Check file permissions on data directory</p> </li> <li> <p>Verification Failures</p> </li> <li>File may be corrupted during download</li> <li>Source file may have changed</li> <li>Disk errors could cause corruption</li> <li>Try re-downloading the binary</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#log-locations","title":"Log Locations","text":"<p>Binary management logs are written to: - Backend logs: Check for <code>[Binary Manager]</code> entries - Download attempts: Look for HTTP client errors - Extraction logs: Command output is logged - Verification results: Hash comparison details</p>"},{"location":"admin-guide/resource-management/binaries/#manual-recovery","title":"Manual Recovery","text":"<p>If automated processes fail:</p> <ol> <li>Manual Download: Download binary to a temporary location</li> <li>Manual Upload: Place in <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Update Database: Set correct hash and file size</li> <li>Manual Extraction: Extract to local directory</li> <li>Verify Permissions: Ensure correct file permissions</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#versionjson-file","title":"Version.json File","text":"<p>The <code>versions.json</code> file in the repository root tracks component versions:</p> <pre><code>{\n    \"backend\": \"0.1.0\",\n    \"frontend\": \"0.1.0\",\n    \"agent\": \"0.1.0\",\n    \"api\": \"0.1.0\",\n    \"database\": \"0.1.0\"\n}\n</code></pre> <p>This file is used for: - Build-time version embedding - API version compatibility checks - Component version tracking - Release management</p> <p>Note: This tracks KrakenHashes component versions, not binary tool versions.</p>"},{"location":"admin-guide/resource-management/binaries/#api-reference","title":"API Reference","text":""},{"location":"admin-guide/resource-management/binaries/#admin-endpoints","title":"Admin Endpoints","text":"Method Endpoint Description POST <code>/api/admin/binary</code> Add new binary version via URL download POST <code>/api/admin/binary/upload</code> Add new binary version via direct upload GET <code>/api/admin/binary</code> List all versions GET <code>/api/admin/binary/{id}</code> Get specific version DELETE <code>/api/admin/binary/{id}</code> Delete/deactivate version POST <code>/api/admin/binary/{id}/verify</code> Verify binary integrity"},{"location":"admin-guide/resource-management/binaries/#agent-endpoints","title":"Agent Endpoints","text":"Method Endpoint Description GET <code>/api/binary/latest</code> Get latest active version GET <code>/api/binary/download/{id}</code> Download binary file"},{"location":"admin-guide/resource-management/binaries/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements to the binary management system:</p> <ol> <li>Automatic Updates: Check for new releases periodically</li> <li>Version Channels: Support for stable/beta/nightly channels</li> <li>Platform Detection: Automatic platform-specific binary selection</li> <li>Signature Verification: GPG signature verification for downloads</li> <li>Delta Updates: Differential updates for minor versions</li> <li>Binary Caching: CDN integration for faster agent downloads</li> <li>Performance Metrics: Track binary performance across versions</li> </ol>"},{"location":"admin-guide/resource-management/rules/","title":"Rules Management","text":"<p>KrakenHashes provides comprehensive management of rule files used for password cracking operations. This document explains how the system handles rule files and provides best practices for administrators.</p>"},{"location":"admin-guide/resource-management/rules/#overview","title":"Overview","text":"<p>Rules are transformation patterns applied to wordlists during password cracking operations. They allow you to generate password variations without storing massive wordlists. For example, a single rule can transform \"password\" into \"Password123!\", \"p@ssw0rd\", and many other variations.</p> <p> The Rule Management interface displaying uploaded rule files with status, type, size, and rule count information. The interface provides filtering options and actions for managing Hashcat and John the Ripper rule files.</p>"},{"location":"admin-guide/resource-management/rules/#directory-structure","title":"Directory Structure","text":"<p>The system monitors the rules directory and automatically creates the following subdirectory structure:</p> <pre><code>rules/\n\u251c\u2500\u2500 hashcat/       # Hashcat-compatible rules\n\u251c\u2500\u2500 john/          # John the Ripper rules\n\u2514\u2500\u2500 custom/        # User-created or modified rules\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#rule-file-formats-and-best-practices","title":"Rule File Formats and Best Practices","text":""},{"location":"admin-guide/resource-management/rules/#hashcat-rules","title":"Hashcat Rules","text":"<p>Hashcat rules use a simple syntax where each character represents an operation:</p> <pre><code># Example hashcat rules\n:                    # Try the original word\nl                    # Convert to lowercase\nu                    # Convert to uppercase\nc                    # Capitalize first letter, lowercase rest\n$1                   # Append '1' to the word\n^a                   # Prepend 'a' to the word\nsa@                  # Replace all 'a' with '@'\n$1$2$3               # Append '123'\nc$!                  # Capitalize and append '!'\n</code></pre> <p>Best Practices for Hashcat Rules: - Use comments (lines starting with #) to document complex rules - Group related rules together - Test rules with <code>--stdout</code> before using in production - Keep rule files focused on specific transformation types</p>"},{"location":"admin-guide/resource-management/rules/#john-the-ripper-rules","title":"John the Ripper Rules","text":"<p>John rules have a similar but slightly different syntax:</p> <pre><code># Example John the Ripper rules\n:                    # Try the original word\n[lL]                 # Convert to lowercase\n[uU]                 # Convert to uppercase\n[cC]                 # Capitalize\n$[0-9]               # Append a digit\n^[aA]                # Prepend 'a' or 'A'\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#custom-rules","title":"Custom Rules","text":"<p>Custom rules should follow either Hashcat or John syntax depending on your cracking engine.</p>"},{"location":"admin-guide/resource-management/rules/#rule-type-detection","title":"Rule Type Detection","text":"<p>The system automatically assigns a rule type based on the file path: - If the path contains \"john\" (case-insensitive), it's classified as John the Ripper - Otherwise, it's classified as Hashcat - Administrators can change the type after import if needed</p>"},{"location":"admin-guide/resource-management/rules/#rule-splitting-for-large-files","title":"Rule Splitting for Large Files","text":"<p>KrakenHashes includes an intelligent rule splitting system for distributing work across multiple agents:</p>"},{"location":"admin-guide/resource-management/rules/#automatic-rule-splitting","title":"Automatic Rule Splitting","text":"<p>When rule splitting is enabled, the system can automatically split large rule files:</p> <p>Configuration Settings: - <code>rule_split_enabled</code>: Enable/disable automatic rule splitting - <code>rule_split_threshold</code>: Threshold ratio for triggering splits (default: 0.8) - <code>rule_split_min_rules</code>: Minimum number of rules before splitting (default: 10,000) - <code>rule_split_max_chunks</code>: Maximum number of chunks to create (default: 100)</p>"},{"location":"admin-guide/resource-management/rules/#how-rule-splitting-works","title":"How Rule Splitting Works","text":"<ol> <li>Detection: When a job uses a rule file with more rules than the threshold, splitting is triggered</li> <li>Chunking: The rule file is divided into equal chunks based on available agents</li> <li>Distribution: Each agent receives a chunk of rules to process</li> <li>Cleanup: Temporary chunk files are removed after job completion</li> </ol>"},{"location":"admin-guide/resource-management/rules/#manual-rule-splitting","title":"Manual Rule Splitting","text":"<p>For optimal performance, you can pre-split large rule files:</p> <pre><code># Split a rule file into 10 parts\nsplit -n 10 large_rules.rule rules_part_\n\n# Split by number of lines (1000 rules per file)\nsplit -l 1000 large_rules.rule rules_chunk_\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/resource-management/rules/#rule-complexity","title":"Rule Complexity","text":"<p>Different rule types have varying performance impacts:</p> <ol> <li>Simple Rules (minimal impact):</li> <li>Case changes (l, u, c)</li> <li> <p>Single character operations ($x, ^x)</p> </li> <li> <p>Moderate Rules (noticeable impact):</p> </li> <li>Multiple substitutions (sa@sb$sc()</li> <li> <p>Positional operations</p> </li> <li> <p>Complex Rules (significant impact):</p> </li> <li>Multiple operations per rule</li> <li>Conditional rules</li> <li>Memory operations</li> </ol>"},{"location":"admin-guide/resource-management/rules/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Order Rules by Frequency: Place most likely successful rules first</li> <li>Avoid Redundancy: Remove duplicate or overlapping rules</li> <li>Benchmark First: Test rule performance with small wordlists</li> <li>Use Rule Splitting: For rules &gt;10,000 lines, enable splitting</li> <li>Monitor Memory: Complex rules can increase memory usage</li> </ol>"},{"location":"admin-guide/resource-management/rules/#common-rule-sets-and-their-uses","title":"Common Rule Sets and Their Uses","text":""},{"location":"admin-guide/resource-management/rules/#basic-password-variations","title":"Basic Password Variations","text":"<pre><code># basic_variations.rule\n:                    # Original\nc                    # Capitalize\nu                    # Uppercase\nl                    # Lowercase\nc$1                  # Capitalize + append 1\nc$!                  # Capitalize + append !\n$2023                # Append year\n$2024                # Append year\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#leetspeak-transformations","title":"Leetspeak Transformations","text":"<pre><code># leetspeak.rule\nsa@                  # a -&gt; @\nse3                  # e -&gt; 3\nsi1                  # i -&gt; 1\nso0                  # o -&gt; 0\nss$                  # s -&gt; $\nsa@se3              # Multiple substitutions\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#corporate-passwords","title":"Corporate Passwords","text":"<pre><code># corporate.rule\nc$1$2$3              # Capitalize + 123\nc$!$@$#              # Capitalize + special chars\n$@company            # Append company name\n^Company             # Prepend company name\nc$2023               # Capitalize + year\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#keyboard-patterns","title":"Keyboard Patterns","text":"<pre><code># keyboard_patterns.rule\n$!@#                 # Common keyboard pattern\n$123                 # Sequential numbers\n$qwe                 # Keyboard row\n$!qaz                # Vertical keyboard pattern\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#creating-custom-rules","title":"Creating Custom Rules","text":""},{"location":"admin-guide/resource-management/rules/#rule-development-workflow","title":"Rule Development Workflow","text":"<ol> <li>Analyze Target Patterns: Study password patterns from previous cracks</li> <li>Write Initial Rules: Create rules based on observed patterns</li> <li>Test with Hashcat: Use <code>--stdout</code> to verify transformations</li> <li>Refine and Optimize: Remove ineffective rules, add variations</li> <li>Document: Add comments explaining rule purpose</li> </ol>"},{"location":"admin-guide/resource-management/rules/#example-creating-domain-specific-rules","title":"Example: Creating Domain-Specific Rules","text":"<pre><code># finance_sector.rule\n# Common patterns in financial sector\n$2023                # Current year\n$Q1                  # Quarter notation\n$USD                 # Currency\n^FIN                 # Department prefix\nsa@s$$               # Common substitutions\nc$123                # Compliance requirement\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#testing-custom-rules","title":"Testing Custom Rules","text":"<pre><code># Test rules with sample wordlist\nhashcat --stdout wordlist.txt -r custom.rule | head -20\n\n# Count generated candidates\nhashcat --stdout wordlist.txt -r custom.rule | wc -l\n\n# Verify specific transformations\necho \"password\" | hashcat --stdout -r custom.rule\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#file-management","title":"File Management","text":""},{"location":"admin-guide/resource-management/rules/#uploading-rules","title":"Uploading Rules","text":"<p>When uploading rule files:</p> <ol> <li>Choose appropriate rule type (Hashcat/John/Custom)</li> <li>Add descriptive tags for organization</li> <li>Include documentation in description field</li> <li>For large files (&gt;10MB), upload is processed asynchronously</li> </ol>"},{"location":"admin-guide/resource-management/rules/#automatic-processing","title":"Automatic Processing","text":"<p>The system automatically: - Calculates MD5 hash for integrity - Counts rules (excluding comments and empty lines) - Verifies file accessibility - Tags auto-imported files</p>"},{"location":"admin-guide/resource-management/rules/#duplicate-handling","title":"Duplicate Handling","text":"<ul> <li>Same filename + same content = Skip upload</li> <li>Same filename + different content = Update existing</li> <li>Different filename + same content = Create new entry</li> </ul>"},{"location":"admin-guide/resource-management/rules/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/resource-management/rules/#organization","title":"Organization","text":"<ol> <li>Categorize by Purpose: Use subdirectories for different rule types</li> <li>Version Control: Include version numbers in filenames</li> <li>Documentation: Include README files explaining rule sets</li> </ol>"},{"location":"admin-guide/resource-management/rules/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Good naming examples\nbasic_english_v2.rule\ncorporate_2024Q1.rule\nweb_app_patterns.rule\nfinance_sector_specific.rule\n\n# Avoid\nrules1.txt\nnew.rule\ntest.rule\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#maintenance","title":"Maintenance","text":"<ol> <li>Regular Reviews: Audit rule effectiveness quarterly</li> <li>Update Patterns: Add new patterns as they emerge</li> <li>Remove Obsolete: Delete rules for outdated patterns</li> <li>Benchmark Performance: Test rule speed on new hardware</li> </ol>"},{"location":"admin-guide/resource-management/rules/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"admin-guide/resource-management/rules/#import-status","title":"Import Status","text":"<p>Check rule import status through: - Admin dashboard for overview - Server logs for detailed processing info - Database <code>verification_status</code> field</p>"},{"location":"admin-guide/resource-management/rules/#common-issues","title":"Common Issues","text":"<ol> <li>Large File Processing: Files &gt;1GB may take time to verify</li> <li>Rule Syntax Errors: Invalid rules are skipped during counting</li> <li>File Access: Ensure proper permissions on rule directories</li> </ol>"},{"location":"admin-guide/resource-management/rules/#performance-metrics","title":"Performance Metrics","text":"<p>Monitor rule performance: - Rules/second processing rate - Memory usage during rule application - Success rate (cracks per rule application)</p>"},{"location":"admin-guide/resource-management/rules/#security-considerations","title":"Security Considerations","text":"<ol> <li>Access Control: Limit rule file access to authorized users</li> <li>Validation: System validates rule syntax during import</li> <li>Audit Trail: All rule modifications are logged</li> <li>Sensitive Patterns: Avoid hardcoding sensitive data in rules</li> </ol>"},{"location":"admin-guide/resource-management/rules/#integration-with-jobs","title":"Integration with Jobs","text":"<p>Rules are selected during job creation: 1. Browse available verified rules 2. Select appropriate rule for attack type 3. System handles rule distribution to agents 4. Progress tracking shows rules processed</p> <p>For optimal performance, match rule complexity to available computational resources and expected password patterns.</p>"},{"location":"admin-guide/resource-management/storage/","title":"Storage Architecture","text":""},{"location":"admin-guide/resource-management/storage/#overview","title":"Overview","text":"<p>KrakenHashes implements a centralized file storage system with intelligent deduplication, hash verification, and performance optimizations. This guide covers the storage architecture, capacity planning, and maintenance procedures.</p>"},{"location":"admin-guide/resource-management/storage/#storage-directory-structure","title":"Storage Directory Structure","text":"<p>The system organizes files into a hierarchical structure under the configured data directory (default: <code>/var/lib/krakenhashes</code> in Docker, <code>~/.krakenhashes-data</code> locally):</p> <pre><code>/var/lib/krakenhashes/           # Root data directory (KH_DATA_DIR)\n\u251c\u2500\u2500 binaries/                    # Hashcat/John binaries\n\u2502   \u251c\u2500\u2500 hashcat_7.6.0_linux64.tar.gz\n\u2502   \u2514\u2500\u2500 john_1.9.0_linux64.tar.gz\n\u251c\u2500\u2500 wordlists/                   # Wordlist files by category\n\u2502   \u251c\u2500\u2500 general/                 # Common wordlists\n\u2502   \u251c\u2500\u2500 specialized/             # Domain-specific lists\n\u2502   \u251c\u2500\u2500 targeted/                # Custom targeted lists\n\u2502   \u2514\u2500\u2500 custom/                  # User-uploaded lists\n\u251c\u2500\u2500 rules/                       # Rule files by type\n\u2502   \u251c\u2500\u2500 hashcat/                 # Hashcat-compatible rules\n\u2502   \u251c\u2500\u2500 john/                    # John-compatible rules\n\u2502   \u2514\u2500\u2500 custom/                  # Custom rule sets\n\u251c\u2500\u2500 hashlists/                   # Processed hashlist files\n\u2502   \u251c\u2500\u2500 1.hash                   # Uncracked hashes for job ID 1\n\u2502   \u2514\u2500\u2500 2.hash                   # Uncracked hashes for job ID 2\n\u251c\u2500\u2500 hashlist_uploads/            # Temporary upload storage\n\u2502   \u2514\u2500\u2500 &lt;user-id&gt;/              # User-specific upload directories\n\u2514\u2500\u2500 local/                       # Extracted binaries (server-side)\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#directory-permissions","title":"Directory Permissions","text":"<p>All directories are created with mode <code>0750</code> (rwxr-x---) to ensure: - Owner has full access - Group has read and execute access - Others have no access</p>"},{"location":"admin-guide/resource-management/storage/#file-deduplication-and-hash-verification","title":"File Deduplication and Hash Verification","text":""},{"location":"admin-guide/resource-management/storage/#md5-based-deduplication","title":"MD5-Based Deduplication","text":"<p>KrakenHashes uses MD5 hashes for file deduplication across all resource types:</p> <ol> <li>Upload Processing</li> <li>Calculate MD5 hash of uploaded file</li> <li>Check database for existing file with same hash</li> <li>If exists, reference existing file instead of storing duplicate</li> <li> <p>If new, store file and record hash in database</p> </li> <li> <p>Verification States</p> </li> <li><code>pending</code> - File uploaded but not yet verified</li> <li><code>verified</code> - File hash matches database record</li> <li><code>failed</code> - Hash mismatch or file corrupted</li> <li> <p><code>deleted</code> - File removed from storage</p> </li> <li> <p>Database Schema <pre><code>-- Example: Wordlists table\nCREATE TABLE wordlists (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    file_name VARCHAR(255) NOT NULL,\n    md5_hash VARCHAR(32) NOT NULL,\n    file_size BIGINT NOT NULL,\n    verification_status VARCHAR(20) DEFAULT 'pending',\n    UNIQUE(md5_hash)  -- Ensures deduplication\n);\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#file-synchronization","title":"File Synchronization","text":"<p>The agent file sync system ensures consistency across distributed agents:</p> <ol> <li>Sync Protocol</li> <li>Agent reports current files with MD5 hashes</li> <li>Server compares against master file list</li> <li>Server sends list of files to download</li> <li> <p>Agent downloads only missing/changed files</p> </li> <li> <p>Hash Verification</p> </li> <li>Files are verified after download</li> <li>Failed verifications trigger re-download</li> <li>Corrupted files are automatically replaced</li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-requirements-and-capacity-planning","title":"Storage Requirements and Capacity Planning","text":""},{"location":"admin-guide/resource-management/storage/#estimating-storage-needs","title":"Estimating Storage Needs","text":"<p>Calculate storage requirements based on:</p> <ol> <li>Wordlists</li> <li>Common wordlists: 10-50 GB</li> <li>Specialized lists: 50-200 GB</li> <li> <p>Large collections: 500+ GB</p> </li> <li> <p>Rules</p> </li> <li>Basic rule sets: 1-10 MB</li> <li> <p>Comprehensive sets: 100-500 MB</p> </li> <li> <p>Hashlists</p> </li> <li>Original uploads: Variable</li> <li>Processed files: ~32 bytes per hash</li> <li> <p>Example: 1M hashes \u2248 32 MB</p> </li> <li> <p>Binaries</p> </li> <li>Hashcat package: ~100 MB</li> <li>John package: ~50 MB</li> <li>Multiple versions: Plan for 3-5 versions</li> </ol>"},{"location":"admin-guide/resource-management/storage/#recommended-minimums","title":"Recommended Minimums","text":"Deployment Size Storage Rationale Development 50 GB Basic wordlists and testing Small Team 200 GB Standard wordlists + custom data Enterprise 1 TB+ Comprehensive wordlists + history"},{"location":"admin-guide/resource-management/storage/#growth-considerations","title":"Growth Considerations","text":"<ul> <li>Hashlist accumulation: ~10-20% monthly growth typical</li> <li>Wordlist expansion: New lists added periodically</li> <li>Binary versions: Keep 3-5 recent versions</li> <li>Backup overhead: 2x storage for full backups</li> </ul>"},{"location":"admin-guide/resource-management/storage/#backup-considerations","title":"Backup Considerations","text":""},{"location":"admin-guide/resource-management/storage/#what-to-backup","title":"What to Backup","text":"<ol> <li>Critical Data</li> <li>PostgreSQL database (contains all metadata)</li> <li>Custom wordlists and rules</li> <li> <p>Configuration files (<code>/etc/krakenhashes</code>)</p> </li> <li> <p>Recoverable Data</p> </li> <li>Standard wordlists (can be re-downloaded)</li> <li>Binaries (can be re-downloaded)</li> <li>Processed hashlists (can be regenerated)</li> </ol>"},{"location":"admin-guide/resource-management/storage/#backup-strategy","title":"Backup Strategy","text":"<pre><code>#!/bin/bash\n# Example backup script\n\n# Backup database\npg_dump -h postgres -U krakenhashes krakenhashes &gt; backup/db_$(date +%Y%m%d).sql\n\n# Backup custom data\nrsync -av /var/lib/krakenhashes/wordlists/custom/ backup/wordlists/\nrsync -av /var/lib/krakenhashes/rules/custom/ backup/rules/\nrsync -av /etc/krakenhashes/ backup/config/\n\n# Backup file metadata\ndocker-compose exec backend \\\n  psql -c \"COPY (SELECT * FROM wordlists) TO STDOUT CSV\" &gt; backup/wordlists_meta.csv\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#restore-procedures","title":"Restore Procedures","text":"<ol> <li> <p>Database Restore <pre><code>psql -h postgres -U krakenhashes krakenhashes &lt; backup/db_20240115.sql\n</code></pre></p> </li> <li> <p>File Restore <pre><code>rsync -av backup/wordlists/ /var/lib/krakenhashes/wordlists/custom/\nrsync -av backup/rules/ /var/lib/krakenhashes/rules/custom/\n</code></pre></p> </li> <li> <p>Verify Integrity</p> </li> <li>Run file verification for all restored files</li> <li>Check MD5 hashes against database records</li> </ol>"},{"location":"admin-guide/resource-management/storage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"admin-guide/resource-management/storage/#file-system-considerations","title":"File System Considerations","text":"<ol> <li>File System Choice</li> <li>ext4: Good general performance</li> <li>XFS: Better for large files</li> <li> <p>ZFS: Built-in deduplication and compression</p> </li> <li> <p>Mount Options <pre><code># Example /etc/fstab entry with optimizations\n/dev/sdb1 /var/lib/krakenhashes ext4 defaults,noatime,nodiratime 0 2\n</code></pre></p> </li> <li> <p>Storage Layout</p> </li> <li>Use separate volumes for different data types</li> <li>Consider SSD for hashlists (frequent reads)</li> <li>HDDs acceptable for wordlists (sequential reads)</li> </ol>"},{"location":"admin-guide/resource-management/storage/#caching-strategy","title":"Caching Strategy","text":"<ol> <li>Application-Level Caching</li> <li>Recently used wordlists kept in memory</li> <li>Hash type definitions cached</li> <li> <p>File metadata cached for 15 minutes</p> </li> <li> <p>File System Caching</p> </li> <li>Linux page cache handles frequently accessed files</li> <li>Monitor with <code>free -h</code> and adjust <code>vm.vfs_cache_pressure</code></li> </ol>"},{"location":"admin-guide/resource-management/storage/#io-optimization","title":"I/O Optimization","text":"<pre><code># Tune kernel parameters for better I/O\necho 'vm.dirty_ratio = 5' &gt;&gt; /etc/sysctl.conf\necho 'vm.dirty_background_ratio = 2' &gt;&gt; /etc/sysctl.conf\necho 'vm.vfs_cache_pressure = 50' &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#docker-volume-management","title":"Docker Volume Management","text":""},{"location":"admin-guide/resource-management/storage/#volume-configuration","title":"Volume Configuration","text":"<p>Docker Compose creates named volumes for persistent storage:</p> <pre><code>volumes:\n  krakenhashes_data:        # Main data directory\n    name: krakenhashes_app_data\n  postgres_data:            # Database storage\n    name: krakenhashes_postgres_data\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#volume-operations","title":"Volume Operations","text":"<ol> <li> <p>Inspect Volumes <pre><code>docker volume inspect krakenhashes_app_data\ndocker volume ls\n</code></pre></p> </li> <li> <p>Backup Volumes <pre><code># Backup data volume\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd)/backup:/backup \\\n  alpine tar czf /backup/data_backup.tar.gz -C /data .\n</code></pre></p> </li> <li> <p>Restore Volumes <pre><code># Restore data volume\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd)/backup:/backup \\\n  alpine tar xzf /backup/data_backup.tar.gz -C /data\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-driver-optimization","title":"Storage Driver Optimization","text":"<p>For production deployments:</p> <pre><code>{\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#file-cleanup-and-maintenance","title":"File Cleanup and Maintenance","text":""},{"location":"admin-guide/resource-management/storage/#automated-cleanup","title":"Automated Cleanup","text":"<p>The system includes automated cleanup for:</p> <ol> <li>Temporary Upload Files</li> <li>Deleted after successful processing</li> <li> <p>Orphaned files cleaned after 24 hours</p> </li> <li> <p>Old Hashlist Files</p> </li> <li>Configurable retention period</li> <li>Default: Keep for job lifetime + 30 days</li> </ol>"},{"location":"admin-guide/resource-management/storage/#manual-cleanup-procedures","title":"Manual Cleanup Procedures","text":"<ol> <li> <p>Remove Orphaned Files <pre><code># Find files not referenced in database\ndocker-compose exec backend bash\ncd /var/lib/krakenhashes\n\n# Check for orphaned wordlists\nfind wordlists -type f -name \"*.txt\" | while read f; do\n  hash=$(md5sum \"$f\" | cut -d' ' -f1)\n  # Query database for hash\ndone\n</code></pre></p> </li> <li> <p>Clean Old Hashlists <pre><code>-- Remove hashlists older than 90 days with no active jobs\nDELETE FROM hashlists \nWHERE updated_at &lt; NOW() - INTERVAL '90 days'\nAND id NOT IN (\n  SELECT DISTINCT hashlist_id \n  FROM job_executions \n  WHERE status IN ('pending', 'running')\n);\n</code></pre></p> </li> <li> <p>Vacuum Database <pre><code>docker-compose exec postgres \\\n  psql -U krakenhashes -c \"VACUUM ANALYZE;\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-monitoring","title":"Storage Monitoring","text":"<ol> <li> <p>Disk Usage Monitoring <pre><code># Monitor storage usage\ndf -h /var/lib/krakenhashes\ndu -sh /var/lib/krakenhashes/*\n\n# Set up alerts\necho '0 * * * * root df -h | grep krakenhashes | \\\n  awk '\\''$5+0 &gt; 80 {print \"Storage warning: \" $0}'\\''' \\\n  &gt;&gt; /etc/crontab\n</code></pre></p> </li> <li> <p>File Count Monitoring <pre><code>-- Monitor file counts\nSELECT \n  'wordlists' as type, COUNT(*) as count,\n  SUM(file_size)/1024/1024/1024 as size_gb\nFROM wordlists\nWHERE verification_status = 'verified'\nUNION ALL\nSELECT 'rules', COUNT(*), SUM(file_size)/1024/1024/1024\nFROM rules\nWHERE verification_status = 'verified';\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Maintenance Schedule</li> <li>Weekly: Check disk usage and clean temp files</li> <li>Monthly: Verify file integrity and clean old hashlists</li> <li> <p>Quarterly: Full backup and storage audit</p> </li> <li> <p>Monitoring Alerts</p> </li> <li>Set up alerts for &gt;80% disk usage</li> <li>Monitor file verification failures</li> <li> <p>Track deduplication efficiency</p> </li> <li> <p>Documentation</p> </li> <li>Document custom wordlist sources</li> <li>Maintain changelog for rule modifications</li> <li>Record storage growth trends</li> </ol>"},{"location":"admin-guide/resource-management/storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/resource-management/storage/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Disk Space Exhaustion <pre><code># Emergency cleanup\nfind /var/lib/krakenhashes/hashlist_uploads -mtime +1 -delete\ndocker system prune -f\n</code></pre></p> </li> <li> <p>File Verification Failures <pre><code>-- Find failed verifications\nSELECT * FROM wordlists \nWHERE verification_status = 'failed';\n\n-- Reset for re-verification\nUPDATE wordlists \nSET verification_status = 'pending' \nWHERE verification_status = 'failed';\n</code></pre></p> </li> <li> <p>Permission Issues <pre><code># Fix permissions\nchown -R 1000:1000 /var/lib/krakenhashes\nchmod -R 750 /var/lib/krakenhashes\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#debug-commands","title":"Debug Commands","text":"<pre><code># Check file system integrity\nfsck -n /dev/sdb1\n\n# Monitor I/O performance\niostat -x 1\n\n# Check open files\nlsof | grep krakenhashes\n\n# Verify Docker volumes\ndocker volume inspect krakenhashes_app_data\n</code></pre>"},{"location":"admin-guide/resource-management/wordlists/","title":"Wordlists and Rules Management","text":"<p>KrakenHashes provides automatic monitoring and management of wordlists and rules used for password cracking operations. This document explains how the system handles these files and provides best practices for administrators.</p>"},{"location":"admin-guide/resource-management/wordlists/#directory-structure","title":"Directory Structure","text":"<p>The system monitors two specific directories for files:</p> <ul> <li>Wordlists Directory: <code>&lt;data_dir&gt;/wordlists/</code></li> <li>Rules Directory: <code>&lt;data_dir&gt;/rules/</code></li> </ul> <p>The system automatically creates the following subdirectory structure:</p> <p>Wordlists: <pre><code>wordlists/\n\u251c\u2500\u2500 general/       # Common wordlists for general use\n\u251c\u2500\u2500 specialized/   # Domain-specific wordlists\n\u251c\u2500\u2500 targeted/      # Target-specific wordlists\n\u2514\u2500\u2500 custom/        # User-created or modified wordlists\n</code></pre></p> <p>Rules: <pre><code>rules/\n\u251c\u2500\u2500 hashcat/       # Hashcat-compatible rules\n\u251c\u2500\u2500 john/          # John the Ripper rules\n\u2514\u2500\u2500 custom/        # User-created or modified rules\n</code></pre></p>"},{"location":"admin-guide/resource-management/wordlists/#file-formats","title":"File Formats","text":""},{"location":"admin-guide/resource-management/wordlists/#wordlists","title":"Wordlists","text":"<p>The system supports the following wordlist formats: - Plaintext: <code>.txt</code>, <code>.lst</code>, <code>.dict</code> files - Compressed: <code>.gz</code>, <code>.zip</code> files</p>"},{"location":"admin-guide/resource-management/wordlists/#rules","title":"Rules","text":"<p>The system supports the following rule formats: - Hashcat: Standard hashcat rule files - John: John the Ripper rule files</p>"},{"location":"admin-guide/resource-management/wordlists/#auto-monitoring-process","title":"Auto-Monitoring Process","text":"<p>When files are added to these directories, the system automatically:</p> <ol> <li>Detects new or modified files</li> <li>Calculates MD5 hashes for integrity verification</li> <li>Imports metadata into the database</li> <li>Counts words/rules in the files</li> <li>Makes them available for use in password cracking jobs</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#file-upload-handling","title":"File Upload Handling","text":"<p>When uploading files through the web interface:</p> <ol> <li>The system preserves the original filename (with sanitization for security)</li> <li>Files are automatically placed in the appropriate subdirectory based on their type</li> <li>Duplicate detection is performed based on filename:</li> <li>If a file with the same name exists and has the same MD5 hash, the upload is skipped</li> <li>If a file with the same name exists but has a different MD5 hash, the file is updated</li> <li>The system automatically calculates the MD5 hash and counts words/rules</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#duplicate-detection","title":"Duplicate Detection","text":"<p>The system handles duplicate files intelligently:</p> <ul> <li>Same filename, same content: The system will recognize the file as already existing and return the existing entry</li> <li>Same filename, different content: The system will update the existing file with the new content</li> <li>Different filename, same content: The system will store both files separately</li> </ul> <p>This approach ensures that: - Files are not unnecessarily duplicated - Updates to existing files are properly tracked - Users can maintain multiple versions of similar files with different names</p>"},{"location":"admin-guide/resource-management/wordlists/#auto-monitoring-details","title":"Auto-Monitoring Details","text":""},{"location":"admin-guide/resource-management/wordlists/#system-user","title":"System User","text":"<p>All auto-imported wordlists and rules are created in the database using a special system user (UUID: <code>00000000-0000-0000-0000-000000000000</code>). This user:</p> <ul> <li>Cannot be used for frontend login</li> <li>Is used exclusively for system-generated actions</li> <li>Helps track which files were auto-imported vs. manually added</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#monitoring-interval","title":"Monitoring Interval","text":"<ul> <li>The system checks for new files every 5 minutes</li> <li>Initial scan happens immediately when the server starts</li> <li>There's a small delay (2 seconds) after database migrations complete before monitoring starts to prevent race conditions</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#file-detection-process","title":"File Detection Process","text":"<ol> <li>The system detects new or modified files in the monitored directories</li> <li>Files are checked to ensure they're not still being transferred</li> <li>MD5 hash is calculated for each file</li> <li>If a file with the same name exists but has a different hash, it's updated</li> <li>If a file with the same name and hash exists, it's skipped</li> <li>New files are added to the database with \"pending\" verification status</li> <li>File contents are counted (words or rules)</li> <li>Status is updated to \"verified\" once counting is complete</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#file-transfer-considerations","title":"File Transfer Considerations","text":"<p>When transferring files to the monitored directories, be aware of the following:</p> <ul> <li>File Stability Check: The system waits for files to be stable (not actively changing) before processing them</li> <li>SCP/SFTP Transfer Time: For large files transferred via SCP or SFTP, allow sufficient time for the transfer to complete before the file will be processed</li> <li>The system waits 30 seconds after the last modification before considering a file stable</li> <li>For files larger than 100MB, the system also checks if the file size has changed</li> </ul> <p>Note: When using SCP to transfer large wordlists, the file may not be detected for import until 30 seconds after the transfer completes.</p>"},{"location":"admin-guide/resource-management/wordlists/#file-size-and-format-restrictions","title":"File Size and Format Restrictions","text":""},{"location":"admin-guide/resource-management/wordlists/#size-restrictions","title":"Size Restrictions","text":"<ul> <li>No file size limits: The system can handle wordlists and rules of any size</li> <li>For very large files (&gt;1GB), be aware that:</li> <li>Initial hash calculation may take longer</li> <li>Word/rule counting operations run in the background</li> <li>The file will be available with a \"pending\" status until counting completes</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#automatic-tagging","title":"Automatic Tagging","text":"<p>Auto-imported files are automatically tagged for easy identification:</p> <ul> <li>All auto-imported files receive the tag <code>auto-imported</code></li> <li>Updated files additionally receive the tag <code>updated</code></li> </ul> <p>The system does not automatically tag files based on subdirectories or other criteria.</p>"},{"location":"admin-guide/resource-management/wordlists/#rule-type-detection","title":"Rule Type Detection","text":"<p>The system automatically assigns a default rule type based on the file path:</p> <ul> <li>If the path contains \"john\" (case-insensitive), the rule is classified as a John the Ripper rule</li> <li>Otherwise, it's classified as a Hashcat rule</li> </ul> <p>Administrators can edit the rule type after import if needed.</p>"},{"location":"admin-guide/resource-management/wordlists/#wordlist-type-classification","title":"Wordlist Type Classification","text":"<p>Wordlists are classified based on their subdirectory:</p> <ul> <li>General: Common wordlists suitable for most cracking jobs</li> <li>Specialized: Wordlists focused on specific patterns or domains</li> <li>Targeted: Wordlists tailored for specific targets</li> <li>Custom: User-created or modified wordlists</li> </ul> <p>When uploading through the web interface, you can specify the wordlist type, which determines the subdirectory where the file will be stored.</p>"},{"location":"admin-guide/resource-management/wordlists/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive filenames: Filenames are used as the default name in the database</li> <li>Pre-verify large files: For very large wordlists, consider pre-calculating word counts to display immediately</li> <li>Monitor the logs: Check server logs for any import errors or issues</li> <li>Avoid frequent updates: Updating large files frequently can cause unnecessary processing overhead</li> <li>Organize by type: Use the appropriate wordlist type when uploading to keep files organized</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#monitoring-import-status","title":"Monitoring Import Status","text":"<p>Administrators can check the status of file imports through:</p> <ol> <li>The admin dashboard in the web interface</li> <li>Server logs, which show detailed information about file processing</li> <li>The database, where each wordlist and rule has a <code>verification_status</code> field</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#manual-management","title":"Manual Management","text":"<p>While auto-importing is convenient, you can also manually:</p> <ol> <li>Add wordlists and rules through the web interface</li> <li>Update metadata for auto-imported files</li> <li>Delete files that are no longer needed</li> </ol> <p>Important: Deleting a file from the database does not remove it from the filesystem. Similarly, removing a file from the filesystem will not automatically remove it from the database.</p>"},{"location":"admin-guide/resource-management/wordlists/#wordlist-types","title":"Wordlist Types","text":"<p>Wordlists are categorized into the following types: - General: Common wordlists for general use - Specialized: Domain-specific wordlists - Targeted: Target-specific wordlists - Custom: User-created or modified wordlists </p>"},{"location":"admin-guide/system-setup/authentication/","title":"Authentication Settings Administration","text":""},{"location":"admin-guide/system-setup/authentication/#overview","title":"Overview","text":"<p>KrakenHashes provides robust authentication settings to ensure system security. This document covers the configuration of password policies, account security settings, and multi-factor authentication (MFA) options.</p>"},{"location":"admin-guide/system-setup/authentication/#password-policy","title":"Password Policy","text":"<p>The password policy settings define the requirements for user passwords across the system.</p>"},{"location":"admin-guide/system-setup/authentication/#configuration-options","title":"Configuration Options","text":"<ol> <li>Minimum Password Length</li> <li>Default: 15 characters</li> <li>Must be a positive integer</li> <li>Recommended: 15+ characters minimum</li> <li> <p>Enforced during password creation and changes</p> </li> <li> <p>Character Requirements</p> </li> <li>Require Uppercase Letters: When enabled, passwords must contain at least one uppercase letter (A-Z)</li> <li>Require Lowercase Letters: When enabled, passwords must contain at least one lowercase letter (a-z)</li> <li>Require Numbers: When enabled, passwords must contain at least one number (0-9)</li> <li>Require Special Characters: When enabled, passwords must contain at least one special character (!@#$%^&amp;*(),.?\":{}|&lt;&gt;)</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices","title":"Best Practices","text":"<ul> <li>Enable all character requirements for maximum security</li> <li>Balance security with usability when setting minimum length</li> <li>Consider industry standards (NIST, OWASP) when configuring</li> <li>Document password requirements clearly for users</li> </ul>"},{"location":"admin-guide/system-setup/authentication/#account-security","title":"Account Security","text":"<p>Account security settings manage login attempts, session duration, and security notifications.</p>"},{"location":"admin-guide/system-setup/authentication/#configuration-options_1","title":"Configuration Options","text":"<ol> <li>Maximum Failed Login Attempts</li> <li>Default: 5 attempts</li> <li>Defines how many failed login attempts are allowed before account lockout</li> <li>Must be a positive integer</li> <li> <p>Recommended range: 3-5 attempts</p> </li> <li> <p>Account Lockout Duration</p> </li> <li>Default: 60 minutes</li> <li>Duration in minutes before a locked account is automatically unlocked</li> <li>Must be a positive integer</li> <li> <p>Affects accounts locked due to exceeded login attempts</p> </li> <li> <p>JWT Token Expiry (Sliding Window Sessions)</p> </li> <li>Default: 60 minutes</li> <li>Base duration for authentication sessions</li> <li>Sliding Window Behavior: Sessions automatically extend when you're actively using the system. The session refreshes after \u2153 of the session time (e.g., 20 minutes for a 60-minute session) when you perform actions like navigating between pages.</li> <li>Activity-Based Extension: Only actual user actions trigger session extension. Background polling, auto-refresh, and SSE streams do not extend the session.</li> <li>Grace Period: When a session refreshes, the old token remains valid for 5 minutes to handle concurrent requests from multiple browser tabs.</li> <li>If you remain idle for the full session duration, you'll need to re-authenticate</li> <li> <p>Balances security with user convenience</p> </li> <li> <p>Notification Aggregation Interval</p> </li> <li>Default: 60 minutes</li> <li>How often to aggregate and send security notifications</li> <li>Prevents notification fatigue while maintaining awareness</li> <li>Groups similar security events within the interval</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices_1","title":"Best Practices","text":"<ul> <li>Adjust lockout duration based on threat model</li> <li>Consider user experience when setting token expiry</li> <li>Monitor failed login attempts for attack patterns</li> <li>Review security notifications regularly</li> </ul>"},{"location":"admin-guide/system-setup/authentication/#multi-factor-authentication-mfa-settings","title":"Multi-Factor Authentication (MFA) Settings","text":"<p>MFA provides an additional layer of security beyond passwords.</p>"},{"location":"admin-guide/system-setup/authentication/#general-settings","title":"General Settings","text":"<ol> <li>Require MFA for All Users</li> <li>Toggle to enforce MFA across all user accounts</li> <li>To enable an email provider must be configured as email is the default MFA</li> <li>Affects new and existing users</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#allowed-mfa-methods","title":"Allowed MFA Methods","text":"<p>The system supports multiple MFA methods:</p> <ol> <li>Email Authentication</li> <li>Sends verification codes to user's registered email</li> <li>Requires configured email provider</li> <li> <p>Good balance of security and convenience</p> </li> <li> <p>Authenticator Apps</p> </li> <li>Compatible with standard TOTP authenticator apps</li> <li>More secure than email-based authentication</li> <li>Works offline once configured</li> <li> <p>Examples: Bitwarden, Google Authenticator, Authy, Microsoft Authenticator</p> </li> <li> <p>Passkey (WebAuthn/FIDO2)</p> </li> <li>Supports FIDO2/WebAuthn standard</li> <li>Provides highest security level</li> <li>Works with security keys (YubiKey), platform authenticators (Windows Hello, Face ID, Touch ID), and password managers (Bitwarden, 1Password)</li> <li>Requires WebAuthn-compatible browser and configured RP ID</li> <li>Note: WebAuthn does NOT support IP addresses as RP IDs - requires domain name</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#webauthn-configuration","title":"WebAuthn Configuration","text":"<p>Before users can register passkeys, administrators must configure WebAuthn settings in the Authentication Settings page:</p> <ol> <li>Relying Party ID (RP ID)</li> <li>Domain name where KrakenHashes is hosted</li> <li>Examples: <code>localhost</code> (dev), <code>krakenhashes.example.com</code> (prod)</li> <li>Cannot be an IP address (WebAuthn specification limitation)</li> <li> <p>Warning: Once set and credentials registered, changing this will invalidate all existing passkeys</p> </li> <li> <p>Allowed Origins</p> </li> <li>Full URLs where the application is accessed</li> <li>Examples: <code>https://localhost:3000</code>, <code>https://krakenhashes.example.com</code></li> <li> <p>Include all URLs users might use to access the system</p> </li> <li> <p>Display Name</p> </li> <li>Friendly name shown during passkey registration</li> <li>Default: \"KrakenHashes\"</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#code-settings","title":"Code Settings","text":"<ol> <li>Email Code Validity</li> <li>Default: 5 minutes</li> <li>How long email-based MFA codes remain valid</li> <li>Must be at least 1 minute</li> <li> <p>Balance security with delivery delays</p> </li> <li> <p>Code Cooldown Period</p> </li> <li>Default: 1 minute</li> <li>Minimum time between code requests</li> <li>Prevents code request spam</li> <li> <p>Must be at least 1 minute</p> </li> <li> <p>Code Expiry Time</p> </li> <li>Default: 5 minutes</li> <li>How long codes remain valid after generation</li> <li>Applies to all MFA methods</li> <li> <p>Should account for potential delays</p> </li> <li> <p>Maximum Code Attempts</p> </li> <li>Default: 3 attempts</li> <li>Maximum invalid code entries before invalidation</li> <li>Requires new code generation after exceeded</li> <li> <p>Prevents brute force attacks</p> </li> <li> <p>Number of Backup Codes</p> </li> <li>Default: 8 codes</li> <li>One-time use backup codes for account recovery</li> <li>Must be at least 1 code</li> <li>Recommended: 8-10 codes</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices_2","title":"Best Practices","text":"<ol> <li>MFA Implementation</li> <li>Consider enforcing MFA for all users</li> <li>Enable multiple MFA methods for flexibility</li> <li>Educate users about backup codes importance</li> <li> <p>Regular review of MFA settings</p> </li> <li> <p>Code Security</p> </li> <li>Keep validity periods short (5-15 minutes)</li> <li>Implement reasonable cooldown periods</li> <li>Limit invalid attempts</li> <li> <p>Generate sufficient backup codes</p> </li> <li> <p>User Experience</p> </li> <li>Clear communication about MFA requirements</li> <li>Document recovery procedures</li> <li>Train support staff on MFA issues</li> <li> <p>Regular testing of MFA workflows</p> </li> <li> <p>Monitoring and Maintenance</p> </li> <li>Regular review of MFA logs</li> <li>Monitor failed MFA attempts</li> <li>Update settings based on security needs</li> <li>Keep documentation current</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/authentication/#common-issues","title":"Common Issues","text":"<ol> <li>Users Unable to Enable MFA</li> <li>Verify email provider configuration</li> <li>Check user permissions</li> <li>Confirm supported authenticator app</li> <li> <p>Review error messages</p> </li> <li> <p>Locked Accounts</p> </li> <li>Verify lockout duration settings</li> <li>Check failed attempt count</li> <li>Review security logs</li> <li> <p>Consider administrative unlock</p> </li> <li> <p>MFA Code Issues</p> </li> <li>Verify code validity period</li> <li>Check cooldown period</li> <li>Confirm correct email delivery</li> <li> <p>Review time synchronization</p> </li> <li> <p>Password Policy Problems</p> </li> <li>Review current policy settings</li> <li>Check character requirement conflicts</li> <li>Verify minimum length appropriateness</li> <li>Consider user feedback </li> </ol>"},{"location":"admin-guide/system-setup/configuration/","title":"KrakenHashes System Configuration Guide","text":"<p>This guide provides comprehensive documentation for all configuration options available in the KrakenHashes system, including environment variables, configuration files, and settings for all components.</p>"},{"location":"admin-guide/system-setup/configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Environment Variables</li> <li>Core System Settings</li> <li>Database Configuration</li> <li>Backend Configuration</li> <li>Frontend Configuration</li> <li>Agent Configuration</li> <li>TLS/SSL Configuration</li> <li>Debug and Logging</li> <li>Docker-Specific Settings</li> <li>Configuration Files</li> <li>Common Configuration Scenarios</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#overview","title":"Overview","text":"<p>KrakenHashes uses environment variables for all runtime configuration. Configuration can be provided through:</p> <ol> <li>Environment Variables: Direct system environment variables</li> <li><code>.env</code> Files: Local environment files (development)</li> <li>Docker Compose: Environment variables in docker-compose.yml</li> <li>Kubernetes ConfigMaps: For production deployments</li> </ol>"},{"location":"admin-guide/system-setup/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>The system follows this precedence order (highest to lowest): 1. Runtime environment variables 2. Docker Compose environment settings 3. <code>.env</code> file values 4. Default values in code</p>"},{"location":"admin-guide/system-setup/configuration/#configuration-types","title":"Configuration Types","text":"<p>KrakenHashes uses two types of configuration:</p>"},{"location":"admin-guide/system-setup/configuration/#1-environment-variables-system-configuration","title":"1. Environment Variables (System Configuration)","text":"<p>These settings control core system behavior and are set at deployment time. They configure infrastructure elements like database connections, ports, and file paths.</p>"},{"location":"admin-guide/system-setup/configuration/#2-admin-panel-settings-runtime-configuration","title":"2. Admin Panel Settings (Runtime Configuration)","text":"<p>These settings can be changed through the web interface without restarting services. They control operational behavior like job execution, chunking, and agent coordination.</p> <p>Configuration Best Practice</p> <p>Use environment variables for infrastructure settings that rarely change. Use admin panel settings for operational parameters that need frequent adjustment.</p>"},{"location":"admin-guide/system-setup/configuration/#admin-panel-settings","title":"Admin Panel Settings","text":"<p>Several configuration options are available through the Admin Panel UI rather than environment variables. These settings can be changed at runtime without restarting services.</p>"},{"location":"admin-guide/system-setup/configuration/#available-admin-panel-settings","title":"Available Admin Panel Settings","text":"<ul> <li>Job Execution Settings: Control job chunking, agent behavior, and task distribution</li> <li>Default chunk duration</li> <li>Reconnect grace period</li> <li>Progress reporting intervals</li> <li> <p>Rule splitting configuration</p> </li> <li> <p>Data Retention Settings: Configure automatic data cleanup</p> </li> <li>Hashlist retention periods</li> <li>Job history retention</li> <li> <p>Metrics retention</p> </li> <li> <p>Agent Scheduling: Define when agents are available</p> </li> <li>Daily schedules per agent</li> <li>Global scheduling enable/disable</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#accessing-admin-panel-settings","title":"Accessing Admin Panel Settings","text":"<ol> <li>Log in as an administrator</li> <li>Navigate to the Admin Panel</li> <li>Click Settings in the navigation menu</li> <li>Select the appropriate settings category</li> </ol> <p>Changes to admin panel settings take effect immediately without requiring service restarts.</p>"},{"location":"admin-guide/system-setup/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"admin-guide/system-setup/configuration/#core-system-settings","title":"Core System Settings","text":"Variable Description Default Example <code>PUID</code> User ID for file permissions <code>1000</code> <code>1001</code> <code>PGID</code> Group ID for file permissions <code>1000</code> <code>1001</code> <code>TZ</code> Timezone <code>UTC</code> <code>America/New_York</code> <code>KH_IN_DOCKER</code> Whether running in Docker <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#database-configuration","title":"Database Configuration","text":"Variable Description Default Example <code>DB_HOST</code> PostgreSQL host <code>localhost</code> <code>postgres</code> <code>DB_PORT</code> PostgreSQL port <code>5432</code> <code>5432</code> <code>DB_NAME</code> Database name <code>krakenhashes</code> <code>krakenhashes_prod</code> <code>DB_USER</code> Database username <code>krakenhashes</code> <code>khuser</code> <code>DB_PASSWORD</code> Database password <code>krakenhashes</code> <code>secure_password</code> <code>DB_CONNECTION_STRING</code> Full connection string (alternative) - <code>postgres://user:pass@host:port/db?sslmode=disable</code>"},{"location":"admin-guide/system-setup/configuration/#backend-configuration","title":"Backend Configuration","text":""},{"location":"admin-guide/system-setup/configuration/#server-settings","title":"Server Settings","text":"Variable Description Default Example <code>KH_HOST</code> Backend host binding <code>localhost</code> (or <code>0.0.0.0</code> in Docker) <code>0.0.0.0</code> <code>KH_HTTPS_PORT</code> HTTPS API port <code>31337</code> <code>8443</code> <code>KH_HTTP_PORT</code> HTTP port (CA certificate) <code>1337</code> <code>8080</code> <code>KH_CONFIG_DIR</code> Configuration directory <code>~/.krakenhashes</code> <code>/etc/krakenhashes</code> <code>KH_DATA_DIR</code> Data storage directory <code>~/.krakenhashes-data</code> <code>/var/lib/krakenhashes</code> <code>KH_CERTS_DIR</code> Certificate directory <code>{KH_CONFIG_DIR}/certs</code> <code>/etc/krakenhashes/certs</code>"},{"location":"admin-guide/system-setup/configuration/#file-handling","title":"File Handling","text":"Variable Description Default Example <code>KH_HASHLIST_BATCH_SIZE</code> Max hashes per DB batch <code>1000</code> <code>5000</code> <code>KH_MAX_UPLOAD_SIZE_MB</code> Max file upload size (MB) <code>32</code> <code>100</code> <code>KH_HASH_UPLOAD_DIR</code> Hash upload directory <code>{KH_DATA_DIR}/hashlist_uploads</code> <code>/var/lib/krakenhashes/uploads</code>"},{"location":"admin-guide/system-setup/configuration/#jwt-authentication","title":"JWT Authentication","text":"Variable Description Default Example <code>JWT_SECRET</code> JWT signing secret - <code>your-secret-key</code> <code>JWT_EXPIRATION</code> Token expiration time <code>24h</code> <code>7d</code> <code>DEFAULT_ADMIN_ID</code> Default admin user ID - <code>uuid-here</code>"},{"location":"admin-guide/system-setup/configuration/#websocket-configuration","title":"WebSocket Configuration","text":"Variable Description Default Example <code>KH_WRITE_WAIT</code> Time allowed to write messages <code>4s</code> <code>10s</code> <code>KH_PONG_WAIT</code> Time to wait for pong response <code>10s</code> <code>30s</code> <code>KH_PING_PERIOD</code> How often to send pings <code>6s</code> <code>15s</code>"},{"location":"admin-guide/system-setup/configuration/#frontend-configuration","title":"Frontend Configuration","text":"Variable Description Default Example <code>REACT_APP_API_URL</code> HTTPS API endpoint <code>https://localhost:31337</code> <code>https://api.example.com</code> <code>REACT_APP_HTTP_API_URL</code> HTTP API endpoint <code>http://localhost:1337</code> <code>http://api.example.com:8080</code> <code>REACT_APP_WS_URL</code> WebSocket endpoint <code>wss://localhost:31337</code> <code>wss://api.example.com</code> <code>FRONTEND_PORT</code> Frontend HTTPS port <code>443</code> <code>3000</code> <code>PORT</code> Development server port <code>3000</code> <code>3001</code> <code>NODE_ENV</code> Node environment <code>development</code> <code>production</code> <code>HTTPS</code> Enable HTTPS in dev server <code>true</code> <code>false</code> <code>SSL_CRT_FILE</code> Dev server SSL certificate - <code>../certs/server.crt</code> <code>SSL_KEY_FILE</code> Dev server SSL key - <code>../certs/server.key</code>"},{"location":"admin-guide/system-setup/configuration/#agent-configuration","title":"Agent Configuration","text":"Variable Description Default Example <code>KH_CONFIG_DIR</code> Agent config directory <code>{executable_dir}/config</code> <code>/opt/krakenhashes/config</code> <code>KH_DATA_DIR</code> Agent data directory <code>{executable_dir}/data</code> <code>/opt/krakenhashes/data</code> <code>HASHCAT_EXTRA_PARAMS</code> Extra hashcat parameters - <code>-O -w 3</code>"},{"location":"admin-guide/system-setup/configuration/#tlsssl-configuration","title":"TLS/SSL Configuration","text":""},{"location":"admin-guide/system-setup/configuration/#general-tls-settings","title":"General TLS Settings","text":"Variable Description Default Example <code>KH_TLS_MODE</code> TLS provider mode <code>self-signed</code> <code>certbot</code>, <code>provided</code> <code>KH_CERT_KEY_SIZE</code> RSA key size (bits) <code>4096</code> <code>2048</code> <code>KH_CERT_VALIDITY_DAYS</code> Server cert validity (days) <code>365</code> <code>730</code> <code>KH_CA_VALIDITY_DAYS</code> CA cert validity (days) <code>3650</code> <code>7300</code>"},{"location":"admin-guide/system-setup/configuration/#certificate-details","title":"Certificate Details","text":"Variable Description Default Example <code>KH_CA_COUNTRY</code> CA country code <code>US</code> <code>UK</code> <code>KH_CA_ORGANIZATION</code> CA organization <code>KrakenHashes</code> <code>YourOrg</code> <code>KH_CA_ORGANIZATIONAL_UNIT</code> CA organizational unit <code>KrakenHashes CA</code> <code>IT Department</code> <code>KH_CA_COMMON_NAME</code> CA common name <code>KrakenHashes Root CA</code> <code>YourOrg Root CA</code> <code>KH_ADDITIONAL_DNS_NAMES</code> Additional DNS names (comma-separated) - <code>localhost,app.local,*.example.com</code> <code>KH_ADDITIONAL_IP_ADDRESSES</code> Additional IP addresses (comma-separated) - <code>192.168.1.100,10.0.0.5</code>"},{"location":"admin-guide/system-setup/configuration/#provided-certificate-mode","title":"Provided Certificate Mode","text":"Variable Description Default Example <code>KH_CERT_FILE</code> Path to certificate file <code>{KH_CERTS_DIR}/server.crt</code> <code>/etc/ssl/server.crt</code> <code>KH_KEY_FILE</code> Path to private key file <code>{KH_CERTS_DIR}/server.key</code> <code>/etc/ssl/server.key</code> <code>KH_CA_FILE</code> Path to CA certificate <code>{KH_CERTS_DIR}/ca.crt</code> <code>/etc/ssl/ca.crt</code>"},{"location":"admin-guide/system-setup/configuration/#certbot-mode-lets-encrypt","title":"Certbot Mode (Let's Encrypt)","text":"Variable Description Default Example <code>KH_CERTBOT_DOMAIN</code> Domain for certificate - <code>kraken.example.com</code> <code>KH_CERTBOT_EMAIL</code> Email for notifications - <code>admin@example.com</code> <code>KH_CERTBOT_STAGING</code> Use staging server <code>false</code> <code>true</code> <code>KH_CERTBOT_AUTO_RENEW</code> Enable auto-renewal <code>true</code> <code>false</code> <code>KH_CERTBOT_RENEW_HOOK</code> Post-renewal hook script - <code>/opt/scripts/reload.sh</code> <code>CLOUDFLARE_API_TOKEN</code> Cloudflare API token for DNS-01 - <code>your-api-token</code>"},{"location":"admin-guide/system-setup/configuration/#debug-and-logging","title":"Debug and Logging","text":""},{"location":"admin-guide/system-setup/configuration/#general-debug-settings","title":"General Debug Settings","text":"Variable Description Default Example <code>DEBUG</code> Enable debug mode <code>false</code> <code>true</code> <code>LOG_LEVEL</code> Logging level <code>INFO</code> <code>DEBUG</code>, <code>WARNING</code>, <code>ERROR</code>"},{"location":"admin-guide/system-setup/configuration/#component-specific-debug-flags","title":"Component-Specific Debug Flags","text":"Variable Description Default Example <code>DEBUG_SQL</code> Enable SQL query logging <code>false</code> <code>true</code> <code>DEBUG_HTTP</code> Enable HTTP request/response logging <code>false</code> <code>true</code> <code>DEBUG_WEBSOCKET</code> Enable WebSocket message logging <code>false</code> <code>true</code> <code>DEBUG_AUTH</code> Enable authentication debugging <code>false</code> <code>true</code> <code>DEBUG_JOBS</code> Enable job processing debugging <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#frontend-debug-settings","title":"Frontend Debug Settings","text":"Variable Description Default Example <code>REACT_APP_DEBUG</code> Enable frontend debugging <code>false</code> <code>true</code> <code>REACT_APP_DEBUG_REDUX</code> Enable Redux debugging <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#log-directories","title":"Log Directories","text":"Variable Description Default Example <code>LOG_DIR</code> Base log directory <code>/var/log/krakenhashes</code> <code>/logs</code> <code>BACKEND_LOG_DIR</code> Backend logs <code>${LOG_DIR}/backend</code> <code>/logs/backend</code> <code>FRONTEND_LOG_DIR</code> Frontend logs <code>${LOG_DIR}/frontend</code> <code>/logs/frontend</code> <code>NGINX_LOG_DIR</code> Nginx logs <code>${LOG_DIR}/nginx</code> <code>/logs/nginx</code> <code>POSTGRES_LOG_DIR</code> PostgreSQL logs <code>${LOG_DIR}/postgres</code> <code>/logs/postgres</code>"},{"location":"admin-guide/system-setup/configuration/#docker-specific-settings","title":"Docker-Specific Settings","text":""},{"location":"admin-guide/system-setup/configuration/#volume-mounts","title":"Volume Mounts","text":"Variable Description Default Example <code>KH_CONFIG_DIR_HOST</code> Host config directory <code>/etc/krakenhashes</code> <code>./config</code> <code>KH_DATA_DIR_HOST</code> Host data directory <code>/var/lib/krakenhashes</code> <code>./data</code>"},{"location":"admin-guide/system-setup/configuration/#nginx-configuration","title":"Nginx Configuration","text":"Variable Description Default Example <code>NGINX_ACCESS_LOG_LEVEL</code> Nginx access log level <code>info</code> <code>debug</code> <code>NGINX_ERROR_LOG_LEVEL</code> Nginx error log level <code>warn</code> <code>error</code> <code>NGINX_CLIENT_MAX_BODY_SIZE</code> Max request body size <code>50M</code> <code>100M</code>"},{"location":"admin-guide/system-setup/configuration/#cors-configuration","title":"CORS Configuration","text":"Variable Description Default Example <code>CORS_ALLOWED_ORIGIN</code> Allowed CORS origins <code>https://localhost:443</code> <code>https://app.example.com</code>"},{"location":"admin-guide/system-setup/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"admin-guide/system-setup/configuration/#directory-structure","title":"Directory Structure","text":"<p>The system creates and uses the following directory structure:</p> <pre><code>${KH_CONFIG_DIR}/\n\u251c\u2500\u2500 certs/              # TLS certificates\n\u2502   \u251c\u2500\u2500 ca.crt         # CA certificate\n\u2502   \u251c\u2500\u2500 ca.key         # CA private key\n\u2502   \u251c\u2500\u2500 server.crt     # Server certificate\n\u2502   \u2514\u2500\u2500 server.key     # Server private key\n\u2514\u2500\u2500 config/            # Application configuration\n\n${KH_DATA_DIR}/\n\u251c\u2500\u2500 binaries/          # Hashcat and other tools\n\u251c\u2500\u2500 wordlists/         # Wordlist files\n\u2502   \u251c\u2500\u2500 general/       # General purpose wordlists\n\u2502   \u251c\u2500\u2500 specialized/   # Domain-specific wordlists\n\u2502   \u251c\u2500\u2500 targeted/      # Target-specific wordlists\n\u2502   \u2514\u2500\u2500 custom/        # User-uploaded wordlists\n\u251c\u2500\u2500 rules/             # Rule files\n\u2502   \u251c\u2500\u2500 hashcat/       # Hashcat rule files\n\u2502   \u251c\u2500\u2500 john/          # John the Ripper rules\n\u2502   \u2514\u2500\u2500 custom/        # Custom rule files\n\u251c\u2500\u2500 hashlists/         # Hash files\n\u2514\u2500\u2500 hashlist_uploads/  # Temporary upload directory\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<ul> <li>Backend: No configuration files - all settings via environment variables</li> <li>Frontend: <code>.env</code> file in frontend directory (development only)</li> <li>Agent: Configuration stored in <code>${KH_CONFIG_DIR}/agent.json</code> (auto-generated)</li> <li>Docker: <code>.env</code> file in project root for docker-compose</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#common-configuration-scenarios","title":"Common Configuration Scenarios","text":""},{"location":"admin-guide/system-setup/configuration/#development-setup","title":"Development Setup","text":"<pre><code># .env file for development\nDEBUG=true\nLOG_LEVEL=DEBUG\nKH_TLS_MODE=self-signed\nJWT_SECRET=dev-secret-key\nDB_HOST=localhost\nDB_PASSWORD=dev-password\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#production-with-lets-encrypt","title":"Production with Let's Encrypt","text":"<pre><code># .env file for production\nDEBUG=false\nLOG_LEVEL=INFO\nKH_TLS_MODE=certbot\nKH_CERTBOT_DOMAIN=kraken.example.com\nKH_CERTBOT_EMAIL=admin@example.com\nCLOUDFLARE_API_TOKEN=your-token\nJWT_SECRET=secure-random-secret\nDB_PASSWORD=strong-password\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#high-security-environment","title":"High-Security Environment","text":"<pre><code># .env file for high security\nKH_TLS_MODE=provided\nKH_CERT_FILE=/etc/ssl/certs/server.crt\nKH_KEY_FILE=/etc/ssl/private/server.key\nKH_CA_FILE=/etc/ssl/certs/ca-bundle.crt\nKH_CERT_KEY_SIZE=4096\nJWT_SECRET=very-long-secure-secret\nDEBUG=false\nDEBUG_SQL=false\nDEBUG_HTTP=false\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#agent-configuration_1","title":"Agent Configuration","text":"<pre><code># Agent environment\nKH_CONFIG_DIR=/opt/krakenhashes/config\nKH_DATA_DIR=/opt/krakenhashes/data\nHASHCAT_EXTRA_PARAMS=-O -w 3\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#docker-production-deployment","title":"Docker Production Deployment","text":"<pre><code># Production docker-compose override\nPUID=1000\nPGID=1000\nKH_CONFIG_DIR_HOST=/opt/krakenhashes/config\nKH_DATA_DIR_HOST=/data/krakenhashes\nLOG_DIR=/var/log/krakenhashes\nNGINX_CLIENT_MAX_BODY_SIZE=100M\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Security:</li> <li>Always use strong, unique values for <code>JWT_SECRET</code> in production</li> <li>Never commit <code>.env</code> files with secrets to version control</li> <li> <p>Use environment-specific configurations</p> </li> <li> <p>File Permissions:</p> </li> <li>Set <code>PUID</code> and <code>PGID</code> to match your host user for proper permissions</li> <li> <p>Ensure certificate files have restricted permissions (600 or 640)</p> </li> <li> <p>TLS/SSL:</p> </li> <li>Use Let's Encrypt (<code>certbot</code> mode) for production</li> <li>Self-signed certificates only for development</li> <li> <p>Always include all required DNS names and IP addresses</p> </li> <li> <p>Performance:</p> </li> <li>Adjust <code>KH_HASHLIST_BATCH_SIZE</code> based on available memory</li> <li>Configure <code>NGINX_CLIENT_MAX_BODY_SIZE</code> for expected file sizes</li> <li> <p>Set appropriate WebSocket timeouts for your network</p> </li> <li> <p>Logging:</p> </li> <li>Use <code>INFO</code> level for production</li> <li>Enable component-specific debugging only when needed</li> <li> <p>Regularly rotate log files</p> </li> <li> <p>Database:</p> </li> <li>Use strong passwords in production</li> <li>Consider using SSL for database connections in production</li> <li>Regular backups of the PostgreSQL data volume</li> </ol>"},{"location":"admin-guide/system-setup/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/configuration/#common-issues","title":"Common Issues","text":"<ol> <li>Certificate Errors:</li> <li>Check <code>KH_ADDITIONAL_DNS_NAMES</code> includes all hostnames</li> <li>Verify certificate paths are correct</li> <li> <p>Ensure proper file permissions</p> </li> <li> <p>Database Connection:</p> </li> <li>Verify database credentials</li> <li>Check network connectivity between containers</li> <li> <p>Ensure PostgreSQL is healthy before backend starts</p> </li> <li> <p>File Upload Issues:</p> </li> <li>Check <code>KH_MAX_UPLOAD_SIZE_MB</code> setting</li> <li>Verify <code>NGINX_CLIENT_MAX_BODY_SIZE</code> is sufficient</li> <li> <p>Ensure data directories have proper permissions</p> </li> <li> <p>WebSocket Disconnections:</p> </li> <li>Adjust timeout values (<code>KH_PONG_WAIT</code>, <code>KH_PING_PERIOD</code>)</li> <li>Check for proxy/firewall interference</li> <li>Verify WebSocket URL configuration</li> </ol>"},{"location":"admin-guide/system-setup/email/","title":"Email Settings Administration","text":""},{"location":"admin-guide/system-setup/email/#overview","title":"Overview","text":"<p>KrakenHashes supports email functionality through multiple providers: SendGrid, Mailgun, and SMTP. This document covers the configuration and management of email settings through the admin interface.</p>"},{"location":"admin-guide/system-setup/email/#provider-configuration","title":"Provider Configuration","text":"<p> Admin Settings Email Configuration page showing Mailgun provider setup with API Key, Domain, From Name, From Email, and Monthly Limit fields</p>"},{"location":"admin-guide/system-setup/email/#sendgrid","title":"SendGrid","text":"<p>To configure SendGrid as your email provider:</p> <ol> <li>Select \"SendGrid\" from the Provider dropdown</li> <li>Configure the following fields:</li> <li>API Key: Your SendGrid API key with email sending permissions</li> <li>From Email: The verified sender email address</li> <li>From Name: Display name for the sender (defaults to \"KrakenHashes\")</li> <li>Monthly Limit: (Optional) Set a monthly email sending limit</li> </ol>"},{"location":"admin-guide/system-setup/email/#mailgun","title":"Mailgun","text":"<p>To configure Mailgun as your email provider:</p> <ol> <li>Select \"Mailgun\" from the Provider dropdown</li> <li>Configure the following fields:</li> <li>API Key: Your Mailgun API key</li> <li>Domain: Your verified Mailgun domain</li> <li>From Email: The verified sender email address</li> <li>From Name: Display name for the sender (defaults to \"KrakenHashes\")</li> <li>Monthly Limit: (Optional) Set a monthly email sending limit</li> </ol>"},{"location":"admin-guide/system-setup/email/#smtp","title":"SMTP","text":"<p>To configure a custom SMTP server as your email provider:</p> <ol> <li>Select \"SMTP\" from the Provider dropdown</li> <li>Configure the following fields:</li> <li>Host: Your SMTP server hostname (e.g., <code>smtp.gmail.com</code>, <code>smtp.office365.com</code>)</li> <li>Port: SMTP server port (auto-selected based on encryption if not specified)</li> <li>Username: SMTP authentication username</li> <li>Password: SMTP authentication password</li> <li>From Email: The sender email address</li> <li>From Name: Display name for the sender (defaults to \"KrakenHashes\")</li> <li>Encryption: Select encryption mode:<ul> <li>None: Plain SMTP without encryption (port 25, not recommended for production)</li> <li>STARTTLS: Start with plain connection and upgrade to TLS (port 587, recommended)</li> <li>TLS/SSL: Direct TLS connection (port 465, legacy but widely supported)</li> </ul> </li> <li>Skip TLS Verify: \u26a0\ufe0f Only enable for self-signed certificates (security risk)</li> </ol>"},{"location":"admin-guide/system-setup/email/#smtp-configuration-notes","title":"SMTP Configuration Notes","text":"<p>Default Ports: - None (no encryption): Port 25 - STARTTLS: Port 587 (recommended for most providers) - TLS/SSL: Port 465 (implicit TLS)</p> <p>Connection Timeouts: - All SMTP connections have a 30-second timeout to prevent indefinite hanging - If connection fails, check firewall rules and server availability</p> <p>Password Management: - When updating existing SMTP configuration, you can leave the password field empty to keep the current password - The password is stored securely and displayed as <code>[REDACTED]</code> in the UI - Only enter a password when creating new configuration or changing the existing password</p> <p>Supported SMTP Providers: - Gmail (requires App Password if 2FA enabled) - Office 365 / Outlook.com - Custom mail servers - Self-hosted SMTP servers</p> <p>Security Warning: - The \"Skip TLS Verify\" option should only be used for self-signed certificates in trusted environments - Always use STARTTLS or TLS/SSL encryption in production - Never use \"None\" encryption for production email</p>"},{"location":"admin-guide/system-setup/email/#monthly-limit","title":"Monthly Limit","text":"<p>The monthly limit field is optional: - Leave empty for unlimited emails - Set a numeric value to limit monthly email sending - Helps prevent unexpected costs from email service providers</p>"},{"location":"admin-guide/system-setup/email/#testing-and-saving-configuration","title":"Testing and Saving Configuration","text":""},{"location":"admin-guide/system-setup/email/#configuration-options","title":"Configuration Options","text":"<p>When saving email provider settings, you have three options:</p> <ol> <li>Cancel: Discard changes and return to previous settings</li> <li>Save Configuration: Save settings without testing</li> <li>Test and Save: Test the configuration before saving</li> </ol>"},{"location":"admin-guide/system-setup/email/#testing-process","title":"Testing Process","text":"<p>When using \"Test and Save\":</p> <ol> <li>Enter a test email address</li> <li>System sends a test email to verify configuration</li> <li>If successful:</li> <li>Configuration is saved</li> <li>Confirmation message displayed</li> <li>If failed:</li> <li>Error message displayed</li> <li>Configuration not saved</li> <li>Troubleshooting information provided</li> </ol>"},{"location":"admin-guide/system-setup/email/#email-templates","title":"Email Templates","text":"<p>Email templates are managed separately from provider configuration.</p>"},{"location":"admin-guide/system-setup/email/#best-practices","title":"Best Practices","text":"<ol> <li>Provider Selection</li> <li>Choose based on your volume needs</li> <li>Consider provider-specific features</li> <li> <p>Review pricing structures</p> </li> <li> <p>Configuration Testing</p> </li> <li>Always test configuration before deployment</li> <li>Verify emails are received</li> <li> <p>Check spam folder during testing</p> </li> <li> <p>Monthly Limits</p> </li> <li>Set based on expected usage</li> <li>Include buffer for unexpected spikes</li> <li> <p>Monitor usage through provider dashboards</p> </li> <li> <p>Security Considerations</p> </li> <li>Store API keys securely</li> <li>Use dedicated sending domains</li> <li>Regularly rotate API keys</li> <li>Monitor for unusual activity</li> </ol>"},{"location":"admin-guide/system-setup/email/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/email/#common-issues","title":"Common Issues","text":"<ol> <li>Emails Not Sending</li> <li>Verify API key permissions</li> <li>Check monthly limit hasn't been reached</li> <li>Confirm sender email is verified</li> <li> <p>Review provider dashboard for blocks</p> </li> <li> <p>Test Emails Failing</p> </li> <li>Verify API key is correct (SendGrid, Mailgun)</li> <li>Verify SMTP credentials are correct (SMTP)</li> <li>Check domain configuration (Mailgun)</li> <li>Check port and encryption settings (SMTP)</li> <li>Ensure test email address is valid</li> <li> <p>Review error messages in admin interface</p> </li> <li> <p>SMTP Connection Issues</p> </li> <li>Verify hostname is correct and reachable</li> <li>Check if port is blocked by firewall</li> <li>Confirm encryption mode matches server requirements</li> <li>For Gmail: Use App Password, not account password</li> <li>For Office 365: Ensure SMTP AUTH is enabled</li> <li> <p>Connection timeout (30 seconds) may indicate network issues</p> </li> <li> <p>Template Issues</p> </li> <li>Verify template syntax</li> <li>Check variable names match expected format</li> <li>Preview templates before saving</li> <li>Test with various data scenarios </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/","title":"SSL/TLS Setup Guide for KrakenHashes","text":"<p>This guide explains how to set up and trust the self-signed SSL certificates used by KrakenHashes on various platforms.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#overview","title":"Overview","text":"<p>KrakenHashes uses self-signed certificates by default to secure communications between components. When you first access the web interface, your browser will warn you about the untrusted certificate. This is expected behavior for self-signed certificates.</p> <p>To avoid these warnings, you need to install the KrakenHashes Certificate Authority (CA) certificate as a trusted root certificate on your system.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#downloading-the-ca-certificate","title":"Downloading the CA Certificate","text":"<p>The CA certificate can be downloaded from: - HTTP endpoint: <code>http://your-server:1337/ca.crt</code> - Via the web interface: When you see the certificate warning, there's usually an option to download the CA certificate</p> <p>Save the file as <code>krakenhashes-ca.crt</code> for the following instructions.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#installation-instructions-by-platform","title":"Installation Instructions by Platform","text":""},{"location":"admin-guide/system-setup/ssl-tls/#linux-systems","title":"Linux Systems","text":""},{"location":"admin-guide/system-setup/ssl-tls/#debianubuntu","title":"Debian/Ubuntu","text":"<ol> <li> <p>Copy the CA certificate to the system certificate directory:    <pre><code>sudo cp krakenhashes-ca.crt /usr/local/share/ca-certificates/\n</code></pre></p> </li> <li> <p>Update the certificate store:    <pre><code>sudo update-ca-certificates\n</code></pre></p> </li> <li> <p>You should see output indicating the certificate was added:    <pre><code>Updating certificates in /etc/ssl/certs...\n1 added, 0 removed; done.\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#fedorarhelcentosrocky-linux","title":"Fedora/RHEL/CentOS/Rocky Linux","text":"<ol> <li> <p>Copy the CA certificate to the trust anchors directory:    <pre><code>sudo cp krakenhashes-ca.crt /etc/pki/ca-trust/source/anchors/\n</code></pre></p> </li> <li> <p>Update the trust store:    <pre><code>sudo update-ca-trust\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#arch-linux","title":"Arch Linux","text":"<p>Using trust anchor command (Recommended): <pre><code># This is the official Arch way - it handles everything automatically\nsudo trust anchor --store krakenhashes-ca.crt\n</code></pre></p> <p>Note: The <code>trust anchor --store</code> command automatically: - Places the certificate in <code>/etc/ca-certificates/trust-source/anchors/</code> - Updates the trust database with <code>update-ca-trust</code> - Handles p11-kit integration properly</p>"},{"location":"admin-guide/system-setup/ssl-tls/#additional-step-for-chromechromium-on-linux","title":"Additional Step for Chrome/Chromium on Linux","text":"<p>Chrome and Chromium browsers maintain their own certificate store on Linux. After installing the system certificate, you also need to:</p> <ol> <li> <p>Install the certificate in the NSS database:    <pre><code>certutil -d sql:$HOME/.pki/nssdb -A -t \"CP,CP,\" -n \"KrakenHashes Root CA\" -i krakenhashes-ca.crt\n</code></pre></p> </li> <li> <p>Restart Chrome/Chromium for the changes to take effect.</p> </li> </ol> <p>To verify the certificate was installed: <pre><code>certutil -d sql:$HOME/.pki/nssdb -L\n</code></pre></p> <p>To remove the certificate later if needed: <pre><code>certutil -d sql:$HOME/.pki/nssdb -D -n \"KrakenHashes Root CA\"\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#windows","title":"Windows","text":""},{"location":"admin-guide/system-setup/ssl-tls/#using-certificate-manager-gui","title":"Using Certificate Manager (GUI)","text":"<ol> <li>Press <code>Win + R</code>, type <code>certmgr.msc</code>, and press Enter</li> <li>Navigate to \"Trusted Root Certification Authorities\" &gt; \"Certificates\"</li> <li>Right-click on \"Certificates\" and select \"All Tasks\" &gt; \"Import...\"</li> <li>Follow the Certificate Import Wizard:</li> <li>Click \"Next\"</li> <li>Browse and select the <code>krakenhashes-ca.crt</code> file</li> <li>Click \"Next\"</li> <li>Ensure \"Place all certificates in the following store\" is selected with \"Trusted Root Certification Authorities\"</li> <li>Click \"Next\" and then \"Finish\"</li> <li>When prompted with a security warning, click \"Yes\" to install the certificate</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#using-powershell-administrator","title":"Using PowerShell (Administrator)","text":"<pre><code># Import the certificate to the Trusted Root store\nImport-Certificate -FilePath \"C:\\path\\to\\krakenhashes-ca.crt\" -CertStoreLocation Cert:\\LocalMachine\\Root\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#using-command-prompt-administrator","title":"Using Command Prompt (Administrator)","text":"<pre><code>certutil -addstore \"Root\" \"C:\\path\\to\\krakenhashes-ca.crt\"\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#macos","title":"macOS","text":""},{"location":"admin-guide/system-setup/ssl-tls/#using-keychain-access-gui","title":"Using Keychain Access (GUI)","text":"<ol> <li>Double-click the <code>krakenhashes-ca.crt</code> file</li> <li>Keychain Access will open</li> <li>Select \"System\" keychain (you may need to authenticate)</li> <li>The certificate will be added but marked as untrusted</li> <li>Double-click on the \"KrakenHashes Root CA\" certificate</li> <li>Expand the \"Trust\" section</li> <li>Change \"When using this certificate\" to \"Always Trust\"</li> <li>Close the window and authenticate to save changes</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#using-command-line","title":"Using Command Line","text":"<pre><code># Add certificate to system keychain\nsudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain krakenhashes-ca.crt\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#verification","title":"Verification","text":"<p>After installing the certificate, you can verify it's working:</p> <ol> <li>Browser Test: Navigate to <code>https://your-server:31337</code> - you should no longer see certificate warnings</li> <li>Command Line Test:     <pre><code>curl https://your-server:31337/health\n# Should work without certificate errors\n</code></pre></li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-details","title":"Certificate Details","text":"<p>The self-signed certificates generated by KrakenHashes include:</p> <ul> <li>CA Certificate: Valid for 10 years (3650 days)</li> <li>Server Certificate: Valid for 1 year (365 days)</li> <li>Key Size: 4096-bit RSA (configurable)</li> <li>Extensions: </li> <li>SubjectKeyIdentifier and AuthorityKeyIdentifier for proper chain validation</li> <li>BasicConstraints properly set (CA:TRUE for CA, CA:FALSE for server)</li> <li>Appropriate KeyUsage and ExtendedKeyUsage flags</li> </ul>"},{"location":"admin-guide/system-setup/ssl-tls/#environment-variables","title":"Environment Variables","text":"<p>You can customize certificate generation with these environment variables:</p> <ul> <li><code>KH_ADDITIONAL_DNS_NAMES</code>: Comma-separated additional DNS names (e.g., \"krakenhashes.local,kraken.internal\")</li> <li><code>KH_ADDITIONAL_IP_ADDRESSES</code>: Comma-separated additional IP addresses (e.g., \"192.168.1.100,10.0.0.50\")</li> <li><code>KH_CERT_KEY_SIZE</code>: RSA key size (2048 or 4096, default: 4096)</li> <li><code>KH_CERT_VALIDITY_DAYS</code>: Server certificate validity in days (default: 365)</li> <li><code>KH_CA_VALIDITY_DAYS</code>: CA certificate validity in days (default: 3650)</li> </ul>"},{"location":"admin-guide/system-setup/ssl-tls/#security-considerations","title":"Security Considerations","text":"<ol> <li>Self-signed certificates are suitable for:</li> <li>Development environments</li> <li>Internal networks</li> <li>Testing and staging</li> <li> <p>Environments where you control all clients</p> </li> <li> <p>For production use, consider:</p> </li> <li>Using certificates from a trusted CA (Let's Encrypt, etc.)</li> <li>Implementing proper certificate rotation</li> <li> <p>Using the <code>certbot</code> mode if you have a public domain</p> </li> <li> <p>Certificate Storage:</p> </li> <li>Private keys are stored with 0600 permissions</li> <li>Certificates are stored in the configured certs directory</li> <li>Default location: <code>~/.krakenhashes/certs/</code> or <code>/etc/krakenhashes/certs/</code> in Docker</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#regenerating-certificates","title":"Regenerating Certificates","text":"<p>If you need to regenerate certificates (e.g., to add new SANs):</p> <ol> <li>Stop the KrakenHashes backend</li> <li>Delete the existing certificates:    <pre><code>rm -rf ~/.krakenhashes/certs/*\n# Or your configured certs directory\n</code></pre></li> <li>Start the backend - new certificates will be generated automatically</li> <li>Reinstall the new CA certificate on client systems</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-modes","title":"Certificate Modes","text":"<p>KrakenHashes supports three TLS modes:</p> <ol> <li>self-signed (default): Automatically generates and manages certificates</li> <li>provided: Use your own certificates (set paths via environment variables)</li> <li>certbot: Integration with Let's Encrypt for public domains</li> </ol> <p>To change modes, set the <code>KH_TLS_MODE</code> environment variable.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#certbot-mode-lets-encrypt","title":"Certbot Mode (Let's Encrypt)","text":"<p>This mode allows you to use Let's Encrypt certificates via Certbot with Cloudflare DNS-01 challenge. This is ideal for internal applications that aren't publicly accessible.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#prerequisites","title":"Prerequisites","text":"<ol> <li>Domain Name: You need to own a domain (e.g., <code>zerkersec.io</code>)</li> <li>Cloudflare Account: Your domain must be managed by Cloudflare</li> <li>API Token: Create a Cloudflare API token with proper permissions</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-1-create-cloudflare-api-token","title":"Step 1: Create Cloudflare API Token","text":"<ol> <li>Log in to your Cloudflare dashboard</li> <li>Go to My Profile \u2192 API Tokens</li> <li>Click Create Token</li> <li>Use the Custom token template with these permissions:</li> <li>Zone \u2192 DNS \u2192 Edit</li> <li>Zone \u2192 Zone \u2192 Read (optional but recommended)</li> <li>Under Zone Resources, select:</li> <li>Include \u2192 Specific zone \u2192 Your domain (e.g., <code>zerkersec.io</code>)</li> <li>Click Continue to summary and Create Token</li> <li>Save the token securely - you won't be able to see it again!</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-2-dns-configuration","title":"Step 2: DNS Configuration","text":"<p>Create an A record for your subdomain:</p> <ol> <li>In Cloudflare dashboard, go to your domain</li> <li>Navigate to DNS \u2192 Records</li> <li>Add a new A record:</li> <li>Type: A</li> <li>Name: <code>kraken</code> (or your chosen subdomain)</li> <li>IPv4 address: Your internal IP (e.g., <code>10.0.0.100</code>)</li> <li>Proxy status: DNS only (gray cloud)</li> <li>TTL: Auto</li> </ol> <p>Note: The IP address doesn't need to be publicly accessible. Certbot only needs to verify domain ownership via DNS TXT records.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#step-3-configure-krakenhashes","title":"Step 3: Configure KrakenHashes","text":"<p>Edit your <code>.env</code> file:</p> <pre><code># Change TLS mode to certbot\nKH_TLS_MODE=certbot\n\n# Certbot Configuration\nKH_CERTBOT_DOMAIN=kraken.zerkersec.io    # Your full domain\nKH_CERTBOT_EMAIL=admin@zerkersec.io      # Your email for Let's Encrypt\nKH_CERTBOT_STAGING=true                  # Start with staging for testing!\nKH_CERTBOT_AUTO_RENEW=true               # Enable automatic renewal\n\n# Cloudflare Configuration\nCLOUDFLARE_API_TOKEN=your-token-here     # The token from Step 1\n\n# Update frontend URLs to use your domain\nREACT_APP_API_URL=https://kraken.zerkersec.io:31337\nREACT_APP_WS_URL=wss://kraken.zerkersec.io:31337\n\n# Update CORS to allow your domain\nCORS_ALLOWED_ORIGIN=https://kraken.zerkersec.io\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#step-4-initial-setup-with-staging","title":"Step 4: Initial Setup with Staging","text":"<p>Important: Always test with Let's Encrypt staging environment first to avoid rate limits!</p> <ol> <li> <p>Stop existing containers:    <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>Start with certbot mode:    <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>Monitor the logs:    <pre><code>docker-compose logs -f krakenhashes\n</code></pre></p> </li> <li> <p>Look for messages indicating successful certificate generation:    <pre><code>Obtaining certificates for domain: kraken.zerkersec.io\nSuccessfully obtained certificates\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-5-verify-staging-certificates","title":"Step 5: Verify Staging Certificates","text":"<ol> <li> <p>Check certificate files:    <pre><code>ls -la kh-backend/config/certs/live/kraken.zerkersec.io/\n</code></pre></p> </li> <li> <p>You should see:</p> </li> <li><code>fullchain.pem</code> - Certificate chain</li> <li><code>privkey.pem</code> - Private key</li> <li><code>chain.pem</code> - Intermediate certificates</li> <li> <p><code>cert.pem</code> - Domain certificate</p> </li> <li> <p>Test the application (you'll get a browser warning for staging certificates)</p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-6-switch-to-production","title":"Step 6: Switch to Production","text":"<p>Once staging certificates work:</p> <ol> <li> <p>Update <code>.env</code>:    <pre><code>KH_CERTBOT_STAGING=false\n</code></pre></p> </li> <li> <p>Remove staging certificates:    <pre><code>rm -rf kh-backend/config/certs/live/*\nrm -rf kh-backend/config/certs/archive/*\nrm -rf kh-backend/config/certs/renewal/*\n</code></pre></p> </li> <li> <p>Restart to get production certificates:    <pre><code>docker-compose down\ndocker-compose up -d\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-renewal","title":"Certificate Renewal","text":"<p>Certificates are automatically renewed:</p> <ul> <li>Renewal checks run twice daily (3 AM and 3 PM)</li> <li>Certificates renew when less than 30 days remain</li> <li>Services reload automatically after renewal</li> </ul> <p>Manual Renewal: <pre><code>docker exec krakenhashes /usr/local/bin/certbot-renew.sh\n</code></pre></p> <p>Monitor Renewal: <pre><code>docker exec krakenhashes tail -f /var/log/krakenhashes/certbot-renew.log\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#certbot-troubleshooting","title":"Certbot Troubleshooting","text":""},{"location":"admin-guide/system-setup/ssl-tls/#common-issues","title":"Common Issues","text":"<ol> <li>\"CLOUDFLARE_API_TOKEN environment variable is required\"</li> <li>Ensure the token is set in your <code>.env</code> file</li> <li> <p>Token must have DNS:Edit permissions</p> </li> <li> <p>\"Failed to obtain certificates\"</p> </li> <li>Check Cloudflare API token permissions</li> <li>Verify domain ownership</li> <li> <p>Check certbot logs: <code>docker exec krakenhashes cat /etc/krakenhashes/certs/logs/letsencrypt.log</code></p> </li> <li> <p>Browser still shows certificate warnings</p> </li> <li>Ensure you switched from staging to production</li> <li>Clear browser cache</li> <li> <p>Verify certificates: <code>docker exec krakenhashes openssl x509 -in /etc/krakenhashes/certs/live/kraken.zerkersec.io/cert.pem -text -noout</code></p> </li> <li> <p>Rate Limits</p> </li> <li>Let's Encrypt has rate limits (50 certificates per domain per week)</li> <li>Always test with staging first</li> <li>See: https://letsencrypt.org/docs/rate-limits/</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-information","title":"Certificate Information","text":"<p>View certificate details: <pre><code># Inside container\ndocker exec krakenhashes certbot certificates --config-dir /etc/krakenhashes/certs\n\n# Certificate expiry\ndocker exec krakenhashes openssl x509 -enddate -noout -in /etc/krakenhashes/certs/live/kraken.zerkersec.io/cert.pem\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#additional-domains","title":"Additional Domains","text":"<p>To add more domains/subdomains:</p> <ol> <li> <p>Add them to <code>KH_ADDITIONAL_DNS_NAMES</code> in <code>.env</code>:    <pre><code>KH_ADDITIONAL_DNS_NAMES=kraken.zerkersec.io,api.zerkersec.io,*.kraken.zerkersec.io\n</code></pre></p> </li> <li> <p>Ensure all domains are in Cloudflare and accessible by your API token</p> </li> <li> <p>Restart the container to obtain certificates for all domains</p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#migration-from-self-signed","title":"Migration from Self-Signed","text":"<p>If migrating from self-signed certificates:</p> <ol> <li>Back up existing certificates (optional)</li> <li>Update <code>.env</code> as shown above</li> <li>Restart containers</li> <li>Update any clients/browsers that have the old CA certificate cached</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/ssl-tls/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"admin-guide/system-setup/ssl-tls/#browser-still-shows-certificate-warnings","title":"Browser Still Shows Certificate Warnings","text":"<ol> <li>Certificate not properly installed:</li> <li>Verify the certificate is in the correct store (Trusted Root, not Personal or Intermediate)</li> <li>Check that you installed the CA certificate, not the server certificate</li> <li> <p>Some browsers cache certificate decisions - try clearing browser data or using incognito mode</p> </li> <li> <p>Browser-specific issues:</p> </li> <li>Chrome/Edge: May require restart after certificate installation</li> <li>Firefox: Has its own certificate store - you may need to import via Firefox settings</li> <li> <p>Safari: Requires explicit trust settings in Keychain Access</p> </li> <li> <p>Certificate regenerated:</p> </li> <li>If the backend regenerated certificates, you need to reinstall the new CA certificate</li> <li>Remove the old certificate first to avoid conflicts</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#err_ssl_key_usage_incompatible-chromeedge","title":"ERR_SSL_KEY_USAGE_INCOMPATIBLE (Chrome/Edge)","text":"<p>This error indicates the certificate doesn't have the correct key usage flags. The updated certificate generation should prevent this, but if you encounter it:</p> <ol> <li>Ensure you're running the latest version with the certificate improvements</li> <li>Delete existing certificates and let them regenerate</li> <li>As a temporary workaround on Windows:    <pre><code># Create registry key to disable the check (use with caution)\nNew-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Edge\" -Name \"RSAKeyUsageForLocalAnchorsEnabled\" -Value 0 -PropertyType DWORD\n# For Chrome:\nNew-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Google\\Chrome\" -Name \"RSAKeyUsageForLocalAnchorsEnabled\" -Value 0 -PropertyType DWORD\n</code></pre></li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-not-trusted-on-linux","title":"Certificate Not Trusted on Linux","text":"<ol> <li> <p>Check certificate format:    <pre><code># Verify it's a valid PEM certificate\nopenssl x509 -in krakenhashes-ca.crt -text -noout\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code># Check if certificate is in the trust store\ntrust list | grep -i krakenhashes\n\n# For Debian/Ubuntu, check the symlink was created\nls -la /etc/ssl/certs/ | grep krakenhashes\n</code></pre></p> </li> <li> <p>SELinux issues (RHEL/Fedora):    <pre><code># Check for SELinux denials\nsudo ausearch -m avc -ts recent\n\n# If needed, restore context\nsudo restorecon -v /etc/pki/ca-trust/source/anchors/krakenhashes-ca.crt\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-chain-issues","title":"Certificate Chain Issues","text":"<ol> <li> <p>Verify the full chain is being sent:    <pre><code># Check what certificates the server is presenting\nopenssl s_client -connect your-server:31337 -showcerts\n</code></pre></p> </li> <li> <p>Validate certificate chain:    <pre><code># Download server certificate\necho | openssl s_client -connect your-server:31337 -servername your-server 2&gt;/dev/null | \\\n  openssl x509 &gt; server.crt\n\n# Verify against CA\nopenssl verify -CAfile krakenhashes-ca.crt server.crt\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#connection-refused-or-timeout","title":"Connection Refused or Timeout","text":"<ol> <li> <p>Check the service is running:    <pre><code># Check if ports are listening\nnetstat -tlnp | grep -E \"1337|31337\"\n# or\nss -tlnp | grep -E \"1337|31337\"\n</code></pre></p> </li> <li> <p>Firewall rules:    <pre><code># Check firewall status\nsudo iptables -L -n | grep -E \"1337|31337\"\n# or for firewalld\nsudo firewall-cmd --list-all\n</code></pre></p> </li> <li> <p>Docker networking (if using Docker):</p> </li> <li>Ensure ports are properly mapped in docker-compose.yml</li> <li>Check Docker network connectivity</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"admin-guide/system-setup/ssl-tls/#view-certificate-details","title":"View Certificate Details","text":"<pre><code># View CA certificate details\nopenssl x509 -in krakenhashes-ca.crt -text -noout\n\n# Check certificate dates\nopenssl x509 -in krakenhashes-ca.crt -noout -dates\n\n# Check certificate subject and issuer\nopenssl x509 -in krakenhashes-ca.crt -noout -subject -issuer\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#test-tls-connection","title":"Test TLS Connection","text":"<pre><code># Test with curl (verbose)\ncurl -v https://your-server:31337/health\n\n# Test with openssl\nopenssl s_client -connect your-server:31337 -CAfile krakenhashes-ca.crt\n\n# Test cipher suites\nnmap --script ssl-enum-ciphers -p 31337 your-server\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#browser-certificate-debugging","title":"Browser Certificate Debugging","text":"<ol> <li>Chrome: Navigate to <code>chrome://settings/certificates</code> to manage certificates</li> <li>Firefox: Navigate to <code>about:preferences#privacy</code> &gt; View Certificates</li> <li>Edge: Navigate to <code>edge://settings/privacy</code> &gt; Manage certificates</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#removing-old-certificates","title":"Removing Old Certificates","text":"<p>If you need to remove old certificates before installing new ones:</p> <p>Arch Linux: <pre><code># Find and remove the certificate\nsudo rm -f /etc/ca-certificates/trust-source/anchors/krakenhashes-ca.crt\nsudo update-ca-trust\n\n# Or use trust anchor to remove by name\nsudo trust anchor --remove \"KrakenHashes Root CA\"\n\n# Remove from Chrome/Chromium\ncertutil -d sql:$HOME/.pki/nssdb -D -n \"KrakenHashes Root CA\"\n</code></pre></p> <p>Debian/Ubuntu: <pre><code>sudo rm /usr/local/share/ca-certificates/krakenhashes-ca.crt\nsudo update-ca-certificates --fresh\n</code></pre></p> <p>Fedora/RHEL: <pre><code>sudo rm /etc/pki/ca-trust/source/anchors/krakenhashes-ca.crt\nsudo update-ca-trust\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues:</p> <ol> <li> <p>Check the backend logs for certificate generation errors:    <pre><code>grep -i \"certificate\\|tls\\|ssl\" /path/to/backend.log\n</code></pre></p> </li> <li> <p>Ensure your environment variables are set correctly</p> </li> <li>Try regenerating certificates with a clean state</li> <li>Report issues at: https://github.com/ZerkerEOD/krakenhashes/issues</li> </ol>"},{"location":"admin-guide/system-setup/sso/","title":"SSO Authentication","text":""},{"location":"admin-guide/system-setup/sso/#overview","title":"Overview","text":"<p>KrakenHashes supports Single Sign-On (SSO) authentication, allowing users to log in using their existing enterprise identity providers. This reduces password fatigue, centralizes access control, and improves security through federated authentication.</p>"},{"location":"admin-guide/system-setup/sso/#supported-provider-types","title":"Supported Provider Types","text":"<p>KrakenHashes supports four SSO provider types:</p> Provider Type Description Use Case LDAP/AD Lightweight Directory Access Protocol Active Directory, OpenLDAP SAML 2.0 Security Assertion Markup Language Enterprise IdPs (Okta, OneLogin, ADFS) OpenID Connect Modern OAuth2-based identity layer Authentik, Keycloak, Azure AD, Google OAuth 2.0 Authorization framework GitHub, custom OAuth providers"},{"location":"admin-guide/system-setup/sso/#configuration","title":"Configuration","text":""},{"location":"admin-guide/system-setup/sso/#global-sso-settings","title":"Global SSO Settings","text":"<p>Navigate to Admin Panel \u2192 SSO Settings to configure global authentication options:</p> <ol> <li>Local Authentication Enabled</li> <li>Toggle to enable/disable username/password login</li> <li>Default: Enabled</li> <li> <p>Warning: Ensure at least one admin has SSO access before disabling</p> </li> <li> <p>Auto-Create Users (JIT Provisioning)</p> </li> <li>When enabled, new users are automatically created on first SSO login</li> <li>Default: Enabled</li> <li> <p>User information is populated from the identity provider</p> </li> <li> <p>Auto-Enable Users</p> </li> <li>When enabled, auto-created users are immediately active</li> <li>Default: Disabled (requires admin approval)</li> <li>When disabled, new users see \"Pending Approval\" message</li> </ol>"},{"location":"admin-guide/system-setup/sso/#provider-specific-settings","title":"Provider-Specific Settings","text":"<p>Each provider can override global settings:</p> <ul> <li>Override Auto-Create: Enable/disable JIT provisioning per provider</li> <li>Override Auto-Enable: Enable/disable auto-activation per provider</li> </ul>"},{"location":"admin-guide/system-setup/sso/#sso-encryption-key","title":"SSO Encryption Key","text":""},{"location":"admin-guide/system-setup/sso/#purpose","title":"Purpose","text":"<p>The <code>SSO_ENCRYPTION_KEY</code> environment variable protects sensitive SSO secrets stored in the database using AES-256-GCM encryption.</p>"},{"location":"admin-guide/system-setup/sso/#what-gets-encrypted","title":"What Gets Encrypted","text":"Secret Type Description LDAP Bind Password Service account password for LDAP queries SAML SP Private Key Private key for signing SAML requests OAuth Client Secret Secret for OAuth token exchange"},{"location":"admin-guide/system-setup/sso/#key-requirements","title":"Key Requirements","text":"<ul> <li>Size: Exactly 32 bytes (256 bits)</li> <li>Format: Base64-encoded string OR raw 32-byte string</li> <li>Algorithm: AES-256-GCM with 96-bit random nonce per encryption</li> </ul>"},{"location":"admin-guide/system-setup/sso/#generating-a-key","title":"Generating a Key","text":"<pre><code># Generate a secure encryption key\nopenssl rand -base64 32\n</code></pre> <p>Example output: <code>K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=</code></p>"},{"location":"admin-guide/system-setup/sso/#configuration_1","title":"Configuration","text":"<p>Add to your environment or <code>.env</code> file:</p> <pre><code>SSO_ENCRYPTION_KEY=K7gNU3sdo+OL0wNhqoVWhr3g6s1xYv72ol/pe/Unols=\n</code></pre> <p>Production Requirement</p> <p>Always set <code>SSO_ENCRYPTION_KEY</code> in production. Without it, the system generates an ephemeral key that is lost on restart, making all encrypted secrets unrecoverable.</p> <p>Ephemeral Key Behavior</p> <p>If <code>SSO_ENCRYPTION_KEY</code> is not set:</p> <ul> <li>A random 32-byte key is generated at startup</li> <li>Log warning: \"SSO_ENCRYPTION_KEY not set - generating ephemeral key\"</li> <li>All encrypted secrets become invalid after restart</li> <li>Suitable only for development/testing</li> </ul>"},{"location":"admin-guide/system-setup/sso/#high-availability-deployments","title":"High Availability Deployments","text":"<p>When running multiple backend instances:</p> <ul> <li>All instances must use the same <code>SSO_ENCRYPTION_KEY</code></li> <li>Store the key in a secrets manager (Vault, AWS Secrets Manager, etc.)</li> <li>Rotate keys by re-encrypting all secrets with a new key</li> </ul>"},{"location":"admin-guide/system-setup/sso/#ldap-configuration","title":"LDAP Configuration","text":""},{"location":"admin-guide/system-setup/sso/#basic-setup","title":"Basic Setup","text":"<ol> <li>Navigate to Admin Panel \u2192 SSO Settings \u2192 Add Provider</li> <li>Select LDAP/Active Directory</li> <li>Configure the following:</li> </ol> Field Description Example Name Display name for login page <code>Corporate LDAP</code> Server URL LDAP server address <code>ldap://ldap.example.com:389</code> or <code>ldaps://ldap.example.com:636</code> Bind DN Service account distinguished name <code>cn=svc-krakenhashes,ou=Service Accounts,dc=example,dc=com</code> Bind Password Service account password (encrypted in database) Base DN Search base for users <code>ou=Users,dc=example,dc=com</code> User Filter LDAP filter for user lookup <code>(&amp;(objectClass=person)(sAMAccountName=%s))</code>"},{"location":"admin-guide/system-setup/sso/#attribute-mappings","title":"Attribute Mappings","text":"Attribute Purpose Common Values Email User's email address <code>mail</code>, <code>userPrincipalName</code> Username Login identifier <code>sAMAccountName</code>, <code>uid</code> Display Name Friendly name <code>displayName</code>, <code>cn</code>"},{"location":"admin-guide/system-setup/sso/#tls-settings","title":"TLS Settings","text":"Option Description Use StartTLS Upgrade connection to TLS (port 389) Skip Certificate Verification Disable cert validation (not recommended) Custom CA Certificate PEM-encoded CA cert for self-signed servers"},{"location":"admin-guide/system-setup/sso/#mfa-behavior","title":"MFA Behavior","text":"<p>LDAP + Local MFA</p> <p>LDAP authentication requires local MFA verification after successful bind. This provides an additional security layer since LDAP doesn't inherently support MFA.</p>"},{"location":"admin-guide/system-setup/sso/#saml-20-configuration","title":"SAML 2.0 Configuration","text":""},{"location":"admin-guide/system-setup/sso/#prerequisites","title":"Prerequisites","text":"<p>Before configuring SAML, you'll need from your Identity Provider (IdP):</p> <ul> <li>IdP Entity ID</li> <li>IdP SSO URL</li> <li>IdP Certificate (for signature verification)</li> </ul>"},{"location":"admin-guide/system-setup/sso/#basic-setup_1","title":"Basic Setup","text":"<ol> <li>Navigate to Admin Panel \u2192 SSO Settings \u2192 Add Provider</li> <li>Select SAML 2.0</li> <li>Configure the required fields:</li> </ol> Field Description Example Name Display name <code>Okta SSO</code> SP Entity ID KrakenHashes identifier <code>https://krakenhashes.example.com</code> IdP Entity ID Identity provider identifier <code>http://www.okta.com/exk123abc</code> IdP SSO URL Login endpoint <code>https://dev-123.okta.com/app/sso/saml</code> IdP Certificate Public cert for signature validation PEM format"},{"location":"admin-guide/system-setup/sso/#rsa-key-requirements-for-saml-trust","title":"RSA Key Requirements for SAML Trust","text":""},{"location":"admin-guide/system-setup/sso/#service-provider-sp-keys","title":"Service Provider (SP) Keys","text":"<p>SP keys are automatically generated when you create or save a SAML provider. This simplifies configuration since manual key generation is no longer required.</p> <p>Automatic Key Generation:</p> Property Value Key Size 2048-bit RSA Certificate Validity 10 years Algorithm RSA with SHA-256 Common Name SP Entity ID <p>When you create a new SAML provider or save an existing one without keys, KrakenHashes automatically:</p> <ol> <li>Generates a 2048-bit RSA private key</li> <li>Creates a self-signed certificate valid for 10 years</li> <li>Encrypts and stores the private key in the database</li> <li>Makes the certificate available via the SP metadata endpoint</li> </ol> <p>Manual Key Generation (Optional):</p> <p>If you prefer to use your own keys (e.g., CA-signed certificates), you can still provide them:</p> <p>Key Format Requirements:</p> Requirement Details Key Format PKCS#8 (recommended) or PKCS#1, PEM-encoded Certificate Format PEM or base64-encoded DER Self-Signed Fully supported CA-Signed Fully supported Key-Cert Match Certificate must correspond to private key <p>Generating Custom Self-Signed SP Certificates:</p> <pre><code># Generate a self-signed certificate valid for 1 year\nopenssl req -x509 -newkey rsa:2048 \\\n  -keyout sp-key.pem \\\n  -out sp-cert.pem \\\n  -days 365 \\\n  -nodes \\\n  -subj \"/CN=KrakenHashes SAML SP\"\n\n# View the certificate\nopenssl x509 -in sp-cert.pem -text -noout\n</code></pre> <ul> <li>Copy contents of <code>sp-key.pem</code> to SP Private Key field</li> <li>Copy contents of <code>sp-cert.pem</code> to SP Certificate field</li> </ul>"},{"location":"admin-guide/system-setup/sso/#identity-provider-idp-certificate","title":"Identity Provider (IdP) Certificate","text":"<ul> <li>Always required for SAML providers</li> <li>Used to verify assertion signatures</li> <li>Obtain from your IdP's metadata or admin console</li> <li>Format: PEM or base64-encoded DER</li> </ul>"},{"location":"admin-guide/system-setup/sso/#sp-metadata-endpoint","title":"SP Metadata Endpoint","text":"<p>KrakenHashes exposes SP metadata for IdP configuration:</p> <pre><code>GET /api/auth/saml/{provider_id}/metadata\n</code></pre> <p>This endpoint:</p> <ul> <li>Is publicly accessible (no authentication required)</li> <li>Returns XML metadata in SAML 2.0 format</li> <li>Includes SP certificate in <code>KeyDescriptor</code> element</li> <li>Provides ACS URL and entity ID</li> </ul> <p>ACS URL Format: <pre><code>https://your-domain.com/api/auth/saml/{provider_id}/acs\n</code></pre></p>"},{"location":"admin-guide/system-setup/sso/#assertion-settings","title":"Assertion Settings","text":"Option Description Default Sign Requests Sign AuthnRequests with SP key Yes (always enabled) Require Signed Assertions Verify IdP signature Yes Require Encrypted Assertions Decrypt assertions with SP key No <p>Request Signing</p> <p>Request signing is always enabled because SP keys are automatically generated. This improves security by ensuring all AuthnRequests are cryptographically signed.</p>"},{"location":"admin-guide/system-setup/sso/#mfa-behavior_1","title":"MFA Behavior","text":"<p>SAML MFA Trust</p> <p>SAML authentication trusts the IdP's MFA. If your IdP requires MFA, KrakenHashes will not prompt for additional local MFA.</p>"},{"location":"admin-guide/system-setup/sso/#oauth-20-openid-connect-configuration","title":"OAuth 2.0 / OpenID Connect Configuration","text":""},{"location":"admin-guide/system-setup/sso/#oidc-vs-oauth-20","title":"OIDC vs OAuth 2.0","text":"Feature OIDC OAuth 2.0 ID Token Yes No Discovery Yes (<code>/.well-known/openid-configuration</code>) No User Info Standardized Provider-specific Use Case Identity + Authorization Authorization only"},{"location":"admin-guide/system-setup/sso/#basic-setup-oidc","title":"Basic Setup (OIDC)","text":"<ol> <li>Navigate to Admin Panel \u2192 SSO Settings \u2192 Add Provider</li> <li>Select OpenID Connect</li> <li>Configure:</li> </ol> Field Description Example Name Display name <code>Authentik</code> Client ID Application identifier <code>krakenhashes-app</code> Client Secret Application secret (encrypted in database) Discovery URL OIDC configuration endpoint <code>https://auth.example.com/application/o/krakenhashes/.well-known/openid-configuration</code>"},{"location":"admin-guide/system-setup/sso/#callback-url","title":"Callback URL","text":"<p>Your IdP needs the OAuth callback URL:</p> <pre><code>https://your-domain.com/api/auth/oauth/{provider_id}/callback\n</code></pre> <p>This URL is displayed in the provider card after creation.</p>"},{"location":"admin-guide/system-setup/sso/#oauth-20-manual-configuration","title":"OAuth 2.0 Manual Configuration","text":"<p>For non-OIDC providers, disable discovery and configure manually:</p> Field Description Authorization URL OAuth authorize endpoint Token URL Token exchange endpoint User Info URL User profile endpoint JWKS URL JSON Web Key Set endpoint (optional)"},{"location":"admin-guide/system-setup/sso/#scopes","title":"Scopes","text":"<p>Default scopes: <code>openid profile email</code></p> <p>Additional scopes may be required depending on your IdP and attribute needs.</p>"},{"location":"admin-guide/system-setup/sso/#signature-algorithm-requirements","title":"Signature Algorithm Requirements","text":"<p>RS256 Required</p> <p>KrakenHashes requires ID tokens signed with RS256 (RSA-SHA256). If your IdP uses HS256 (HMAC-SHA256), you must reconfigure it.</p> <p>Why RS256?</p> <ul> <li>RS256 uses asymmetric cryptography (only IdP can sign)</li> <li>HS256 uses symmetric cryptography (anyone with client_secret can forge tokens)</li> </ul> <p>Configuring Authentik for RS256:</p> <ol> <li>Go to Authentik Admin \u2192 Applications \u2192 Providers</li> <li>Edit your OIDC provider</li> <li>Set Signing Key to an RSA key</li> <li>Save changes</li> </ol>"},{"location":"admin-guide/system-setup/sso/#mfa-behavior_2","title":"MFA Behavior","text":"<p>OAuth/OIDC MFA Trust</p> <p>OAuth and OIDC authentication trusts the IdP's MFA. No additional local MFA is required.</p>"},{"location":"admin-guide/system-setup/sso/#username-attribute-fallback","title":"Username Attribute Fallback","text":"<p>KrakenHashes automatically detects usernames from common claim names. The configured Username Attribute is tried first, followed by these fallbacks:</p> Order Claim Name Description 1 (configured attribute) Your custom username attribute 2 <code>preferred_username</code> OIDC standard claim 3 <code>username</code> Common claim name 4 <code>user_name</code> Some providers use this 5 <code>login</code> GitHub uses this 6 <code>nickname</code> Some providers 7 <code>name</code> Fallback to display name <p>If no username is found after trying all fallbacks, the email address is used as the username.</p> <p>Provider-Specific Notes</p> <ul> <li>GitHub: Uses <code>login</code> claim for username</li> <li>Google: Uses <code>email</code> as username (no username claim)</li> <li>Azure AD: Uses <code>preferred_username</code> or <code>upn</code></li> <li>Authentik/Keycloak: Uses <code>preferred_username</code> by default</li> </ul>"},{"location":"admin-guide/system-setup/sso/#sso-user-accounts","title":"SSO User Accounts","text":""},{"location":"admin-guide/system-setup/sso/#just-in-time-jit-provisioning","title":"Just-In-Time (JIT) Provisioning","text":"<p>When a user authenticates via SSO for the first time:</p> <ol> <li>Email Matching: System checks for existing user with matching email</li> <li>Identity Linking: If found, SSO identity is linked to existing account</li> <li>User Creation: If not found and auto-create is enabled, new user is created</li> <li>Account Status: New users are enabled or disabled based on auto-enable setting</li> </ol>"},{"location":"admin-guide/system-setup/sso/#account-linking","title":"Account Linking","text":"<p>Users can have multiple SSO identities linked:</p> <ul> <li>View linked accounts in User Settings \u2192 Linked Accounts</li> <li>Unlink accounts (if not the last authentication method)</li> <li>Link additional providers while logged in</li> </ul>"},{"location":"admin-guide/system-setup/sso/#sso-user-password-generation","title":"SSO User Password Generation","text":""},{"location":"admin-guide/system-setup/sso/#why-random-passwords","title":"Why Random Passwords?","text":"<p>SSO users authenticate through external identity providers and don't need local passwords. However, the database requires a <code>password_hash</code> value. Instead of modifying the database schema, KrakenHashes generates an unguessable random password.</p>"},{"location":"admin-guide/system-setup/sso/#how-it-works","title":"How It Works","text":"<ol> <li>Generate 48 cryptographically random bytes using <code>crypto/rand</code></li> <li>Encode to base64 \u2192 64-character string</li> <li>Hash with bcrypt (cost factor 10)</li> <li>Store the hash in the database</li> </ol>"},{"location":"admin-guide/system-setup/sso/#security-properties","title":"Security Properties","text":"Property Value Entropy 384 bits (48 random bytes) Password Length 64 characters (base64) Bcrypt Compliance Under 72-byte maximum Guessability Computationally impossible Local Login Impossible without admin reset"},{"location":"admin-guide/system-setup/sso/#admin-override","title":"Admin Override","text":"<p>Administrators can reset an SSO user's password through User Management, enabling local login as a fallback if the SSO provider is unavailable.</p>"},{"location":"admin-guide/system-setup/sso/#per-user-authentication-overrides","title":"Per-User Authentication Overrides","text":"<p>Administrators can override global authentication settings per user:</p> Override Purpose Local Auth Override Force enable/disable local login for specific user SSO Auth Override Force enable/disable SSO for specific user Override Notes Document reason for override <p>Use Cases:</p> <ul> <li>Emergency admin access when SSO is down</li> <li>Restricting specific users to SSO only</li> <li>Service accounts that require local authentication</li> </ul>"},{"location":"admin-guide/system-setup/sso/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/system-setup/sso/#security","title":"Security","text":"<ol> <li>Always set <code>SSO_ENCRYPTION_KEY</code> in production</li> <li>Use RS256 for OIDC token signing</li> <li>Enable assertion signing for SAML</li> <li>Use LDAPS or StartTLS for LDAP connections</li> <li>Regularly rotate SP certificates and encryption keys</li> <li>Monitor failed login attempts in audit logs</li> </ol>"},{"location":"admin-guide/system-setup/sso/#deployment","title":"Deployment","text":"<ol> <li>Test thoroughly before disabling local authentication</li> <li>Keep at least one admin with local auth override</li> <li>Document recovery procedures for IdP outages</li> <li>Use the same encryption key across all backend instances</li> </ol>"},{"location":"admin-guide/system-setup/sso/#user-experience","title":"User Experience","text":"<ol> <li>Provide clear error messages for SSO failures</li> <li>Document SSO options for end users</li> <li>Configure appropriate session timeouts</li> <li>Test all provider types before production deployment</li> </ol>"},{"location":"admin-guide/system-setup/sso/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/sso/#common-issues","title":"Common Issues","text":"<ol> <li>\"Invalid signature algorithm HS256\"</li> <li>Cause: IdP signing tokens with HS256 instead of RS256</li> <li> <p>Solution: Configure IdP to use RS256 signing key</p> </li> <li> <p>\"Issuer mismatch\"</p> </li> <li>Cause: Trailing slash inconsistency in discovery URL</li> <li> <p>Solution: Ensure discovery URL matches IdP exactly (including trailing slashes)</p> </li> <li> <p>\"Account processing failed\"</p> </li> <li>Cause: Various server-side errors</li> <li> <p>Solution: Check backend logs for specific error message</p> </li> <li> <p>\"Pending approval\"</p> </li> <li>Cause: Auto-enable users is disabled</li> <li> <p>Solution: Admin must enable the user account</p> </li> <li> <p>\"SSO provider not found\"</p> </li> <li>Cause: Provider failed to load after creation</li> <li> <p>Solution: Check provider configuration and use Test Connection</p> </li> <li> <p>\"Failed to decrypt assertion\"</p> </li> <li>Cause: Missing or incorrect SP private key</li> <li> <p>Solution: Ensure SP private key matches the certificate sent to IdP</p> </li> <li> <p>LDAP \"Invalid credentials\"</p> </li> <li>Cause: Wrong bind DN or password</li> <li>Solution: Verify service account credentials with <code>ldapsearch</code></li> </ol>"},{"location":"admin-guide/system-setup/sso/#diagnostic-steps","title":"Diagnostic Steps","text":"<ol> <li>Check backend logs for detailed error messages</li> <li>Use Test Connection to verify provider configuration</li> <li>Verify IdP configuration matches KrakenHashes settings</li> <li>Check network connectivity to IdP servers</li> <li>Validate certificates haven't expired</li> </ol>"},{"location":"admin-guide/system-setup/sso/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the GitHub Issues</li> <li>Join the Discord community</li> <li>Review IdP-specific documentation</li> </ol>"},{"location":"agent-guide/","title":"Agent Guide","text":""},{"location":"agent-guide/#overview","title":"Overview","text":"<p>Agents are the computational workhorses of KrakenHashes, responsible for executing password cracking jobs using hashcat. This guide covers all aspects of agent deployment, configuration, and management.</p>"},{"location":"agent-guide/#quick-start","title":"Quick Start","text":"<ol> <li>Download the agent binary from the Releases page</li> <li>Generate a claim code in the Admin UI</li> <li>Register the agent:    <pre><code>./krakenhashes-agent -claim YOUR_CLAIM_CODE -host IP:31337\n</code></pre></li> <li>Start the agent:    <pre><code>./krakenhashes-agent\n</code></pre></li> </ol>"},{"location":"agent-guide/#guide-contents","title":"Guide Contents","text":""},{"location":"agent-guide/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Installation - Installing and setting up agents</li> <li>Configuration - Agent configuration options</li> <li>Systemd Service Setup - Running agents as systemd services</li> <li>File Synchronization - How agents sync files with the backend</li> </ul>"},{"location":"agent-guide/#operations","title":"Operations","text":"<ul> <li>Scheduling - Configure working hours and availability</li> <li>Device Management - GPU and device configuration</li> <li>Monitoring - Performance metrics and health checks</li> </ul>"},{"location":"agent-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Common Issues - Solutions to frequent problems</li> <li>Logs and Debugging - Finding and understanding agent logs</li> </ul>"},{"location":"agent-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"agent-guide/#agent-registration","title":"Agent Registration","text":"<p>Agents use a claim code system for secure registration: - One-time codes: Single use, automatically deactivated - Continuous codes: Can register multiple agents - API keys: Generated during registration for ongoing authentication</p>"},{"location":"agent-guide/#device-support","title":"Device Support","text":"<p>KrakenHashes agents support multiple device types: - NVIDIA GPUs (CUDA) - AMD GPUs (OpenCL) - Intel GPUs (OpenCL) - CPU-based cracking (fallback)</p>"},{"location":"agent-guide/#file-management","title":"File Management","text":"<p>Agents automatically manage required files: - Wordlists are downloaded on-demand - Rules are cached locally - Hashcat binaries are auto-updated - All files are verified using checksums</p>"},{"location":"agent-guide/#security","title":"Security","text":"<p>Agent security features: - TLS encrypted communication - API key authentication - No inbound connections required - Certificate validation</p>"},{"location":"agent-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Install your first agent</li> <li>Configure agent scheduling</li> <li>Learn about file synchronization</li> </ul>"},{"location":"agent-guide/configuration/","title":"Agent Configuration","text":""},{"location":"agent-guide/configuration/#overview","title":"Overview","text":"<p>This guide covers all configuration options available for KrakenHashes agents.</p>"},{"location":"agent-guide/configuration/#configuration-file","title":"Configuration File","text":"<p>The agent uses a <code>.env</code> configuration file that is automatically created on first run. The file is located in the agent's working directory and contains all necessary configuration.</p>"},{"location":"agent-guide/configuration/#location","title":"Location","text":"<ul> <li>Default: <code>./.env</code> (in the current working directory)</li> <li>Custom: Specified via command-line flags on first run</li> </ul>"},{"location":"agent-guide/configuration/#configuration-management","title":"Configuration Management","text":"<p>The <code>.env</code> file is automatically managed by the agent: - Created on first run with values from command-line flags - Updated with any missing configuration keys on subsequent runs - Preserves existing values when new options are added</p>"},{"location":"agent-guide/configuration/#manual-editing","title":"Manual Editing","text":"<p>You can manually edit the <code>.env</code> file to adjust configuration:</p> <pre><code># Stop the agent (choose based on your setup)\n\n# For manual run: Ctrl+C in the terminal or kill the process\n# For user service:\nsystemctl --user stop krakenhashes-agent\n# For system service:\nsudo systemctl stop krakenhashes-agent\n\n# Edit the configuration\nnano .env\n\n# Restart the agent\n\n# For manual run:\n./krakenhashes-agent\n# For user service:\nsystemctl --user start krakenhashes-agent\n# For system service:\nsudo systemctl start krakenhashes-agent\n</code></pre> <p>Note: If you haven't set up systemd yet, see the Systemd Service Setup guide for automatic startup and easier management.</p>"},{"location":"agent-guide/configuration/#environment-variables-env-file","title":"Environment Variables (.env File)","text":"<p>The agent uses a <code>.env</code> file for configuration, which is automatically created during first run. The file is loaded at startup and values are NOT taken from system environment variables to avoid conflicts when running on the same host as the backend.</p>"},{"location":"agent-guide/configuration/#complete-env-file-example","title":"Complete .env File Example","text":"<p>This is the actual <code>.env</code> file generated by the agent on first run:</p> <pre><code># KrakenHashes Agent Configuration\n# Generated on: 2025-09-05T12:05:32+01:00\n\n# Server Configuration\nKH_HOST=your-server.example.com  # Backend server hostname\nKH_PORT=31337                    # Backend server port\nUSE_TLS=true                     # Use TLS for secure communication (wss:// and https://)\nLISTEN_INTERFACE=                # Network interface to bind to (leave empty for all)\nHEARTBEAT_INTERVAL=5             # Heartbeat interval in seconds\n\n# Agent Configuration\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE  # Claim code for first-time registration (auto-commented after success)\n\n# Directory Configuration\nKH_CONFIG_DIR=./config  # Configuration directory for certificates and credentials\nKH_DATA_DIR=./data      # Data directory for binaries, wordlists, rules, and hashlists\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s   # Timeout for writing messages to WebSocket\nKH_PONG_WAIT=60s    # Timeout for receiving pong from server\nKH_PING_PERIOD=54s  # Interval for sending ping to server (must be less than pong wait)\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Maximum number of concurrent file downloads\nKH_DOWNLOAD_TIMEOUT=1h         # Timeout for large file downloads\n\n# Hashcat Configuration\nHASHCAT_EXTRA_PARAMS=  # Extra parameters to pass to hashcat (e.g., \"-O -w 3\" for optimized kernels and high workload)\n\n# Logging Configuration\nDEBUG=false            # Enable debug logging\nLOG_LEVEL=INFO        # Log level (DEBUG, INFO, WARNING, ERROR)\n</code></pre>"},{"location":"agent-guide/configuration/#important-hashcat-parameter-precedence","title":"Important: Hashcat Parameter Precedence","text":"<p>HASHCAT_EXTRA_PARAMS Behavior: - Parameters configured in the frontend/backend (per-agent settings) take precedence over the agent's .env file - The agent's .env <code>HASHCAT_EXTRA_PARAMS</code> is only used as a fallback when the backend doesn't send any parameters - Best Practice: Configure agent parameters via the frontend UI for centralized management - Only use .env parameters for local overrides that should NOT be managed by the backend</p> <p>Parameter Priority (highest to lowest): 1. Backend/Frontend per-agent settings (stored in database) 2. Agent .env file <code>HASHCAT_EXTRA_PARAMS</code> (fallback only)</p>"},{"location":"agent-guide/configuration/#manual-env-file-creation","title":"Manual .env File Creation","text":"<p>You can manually create a <code>.env</code> file for agent registration instead of using command-line flags:</p> <ol> <li>Create a <code>.env</code> file in the agent's working directory</li> <li>Copy the example above and fill in your values:</li> <li>Set <code>KH_HOST</code> to your backend server hostname</li> <li>Set <code>KH_PORT</code> to your backend server port (usually 31337)</li> <li>Set <code>KH_CLAIM_CODE</code> to your claim code (get from Admin UI)</li> <li>Adjust directory paths as needed</li> <li>Run the agent without any flags: <code>./krakenhashes-agent</code></li> <li>After successful registration, the claim code will be automatically commented out</li> </ol> <p>Note: The agent reads from the <code>.env</code> file, not from system environment variables. This prevents conflicts when running the agent and backend on the same host.</p>"},{"location":"agent-guide/configuration/#command-line-options","title":"Command Line Options","text":"<pre><code>krakenhashes-agent [flags]\n\nFlags:\n  -host string           Backend server host (e.g., localhost:31337)\n  -tls                   Use TLS for secure communication (default: true)\n  -interface string      Network interface to listen on (optional)\n  -heartbeat int         Heartbeat interval in seconds (default: 5)\n  -claim string          Agent claim code (required only for first-time registration)\n  -debug                 Enable debug logging (default: false)\n  -hashcat-params string Extra parameters to pass to hashcat (e.g., '-O -w 3')\n  -config-dir string     Configuration directory for certificates and credentials\n  -data-dir string       Data directory for binaries, wordlists, rules, and hashlists\n  -help                  Show help\n</code></pre>"},{"location":"agent-guide/configuration/#example-usage","title":"Example Usage","text":"<pre><code># First-time registration from agent directory\ncd ~/krakenhashes-agent\n./krakenhashes-agent \\\n  -host your-server:31337 \\\n  -claim YOUR_CLAIM_CODE \\\n  -debug\n\n# Subsequent runs (uses .env file created during first run)\ncd ~/krakenhashes-agent\n./krakenhashes-agent\n\n# Override specific settings\n./krakenhashes-agent -debug -hashcat-params \"-O -w 4\"\n</code></pre>"},{"location":"agent-guide/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are applied in this order (later overrides earlier): 1. Default values 2. <code>.env</code> file values (created/updated on first run) 3. Command line flags</p> <p>Important: The agent does NOT read from system environment variables to avoid conflicts when running on the same host as the backend. All configuration is handled through the <code>.env</code> file and command-line flags.</p>"},{"location":"agent-guide/configuration/#device-configuration","title":"Device Configuration","text":""},{"location":"agent-guide/configuration/#enablingdisabling-devices","title":"Enabling/Disabling Devices","text":"<p>You can control which devices the agent uses:</p> <pre><code>devices:\n  # Disable specific device IDs\n  disabled_devices:\n    - 0  # Disable first GPU\n\n  # Or only enable specific devices\n  enabled_devices:\n    - 1\n    - 2\n</code></pre>"},{"location":"agent-guide/configuration/#device-specific-settings","title":"Device-Specific Settings","text":"<pre><code>devices:\n  # Per-device temperature limits\n  device_temps:\n    0: 80  # Device 0 max temp\n    1: 85  # Device 1 max temp\n\n  # Per-device workload\n  device_workloads:\n    0: 2  # Lower workload for device 0\n    1: 4  # Higher workload for device 1\n</code></pre>"},{"location":"agent-guide/configuration/#security-configuration","title":"Security Configuration","text":""},{"location":"agent-guide/configuration/#tlsssl-settings","title":"TLS/SSL Settings","text":"<pre><code>tls:\n  # Skip certificate verification (not recommended)\n  insecure_skip_verify: false\n\n  # Custom CA certificate\n  ca_cert_file: /etc/krakenhashes/ca.crt\n\n  # Client certificates (if required)\n  client_cert_file: /etc/krakenhashes/client.crt\n  client_key_file: /etc/krakenhashes/client.key\n</code></pre>"},{"location":"agent-guide/configuration/#api-key-security","title":"API Key Security","text":"<ul> <li>API keys are stored encrypted in the config file</li> <li>Keys are never logged or displayed after registration</li> <li>Regenerate keys if compromised</li> </ul>"},{"location":"agent-guide/configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"agent-guide/configuration/#memory-management","title":"Memory Management","text":"<pre><code>performance:\n  # Hashcat memory settings\n  hashcat_memory_limit: 4096  # MB per device\n\n  # System memory reservation\n  system_memory_reserve: 2048  # MB to leave free\n\n  # File cache settings\n  max_cache_size: 10240  # MB for wordlists/rules\n</code></pre>"},{"location":"agent-guide/configuration/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>performance:\n  # GPU utilization target\n  gpu_utilization_target: 90  # Percent\n\n  # Kernel tuning\n  kernel_accel: 0  # 0=auto, or specific value\n  kernel_loops: 0  # 0=auto, or specific value\n\n  # Power management\n  gpu_power_tune: 0  # Percent adjustment (-50 to +50)\n</code></pre>"},{"location":"agent-guide/configuration/#monitoring-configuration","title":"Monitoring Configuration","text":"<pre><code>monitoring:\n  # Metrics collection\n  collect_metrics: true\n  metrics_interval: 30  # seconds\n\n  # Hardware monitoring\n  monitor_temps: true\n  monitor_fan_speed: true\n  monitor_power: true\n  monitor_memory: true\n\n  # Alerts\n  alerts:\n    high_temp_threshold: 85\n    low_hashrate_threshold: 1000000  # H/s\n    error_rate_threshold: 0.05  # 5%\n</code></pre>"},{"location":"agent-guide/configuration/#scheduling-configuration","title":"Scheduling Configuration","text":"<p>See Agent Scheduling for detailed scheduling configuration.</p>"},{"location":"agent-guide/configuration/#troubleshooting-configuration-issues","title":"Troubleshooting Configuration Issues","text":""},{"location":"agent-guide/configuration/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to see configuration loading: <pre><code>cd ~/krakenhashes-agent\n./krakenhashes-agent -debug\n</code></pre></p> <p>This will show: - Configuration file loading - Connection attempts - Certificate validation - File synchronization</p>"},{"location":"agent-guide/configuration/#common-issues","title":"Common Issues","text":"<ol> <li>Permission Denied: Ensure agent user can read config file</li> <li>Invalid YAML: Use a YAML validator</li> <li>Missing Required Fields: Check server URL and data directory</li> <li>Environment Variable Conflicts: Check for conflicting env vars</li> </ol>"},{"location":"agent-guide/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use Configuration Management: Store configs in Git/Ansible</li> <li>Secure API Keys: Use appropriate file permissions (600)</li> <li>Monitor Logs: Set up log rotation and monitoring</li> <li>Test Changes: Validate config before restarting agent</li> <li>Document Custom Settings: Keep notes on non-default values</li> </ol>"},{"location":"agent-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Set up agent scheduling</li> <li>Configure file synchronization</li> <li>Monitor agent performance</li> </ul>"},{"location":"agent-guide/debugging/","title":"Agent Debugging Guide","text":"<p>This guide covers debugging the KrakenHashes agent, including enabling debug mode, interpreting logs, and using development tools to diagnose issues.</p>"},{"location":"agent-guide/debugging/#quick-start-enable-debug-mode","title":"Quick Start - Enable Debug Mode","text":"<p>The fastest way to enable debug logging:</p> <pre><code># Method 1: Command line flag\n./krakenhashes-agent --debug --host backend.example.com:31337\n\n# Method 2: Environment variable\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n./krakenhashes-agent --host backend.example.com:31337\n\n# Method 3: Edit .env file\necho \"DEBUG=true\" &gt;&gt; .env\necho \"LOG_LEVEL=DEBUG\" &gt;&gt; .env\n./krakenhashes-agent --host backend.example.com:31337\n</code></pre>"},{"location":"agent-guide/debugging/#debug-configuration-options","title":"Debug Configuration Options","text":""},{"location":"agent-guide/debugging/#command-line-flags","title":"Command Line Flags","text":"<p>The agent supports several debugging-related command line flags:</p> <pre><code>./krakenhashes-agent --help\n\n  --debug               Enable debug logging (default: false)\n  --host string         Backend server host (e.g., localhost:31337)\n  --tls                 Use TLS for secure communication (default: true)\n  --interface string    Network interface to listen on (optional)\n  --heartbeat int       Heartbeat interval in seconds (default: 5)\n  --config-dir string   Configuration directory for certificates and credentials\n  --data-dir string     Data directory for binaries, wordlists, rules, and hashlists\n  --hashcat-params string  Extra parameters to pass to hashcat (e.g., '-O -w 3')\n</code></pre>"},{"location":"agent-guide/debugging/#environment-variables","title":"Environment Variables","text":"<p>Configure debugging through environment variables in <code>.env</code> file:</p> <pre><code># Logging Configuration\nDEBUG=true                    # Enable debug logging\nLOG_LEVEL=DEBUG              # Set minimum log level (DEBUG, INFO, WARNING, ERROR)\n\n# Server Configuration\nKH_HOST=backend.example.com  # Backend hostname\nKH_PORT=31337               # Backend port\nUSE_TLS=true                # Use secure connections\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s           # WebSocket write timeout\nKH_PONG_WAIT=60s            # Server pong timeout\nKH_PING_PERIOD=54s          # Ping interval\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Concurrent download limit\nKH_DOWNLOAD_TIMEOUT=1h         # Download timeout\nKH_MAX_DOWNLOAD_RETRIES=3      # Download retry attempts\n\n# Development Configuration\nHEARTBEAT_INTERVAL=5        # Heartbeat frequency (seconds)\n</code></pre>"},{"location":"agent-guide/debugging/#log-levels","title":"Log Levels","text":"<p>The agent supports four log levels in order of severity:</p> <ol> <li>DEBUG - Detailed diagnostic information</li> <li>INFO - General operational messages</li> <li>WARNING - Potential issues that don't stop operation</li> <li>ERROR - Serious problems that may cause failures</li> </ol> <p>Set the minimum level with <code>LOG_LEVEL</code> environment variable:</p> <pre><code># Show all messages\nLOG_LEVEL=DEBUG\n\n# Show info, warnings, and errors\nLOG_LEVEL=INFO\n\n# Show only warnings and errors\nLOG_LEVEL=WARNING\n\n# Show only errors\nLOG_LEVEL=ERROR\n</code></pre>"},{"location":"agent-guide/debugging/#debug-output-interpretation","title":"Debug Output Interpretation","text":""},{"location":"agent-guide/debugging/#log-message-format","title":"Log Message Format","text":"<p>Debug messages follow this format: <pre><code>[LEVEL] [TIMESTAMP] [FILE:LINE] [FUNCTION] MESSAGE\n</code></pre></p> <p>Example: <pre><code>[DEBUG] [2025-01-10 15:04:05.123] [/path/to/file.go:42] [package.Function] Connecting to backend server\n</code></pre></p>"},{"location":"agent-guide/debugging/#common-debug-messages","title":"Common Debug Messages","text":""},{"location":"agent-guide/debugging/#startup-and-configuration","title":"Startup and Configuration","text":"<pre><code>[INFO] Debug logging initialized - Debug enabled: true\n[INFO] Current working directory: /path/to/agent\n[INFO] Loading agent configuration...\n[INFO] Using config directory: /path/to/config\n[INFO] Using data directory: /path/to/data\n</code></pre>"},{"location":"agent-guide/debugging/#websocket-connection","title":"WebSocket Connection","text":"<pre><code>[DEBUG] Starting WebSocket connection process\n[INFO] Connection attempt 1 of 3\n[DEBUG] WebSocket connected to wss://backend.example.com:31337/ws/agent\n[INFO] Connection attempt 1 successful\n</code></pre>"},{"location":"agent-guide/debugging/#hardware-detection","title":"Hardware Detection","text":"<pre><code>[INFO] Detecting compute devices at startup...\n[DEBUG] Found GPU: NVIDIA RTX 4090 (Device ID: 0)\n[DEBUG] Found GPU: NVIDIA RTX 4080 (Device ID: 1)\n[INFO] Successfully detected and sent device information to server\n</code></pre>"},{"location":"agent-guide/debugging/#file-synchronization","title":"File Synchronization","text":"<pre><code>[INFO] Initializing file sync with max downloads: 3, timeout: 1h0m0s, max retries: 3\n[DEBUG] Scanning wordlists directory: /path/to/data/wordlists\n[INFO] File sync: Found 15 local files, backend has 23 files\n[DEBUG] Downloading missing file: rockyou.txt (14344391 bytes)\n</code></pre>"},{"location":"agent-guide/debugging/#job-execution","title":"Job Execution","text":"<pre><code>[INFO] Received job assignment for task: task_abc123\n[DEBUG] Starting hashcat with command: hashcat -m 1000 -a 0 hashes.txt wordlist.txt\n[DEBUG] Hashcat process started with PID: 12345\n[INFO] Job progress: Task task_abc123, Keyspace 1000000, Hash rate 2500000 H/s\n</code></pre>"},{"location":"agent-guide/debugging/#component-specific-debugging","title":"Component-Specific Debugging","text":""},{"location":"agent-guide/debugging/#websocket-connection-debugging","title":"WebSocket Connection Debugging","text":"<p>Enable verbose WebSocket debugging:</p> <pre><code># Add to .env file\nDEBUG=true\nLOG_LEVEL=DEBUG\n\n# Monitor WebSocket messages\ntail -f agent.log | grep -E \"(WebSocket|WSMessage|connection)\"\n</code></pre> <p>Common WebSocket issues and debugging:</p> <pre><code># Connection timeout\n[ERROR] Failed to create connection on attempt 1: dial tcp: i/o timeout\n\n# Certificate issues\n[ERROR] Failed to load CA certificate: certificate signed by unknown authority\n\n# Authentication failures\n[ERROR] WebSocket handshake failed: HTTP 401 Unauthorized\n</code></pre>"},{"location":"agent-guide/debugging/#file-synchronization-debugging","title":"File Synchronization Debugging","text":"<p>Monitor file sync operations:</p> <pre><code># Filter sync-related logs\ntail -f agent.log | grep -E \"(sync|download|FileInfo)\"\n\n# Debug specific file types\ntail -f agent.log | grep -E \"(wordlist|rule|binary)\"\n</code></pre> <p>File sync debug messages: <pre><code>[DEBUG] Calculating MD5 hash for file: /path/to/wordlist.txt\n[INFO] File sync: Downloading wordlist: rockyou.txt (14MB)\n[WARNING] Download retry 2/3 for file: large_wordlist.txt\n[ERROR] Failed to download file after 3 attempts: connection timeout\n</code></pre></p>"},{"location":"agent-guide/debugging/#job-execution-debugging","title":"Job Execution Debugging","text":"<p>Monitor hashcat job execution:</p> <pre><code># Job-specific logs\ntail -f agent.log | grep -E \"(job|task|hashcat|progress)\"\n\n# Real-time hashcat output\ntail -f agent.log | grep \"hashcat_output\"\n</code></pre> <p>Job debug messages: <pre><code>[INFO] Starting hashcat executor with extra params: -O -w 3\n[DEBUG] Hashcat working directory: /tmp/krakenhashes/task_abc123\n[DEBUG] Hashcat stdout: Session..........: hashcat\n[DEBUG] Hashcat stdout: Status...........: Running\n[INFO] Job completed successfully, found 15 cracked hashes\n</code></pre></p>"},{"location":"agent-guide/debugging/#hardware-detection-debugging","title":"Hardware Detection Debugging","text":"<p>Monitor GPU and hardware detection:</p> <pre><code># Hardware detection logs\ntail -f agent.log | grep -E \"(hardware|GPU|device|monitor)\"\n\n# Device capabilities\ntail -f agent.log | grep -E \"(OpenCL|CUDA|compute)\"\n</code></pre> <p>Hardware debug messages: <pre><code>[DEBUG] Detecting NVIDIA GPUs using nvidia-ml-py\n[INFO] Found NVIDIA GPU: GeForce RTX 4090 (12GB VRAM)\n[DEBUG] GPU compute capability: 8.9\n[WARNING] GPU temperature high: 85\u00b0C\n</code></pre></p>"},{"location":"agent-guide/debugging/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"agent-guide/debugging/#building-debug-builds","title":"Building Debug Builds","text":"<p>Create debug builds with additional debugging information:</p> <pre><code># Build with debug symbols (no optimization)\ncd agent\nmake clean\n\n# Set debug build flags\nexport GOFLAGS=\"-gcflags=-N -gcflags=-l\"\nmake build\n\n# Or build with race detection\ngo build -race -o debug-agent ./cmd/agent\n</code></pre>"},{"location":"agent-guide/debugging/#using-go-debugger-delve","title":"Using Go Debugger (Delve)","text":"<p>Install and use Delve for interactive debugging:</p> <pre><code># Install delve\ngo install github.com/go-delve/delve/cmd/dlv@latest\n\n# Build and debug\ncd agent\ngo build -gcflags=\"all=-N -l\" -o debug-agent ./cmd/agent\n\n# Start debugging session\ndlv exec ./debug-agent -- --debug --host localhost:31337\n\n# Common delve commands:\n# (dlv) break main.main\n# (dlv) continue\n# (dlv) print cfg\n# (dlv) step\n# (dlv) next\n</code></pre>"},{"location":"agent-guide/debugging/#using-pprof-for-performance-profiling","title":"Using pprof for Performance Profiling","text":"<p>Add profiling endpoints for performance analysis:</p> <pre><code>// Add to main.go for profiling (development only)\nimport _ \"net/http/pprof\"\n\n// Start profiling server (development builds only)\ngo func() {\n    log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n}()\n</code></pre> <p>Then profile the running agent:</p> <pre><code># CPU profiling\ngo tool pprof http://localhost:6060/debug/pprof/profile\n\n# Memory profiling\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# Goroutine profiling\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n</code></pre>"},{"location":"agent-guide/debugging/#memory-and-goroutine-analysis","title":"Memory and Goroutine Analysis","text":"<p>Monitor resource usage during development:</p> <pre><code># Monitor memory usage\nwhile true; do\n    ps -p $(pgrep krakenhashes-agent) -o pid,rss,vsz,pcpu,pmem,cmd\n    sleep 5\ndone\n\n# Monitor goroutines (with pprof endpoint)\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n</code></pre>"},{"location":"agent-guide/debugging/#common-debugging-scenarios","title":"Common Debugging Scenarios","text":""},{"location":"agent-guide/debugging/#1-agent-wont-connect-to-backend","title":"1. Agent Won't Connect to Backend","text":"<p>Symptoms: - Connection timeout errors - Authentication failures - Certificate errors</p> <p>Debugging steps: <pre><code># Enable debug logging\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n\n# Test network connectivity\ntelnet backend.example.com 31337\ncurl -k https://backend.example.com:31337/health\n\n# Check certificate issues\nopenssl s_client -connect backend.example.com:31337\n\n# Verify API key and agent ID\ncat config/agent_credentials.json\ncat config/api_key.json\n</code></pre></p>"},{"location":"agent-guide/debugging/#2-file-sync-issues","title":"2. File Sync Issues","text":"<p>Symptoms: - Files not downloading - Constant re-downloading - MD5 hash mismatches</p> <p>Debugging steps: <pre><code># Check file permissions\nls -la data/wordlists/\nls -la data/rules/\n\n# Verify network connectivity for downloads\ncurl -I https://backend.example.com:31337/api/files/download/wordlist/1\n\n# Check available disk space\ndf -h data/\n\n# Manual MD5 verification\nmd5sum data/wordlists/rockyou.txt\n</code></pre></p>"},{"location":"agent-guide/debugging/#3-job-execution-problems","title":"3. Job Execution Problems","text":"<p>Symptoms: - Jobs not starting - Hashcat errors - No progress updates</p> <p>Debugging steps: <pre><code># Check hashcat installation\nwhich hashcat\nhashcat --version\n\n# Test hashcat manually\nhashcat -m 1000 -a 3 --stdout ?d?d?d?d | head -10\n\n# Check GPU availability\nhashcat -I\n\n# Monitor system resources\ntop -p $(pgrep hashcat)\nnvidia-smi  # For NVIDIA GPUs\n</code></pre></p>"},{"location":"agent-guide/debugging/#4-high-memory-usage","title":"4. High Memory Usage","text":"<p>Symptoms: - Agent consuming excessive RAM - System becoming slow - Out of memory errors</p> <p>Debugging steps: <pre><code># Enable memory profiling\nexport DEBUG=true\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# Check for memory leaks\n# Monitor over time with:\nwatch \"ps -p $(pgrep krakenhashes-agent) -o pid,rss,vsz\"\n\n# Reduce concurrent operations\n# In .env file:\nKH_MAX_CONCURRENT_DOWNLOADS=1\nHEARTBEAT_INTERVAL=10\n</code></pre></p>"},{"location":"agent-guide/debugging/#debugging-tools-and-utilities","title":"Debugging Tools and Utilities","text":""},{"location":"agent-guide/debugging/#log-analysis-scripts","title":"Log Analysis Scripts","text":"<p>Create helper scripts for log analysis:</p> <pre><code>#!/bin/bash\n# debug-helper.sh\n\n# Show only error messages\nshow_errors() {\n    grep \"\\[ERROR\\]\" agent.log | tail -20\n}\n\n# Show WebSocket connection events\nshow_websocket() {\n    grep -E \"(WebSocket|connection|disconnect)\" agent.log | tail -20\n}\n\n# Show file sync activity\nshow_sync() {\n    grep -E \"(sync|download|upload)\" agent.log | tail -20\n}\n\n# Show job execution\nshow_jobs() {\n    grep -E \"(job|task|hashcat)\" agent.log | tail -20\n}\n\n# Usage: ./debug-helper.sh show_errors\n$1\n</code></pre>"},{"location":"agent-guide/debugging/#real-time-monitoring","title":"Real-time Monitoring","text":"<p>Monitor agent activity in real-time:</p> <pre><code># Multi-pane monitoring with tmux\ntmux new-session -d -s agent-debug\n\n# Pane 1: Agent output\ntmux send-keys -t agent-debug \"tail -f agent.log\" Enter\n\n# Pane 2: System resources\ntmux split-window -v -t agent-debug\ntmux send-keys -t agent-debug \"htop\" Enter\n\n# Pane 3: Network connections\ntmux split-window -h -t agent-debug\ntmux send-keys -t agent-debug \"watch 'netstat -an | grep :31337'\" Enter\n\n# Attach to session\ntmux attach-session -t agent-debug\n</code></pre>"},{"location":"agent-guide/debugging/#configuration-validation","title":"Configuration Validation","text":"<p>Validate agent configuration:</p> <pre><code>#!/bin/bash\n# validate-config.sh\n\necho \"=== Agent Configuration Validation ===\"\n\n# Check required directories\necho \"Checking directories...\"\n[ -d \"config\" ] &amp;&amp; echo \"\u2705 config/\" || echo \"\u274c config/ missing\"\n[ -d \"data\" ] &amp;&amp; echo \"\u2705 data/\" || echo \"\u274c data/ missing\"\n\n# Check .env file\necho \"Checking .env configuration...\"\nif [ -f \".env\" ]; then\n    echo \"\u2705 .env file exists\"\n    grep -q \"KH_HOST=\" .env &amp;&amp; echo \"\u2705 KH_HOST set\" || echo \"\u274c KH_HOST missing\"\n    grep -q \"DEBUG=\" .env &amp;&amp; echo \"\u2705 DEBUG set\" || echo \"\u274c DEBUG missing\"\nelse\n    echo \"\u274c .env file missing\"\nfi\n\n# Check certificates\necho \"Checking certificates...\"\n[ -f \"config/agent.crt\" ] &amp;&amp; echo \"\u2705 Agent certificate\" || echo \"\u274c Agent certificate missing\"\n[ -f \"config/ca.crt\" ] &amp;&amp; echo \"\u2705 CA certificate\" || echo \"\u274c CA certificate missing\"\n\n# Check API key\n[ -f \"config/api_key.json\" ] &amp;&amp; echo \"\u2705 API key\" || echo \"\u274c API key missing\"\n\necho \"=== Validation Complete ===\"\n</code></pre>"},{"location":"agent-guide/debugging/#automated-testing-and-debugging","title":"Automated Testing and Debugging","text":""},{"location":"agent-guide/debugging/#unit-tests-with-debug-output","title":"Unit Tests with Debug Output","text":"<p>Run tests with verbose output:</p> <pre><code>cd agent\n\n# Run all tests with verbose output\ngo test -v ./...\n\n# Run specific package tests\ngo test -v ./internal/config\ngo test -v ./internal/agent\ngo test -v ./pkg/debug\n\n# Run tests with race detection\ngo test -race -v ./...\n\n# Generate test coverage\ngo test -coverprofile=coverage.out ./...\ngo tool cover -html=coverage.out -o coverage.html\n</code></pre>"},{"location":"agent-guide/debugging/#integration-testing","title":"Integration Testing","text":"<p>Test agent integration with a local backend:</p> <pre><code># Start local backend for testing\ncd ../backend\ndocker-compose -f docker-compose.dev-local.yml up -d\n\n# Test agent connection\ncd ../agent\n./krakenhashes-agent --debug --host localhost:31337\n</code></pre>"},{"location":"agent-guide/debugging/#performance-profiling","title":"Performance Profiling","text":""},{"location":"agent-guide/debugging/#cpu-profiling","title":"CPU Profiling","text":"<p>Profile CPU usage during job execution:</p> <pre><code># Start agent with profiling\ngo tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile?seconds=30\n\n# During heavy computation (hashcat jobs)\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=60\n</code></pre>"},{"location":"agent-guide/debugging/#memory-profiling","title":"Memory Profiling","text":"<p>Identify memory usage patterns:</p> <pre><code># Heap profiling\ngo tool pprof -http=:8080 http://localhost:6060/debug/pprof/heap\n\n# Allocation profiling\ngo tool pprof http://localhost:6060/debug/pprof/allocs\n</code></pre>"},{"location":"agent-guide/debugging/#goroutine-analysis","title":"Goroutine Analysis","text":"<p>Monitor concurrent operations:</p> <pre><code># Goroutine dump\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n\n# Interactive analysis\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n</code></pre>"},{"location":"agent-guide/debugging/#logging-best-practices","title":"Logging Best Practices","text":""},{"location":"agent-guide/debugging/#custom-debug-messages","title":"Custom Debug Messages","text":"<p>Add debug messages to your code:</p> <pre><code>import \"github.com/ZerkerEOD/krakenhashes/agent/pkg/debug\"\n\n// Different log levels\ndebug.Debug(\"Detailed diagnostic: variable=%v\", someVar)\ndebug.Info(\"Operation started: %s\", operation)\ndebug.Warning(\"Potential issue detected: %s\", issue)\ndebug.Error(\"Critical error: %v\", err)\n</code></pre>"},{"location":"agent-guide/debugging/#structured-logging","title":"Structured Logging","text":"<p>Organize debug output by component:</p> <pre><code>// Component-specific logging\ndebug.Info(\"[CONNECTION] WebSocket connected to %s\", url)\ndebug.Info(\"[SYNC] Downloaded file: %s (%d bytes)\", filename, size)\ndebug.Info(\"[JOB] Task started: %s\", taskID)\ndebug.Info(\"[HARDWARE] GPU detected: %s\", gpuName)\n</code></pre>"},{"location":"agent-guide/debugging/#contributing-and-bug-reports","title":"Contributing and Bug Reports","text":""},{"location":"agent-guide/debugging/#preparing-debug-information","title":"Preparing Debug Information","text":"<p>When reporting bugs, include:</p> <ol> <li> <p>Agent version and build info: <pre><code>./krakenhashes-agent --version\n</code></pre></p> </li> <li> <p>Complete configuration: <pre><code># Sanitized .env file (remove sensitive data)\ncat .env | sed 's/\\(API_KEY\\|PASSWORD\\)=.*/\\1=***REDACTED***/'\n</code></pre></p> </li> <li> <p>Debug logs: <pre><code># Last 100 lines with debug enabled\nDEBUG=true LOG_LEVEL=DEBUG ./krakenhashes-agent --host backend.example.com &gt; debug.log 2&gt;&amp;1\ntail -100 debug.log\n</code></pre></p> </li> <li> <p>System information: <pre><code>uname -a\nlscpu | grep -E \"(Architecture|CPU|Thread)\"\nnvidia-smi  # If using NVIDIA GPUs\n</code></pre></p> </li> <li> <p>Network connectivity: <pre><code>curl -I https://backend.example.com:31337/health\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/debugging/#debug-build-for-development","title":"Debug Build for Development","text":"<p>Create debug builds for development:</p> <pre><code># Clean build with debug symbols\ncd agent\nmake clean\n\n# Build with debug flags\ngo build -gcflags=\"all=-N -l\" -ldflags=\"-X main.BuildMode=debug\" -o debug-agent ./cmd/agent\n\n# Run with additional debugging\n./debug-agent --debug --host backend.example.com:31337\n</code></pre> <p>This debug build includes: - No compiler optimizations - Full symbol information - Additional runtime checks - Enhanced logging</p>"},{"location":"agent-guide/debugging/#troubleshooting-quick-reference","title":"Troubleshooting Quick Reference","text":"Issue Debug Steps Key Files Won't start Check <code>.env</code>, verify paths <code>.env</code>, <code>config/</code> Connection fails Test network, check certs <code>config/ca.crt</code>, <code>config/agent.crt</code> Auth errors Verify API key and agent ID <code>config/api_key.json</code>, <code>config/agent_credentials.json</code> Files not syncing Check permissions, disk space <code>data/wordlists/</code>, <code>data/rules/</code> Jobs not running Test hashcat, check GPU System hashcat, <code>nvidia-smi</code> High memory Enable profiling, reduce concurrency <code>.env</code> (set lower limits) Slow performance CPU/memory profiling pprof endpoints"},{"location":"agent-guide/debugging/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":""},{"location":"agent-guide/debugging/#custom-debug-builds","title":"Custom Debug Builds","text":"<p>Build with custom debug flags:</p> <pre><code># Build with additional debugging\ngo build -tags debug -gcflags=\"all=-N -l\" ./cmd/agent\n\n# Build with memory debugging\ngo build -gcflags=\"all=-m\" ./cmd/agent\n\n# Build with race detection (development only)\ngo build -race ./cmd/agent\n</code></pre>"},{"location":"agent-guide/debugging/#remote-debugging","title":"Remote Debugging","text":"<p>Debug agent running on remote systems:</p> <pre><code># On remote system\ndlv exec ./krakenhashes-agent --listen=:2345 --headless=true --api-version=2 -- --debug\n\n# From local system\ndlv connect remote-host:2345\n</code></pre>"},{"location":"agent-guide/debugging/#container-debugging","title":"Container Debugging","text":"<p>Debug agent running in containers:</p> <pre><code># Build debug container\ndocker build -f Dockerfile.debug -t agent-debug .\n\n# Run with debug enabled\ndocker run -e DEBUG=true -e LOG_LEVEL=DEBUG agent-debug\n\n# Attach debugger to container\ndocker exec -it &lt;container_id&gt; dlv attach &lt;pid&gt;\n</code></pre> <p>This comprehensive debugging guide should help developers and advanced users effectively debug agent issues, profile performance, and contribute to the project development.</p>"},{"location":"agent-guide/device-management/","title":"Device Management","text":"<p>The KrakenHashes agent provides comprehensive device detection, management, and optimization capabilities for password cracking workloads. This guide covers everything you need to know about configuring and optimizing hardware resources.</p>"},{"location":"agent-guide/device-management/#overview","title":"Overview","text":"<p>The agent uses hashcat's built-in device detection capabilities to identify and manage compute devices. This approach ensures compatibility with hashcat's device handling and provides accurate performance characteristics for each device.</p>"},{"location":"agent-guide/device-management/#key-features","title":"Key Features","text":"<ul> <li>Automatic device detection using hashcat's <code>-I</code> flag</li> <li>Multi-GPU support with intelligent device allocation</li> <li>Cross-platform compatibility (Windows, Linux, macOS)</li> <li>Backend optimization with priority-based device selection</li> <li>Real-time monitoring during job execution</li> <li>Alias filtering to prevent duplicate device entries</li> </ul>"},{"location":"agent-guide/device-management/#supported-hardware","title":"Supported Hardware","text":""},{"location":"agent-guide/device-management/#nvidia-gpus","title":"NVIDIA GPUs","text":"<p>Supported backends: - CUDA (Primary) - OpenCL (Fallback)</p> <p>Requirements: - NVIDIA driver 450.80.02 or newer - CUDA toolkit 11.0 or newer (for CUDA backend) - OpenCL 1.2 or newer (for OpenCL backend)</p> <p>Installation: <pre><code># Ubuntu/Debian\nsudo apt-get install nvidia-driver nvidia-cuda-toolkit\n\n# Verify installation\nnvidia-smi\nnvidia-settings --version\n</code></pre></p> <p>Optimal configuration: - Use CUDA backend when available (higher performance) - Enable GPU boost for maximum clock speeds - Ensure adequate power supply (750W+ for high-end cards) - Monitor temperatures (keep below 83\u00b0C for optimal performance)</p>"},{"location":"agent-guide/device-management/#amd-gpus","title":"AMD GPUs","text":"<p>Supported backends: - HIP (Primary for modern cards) - OpenCL (Universal)</p> <p>Requirements: - AMD Radeon Software 22.7.1 or newer - ROCm 5.2 or newer (for HIP backend) - OpenCL 2.0 or newer</p> <p>Installation: <pre><code># Ubuntu/Debian - ROCm installation\nwget https://repo.radeon.com/amdgpu-install/5.4/ubuntu/jammy/amdgpu-install_5.4.50400-1_all.deb\nsudo dpkg -i amdgpu-install_5.4.50400-1_all.deb\nsudo amdgpu-install --usecase=rocm\n\n# Verify installation\nrocm-smi\nclinfo | grep AMD\n</code></pre></p> <p>Optimal configuration: - Use HIP backend for RX 6000/7000 series and newer - Use OpenCL for older cards (RX 500/Vega series) - Enable GPU memory overclocking for hash-heavy algorithms - Monitor junction temperatures (keep below 110\u00b0C)</p>"},{"location":"agent-guide/device-management/#intel-gpus","title":"Intel GPUs","text":"<p>Supported backends: - OpenCL - Level Zero (Arc series)</p> <p>Requirements: - Intel Graphics Driver 30.0.101.1404 or newer - Intel GPU tools for monitoring - OpenCL runtime</p> <p>Installation: <pre><code># Ubuntu/Debian\nsudo apt-get install intel-gpu-tools intel-opencl-icd\n\n# Verify installation\nintel_gpu_top\nclinfo | grep Intel\n</code></pre></p> <p>Notes: - Intel Arc GPUs provide competitive hash rates for certain algorithms - Integrated graphics can be used for light workloads - Limited hashcat optimization compared to NVIDIA/AMD</p>"},{"location":"agent-guide/device-management/#cpu-processing","title":"CPU Processing","text":"<p>Supported: - All x86-64 processors - ARM processors (limited algorithm support)</p> <p>Requirements: - Modern multi-core processor - Sufficient system memory (8GB+ recommended)</p> <p>Optimal configuration: - Enable all CPU cores for maximum throughput - Ensure adequate cooling for sustained workloads - Consider CPU-only for specific algorithms (bcrypt, scrypt)</p>"},{"location":"agent-guide/device-management/#device-detection","title":"Device Detection","text":""},{"location":"agent-guide/device-management/#automatic-detection","title":"Automatic Detection","text":"<p>The agent automatically detects devices on startup using hashcat's device enumeration:</p> <pre><code>./agent -debug  # Enable debug output to see detection process\n</code></pre> <p>Detection process: 1. Locates latest hashcat binary in data directory 2. Executes <code>hashcat -I</code> command 3. Parses device information and capabilities 4. Filters aliases and invalid devices 5. Stores device configuration for job allocation</p>"},{"location":"agent-guide/device-management/#device-properties","title":"Device Properties","text":"<p>Each detected device includes:</p> <pre><code>{\n  \"device_id\": 1,\n  \"device_name\": \"NVIDIA GeForce RTX 4090\",\n  \"device_type\": \"GPU\",\n  \"enabled\": true,\n  \"processors\": 128,\n  \"clock\": 2520,\n  \"memory_total\": 24576,\n  \"memory_free\": 23552,\n  \"pci_address\": \"01:00.0\",\n  \"backend\": \"CUDA\",\n  \"is_alias\": false\n}\n</code></pre>"},{"location":"agent-guide/device-management/#backend-priority","title":"Backend Priority","text":"<p>When multiple backends are available for the same device, the agent uses this default priority order:</p> <ol> <li>CUDA (NVIDIA optimized - best performance)</li> <li>HIP (AMD optimized - modern cards)</li> <li>OpenCL (Universal fallback - widest compatibility)</li> </ol> <p>This ensures optimal performance by selecting the most efficient backend for each device. However, users can override this selection per device using the runtime selection feature (see below).</p>"},{"location":"agent-guide/device-management/#gpu-runtime-selection","title":"GPU Runtime Selection","text":"<p>Modern GPUs often support multiple compute backends (runtimes) such as CUDA, HIP, and OpenCL. KrakenHashes allows you to select which runtime to use for each physical GPU.</p>"},{"location":"agent-guide/device-management/#understanding-physical-devices-and-runtimes","title":"Understanding Physical Devices and Runtimes","text":"<p>When an agent detects devices, it groups them by physical GPU: - Physical Device: One entry per GPU hardware - Runtime Options: Multiple backends available for that GPU - Selected Runtime: The active backend used for job execution</p> <p>Example: An AMD Radeon RX 7700S might show: - Physical Device ID: 0 - Runtime Options:   - HIP #1 (16 cores, 2208 MHz, 8176 MB)   - OpenCL #3 (16 cores, 2208 MHz, 8176 MB) - Selected Runtime: HIP (default)</p>"},{"location":"agent-guide/device-management/#changing-runtime","title":"Changing Runtime","text":"<p>Via Web Interface: 1. Navigate to Agent Details page 2. Locate the \"Hardware Configuration\" section 3. Find the device you want to modify 4. Use the \"Runtime\" dropdown to select CUDA, HIP, or OpenCL 5. Changes take effect immediately for new jobs</p> <p>Via API: <pre><code>curl -X PATCH http://localhost:8080/api/agents/{agent_id}/devices/{device_id}/runtime \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"runtime\": \"OpenCL\"}'\n</code></pre></p>"},{"location":"agent-guide/device-management/#runtime-selection-strategy","title":"Runtime Selection Strategy","text":"<p>Default priority when multiple runtimes are available: 1. CUDA - NVIDIA GPUs (best performance) 2. HIP - AMD GPUs (optimized for modern cards) 3. OpenCL - Universal fallback (widest compatibility)</p> <p>You can override this default by manually selecting a different runtime for any device.</p>"},{"location":"agent-guide/device-management/#hashcat-version-compatibility","title":"Hashcat Version Compatibility","text":"<p>Different hashcat versions have varying device detection capabilities:</p> <p>Versions 6.2.6+ - Explicit alias information (\"Alias of #X\") - More accurate device grouping - Recommended for most users</p> <p>Versions 6.2.3-6.2.5 - No alias information - Uses positional matching for grouping - Works correctly but less explicit</p> <p>All Tested Versions: 6.2.3, 6.2.4, 6.2.5, 6.2.6, 7.1.2, 7.1.2-47+</p>"},{"location":"agent-guide/device-management/#important-notes","title":"Important Notes","text":"<ul> <li>Changing runtime takes effect immediately for new jobs</li> <li>Each physical GPU can only run under one runtime at a time</li> <li>Some hashcat algorithms perform better on specific runtimes</li> <li>Runtime selection affects benchmark results</li> </ul>"},{"location":"agent-guide/device-management/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":""},{"location":"agent-guide/device-management/#automatic-load-balancing","title":"Automatic Load Balancing","text":"<p>The agent automatically distributes workload across available GPUs:</p> <ul> <li>Equal distribution for identical GPU models</li> <li>Proportional allocation based on compute capability</li> <li>Dynamic adjustment based on real-time performance</li> </ul>"},{"location":"agent-guide/device-management/#manual-device-selection","title":"Manual Device Selection","text":"<p>You can manually enable/disable specific devices:</p> <pre><code># Through the web interface:\n# Agent Details \u2192 Device Management \u2192 Toggle device status\n\n# Or via API:\ncurl -X PUT http://localhost:8080/api/agents/{agent_id}/devices/{device_id} \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"enabled\": false}'\n</code></pre>"},{"location":"agent-guide/device-management/#optimal-multi-gpu-setups","title":"Optimal Multi-GPU Setups","text":"<p>Recommended configurations:</p> <ol> <li> <p>Identical GPUs: Best performance and load balancing    <pre><code>4x RTX 4090 \u2192 ~400 GH/s MD5\n8x RTX 3080 \u2192 ~640 GH/s MD5\n</code></pre></p> </li> <li> <p>Mixed GPUs: Group similar performance tiers    <pre><code>2x RTX 4090 + 2x RTX 4080 \u2192 Separate job allocation\n</code></pre></p> </li> <li> <p>CPU + GPU hybrid: Use CPU for specific algorithms    <pre><code>GPU: Fast hashes (MD5, SHA1, NTLM)\nCPU: Slow hashes (bcrypt, scrypt, Argon2)\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/device-management/#performance-optimization","title":"Performance Optimization","text":""},{"location":"agent-guide/device-management/#gpu-optimization","title":"GPU Optimization","text":"<p>Memory optimization: <pre><code># Enable optimized kernels (uses more VRAM but faster)\n-O\n\n# Set workload tuning\n-w 1  # Low power usage\n-w 2  # Default\n-w 3  # High performance (recommended)\n-w 4  # Insane performance (may cause instability)\n</code></pre></p> <p>Hashcat performance flags: <pre><code># Example optimization for RTX 4090\n--gpu-loops 1024 --gpu-accel 128 --gpu-threads 1024\n</code></pre></p>"},{"location":"agent-guide/device-management/#thermal-management","title":"Thermal Management","text":"<p>Temperature monitoring: - NVIDIA: Use <code>nvidia-smi</code> or built-in monitoring - AMD: Use <code>rocm-smi</code> or <code>amdgpu-pro</code> - Intel: Use <code>intel_gpu_top</code></p> <p>Thermal limits: - NVIDIA: 83\u00b0C (optimal), 91\u00b0C (maximum) - AMD: 110\u00b0C (junction), 95\u00b0C (edge) - Intel: 100\u00b0C (throttling)</p> <p>Cooling recommendations: - Ensure adequate case ventilation - Monitor ambient temperature (keep below 25\u00b0C) - Consider undervolting for 24/7 operations - Use custom fan curves for sustained workloads</p>"},{"location":"agent-guide/device-management/#power-management","title":"Power Management","text":"<p>Power considerations: <pre><code># Check GPU power limits\nnvidia-smi -q -d POWER     # NVIDIA\nrocm-smi --showpower       # AMD\n</code></pre></p> <p>Power optimization: - Ensure adequate PSU capacity (add 20% headroom) - Enable power limit increases where possible - Monitor power consumption during long jobs - Consider efficiency curves for different algorithms</p>"},{"location":"agent-guide/device-management/#device-allocation-strategies","title":"Device Allocation Strategies","text":""},{"location":"agent-guide/device-management/#job-based-allocation","title":"Job-Based Allocation","text":"<p>Strategy selection: 1. Round-robin: Distribute tasks evenly across devices 2. Performance-based: Allocate based on device capability 3. Memory-based: Consider VRAM requirements 4. Thermal-aware: Avoid overheated devices</p>"},{"location":"agent-guide/device-management/#workload-distribution","title":"Workload Distribution","text":"<p>Hash type considerations: - Fast hashes (MD5, SHA1): Use all available GPUs - Medium hashes (SHA256, SHA512): Balance GPU count vs. memory - Slow hashes (bcrypt, scrypt): May benefit from CPU processing - Memory-hard (Argon2): Requires high VRAM devices</p> <p>Example allocations: <pre><code># Large wordlist + fast hash = all devices\nhashcat -m 0 -a 0 hashes.txt wordlist.txt -d 1,2,3,4\n\n# Complex rules + medium hash = subset of devices\nhashcat -m 1000 -a 0 hashes.txt wordlist.txt -r rules.txt -d 1,2\n\n# Brute force + slow hash = single high-end device\nhashcat -m 3200 -a 3 hashes.txt ?a?a?a?a?a?a -d 1\n</code></pre></p>"},{"location":"agent-guide/device-management/#benchmarking-and-capabilities","title":"Benchmarking and Capabilities","text":""},{"location":"agent-guide/device-management/#hashcat-benchmarking","title":"Hashcat Benchmarking","text":"<p>Run benchmarks: <pre><code># Full benchmark suite\nhashcat -b\n\n# Specific algorithm benchmark\nhashcat -b -m 1000  # NTLM benchmark\n\n# Device-specific benchmark\nhashcat -b -d 1     # Benchmark only device 1\n</code></pre></p> <p>Interpreting results: <pre><code>Speed.#1.........:   123.4 GH/s (95.2ms) @ Accel:512 Loops:1024 Thr:64 Vec:1\n</code></pre> - <code>123.4 GH/s</code>: Hash rate (Giga-hashes per second) - <code>95.2ms</code>: Kernel execution time - <code>Accel:512</code>: Acceleration factor - <code>Loops:1024</code>: Iteration loops - <code>Thr:64</code>: Thread count - <code>Vec:1</code>: Vector width</p>"},{"location":"agent-guide/device-management/#performance-baselines","title":"Performance Baselines","text":"<p>Expected performance (RTX 4090): - MD5: ~100 GH/s - SHA1: ~35 GH/s - NTLM: ~180 GH/s - SHA256: ~15 GH/s - bcrypt: ~150 KH/s</p> <p>Expected performance (RX 7900 XTX): - MD5: ~75 GH/s - SHA1: ~25 GH/s - NTLM: ~130 GH/s - SHA256: ~12 GH/s - bcrypt: ~120 KH/s</p>"},{"location":"agent-guide/device-management/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"agent-guide/device-management/#minimum-requirements","title":"Minimum Requirements","text":"<p>System specifications: - CPU: 4 cores, 2.5GHz - RAM: 8GB - Storage: 100GB available space - GPU: Any OpenCL 1.2 compatible device - Network: 100 Mbps for file synchronization</p>"},{"location":"agent-guide/device-management/#recommended-requirements","title":"Recommended Requirements","text":"<p>High-performance setup: - CPU: 16+ cores, 3.0GHz+ - RAM: 32GB+ (64GB for large hashlists) - Storage: 1TB+ NVMe SSD - GPU: RTX 4080/4090 or RX 7900 XT/XTX - Network: 1 Gbps for large file transfers - PSU: 1000W+ 80+ Gold rated</p>"},{"location":"agent-guide/device-management/#enterprise-requirements","title":"Enterprise Requirements","text":"<p>Large-scale deployment: - CPU: 32+ cores server processor - RAM: 128GB+ ECC memory - Storage: 10TB+ enterprise SSD array - GPU: Multiple high-end cards with NVLink/Infinity Cache - Network: 10 Gbps with redundancy - Power: Redundant 1600W+ PSUs - Cooling: Dedicated server room cooling</p>"},{"location":"agent-guide/device-management/#driver-requirements","title":"Driver Requirements","text":""},{"location":"agent-guide/device-management/#nvidia-drivers","title":"NVIDIA Drivers","text":"<p>Recommended versions: - Production: Latest stable driver (535.x+) - Development: Latest beta driver for new features - Enterprise: Long-term support versions (470.x LTS)</p> <p>Installation verification: <pre><code>nvidia-smi\nnvcc --version  # CUDA compiler\nnvidia-settings --version\n</code></pre></p>"},{"location":"agent-guide/device-management/#amd-drivers","title":"AMD Drivers","text":"<p>Recommended versions: - ROCm: 5.4+ for HIP support - AMDGPU-PRO: 23.20+ for OpenCL - Mesa: 23.0+ for open-source stack</p> <p>Installation verification: <pre><code>rocm-smi\nrocminfo | grep \"Agent\"\nclinfo | grep AMD\n</code></pre></p>"},{"location":"agent-guide/device-management/#intel-drivers","title":"Intel Drivers","text":"<p>Recommended versions: - Graphics Driver: 30.0.101.1404+ - OpenCL Runtime: 22.43+ - Level Zero: 1.8+ (Arc series)</p> <p>Installation verification: <pre><code>intel_gpu_top\nclinfo | grep Intel\nvainfo | grep \"Driver version\"\n</code></pre></p>"},{"location":"agent-guide/device-management/#troubleshooting-device-issues","title":"Troubleshooting Device Issues","text":""},{"location":"agent-guide/device-management/#common-issues","title":"Common Issues","text":"<p>Device not detected: 1. Verify driver installation 2. Check hardware compatibility 3. Ensure proper PCI Express connection 4. Verify power supply adequacy 5. Test with hashcat directly: <code>hashcat -I</code></p> <p>Poor performance: 1. Check thermal throttling 2. Verify power limits 3. Update drivers 4. Check for conflicting processes 5. Validate hashcat parameters</p> <p>System instability: 1. Reduce workload tuning (<code>-w 2</code> instead of <code>-w 3</code>) 2. Lower GPU clocks and memory speeds 3. Improve cooling and power delivery 4. Check for hardware defects 5. Verify system memory integrity</p>"},{"location":"agent-guide/device-management/#debug-commands","title":"Debug Commands","text":"<p>Hardware diagnostics: <pre><code># System information\nlspci | grep -i vga\nlshw -c display\n\n# GPU status\nnvidia-smi -l 1        # NVIDIA monitoring\nrocm-smi -l            # AMD monitoring\nintel_gpu_top          # Intel monitoring\n\n# Temperature monitoring\nsensors                # System sensors\nnvidia-smi dmon        # NVIDIA detailed monitoring\n</code></pre></p> <p>Hashcat diagnostics: <pre><code># Device information\nhashcat -I\n\n# Test device functionality\nhashcat -t\n\n# Benchmark specific device\nhashcat -b -d 1\n\n# Debug mode\nhashcat --debug-mode=1 -m 1000 hash.txt wordlist.txt\n</code></pre></p>"},{"location":"agent-guide/device-management/#performance-troubleshooting","title":"Performance Troubleshooting","text":"<p>If hash rates are lower than expected:</p> <ol> <li> <p>Check thermal throttling: <pre><code>nvidia-smi dmon -s pucvmet -c 60  # Monitor for 60 seconds\n</code></pre></p> </li> <li> <p>Verify power limits: <pre><code>nvidia-smi -q -d POWER\n</code></pre></p> </li> <li> <p>Test with different parameters: <pre><code># Conservative settings\nhashcat -w 2 -O -m 1000 hash.txt wordlist.txt\n\n# Aggressive settings (may cause instability)\nhashcat -w 4 -O -m 1000 hash.txt wordlist.txt\n</code></pre></p> </li> <li> <p>Check for competing processes: <pre><code>ps aux | grep -E \"(hashcat|john|nvidia|rocm)\"\nnvidia-smi pmon  # Process monitoring\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/device-management/#best-practices","title":"Best Practices","text":""},{"location":"agent-guide/device-management/#hardware-selection","title":"Hardware Selection","text":"<ol> <li>Match workload to hardware:</li> <li>Fast hashes: High core count GPUs (RTX 4090, RX 7900 XTX)</li> <li>Slow hashes: High memory bandwidth (RTX 3090, RX 6900 XT)</li> <li> <p>Mixed workloads: Balanced systems with CPU + GPU</p> </li> <li> <p>Consider total cost of ownership:</p> </li> <li>Power consumption over lifetime</li> <li>Cooling requirements and costs</li> <li>Maintenance and replacement cycles</li> <li>Performance per dollar ratios</li> </ol>"},{"location":"agent-guide/device-management/#configuration-management","title":"Configuration Management","text":"<ol> <li>Document hardware configurations:</li> <li>GPU models, VRAM, clock speeds</li> <li>Driver versions and update schedules</li> <li>Optimal hashcat parameters for each device</li> <li> <p>Thermal and power limit settings</p> </li> <li> <p>Monitor performance trends:</p> </li> <li>Track hash rates over time</li> <li>Monitor for degradation or throttling</li> <li>Log hardware errors and failures</li> <li> <p>Schedule preventive maintenance</p> </li> <li> <p>Implement redundancy:</p> </li> <li>Deploy multiple agents for high availability</li> <li>Use mixed hardware to avoid single points of failure</li> <li>Maintain spare hardware for critical operations</li> <li>Implement proper backup and recovery procedures</li> </ol>"},{"location":"agent-guide/device-management/#security-considerations","title":"Security Considerations","text":"<ol> <li>Physical security:</li> <li>Secure hardware from unauthorized access</li> <li>Monitor for tampering or theft</li> <li>Implement proper access controls</li> <li> <p>Use hardware-based attestation where possible</p> </li> <li> <p>Driver security:</p> </li> <li>Keep drivers updated for security patches</li> <li>Verify driver signatures and authenticity</li> <li>Monitor for driver-level exploits</li> <li> <p>Use enterprise driver branches when available</p> </li> <li> <p>Performance isolation:</p> </li> <li>Isolate cracking workloads from other processes</li> <li>Use dedicated hardware for sensitive operations</li> <li>Monitor for unauthorized resource usage</li> <li>Implement resource quotas and limits</li> </ol> <p>By following this comprehensive guide, you'll be able to effectively configure, optimize, and manage hardware resources for maximum password cracking performance while maintaining system stability and security.</p>"},{"location":"agent-guide/file-sync/","title":"Agent File Synchronization","text":"<p>This document explains how KrakenHashes agents synchronize files with the backend server.</p>"},{"location":"agent-guide/file-sync/#overview","title":"Overview","text":"<p>KrakenHashes agents need access to the same wordlists and rules as the backend server to perform password cracking operations. The system implements a WebSocket-based file synchronization mechanism to ensure agents have the necessary files.</p>"},{"location":"agent-guide/file-sync/#synchronization-process","title":"Synchronization Process","text":"<p>The file synchronization process follows these steps:</p> <ol> <li>When an agent connects to the backend server via WebSocket, the server initiates a file synchronization request</li> <li>The agent scans its local directories and reports all files with their MD5 hashes</li> <li>The server compares the agent's files with its database and identifies missing or outdated files</li> <li>The server sends a synchronization command with a list of files the agent should download</li> <li>The agent downloads each file in parallel from the backend server</li> </ol>"},{"location":"agent-guide/file-sync/#file-types","title":"File Types","text":"<p>The system synchronizes the following types of files:</p> <ul> <li>Wordlists: Password dictionaries used for cracking</li> <li>Rules: Hashcat and John the Ripper rule files for password mutations</li> <li>Binaries: Tool binaries (future implementation)</li> </ul>"},{"location":"agent-guide/file-sync/#directory-structure","title":"Directory Structure","text":"<p>Agents store synchronized files in a data directory structure:</p> <pre><code>&lt;data_dir&gt;/\n\u251c\u2500\u2500 wordlists/\n\u2502   \u251c\u2500\u2500 general/\n\u2502   \u251c\u2500\u2500 specialized/\n\u2502   \u251c\u2500\u2500 targeted/\n\u2502   \u251c\u2500\u2500 custom/\n\u2502   \u2514\u2500\u2500 association/    # Association wordlists (v1.4.0+)\n\u251c\u2500\u2500 rules/\n\u2502   \u251c\u2500\u2500 hashcat/\n\u2502   \u251c\u2500\u2500 john/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 binaries/\n\u2514\u2500\u2500 hashlists/\n</code></pre> <p>The base data directory location is determined by:</p> <ol> <li>The <code>KH_DATA_DIR</code> environment variable, if set</li> <li>Otherwise, a <code>data</code> directory relative to the agent executable</li> </ol>"},{"location":"agent-guide/file-sync/#websocket-messages","title":"WebSocket Messages","text":"<p>The file synchronization uses the following WebSocket message types:</p>"},{"location":"agent-guide/file-sync/#file-sync-request","title":"File Sync Request","text":"<p>Sent from server to agent to request a list of files:</p> <pre><code>{\n  \"type\": \"file_sync_request\",\n  \"payload\": {\n    \"file_types\": [\"wordlist\", \"rule\", \"binary\"]\n  },\n  \"timestamp\": \"2023-07-01T12:00:00Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-sync-response","title":"File Sync Response","text":"<p>Sent from agent to server with the list of files:</p> <pre><code>{\n  \"type\": \"file_sync_response\",\n  \"payload\": {\n    \"agent_id\": 123,\n    \"files\": [\n      {\n        \"name\": \"rockyou.txt\",\n        \"file_type\": \"wordlist\",\n        \"hash\": \"7bfc9d4df2b5ce4e29ca14d40f7aef1b\",\n        \"size\": 139921507\n      },\n      {\n        \"name\": \"best64.rule\",\n        \"file_type\": \"rule\",\n        \"hash\": \"1e5f4a7e3cc31bd12a0f7a42c6ebab29\",\n        \"size\": 1234\n      }\n    ]\n  },\n  \"timestamp\": \"2023-07-01T12:00:05Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-sync-command","title":"File Sync Command","text":"<p>Sent from server to agent with files to download:</p> <pre><code>{\n  \"type\": \"file_sync_command\",\n  \"payload\": {\n    \"files\": [\n      {\n        \"name\": \"darkweb2017.txt\",\n        \"file_type\": \"wordlist\",\n        \"hash\": \"8b1a9953c4611296a827abf8c47804d7\",\n        \"size\": 8553126\n      }\n    ]\n  },\n  \"timestamp\": \"2023-07-01T12:00:10Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-download-process","title":"File Download Process","text":"<p>When an agent receives a file sync command:</p> <ol> <li>It processes each file in the command asynchronously</li> <li>For each file, it creates the appropriate directory structure if needed</li> <li>It downloads the file from the backend server's file API endpoint</li> <li>It verifies the downloaded file's MD5 hash matches the expected hash</li> <li>If verification fails, it retries the download (up to 3 times)</li> </ol>"},{"location":"agent-guide/file-sync/#synchronization-timing","title":"Synchronization Timing","text":"<p>File synchronization occurs at the following times:</p> <ol> <li>When an agent first connects to the backend server</li> <li>Periodically (every 6 hours by default)</li> <li>When the backend server explicitly requests synchronization (e.g., after new files are added)</li> </ol>"},{"location":"agent-guide/file-sync/#error-handling","title":"Error Handling","text":"<p>The system implements several error handling mechanisms:</p> <ul> <li>Download timeouts (1 hour per file)</li> <li>Retry logic for failed downloads (3 attempts with exponential backoff)</li> <li>Partial file cleanup if a download is interrupted</li> <li>Verification of file integrity via MD5 hash</li> </ul>"},{"location":"agent-guide/file-sync/#security-considerations","title":"Security Considerations","text":"<p>All file transfers occur over secure HTTPS connections with:</p> <ul> <li>TLS encryption for all communications</li> <li>Agent authentication required for file downloads</li> <li>File integrity verification via MD5 hash</li> </ul>"},{"location":"agent-guide/file-sync/#monitoring","title":"Monitoring","text":"<p>Administrators can monitor file synchronization through:</p> <ol> <li>Agent logs, which show detailed information about file downloads</li> <li>Backend server logs, which show synchronization requests and commands</li> <li>The admin dashboard, which displays synchronization status for each agent</li> </ol>"},{"location":"agent-guide/file-sync/#best-practices","title":"Best Practices","text":"<ol> <li>Ensure adequate storage: Agents need sufficient disk space for wordlists and rules</li> <li>Monitor bandwidth usage: Large file transfers may impact network performance</li> <li>Stagger agent registrations: To prevent overwhelming the server with simultaneous downloads</li> <li>Pre-populate common files: For faster agent setup, pre-copy large wordlists to agent machines </li> </ol>"},{"location":"agent-guide/installation/","title":"Agent Installation","text":""},{"location":"agent-guide/installation/#overview","title":"Overview","text":"<p>This guide covers installing and setting up KrakenHashes agents on various platforms.</p>"},{"location":"agent-guide/installation/#system-requirements","title":"System Requirements","text":""},{"location":"agent-guide/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>4GB RAM</li> <li>10GB free disk space (You need enough disk space to cover all wordlists)</li> <li>Linux (Ubuntu 20.04+, Debian 11+, RHEL 8+, or similar)</li> <li>Network connectivity to backend server</li> </ul>"},{"location":"agent-guide/installation/#gpu-requirements-optional-but-recommended","title":"GPU Requirements (Optional but Recommended)","text":"<ul> <li>NVIDIA: CUDA 11.0+ compatible GPU with 4GB+ VRAM</li> <li>AMD: ROCm compatible GPU or OpenCL support</li> <li>Intel: OpenCL compatible GPU</li> </ul>"},{"location":"agent-guide/installation/#installation","title":"Installation","text":"<p>The agent is distributed as a standalone binary that runs from your home directory. No root access is required for basic installation.</p>"},{"location":"agent-guide/installation/#step-1-create-agent-directory","title":"Step 1: Create Agent Directory","text":"<pre><code># Create a directory for the agent in your home folder\nmkdir ~/krakenhashes-agent\ncd ~/krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/installation/#step-2-download-the-agent-binary","title":"Step 2: Download the Agent Binary","text":"<p>Download the appropriate binary for your platform from the GitHub Releases page or directly from your KrakenHashes server:</p>"},{"location":"agent-guide/installation/#option-1-download-from-github-releases","title":"Option 1: Download from GitHub Releases","text":"<pre><code># For Linux AMD64 (most common)\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-linux-amd64 -O krakenhashes-agent\n\n# For Linux ARM64\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-linux-arm64 -O krakenhashes-agent\n\n# For macOS AMD64\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-darwin-amd64 -O krakenhashes-agent\n\n# For macOS ARM64 (Apple Silicon)\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-darwin-arm64 -O krakenhashes-agent\n\n# Make the binary executable\nchmod +x krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/installation/#option-2-download-from-krakenhashes-server","title":"Option 2: Download from KrakenHashes Server","text":"<p>The KrakenHashes web UI provides convenient download options with ready-to-use commands. Navigate to the Agent Downloads page in your installation to access:</p> <ul> <li>Direct Download Buttons: Click to download the agent binary for your platform</li> <li>Copy URL: Get the direct download URL</li> <li>curl Command: One-click copy of curl command with SSL bypass flags for self-signed certificates</li> <li>wget Command: One-click copy of wget command with SSL bypass flags for self-signed certificates</li> </ul> <p>Example commands (replace <code>your-server</code> with your actual server URL):</p> <pre><code># Using curl (Linux AMD64)\ncurl -k -o krakenhashes-agent https://your-server:31337/api/public/agent/download/linux/amd64\nchmod +x krakenhashes-agent\n\n# Using wget (Linux AMD64)\nwget --no-check-certificate -O krakenhashes-agent https://your-server:31337/api/public/agent/download/linux/amd64\nchmod +x krakenhashes-agent\n</code></pre> <p>Note: The <code>-k</code> (curl) and <code>--no-check-certificate</code> (wget) flags bypass SSL certificate validation, which is necessary when using self-signed certificates. All downloads save as <code>krakenhashes-agent</code> regardless of platform.</p>"},{"location":"agent-guide/installation/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<pre><code># Verify the binary is executable\nls -la ~/krakenhashes-agent/krakenhashes-agent\n# Should show executable permissions (x)\n\n# Check available options\n./krakenhashes-agent -help\n\n# Test connectivity (without registering)\n./krakenhashes-agent -host your-server:31337 -debug\n# This will show connection attempts even without a claim code\n</code></pre>"},{"location":"agent-guide/installation/#optional-set-up-as-a-service","title":"Optional: Set up as a Service","text":"<p>For automatic startup and easier management, see the Systemd Service Setup guide. This allows the agent to run in the background and start automatically on boot.</p>"},{"location":"agent-guide/installation/#initial-configuration","title":"Initial Configuration","text":"<p>The agent supports two configuration methods:</p>"},{"location":"agent-guide/installation/#method-1-automatic-configuration-recommended","title":"Method 1: Automatic Configuration (Recommended)","text":"<p>The agent automatically creates a <code>.env</code> configuration file on first run. From the agent directory:</p> <pre><code>cd ~/krakenhashes-agent\n\n# Run with your server details and claim code\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE\n\n# The agent will create:\n# - .env configuration file\n# - config/ directory for certificates\n# - data/ directory for files\n</code></pre>"},{"location":"agent-guide/installation/#method-2-manual-env-file-creation","title":"Method 2: Manual .env File Creation","text":"<p>You can manually create a <code>.env</code> file before running the agent:</p> <ol> <li>Create a <code>.env</code> file in <code>~/krakenhashes-agent/.env</code>:</li> </ol> <pre><code>cd ~/krakenhashes-agent\nnano .env  # or your preferred editor\n</code></pre> <ol> <li>Add the following configuration:</li> </ol> <pre><code># KrakenHashes Agent Configuration\n\n# Server Configuration\nKH_HOST=your-server.example.com  # Backend server hostname\nKH_PORT=31337                    # Backend server port\nUSE_TLS=true                     # Use TLS for secure communication\nLISTEN_INTERFACE=                # Network interface to bind to\nHEARTBEAT_INTERVAL=5             # Heartbeat interval in seconds\n\n# Agent Configuration\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE  # Your claim code from Admin UI\n\n# Directory Configuration\nKH_CONFIG_DIR=./config  # Configuration directory\nKH_DATA_DIR=./data      # Data directory\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s   # Timeout for writing messages\nKH_PONG_WAIT=60s    # Timeout for receiving pong\nKH_PING_PERIOD=54s  # Ping interval\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Max concurrent downloads\nKH_DOWNLOAD_TIMEOUT=1h         # Download timeout\n\n# Hashcat Configuration\nHASHCAT_EXTRA_PARAMS=  # Extra hashcat parameters (see note below)\n\n# Logging Configuration\nDEBUG=false            # Enable debug logging\nLOG_LEVEL=INFO        # Log level\n</code></pre> <ol> <li>Replace the placeholder values with your actual configuration</li> <li>Run the agent: <code>cd ~/krakenhashes-agent &amp;&amp; ./krakenhashes-agent</code></li> </ol> <p>Important Note on HASHCAT_EXTRA_PARAMS: - Parameters configured via the frontend (per-agent settings) take precedence - The .env file parameters are only used as a fallback - Best practice: Configure parameters via the frontend UI for centralized management</p>"},{"location":"agent-guide/installation/#post-configuration","title":"Post-Configuration","text":"<p>After the first run, the agent will use the <code>.env</code> file for all configuration. You can edit this file manually if needed:</p> <pre><code># View/edit the generated configuration\ncat .env\nnano .env  # or your preferred editor\n\n# Note: After successful registration, the KH_CLAIM_CODE will be automatically commented out\n</code></pre>"},{"location":"agent-guide/installation/#agent-registration","title":"Agent Registration","text":""},{"location":"agent-guide/installation/#step-1-generate-a-claim-code","title":"Step 1: Generate a Claim Code","text":"<p>In the KrakenHashes Admin UI: 1. Navigate to Agents \u2192 Manage Vouchers 2. Click \"Create Voucher\" 3. Choose voucher type (one-time or continuous) 4. Copy the generated code</p>"},{"location":"agent-guide/installation/#step-2-register-the-agent","title":"Step 2: Register the Agent","text":"<p>From your agent directory, run the agent with your claim code:</p> <pre><code>cd ~/krakenhashes-agent\n\n# Register and run the agent\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE\n\n# With debug output (helpful for troubleshooting)\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE -debug\n</code></pre> <p>The agent will: - Connect to the backend server - Register using the claim code - Generate certificates and API keys - Create a <code>.env</code> file with your configuration - Automatically comment out the claim code after successful registration</p>"},{"location":"agent-guide/installation/#step-3-running-the-agent","title":"Step 3: Running the Agent","text":"<p>After registration, simply run:</p> <pre><code>cd ~/krakenhashes-agent\n./krakenhashes-agent\n</code></pre> <p>The agent will use the <code>.env</code> file created during registration. You don't need to specify the claim code again.</p> <p>For automatic startup, see the Systemd Service Setup guide.</p>"},{"location":"agent-guide/installation/#gpu-driver-installation","title":"GPU Driver Installation","text":"<p>The following is only to help but not a full list or updated each time. Please review what drivers you need for your distribution.</p>"},{"location":"agent-guide/installation/#nvidia-gpus","title":"NVIDIA GPUs","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y nvidia-driver-525 nvidia-cuda-toolkit\n\n# RHEL/CentOS/Rocky\nsudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\nsudo dnf install -y nvidia-driver cuda\n</code></pre>"},{"location":"agent-guide/installation/#amd-gpus","title":"AMD GPUs","text":"<pre><code># Install ROCm\nwget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -\necho 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/debian/ ubuntu main' | sudo tee /etc/apt/sources.list.d/rocm.list\nsudo apt update\nsudo apt install rocm-dev\n</code></pre>"},{"location":"agent-guide/installation/#verification","title":"Verification","text":"<ol> <li>Check agent status:</li> </ol> <p>For manual run, check if the process is running:    <pre><code>ps aux | grep krakenhashes-agent\n</code></pre></p> <p>For systemd service:    <pre><code># User service\nsystemctl --user status krakenhashes-agent\n\n# System service  \nsudo systemctl status krakenhashes-agent\n</code></pre></p> <ol> <li>View logs:</li> </ol> <p>For manual run, check the terminal output or log files in the agent directory.</p> <p>For systemd service:    <pre><code># User service\njournalctl --user -u krakenhashes-agent -f\n\n# System service\nsudo journalctl -u krakenhashes-agent -f\n</code></pre></p> <ol> <li>Verify in Web UI:</li> <li>Navigate to Agents section</li> <li>Confirm agent appears as \"Online\"</li> <li>Check detected devices</li> </ol>"},{"location":"agent-guide/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure the agent</li> <li>Set up scheduling</li> <li>Learn about file synchronization</li> </ul>"},{"location":"agent-guide/monitoring/","title":"Agent Monitoring Guide","text":"<p>This comprehensive guide covers monitoring distributed agents in KrakenHashes, including real-time metrics, health checks, performance monitoring, and troubleshooting strategies for administrators managing agent fleets.</p>"},{"location":"agent-guide/monitoring/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Real-time Agent Status</li> <li>Heartbeat and Connection Monitoring</li> <li>Metrics Collection and Monitoring</li> <li>Device Performance Monitoring</li> <li>Agent Health Indicators</li> <li>Multi-Agent Fleet Monitoring</li> <li>Status Indicators and States</li> <li>Performance Analysis and Trends</li> <li>Troubleshooting with Monitoring Data</li> <li>Best Practices</li> </ol>"},{"location":"agent-guide/monitoring/#overview","title":"Overview","text":"<p>KrakenHashes provides comprehensive monitoring capabilities for distributed agents, enabling administrators to track agent health, performance, and operational status across their entire fleet. The monitoring system includes real-time metrics collection, WebSocket-based status reporting, and detailed performance analytics.</p>"},{"location":"agent-guide/monitoring/#key-monitoring-features","title":"Key Monitoring Features","text":"<ul> <li>Real-time Agent Status: Live connection status and heartbeat monitoring</li> <li>Device Metrics: GPU temperature, utilization, fan speed, and hash rate tracking</li> <li>WebSocket Communication: Persistent connections with automatic reconnection</li> <li>Performance Analytics: Historical data and trend analysis</li> <li>Multi-Agent Dashboard: Fleet-wide visibility and management</li> <li>Automated Health Checks: Connection validation and failure detection</li> </ul>"},{"location":"agent-guide/monitoring/#real-time-agent-status","title":"Real-time Agent Status","text":""},{"location":"agent-guide/monitoring/#agent-status-overview","title":"Agent Status Overview","text":"<p>The system continuously monitors agent status through multiple channels:</p> <pre><code>Active Agents: Online and ready for work\n\u251c\u2500\u2500 Connected: WebSocket connection established\n\u251c\u2500\u2500 Idle: Available for task assignment\n\u251c\u2500\u2500 Busy: Currently executing tasks\n\u2514\u2500\u2500 Reconnecting: Temporary disconnection with recovery in progress\n\nInactive Agents: Not currently operational\n\u251c\u2500\u2500 Offline: No recent heartbeat or connection\n\u251c\u2500\u2500 Disabled: Administratively disabled\n\u251c\u2500\u2500 Error: Failed connection or system error\n\u2514\u2500\u2500 Pending: Newly registered, awaiting first connection\n</code></pre>"},{"location":"agent-guide/monitoring/#agent-connection-states","title":"Agent Connection States","text":"<p>The monitoring system tracks detailed connection states:</p> State Description Color Indicator Action Required <code>active</code> Connected and operational \ud83d\udfe2 Green None <code>inactive</code> Disconnected or offline \ud83d\udd34 Red Check agent service <code>pending</code> Registration in progress \ud83d\udfe1 Yellow Wait for completion <code>disabled</code> Administratively disabled \u26ab Gray Manual re-enable <code>error</code> System or hardware error \ud83d\udd34 Red Investigate error"},{"location":"agent-guide/monitoring/#agent-dashboard-view","title":"Agent Dashboard View","text":"<p>Access the agent monitoring dashboard at: - Frontend: <code>https://your-server:31337/agents</code> - Agent Details: <code>https://your-server:31337/agents/{agent_id}</code></p> <p>The dashboard provides: - Real-time connection status - Last activity timestamps - Device configuration and status - Performance metrics and charts - Task assignment history</p>"},{"location":"agent-guide/monitoring/#heartbeat-and-connection-monitoring","title":"Heartbeat and Connection Monitoring","text":""},{"location":"agent-guide/monitoring/#websocket-heartbeat-system","title":"WebSocket Heartbeat System","text":"<p>KrakenHashes uses a robust WebSocket-based heartbeat system for monitoring agent connectivity:</p> <pre><code>Backend \u2190\u2192 Agent WebSocket Connection\n\u251c\u2500\u2500 Ping/Pong Messages: Every 54 seconds (configurable)\n\u251c\u2500\u2500 Agent Status Updates: Every 60 seconds\n\u251c\u2500\u2500 Heartbeat Timeout: 60 seconds maximum\n\u2514\u2500\u2500 Automatic Reconnection: Exponential backoff (1s to 30s)\n</code></pre>"},{"location":"agent-guide/monitoring/#connection-timing-configuration","title":"Connection Timing Configuration","text":"<p>The system uses configurable timing parameters:</p> <pre><code># Backend WebSocket Settings (environment variables)\nKH_WRITE_WAIT=10s      # Write operation timeout\nKH_PONG_WAIT=60s       # Pong response timeout  \nKH_PING_PERIOD=54s     # Ping interval\n\n# Agent automatically fetches these from backend\n# No manual configuration required\n</code></pre>"},{"location":"agent-guide/monitoring/#heartbeat-monitoring-queries","title":"Heartbeat Monitoring Queries","text":"<p>Monitor agent heartbeat status using database queries:</p> <pre><code>-- Agents with recent heartbeats (last 5 minutes)\nSELECT \n    id,\n    name,\n    status,\n    last_heartbeat,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) AS seconds_since_heartbeat\nFROM agents \nWHERE last_heartbeat &gt; NOW() - INTERVAL '5 minutes'\nORDER BY last_heartbeat DESC;\n\n-- Agents with stale heartbeats (potential issues)\nSELECT \n    id,\n    name, \n    status,\n    last_heartbeat,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) AS seconds_since_heartbeat\nFROM agents\nWHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes'\n  AND status = 'active'\nORDER BY last_heartbeat ASC;\n\n-- Connection status distribution\nSELECT status, COUNT(*) as count\nFROM agents\nGROUP BY status\nORDER BY count DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#metrics-collection-and-monitoring","title":"Metrics Collection and Monitoring","text":""},{"location":"agent-guide/monitoring/#system-metrics-collection","title":"System Metrics Collection","text":"<p>Agents automatically collect and report system metrics:</p>"},{"location":"agent-guide/monitoring/#cpu-and-memory-metrics","title":"CPU and Memory Metrics","text":"<ul> <li>CPU Usage: Overall processor utilization percentage</li> <li>Memory Usage: System memory utilization percentage</li> <li>Collection Interval: 5 seconds (configurable)</li> <li>Data Retention: Based on monitoring settings</li> </ul>"},{"location":"agent-guide/monitoring/#agent-metrics-structure","title":"Agent Metrics Structure","text":"<pre><code>type MetricsData struct {\n    AgentID     int                `json:\"agent_id\"`\n    CollectedAt time.Time          `json:\"collected_at\"`\n    CPUs        []CPUMetrics       `json:\"cpus\"`\n    GPUs        []GPUMetrics       `json:\"gpus\"`\n    Memory      MemoryMetrics      `json:\"memory\"`\n    Disk        []DiskMetrics      `json:\"disk\"`\n    Network     []NetworkMetrics   `json:\"network\"`\n    Process     []ProcessMetrics   `json:\"process\"`\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#gpu-metrics-integration","title":"GPU Metrics Integration","text":"<p>GPU metrics are obtained from hashcat's JSON status output during job execution:</p> <ul> <li>GPU Utilization: Device usage percentage</li> <li>GPU Temperature: Operating temperature in Celsius</li> <li>GPU Memory: Memory utilization</li> <li>Power Usage: Power consumption in watts</li> <li>Hash Rate: Real-time hashing performance</li> </ul>"},{"location":"agent-guide/monitoring/#metrics-storage-and-retention","title":"Metrics Storage and Retention","text":"<p>The system implements a cascading retention policy:</p> <pre><code>Real-time Metrics (Fine-grained)\n\u251c\u2500\u2500 Retention: 7 days (configurable)  \n\u251c\u2500\u2500 Resolution: 5-second intervals\n\u2514\u2500\u2500 Use: Recent activity monitoring\n\nDaily Aggregates (Medium-term)\n\u251c\u2500\u2500 Retention: 30 days (configurable)\n\u251c\u2500\u2500 Resolution: Daily summaries\n\u2514\u2500\u2500 Use: Performance analysis\n\nWeekly Aggregates (Long-term)  \n\u251c\u2500\u2500 Retention: 365 days (configurable)\n\u251c\u2500\u2500 Resolution: Weekly summaries\n\u2514\u2500\u2500 Use: Historical trends\n</code></pre>"},{"location":"agent-guide/monitoring/#monitoring-settings-configuration","title":"Monitoring Settings Configuration","text":"<p>Configure metrics retention through the admin interface:</p> <pre><code># Access monitoring settings\nhttps://your-server:31337/admin/monitoring\n\n# Available settings:\n- Real-time Data Retention: 7 days\n- Daily Aggregates Retention: 30 days  \n- Weekly Aggregates Retention: 365 days\n- Enable Aggregation: true\n- Aggregation Interval: daily\n</code></pre>"},{"location":"agent-guide/monitoring/#device-performance-monitoring","title":"Device Performance Monitoring","text":""},{"location":"agent-guide/monitoring/#real-time-device-metrics","title":"Real-time Device Metrics","text":"<p>The Agent Details page provides comprehensive device monitoring with live charts:</p>"},{"location":"agent-guide/monitoring/#available-device-charts","title":"Available Device Charts","text":"<ol> <li>Temperature Monitoring</li> <li>Real-time GPU temperature tracking</li> <li>Temperature threshold alerts</li> <li>Historical temperature trends</li> <li> <p>Multi-device comparison</p> </li> <li> <p>Utilization Tracking</p> </li> <li>GPU utilization percentage</li> <li>Device workload distribution</li> <li>Efficiency analysis</li> <li> <p>Performance optimization insights</p> </li> <li> <p>Fan Speed Monitoring</p> </li> <li>Cooling system performance</li> <li>Fan curve analysis</li> <li>Thermal management tracking</li> <li> <p>Hardware health indicators</p> </li> <li> <p>Hash Rate Performance</p> </li> <li>Real-time hashing performance</li> <li>Per-device contribution</li> <li>Cumulative hash rate</li> <li>Performance benchmarking</li> </ol>"},{"location":"agent-guide/monitoring/#chart-configuration-options","title":"Chart Configuration Options","text":"<pre><code>// Time Range Options\nconst timeRanges = [\n    '10m',  // 10 minutes\n    '20m',  // 20 minutes  \n    '1h',   // 1 hour\n    '5h',   // 5 hours\n    '24h'   // 24 hours\n];\n\n// Metric Types\nconst metricTypes = [\n    'temperature',  // GPU temperature in \u00b0C\n    'utilization',  // GPU utilization %\n    'fanspeed',     // Fan speed %\n    'hashrate'      // Hash rate (varies by algorithm)\n];\n</code></pre>"},{"location":"agent-guide/monitoring/#device-metrics-api-endpoints","title":"Device Metrics API Endpoints","text":"<pre><code># Get device metrics for agent\nGET /api/agents/{agent_id}/metrics?timeRange=1h&amp;metrics=temperature,utilization,fanspeed,hashrate\n\n# Response format\n{\n    \"devices\": [\n        {\n            \"deviceId\": 0,\n            \"deviceName\": \"NVIDIA RTX 4090\",\n            \"metrics\": {\n                \"temperature\": [\n                    {\"timestamp\": 1640995200000, \"value\": 65.0},\n                    {\"timestamp\": 1640995205000, \"value\": 67.2}\n                ],\n                \"utilization\": [\n                    {\"timestamp\": 1640995200000, \"value\": 98.5},\n                    {\"timestamp\": 1640995205000, \"value\": 99.1}\n                ]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#device-enabledisable-monitoring","title":"Device Enable/Disable Monitoring","text":"<p>Monitor device status changes through the interface:</p> <pre><code># Update device status\nPUT /api/agents/{agent_id}/devices/{device_id}\n{\n    \"enabled\": true\n}\n\n# Monitor device state changes in logs\ngrep -i \"device.*update.*enabled\" logs/backend/*.log\n</code></pre>"},{"location":"agent-guide/monitoring/#agent-health-indicators","title":"Agent Health Indicators","text":""},{"location":"agent-guide/monitoring/#connection-health-metrics","title":"Connection Health Metrics","text":"<p>Monitor agent connection health through multiple indicators:</p>"},{"location":"agent-guide/monitoring/#connection-status-indicators","title":"Connection Status Indicators","text":"<ul> <li>WebSocket State: Connected/Disconnected</li> <li>Last Heartbeat: Timestamp of last communication</li> <li>Response Time: WebSocket ping-pong latency</li> <li>Reconnection Attempts: Failed connection retry count</li> </ul>"},{"location":"agent-guide/monitoring/#system-health-indicators","title":"System Health Indicators","text":"<ul> <li>CPU Load: System processor utilization</li> <li>Memory Usage: Available system memory</li> <li>Disk Space: Storage availability</li> <li>Network Latency: Communication delays</li> </ul>"},{"location":"agent-guide/monitoring/#hardware-health-indicators","title":"Hardware Health Indicators","text":"<ul> <li>GPU Temperature: Thermal status and limits</li> <li>GPU Utilization: Device workload efficiency</li> <li>Power Consumption: Electrical usage monitoring</li> <li>Fan Performance: Cooling system operation</li> </ul>"},{"location":"agent-guide/monitoring/#agent-status-reporting","title":"Agent Status Reporting","text":"<p>Agents automatically report detailed status information:</p> <pre><code>{\n    \"status\": \"active\",\n    \"version\": \"v0.15.7\",  \n    \"updated_at\": \"2025-09-11T10:30:00Z\",\n    \"environment\": {\n        \"os\": \"linux\",\n        \"arch\": \"amd64\",\n        \"hostname\": \"worker-01\"\n    },\n    \"os_info\": {\n        \"platform\": \"linux\",\n        \"hostname\": \"worker-01\", \n        \"os_name\": \"Ubuntu\",\n        \"os_version\": \"22.04.3 LTS\",\n        \"kernel_version\": \"Linux version 6.5.0\",\n        \"go_version\": \"go1.21.0\"\n    }\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#health-check-queries","title":"Health Check Queries","text":"<pre><code>-- Agent health summary\nSELECT \n    a.name,\n    a.status,\n    a.last_heartbeat,\n    a.version,\n    CASE \n        WHEN a.last_heartbeat &gt; NOW() - INTERVAL '2 minutes' THEN 'Healthy'\n        WHEN a.last_heartbeat &gt; NOW() - INTERVAL '5 minutes' THEN 'Warning'\n        ELSE 'Critical'\n    END as health_status,\n    COUNT(ad.id) as device_count,\n    SUM(CASE WHEN ad.enabled THEN 1 ELSE 0 END) as enabled_devices\nFROM agents a\nLEFT JOIN agent_devices ad ON a.id = ad.agent_id\nGROUP BY a.id, a.name, a.status, a.last_heartbeat, a.version\nORDER BY a.last_heartbeat DESC;\n\n-- Agents with hardware issues\nSELECT \n    a.name,\n    ad.device_name,\n    apm.metric_type,\n    apm.value,\n    apm.timestamp\nFROM agents a\nJOIN agent_devices ad ON a.id = ad.agent_id\nJOIN agent_performance_metrics apm ON ad.agent_id = apm.agent_id\nWHERE (apm.metric_type = 'temperature' AND apm.value &gt; 85)\n   OR (apm.metric_type = 'utilization' AND apm.value &lt; 50)\nORDER BY apm.timestamp DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#multi-agent-fleet-monitoring","title":"Multi-Agent Fleet Monitoring","text":""},{"location":"agent-guide/monitoring/#fleet-overview-dashboard","title":"Fleet Overview Dashboard","text":"<p>Monitor your entire agent fleet from the main agents page:</p> <pre><code># Access fleet monitoring\nhttps://your-server:31337/agents\n\n# Key fleet metrics:\n- Total Agents: Active + Inactive count\n- Agent Distribution: By status and location  \n- Hardware Summary: Total GPU count and types\n- Performance Metrics: Combined hash rates\n- Health Status: Overall fleet health\n</code></pre>"},{"location":"agent-guide/monitoring/#fleet-status-categories","title":"Fleet Status Categories","text":""},{"location":"agent-guide/monitoring/#active-agents","title":"Active Agents","text":"<ul> <li>Online and Ready: Available for job assignment</li> <li>Busy: Currently executing tasks</li> <li>Idle: Connected but not actively working</li> </ul>"},{"location":"agent-guide/monitoring/#inactive-agents","title":"Inactive Agents","text":"<ul> <li>Offline: No recent heartbeat</li> <li>Disabled: Administratively disabled</li> <li>Error State: Hardware or connection issues</li> </ul>"},{"location":"agent-guide/monitoring/#pending-agents","title":"Pending Agents","text":"<ul> <li>Registering: New agent setup in progress</li> <li>Authenticating: Certificate and API key validation</li> </ul>"},{"location":"agent-guide/monitoring/#fleet-wide-monitoring-queries","title":"Fleet-wide Monitoring Queries","text":"<pre><code>-- Fleet status summary\nSELECT \n    status,\n    COUNT(*) as agent_count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\nFROM agents\nGROUP BY status\nORDER BY agent_count DESC;\n\n-- Fleet hardware summary\nSELECT \n    ad.device_type,\n    COUNT(DISTINCT a.id) as agents_with_device,\n    COUNT(ad.id) as total_devices,\n    SUM(CASE WHEN ad.enabled THEN 1 ELSE 0 END) as enabled_devices\nFROM agents a\nJOIN agent_devices ad ON a.id = ad.agent_id\nGROUP BY ad.device_type\nORDER BY total_devices DESC;\n\n-- Fleet performance overview\nSELECT \n    COUNT(DISTINCT a.id) as total_agents,\n    COUNT(DISTINCT CASE WHEN a.status = 'active' THEN a.id END) as active_agents,\n    AVG(CASE WHEN apm.metric_type = 'utilization' THEN apm.value END) as avg_gpu_utilization,\n    AVG(CASE WHEN apm.metric_type = 'temperature' THEN apm.value END) as avg_gpu_temperature\nFROM agents a\nLEFT JOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.timestamp &gt; NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"agent-guide/monitoring/#geographic-and-organizational-fleet-monitoring","title":"Geographic and Organizational Fleet Monitoring","text":"<pre><code>-- Agents by network location (using IP metadata)\nSELECT \n    SUBSTRING(metadata-&gt;&gt;'ipAddress', 1, \n              POSITION('.' IN metadata-&gt;&gt;'ipAddress' || '.')) as network_prefix,\n    COUNT(*) as agent_count,\n    COUNT(CASE WHEN status = 'active' THEN 1 END) as active_count\nFROM agents\nWHERE metadata ? 'ipAddress'\nGROUP BY network_prefix\nORDER BY agent_count DESC;\n\n-- Agents by owner (team/user assignments)\nSELECT \n    COALESCE(u.username, 'Unassigned') as owner,\n    COUNT(a.id) as agent_count,\n    COUNT(CASE WHEN a.status = 'active' THEN 1 END) as active_count\nFROM agents a\nLEFT JOIN users u ON a.owner_id = u.id  \nGROUP BY u.username\nORDER BY agent_count DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#status-indicators-and-states","title":"Status Indicators and States","text":""},{"location":"agent-guide/monitoring/#agent-status-state-machine","title":"Agent Status State Machine","text":"<pre><code>[pending] \u2192 [active] \u2192 [inactive]\n    \u2193         \u2193           \u2193\n[error]   [busy/idle]  [disabled]\n    \u2193         \u2193           \u2193\n[active]  [active]   [active]\n</code></pre>"},{"location":"agent-guide/monitoring/#detailed-status-descriptions","title":"Detailed Status Descriptions","text":"Status Description Typical Causes Recovery Actions <code>pending</code> New agent registration First-time setup Wait for completion <code>active</code> Fully operational Normal state None required <code>busy</code> Executing tasks Job assignment Monitor progress <code>idle</code> Connected, available Between jobs None required <code>inactive</code> Disconnected Network/service issues Restart agent service <code>disabled</code> Manually disabled Admin action Re-enable through UI <code>error</code> System/hardware error Hardware failure, config error Check logs, fix issues"},{"location":"agent-guide/monitoring/#status-transition-triggers","title":"Status Transition Triggers","text":""},{"location":"agent-guide/monitoring/#automatic-transitions","title":"Automatic Transitions","text":"<ul> <li>pending \u2192 active: Successful device detection and registration</li> <li>active \u2192 inactive: Heartbeat timeout (&gt;5 minutes)</li> <li>active \u2192 error: Device detection failure or system error</li> <li>error \u2192 active: Successful reconnection after error resolution</li> </ul>"},{"location":"agent-guide/monitoring/#manual-transitions","title":"Manual Transitions","text":"<ul> <li>Any \u2192 disabled: Administrative disable action</li> <li>disabled \u2192 active: Administrative enable action</li> <li>Any \u2192 active: Force status change through API</li> </ul>"},{"location":"agent-guide/monitoring/#visual-status-indicators","title":"Visual Status Indicators","text":"<p>The web interface uses consistent visual indicators:</p> <pre><code>/* Status indicator colors */\n.status-active     { color: #4caf50; }  /* Green */\n.status-inactive   { color: #f44336; }  /* Red */\n.status-pending    { color: #ff9800; }  /* Orange */\n.status-disabled   { color: #9e9e9e; }  /* Gray */\n.status-error      { color: #f44336; }  /* Red */\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-analysis-and-trends","title":"Performance Analysis and Trends","text":""},{"location":"agent-guide/monitoring/#historical-performance-tracking","title":"Historical Performance Tracking","text":"<p>The system maintains comprehensive historical performance data for trend analysis:</p>"},{"location":"agent-guide/monitoring/#performance-metrics-database-schema","title":"Performance Metrics Database Schema","text":"<pre><code>-- Agent performance metrics table structure\nCREATE TABLE agent_performance_metrics (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER REFERENCES agents(id),\n    device_name VARCHAR(255),\n    metric_type VARCHAR(50),  -- 'temperature', 'utilization', 'fanspeed', 'hashrate'\n    value NUMERIC(10,2),\n    timestamp TIMESTAMP DEFAULT NOW()\n);\n\n-- Benchmark results tracking\nCREATE TABLE agent_benchmarks (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER REFERENCES agents(id),\n    attack_mode INTEGER,\n    hash_type INTEGER, \n    speed BIGINT,\n    device_speeds JSONB,\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-analysis-queries","title":"Performance Analysis Queries","text":"<pre><code>-- Agent performance trends over time\nSELECT \n    DATE_TRUNC('hour', timestamp) as hour,\n    AVG(CASE WHEN metric_type = 'utilization' THEN value END) as avg_utilization,\n    AVG(CASE WHEN metric_type = 'temperature' THEN value END) as avg_temperature,\n    AVG(CASE WHEN metric_type = 'hashrate' THEN value END) as avg_hashrate\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND timestamp &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour;\n\n-- Top performing agents by hash rate\nSELECT \n    a.name,\n    AVG(apm.value) as avg_hashrate,\n    MAX(apm.value) as peak_hashrate,\n    COUNT(apm.id) as measurement_count\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.metric_type = 'hashrate'\n  AND apm.timestamp &gt; NOW() - INTERVAL '1 week'\nGROUP BY a.id, a.name\nORDER BY avg_hashrate DESC\nLIMIT 10;\n\n-- Performance degradation detection\nWITH recent_performance AS (\n    SELECT \n        agent_id,\n        AVG(value) as recent_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hashrate'\n      AND timestamp &gt; NOW() - INTERVAL '24 hours'\n    GROUP BY agent_id\n),\nbaseline_performance AS (\n    SELECT \n        agent_id,\n        AVG(value) as baseline_avg\n    FROM agent_performance_metrics  \n    WHERE metric_type = 'hashrate'\n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '2 days'\n    GROUP BY agent_id\n)\nSELECT \n    a.name,\n    r.recent_avg,\n    b.baseline_avg,\n    ROUND(((r.recent_avg - b.baseline_avg) / b.baseline_avg * 100), 2) as percent_change\nFROM agents a\nJOIN recent_performance r ON a.id = r.agent_id\nJOIN baseline_performance b ON a.id = b.agent_id\nWHERE ABS((r.recent_avg - b.baseline_avg) / b.baseline_avg) &gt; 0.15  -- &gt;15% change\nORDER BY percent_change;\n</code></pre>"},{"location":"agent-guide/monitoring/#benchmark-performance-tracking","title":"Benchmark Performance Tracking","text":"<p>Monitor agent benchmark performance over time:</p> <pre><code>-- Benchmark history for agent\nSELECT \n    attack_mode,\n    hash_type,\n    speed,\n    created_at,\n    LAG(speed) OVER (PARTITION BY attack_mode, hash_type ORDER BY created_at) as previous_speed,\n    speed - LAG(speed) OVER (PARTITION BY attack_mode, hash_type ORDER BY created_at) as speed_change\nFROM agent_benchmarks\nWHERE agent_id = $1\nORDER BY created_at DESC;\n\n-- Fleet benchmark comparison\nSELECT \n    a.name,\n    ab.attack_mode,\n    ab.hash_type,\n    ab.speed,\n    RANK() OVER (PARTITION BY ab.attack_mode, ab.hash_type ORDER BY ab.speed DESC) as rank\nFROM agents a\nJOIN agent_benchmarks ab ON a.id = ab.agent_id\nWHERE ab.updated_at &gt; NOW() - INTERVAL '1 week'\nORDER BY ab.attack_mode, ab.hash_type, ab.speed DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#troubleshooting-with-monitoring-data","title":"Troubleshooting with Monitoring Data","text":""},{"location":"agent-guide/monitoring/#common-issues-and-diagnostic-approaches","title":"Common Issues and Diagnostic Approaches","text":""},{"location":"agent-guide/monitoring/#1-agent-connection-issues","title":"1. Agent Connection Issues","text":"<p>Symptoms: - Agent status shows \"inactive\" or \"error\" - Missing from active agents list - Stale heartbeat timestamps</p> <p>Diagnostic Steps: <pre><code>-- Check agent connection history\nSELECT \n    id,\n    name,\n    status,\n    last_heartbeat,\n    last_error,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) as seconds_offline\nFROM agents\nWHERE name = 'problem-agent'\nOR id = 123;\n\n-- Check for recent WebSocket errors\n</code></pre></p> <p>Log Analysis: <pre><code># Check agent logs for connection issues\ngrep -i \"connection\\|websocket\\|heartbeat\" /path/to/agent.log\n\n# Check backend logs for agent-related errors\ngrep -i \"agent.*error\\|websocket.*close\\|connection.*failed\" logs/backend/*.log\n\n# Look for certificate or authentication issues\ngrep -i \"certificate\\|auth.*fail\\|tls.*error\" logs/backend/*.log\n</code></pre></p>"},{"location":"agent-guide/monitoring/#2-performance-degradation","title":"2. Performance Degradation","text":"<p>Symptoms: - Decreased hash rates - Higher GPU temperatures - Reduced utilization</p> <p>Diagnostic Queries: <pre><code>-- Performance comparison (current vs historical)\nWITH current_perf AS (\n    SELECT AVG(value) as current_hashrate\n    FROM agent_performance_metrics\n    WHERE agent_id = $1 \n      AND metric_type = 'hashrate'\n      AND timestamp &gt; NOW() - INTERVAL '1 hour'\n),\nhistorical_perf AS (\n    SELECT AVG(value) as historical_hashrate\n    FROM agent_performance_metrics\n    WHERE agent_id = $1\n      AND metric_type = 'hashrate' \n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '1 day'\n)\nSELECT \n    c.current_hashrate,\n    h.historical_hashrate,\n    ((c.current_hashrate - h.historical_hashrate) / h.historical_hashrate * 100) as percent_change\nFROM current_perf c, historical_perf h;\n\n-- Temperature analysis\nSELECT \n    DATE_TRUNC('hour', timestamp) as hour,\n    AVG(value) as avg_temp,\n    MAX(value) as max_temp,\n    COUNT(*) as measurements\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND metric_type = 'temperature'\n  AND timestamp &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#3-hardware-health-issues","title":"3. Hardware Health Issues","text":"<p>Symptoms: - High GPU temperatures (&gt;85\u00b0C) - Fan speed abnormalities - Utilization inconsistencies</p> <p>Monitoring Approach: <pre><code>-- Hardware health check\nSELECT \n    device_name,\n    metric_type,\n    value,\n    timestamp,\n    CASE \n        WHEN metric_type = 'temperature' AND value &gt; 85 THEN 'CRITICAL'\n        WHEN metric_type = 'temperature' AND value &gt; 75 THEN 'WARNING'\n        WHEN metric_type = 'utilization' AND value &lt; 80 THEN 'LOW_UTIL'\n        WHEN metric_type = 'fanspeed' AND value &gt; 90 THEN 'HIGH_FAN'\n        ELSE 'NORMAL'\n    END as status\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND timestamp &gt; NOW() - INTERVAL '1 hour'\n  AND (\n    (metric_type = 'temperature' AND value &gt; 75)\n    OR (metric_type = 'utilization' AND value &lt; 80)\n    OR (metric_type = 'fanspeed' AND value &gt; 90)\n  )\nORDER BY timestamp DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#4-task-assignment-issues","title":"4. Task Assignment Issues","text":"<p>Symptoms: - Agents remain idle during jobs - Uneven task distribution - Tasks stuck in \"reconnect_pending\"</p> <p>Diagnostic Queries: <pre><code>-- Check agent busy status and current tasks\nSELECT \n    a.name,\n    a.status,\n    a.metadata-&gt;&gt;'busy_status' as busy_status,\n    a.metadata-&gt;&gt;'current_task_id' as current_task,\n    jt.status as task_status,\n    je.name as job_name\nFROM agents a\nLEFT JOIN job_tasks jt ON jt.id = a.metadata-&gt;&gt;'current_task_id'  \nLEFT JOIN job_executions je ON jt.job_execution_id = je.id\nWHERE a.id = $1;\n\n-- Check for reconnect_pending tasks\nSELECT \n    jt.id,\n    jt.status,\n    jt.agent_id,\n    a.name,\n    jt.keyspace_start,\n    jt.keyspace_end,\n    jt.updated_at\nFROM job_tasks jt\nJOIN agents a ON jt.agent_id = a.id\nWHERE jt.status = 'reconnect_pending'\nORDER BY jt.updated_at DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#log-analysis-techniques","title":"Log Analysis Techniques","text":""},{"location":"agent-guide/monitoring/#structured-log-search","title":"Structured Log Search","text":"<pre><code># Find connection events for specific agent\ngrep -i \"agent.*123\\|Agent 123\" logs/backend/*.log | grep -i \"connect\"\n\n# Track WebSocket message flow\ngrep -i \"websocket\\|message.*type\" logs/backend/*.log | tail -50\n\n# Monitor heartbeat activity\ngrep -i \"heartbeat\\|ping\\|pong\" logs/backend/*.log | tail -20\n\n# Check for error patterns\ngrep -i \"error\\|fail\\|timeout\" logs/backend/*.log | grep -i \"agent\" | tail -10\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-issue-detection","title":"Performance Issue Detection","text":"<pre><code># Find performance-related messages\ngrep -i \"performance\\|slow\\|timeout\\|benchmark\" logs/backend/*.log\n\n# Check for resource issues\ngrep -i \"memory\\|cpu\\|disk\\|resource\" logs/backend/*.log\n\n# Monitor cleanup operations\ngrep -i \"cleanup\\|maintenance\\|retention\" logs/backend/*.log\n</code></pre>"},{"location":"agent-guide/monitoring/#automated-issue-detection","title":"Automated Issue Detection","text":"<p>Set up monitoring alerts for common issues:</p> <pre><code># Script: monitor_agents.sh\n#!/bin/bash\n\n# Check for agents with stale heartbeats\nSTALE_AGENTS=$(psql -t -c \"SELECT COUNT(*) FROM agents WHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes' AND status = 'active';\")\n\nif [ \"$STALE_AGENTS\" -gt 0 ]; then\n    echo \"ALERT: $STALE_AGENTS agents have stale heartbeats\"\n    # Send notification\nfi\n\n# Check for high GPU temperatures\nHOT_GPUS=$(psql -t -c \"SELECT COUNT(*) FROM agent_performance_metrics WHERE metric_type = 'temperature' AND value &gt; 85 AND timestamp &gt; NOW() - INTERVAL '5 minutes';\")\n\nif [ \"$HOT_GPUS\" -gt 0 ]; then\n    echo \"ALERT: $HOT_GPUS GPUs running hot (&gt;85\u00b0C)\"\n    # Send notification\nfi\n\n# Check for agents with no enabled devices\nNO_DEVICE_AGENTS=$(psql -t -c \"SELECT COUNT(DISTINCT a.id) FROM agents a LEFT JOIN agent_devices ad ON a.id = ad.agent_id WHERE a.status = 'active' AND NOT EXISTS (SELECT 1 FROM agent_devices ad2 WHERE ad2.agent_id = a.id AND ad2.enabled = true);\")\n\nif [ \"$NO_DEVICE_AGENTS\" -gt 0 ]; then\n    echo \"ALERT: $NO_DEVICE_AGENTS active agents have no enabled devices\"\n    # Send notification  \nfi\n</code></pre>"},{"location":"agent-guide/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"agent-guide/monitoring/#monitoring-strategy","title":"Monitoring Strategy","text":""},{"location":"agent-guide/monitoring/#1-proactive-monitoring","title":"1. Proactive Monitoring","text":"<ul> <li>Set up automated alerts for critical metrics (heartbeat failures, high temperatures, low utilization)</li> <li>Establish performance baselines for each agent to detect degradation</li> <li>Monitor trends rather than just current values</li> <li>Use multiple monitoring approaches (real-time dashboard + historical analysis)</li> </ul>"},{"location":"agent-guide/monitoring/#2-alert-thresholds","title":"2. Alert Thresholds","text":"<pre><code># Recommended alert thresholds:\nAgent Heartbeat: &gt; 5 minutes offline\nGPU Temperature: &gt; 85\u00b0C sustained  \nGPU Utilization: &lt; 50% during jobs\nConnection Failures: &gt; 3 consecutive failures\nPerformance Degradation: &gt; 20% decrease from baseline\n</code></pre>"},{"location":"agent-guide/monitoring/#3-regular-health-checks","title":"3. Regular Health Checks","text":"<ul> <li>Daily: Review agent status dashboard, check for offline agents</li> <li>Weekly: Analyze performance trends, identify degradation patterns  </li> <li>Monthly: Review hardware utilization, plan capacity changes</li> <li>Quarterly: Update performance baselines, optimize configurations</li> </ul>"},{"location":"agent-guide/monitoring/#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"agent-guide/monitoring/#1-fleet-management","title":"1. Fleet Management","text":"<ul> <li>Group agents by location, hardware type, or team assignment</li> <li>Use naming conventions that reflect agent purpose and location</li> <li>Document agent configurations including hardware specs and special parameters</li> <li>Maintain agent inventory with ownership and responsibility assignments</li> </ul>"},{"location":"agent-guide/monitoring/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Monitor benchmark results and update when hardware changes</li> <li>Balance task distribution across agents based on capability</li> <li>Track device utilization to identify underused resources</li> <li>Optimize extra parameters for each agent's hardware configuration</li> </ul>"},{"location":"agent-guide/monitoring/#3-maintenance-scheduling","title":"3. Maintenance Scheduling","text":"<ul> <li>Plan maintenance windows during low-activity periods</li> <li>Coordinate updates to minimize impact on running jobs</li> <li>Test configuration changes on non-critical agents first</li> <li>Document maintenance activities and their impact on performance</li> </ul>"},{"location":"agent-guide/monitoring/#monitoring-data-retention","title":"Monitoring Data Retention","text":""},{"location":"agent-guide/monitoring/#1-storage-management","title":"1. Storage Management","text":"<pre><code>-- Configure retention policies based on needs:\nReal-time metrics: 7-14 days (high-frequency data)\nDaily aggregates: 30-90 days (performance analysis)  \nWeekly aggregates: 1-2 years (long-term trends)\nBenchmark results: Indefinite (configuration reference)\n</code></pre>"},{"location":"agent-guide/monitoring/#2-data-cleanup-automation","title":"2. Data Cleanup Automation","text":"<ul> <li>Enable automatic aggregation to reduce storage requirements</li> <li>Monitor database growth and adjust retention as needed</li> <li>Archive historical data for compliance or analysis needs</li> <li>Use monitoring settings UI to adjust retention policies</li> </ul>"},{"location":"agent-guide/monitoring/#security-and-access-control","title":"Security and Access Control","text":""},{"location":"agent-guide/monitoring/#1-monitoring-access","title":"1. Monitoring Access","text":"<ul> <li>Restrict monitoring access to appropriate administrators</li> <li>Use role-based permissions for different monitoring functions</li> <li>Audit monitoring activities and configuration changes</li> <li>Secure monitoring endpoints with proper authentication</li> </ul>"},{"location":"agent-guide/monitoring/#2-agent-communication-security","title":"2. Agent Communication Security","text":"<ul> <li>Monitor certificate status and renewal schedules</li> <li>Track authentication failures and suspicious activity</li> <li>Use secure WebSocket connections (WSS) in production</li> <li>Regularly rotate API keys and monitor key usage</li> </ul>"},{"location":"agent-guide/monitoring/#disaster-recovery-and-continuity","title":"Disaster Recovery and Continuity","text":""},{"location":"agent-guide/monitoring/#1-monitoring-system-availability","title":"1. Monitoring System Availability","text":"<ul> <li>Implement monitoring redundancy to avoid single points of failure</li> <li>Backup monitoring configurations and historical data</li> <li>Test monitoring system recovery procedures</li> <li>Document escalation procedures for monitoring system failures</li> </ul>"},{"location":"agent-guide/monitoring/#2-agent-recovery-procedures","title":"2. Agent Recovery Procedures","text":"<ul> <li>Automate agent reconnection with exponential backoff</li> <li>Implement graceful degradation when agents are offline</li> <li>Buffer critical messages during disconnections for recovery</li> <li>Track task recovery and automatic redistribution</li> </ul> <p>This comprehensive monitoring guide enables administrators to effectively manage distributed KrakenHashes agent fleets, ensuring optimal performance, rapid issue detection, and reliable operation across diverse hardware configurations and network environments.</p>"},{"location":"agent-guide/scheduling/","title":"Agent Scheduling","text":""},{"location":"agent-guide/scheduling/#overview","title":"Overview","text":"<p>The Agent Scheduling feature in KrakenHashes allows administrators to define specific time windows when agents are available for job execution. This feature helps optimize resource usage, manage electricity costs, and ensure agents run during appropriate hours.</p>"},{"location":"agent-guide/scheduling/#key-features","title":"Key Features","text":"<ul> <li>Daily Schedule Configuration: Set different working hours for each day of the week</li> <li>Timezone Support: Schedules are configured in the user's local timezone but stored in UTC</li> <li>Overnight Schedule Support: Schedules can span midnight (e.g., 22:00 - 02:00)</li> <li>Global Enable/Disable: System-wide toggle to enable or disable all scheduling</li> <li>Per-Agent Control: Each agent can have scheduling enabled or disabled independently</li> <li>Schedule Preservation: Schedules are preserved even when disabled</li> </ul>"},{"location":"agent-guide/scheduling/#how-it-works","title":"How It Works","text":""},{"location":"agent-guide/scheduling/#schedule-enforcement","title":"Schedule Enforcement","text":"<p>When scheduling is enabled: 1. The system checks if global scheduling is enabled (admin setting) 2. The system checks if the individual agent has scheduling enabled 3. The system checks if the current UTC time falls within the agent's schedule 4. Only agents that pass all checks are assigned jobs</p>"},{"location":"agent-guide/scheduling/#time-storage-and-display","title":"Time Storage and Display","text":"<ul> <li>Storage: All times are stored in UTC in the database</li> <li>Display: Times are shown in the user's local timezone in the UI</li> <li>Conversion: Automatic conversion happens between local and UTC times</li> </ul>"},{"location":"agent-guide/scheduling/#configuration","title":"Configuration","text":""},{"location":"agent-guide/scheduling/#global-settings","title":"Global Settings","text":"<p>The global scheduling setting can be found in Admin Panel \u2192 System Settings:</p> <pre><code>Enable Agent Scheduling System: [Toggle]\n</code></pre> <p>When disabled: - All agent schedules are ignored - Agents are always available for jobs - Individual agent schedules are preserved but not enforced</p>"},{"location":"agent-guide/scheduling/#per-agent-configuration","title":"Per-Agent Configuration","text":"<p>Individual agent scheduling is configured on the agent details page:</p> <ol> <li>Navigate to Agents \u2192 [Agent Name]</li> <li>Find the Scheduling section</li> <li>Toggle Enable Scheduling to activate scheduling for this agent</li> <li>Click Edit All Schedules to configure daily schedules</li> </ol>"},{"location":"agent-guide/scheduling/#schedule-configuration","title":"Schedule Configuration","text":"<p>When editing schedules:</p> <ol> <li>Add Schedule: Click \"Add Schedule\" for any day to create a time window</li> <li>Set Times: Enter start and end times in 24-hour format (HH:MM)</li> <li>Active Toggle: Enable/disable the schedule for specific days</li> <li>Active (ON): Agent works during the specified hours</li> <li>Active (OFF): Agent does not work at all on this day</li> <li>Copy Schedule: Use the copy icon to apply one day's schedule to all other days</li> <li>Delete Schedule: Remove a schedule for a specific day</li> </ol>"},{"location":"agent-guide/scheduling/#time-input-formats","title":"Time Input Formats","text":"<p>The system accepts various time formats: - <code>9</code> \u2192 <code>09:00:00</code> - <code>17</code> \u2192 <code>17:00:00</code> - <code>9:30</code> \u2192 <code>09:30:00</code> - <code>09:00</code> \u2192 <code>09:00:00</code> - <code>09:00:00</code> \u2192 <code>09:00:00</code></p>"},{"location":"agent-guide/scheduling/#examples","title":"Examples","text":""},{"location":"agent-guide/scheduling/#standard-business-hours-9-5-monday-friday","title":"Standard Business Hours (9-5, Monday-Friday)","text":"<pre><code>Monday:    09:00 - 17:00 [Active]\nTuesday:   09:00 - 17:00 [Active]\nWednesday: 09:00 - 17:00 [Active]\nThursday:  09:00 - 17:00 [Active]\nFriday:    09:00 - 17:00 [Active]\nSaturday:  Not scheduled\nSunday:    Not scheduled\n</code></pre>"},{"location":"agent-guide/scheduling/#247-operation-with-weekend-maintenance","title":"24/7 Operation with Weekend Maintenance","text":"<pre><code>Monday:    00:00 - 23:59 [Active]\nTuesday:   00:00 - 23:59 [Active]\nWednesday: 00:00 - 23:59 [Active]\nThursday:  00:00 - 23:59 [Active]\nFriday:    00:00 - 23:59 [Active]\nSaturday:  00:00 - 06:00 [Active]  # Maintenance window 6 AM - Midnight\nSunday:    Not scheduled            # Full day maintenance\n</code></pre>"},{"location":"agent-guide/scheduling/#overnight-processing","title":"Overnight Processing","text":"<pre><code>Monday:    22:00 - 06:00 [Active]  # Runs overnight Mon-Tue\nTuesday:   22:00 - 06:00 [Active]  # Runs overnight Tue-Wed\nWednesday: 22:00 - 06:00 [Active]  # Runs overnight Wed-Thu\nThursday:  22:00 - 06:00 [Active]  # Runs overnight Thu-Fri\nFriday:    22:00 - 06:00 [Active]  # Runs overnight Fri-Sat\nSaturday:  Not scheduled\nSunday:    22:00 - 06:00 [Active]  # Runs overnight Sun-Mon\n</code></pre>"},{"location":"agent-guide/scheduling/#important-behavior-notes","title":"Important Behavior Notes","text":""},{"location":"agent-guide/scheduling/#running-jobs-are-not-interrupted","title":"Running Jobs Are Not Interrupted","text":"<p>The scheduling system only controls when new jobs are assigned, not when running jobs must complete.</p> <p>Key points: - Schedules determine when an agent can receive new jobs - Running jobs will always complete, even if they extend past the scheduled end time - The agent will not accept new jobs outside its schedule, but will finish current work</p>"},{"location":"agent-guide/scheduling/#example-scenario","title":"Example Scenario","text":"<p>If an agent is scheduled to work until 17:00: - At 16:59, the agent receives a job configured for 1-hour chunks - The job will run to completion, potentially until 17:59 or later - No new jobs will be assigned after 17:00 - The agent becomes available for new work at the next scheduled window</p> <p>This design ensures: - No work is lost due to scheduling boundaries - Jobs complete successfully without interruption - Predictable behavior for long-running tasks</p>"},{"location":"agent-guide/scheduling/#schedule-priority","title":"Schedule Priority","text":"<p>The scheduling system follows this priority order:</p> <ol> <li>Global Setting OFF: All schedules ignored, all agents always available</li> <li>Global Setting ON + Agent Scheduling OFF: Agent always available</li> <li>Global Setting ON + Agent Scheduling ON: Agent follows configured schedule</li> </ol>"},{"location":"agent-guide/scheduling/#technical-details","title":"Technical Details","text":""},{"location":"agent-guide/scheduling/#database-schema","title":"Database Schema","text":"<p>Schedules are stored in the <code>agent_schedules</code> table:</p> <pre><code>CREATE TABLE agent_schedules (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER NOT NULL REFERENCES agents(id),\n    day_of_week INTEGER NOT NULL,  -- 0-6 (Sunday-Saturday)\n    start_time TIME NOT NULL,       -- UTC time\n    end_time TIME NOT NULL,         -- UTC time\n    timezone VARCHAR(50) NOT NULL,  -- Original timezone for reference\n    is_active BOOLEAN NOT NULL,\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"agent-guide/scheduling/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>GET /api/agents/{id}/schedules</code> - Get agent schedules</li> <li><code>POST /api/agents/{id}/schedules</code> - Update single schedule</li> <li><code>POST /api/agents/{id}/schedules/bulk</code> - Bulk update schedules</li> <li><code>DELETE /api/agents/{id}/schedules/{day}</code> - Delete schedule for a day</li> <li><code>PUT /api/agents/{id}/scheduling-enabled</code> - Toggle scheduling for agent</li> </ul>"},{"location":"agent-guide/scheduling/#job-assignment-integration","title":"Job Assignment Integration","text":"<p>The job assignment service (<code>GetAvailableAgents</code>) checks scheduling:</p> <pre><code>if agent.SchedulingEnabled {\n    schedulingSetting, err := s.systemSettingsRepo.GetSetting(ctx, \"agent_scheduling_enabled\")\n    if err == nil &amp;&amp; schedulingSetting.Value != nil &amp;&amp; *schedulingSetting.Value == \"true\" {\n        isScheduled, err := s.scheduleRepo.IsAgentScheduledNow(ctx, agent.ID)\n        if err != nil || !isScheduled {\n            continue // Skip this agent\n        }\n    }\n}\n</code></pre>"},{"location":"agent-guide/scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Test Schedules: Always test schedules with non-critical jobs first</li> <li>Timezone Awareness: Be mindful of timezone differences when setting schedules</li> <li>Overlap Planning: Ensure adequate agent coverage during peak hours</li> <li>Maintenance Windows: Schedule maintenance during off-hours</li> <li>Documentation: Document your scheduling strategy for team members</li> </ol>"},{"location":"agent-guide/scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agent-guide/scheduling/#agent-not-getting-jobs-despite-being-scheduled","title":"Agent Not Getting Jobs Despite Being Scheduled","text":"<ol> <li>Check global scheduling is enabled</li> <li>Verify agent scheduling is enabled</li> <li>Confirm current time falls within schedule</li> <li>Check agent is otherwise eligible (enabled, online, etc.)</li> </ol>"},{"location":"agent-guide/scheduling/#schedule-shows-wrong-times","title":"Schedule Shows Wrong Times","text":"<ol> <li>Verify your browser timezone is correct</li> <li>Check the timezone display in the UI</li> <li>Remember all times are stored in UTC</li> </ol>"},{"location":"agent-guide/scheduling/#overnight-schedules-not-working","title":"Overnight Schedules Not Working","text":"<ol> <li>Ensure end time is properly set for next day</li> <li>Verify the schedule spans midnight correctly</li> <li>Check both days involved in the overnight schedule</li> </ol>"},{"location":"agent-guide/scheduling/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements for the scheduling system:</p> <ul> <li>Holiday calendar integration</li> <li>Schedule templates for common patterns</li> <li>Bulk schedule management across multiple agents</li> <li>Schedule conflict detection and warnings</li> <li>Historical schedule effectiveness reporting</li> </ul>"},{"location":"agent-guide/systemd-setup/","title":"Systemd Service Setup","text":""},{"location":"agent-guide/systemd-setup/#overview","title":"Overview","text":"<p>I have yet to test systemd. Please open any issues you may have so that I can address them as needed.</p> <p>This guide explains how to set up the KrakenHashes agent as a systemd service for automatic startup and management. There are two approaches: user services (recommended for personal use) and system services (for production servers).</p>"},{"location":"agent-guide/systemd-setup/#quick-decision-guide","title":"Quick Decision Guide","text":"<ul> <li>User Service: If you're running the agent on your personal machine or don't have root access</li> <li>System Service: If you're setting up on a production server with multiple users or need the agent to start before login</li> </ul>"},{"location":"agent-guide/systemd-setup/#user-service-setup-no-root-required","title":"User Service Setup (No Root Required)","text":"<p>User systemd services run under your user account and don't require sudo privileges. This is the recommended approach for most users.</p>"},{"location":"agent-guide/systemd-setup/#1-create-the-service-directory","title":"1. Create the Service Directory","text":"<pre><code>mkdir -p ~/.config/systemd/user/\n</code></pre>"},{"location":"agent-guide/systemd-setup/#2-create-the-service-file","title":"2. Create the Service File","text":"<p>Create <code>~/.config/systemd/user/krakenhashes-agent.service</code>:</p> <pre><code>[Unit]\nDescription=KrakenHashes Agent\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\n# Working directory where the agent and .env file are located\nWorkingDirectory=%h/krakenhashes-agent\n# Path to the agent executable\nExecStart=%h/krakenhashes-agent/krakenhashes-agent\nRestart=on-failure\nRestartSec=10\n# Optional: Set resource limits\n# MemoryLimit=4G\n# CPUQuota=80%\n\n# Environment variables (optional)\n# Environment=\"DEBUG=true\"\n# Or use an environment file:\n# EnvironmentFile=%h/krakenhashes-agent/.env.systemd\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Note: <code>%h</code> is automatically replaced with your home directory (e.g., <code>/home/username</code>)</p>"},{"location":"agent-guide/systemd-setup/#3-enable-and-start-the-service","title":"3. Enable and Start the Service","text":"<pre><code># Reload systemd to recognize the new service\nsystemctl --user daemon-reload\n\n# Enable the service to start on boot\nsystemctl --user enable krakenhashes-agent\n\n# Start the service now\nsystemctl --user start krakenhashes-agent\n\n# Check status\nsystemctl --user status krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#4-enable-lingering-optional","title":"4. Enable Lingering (Optional)","text":"<p>To start the service at boot (before you log in):</p> <pre><code>sudo loginctl enable-linger $USER\n</code></pre>"},{"location":"agent-guide/systemd-setup/#managing-user-services","title":"Managing User Services","text":"<pre><code># View logs\njournalctl --user -u krakenhashes-agent -f\n\n# Stop the service\nsystemctl --user stop krakenhashes-agent\n\n# Restart the service\nsystemctl --user restart krakenhashes-agent\n\n# Disable auto-start\nsystemctl --user disable krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#system-service-setup-advanced-requires-root","title":"System Service Setup (Advanced - Requires Root)","text":"<p>System services run at the system level and require root/sudo access. This approach is typically used for production servers where the agent needs to run before any user logs in.</p> <p>Note: Most users should use the User Service setup above. Only use system services if you specifically need the agent to run at boot before login.</p>"},{"location":"agent-guide/systemd-setup/#1-create-a-dedicated-user-optional-but-recommended","title":"1. Create a Dedicated User (Optional but Recommended)","text":"<pre><code>sudo useradd -r -s /bin/false -d /var/lib/krakenhashes -m krakenhashes\n</code></pre>"},{"location":"agent-guide/systemd-setup/#2-install-the-agent","title":"2. Install the Agent","text":"<pre><code># Create directory structure\nsudo mkdir -p /opt/krakenhashes-agent\nsudo chown krakenhashes:krakenhashes /opt/krakenhashes-agent\n\n# Copy agent binary from your download location\nsudo cp ~/krakenhashes-agent/krakenhashes-agent /opt/krakenhashes-agent/\nsudo chown krakenhashes:krakenhashes /opt/krakenhashes-agent/krakenhashes-agent\nsudo chmod +x /opt/krakenhashes-agent/krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#3-create-configuration","title":"3. Create Configuration","text":"<p>Create <code>/opt/krakenhashes-agent/.env</code> with your configuration:</p> <pre><code>sudo -u krakenhashes tee /opt/krakenhashes-agent/.env &gt; /dev/null &lt;&lt;EOF\n# Agent configuration\nKH_HOST=your-server.example.com\nKH_PORT=31337\nUSE_TLS=true\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE\nKH_CONFIG_DIR=/opt/krakenhashes-agent/config\nKH_DATA_DIR=/opt/krakenhashes-agent/data\n# Add other configuration as needed\nEOF\n</code></pre>"},{"location":"agent-guide/systemd-setup/#4-create-system-service-file","title":"4. Create System Service File","text":"<p>Create <code>/etc/systemd/system/krakenhashes-agent.service</code>:</p> <pre><code>[Unit]\nDescription=KrakenHashes Agent\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=krakenhashes\nGroup=krakenhashes\nWorkingDirectory=/opt/krakenhashes-agent\nExecStart=/opt/krakenhashes-agent/krakenhashes-agent\nRestart=always\nRestartSec=10\n\n# Security hardening (optional)\nNoNewPrivileges=true\nPrivateTmp=true\nProtectSystem=strict\nProtectHome=true\nReadWritePaths=/opt/krakenhashes-agent\n\n# Resource limits (optional)\n# MemoryLimit=4G\n# CPUQuota=80%\n\n# Environment (optional - .env file is preferred)\n# Environment=\"KH_DATA_DIR=/opt/krakenhashes-agent/data\"\n# Environment=\"KH_CONFIG_DIR=/opt/krakenhashes-agent/config\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"agent-guide/systemd-setup/#5-enable-and-start-the-service","title":"5. Enable and Start the Service","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable service to start on boot\nsudo systemctl enable krakenhashes-agent\n\n# Start the service\nsudo systemctl start krakenhashes-agent\n\n# Check status\nsudo systemctl status krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#managing-system-services","title":"Managing System Services","text":"<pre><code># View logs\nsudo journalctl -u krakenhashes-agent -f\n\n# Stop the service\nsudo systemctl stop krakenhashes-agent\n\n# Restart the service\nsudo systemctl restart krakenhashes-agent\n\n# Disable auto-start\nsudo systemctl disable krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"agent-guide/systemd-setup/#using-environment-files","title":"Using Environment Files","text":"<p>Instead of hardcoding environment variables in the service file, you can use a separate environment file:</p> <ol> <li> <p>Create an environment file (note: different from .env): <pre><code># For user service: ~/.config/krakenhashes-agent.env\n# For system service: /etc/krakenhashes-agent.env\n\nDEBUG=false\nLOG_LEVEL=INFO\n# Don't include sensitive data here as it may be world-readable\n</code></pre></p> </li> <li> <p>Reference it in the service file: <pre><code>[Service]\nEnvironmentFile=/path/to/environment/file\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/systemd-setup/#resource-limits","title":"Resource Limits","text":"<p>Control agent resource usage:</p> <pre><code>[Service]\n# Limit memory usage\nMemoryLimit=4G\nMemoryAccounting=true\n\n# Limit CPU usage (percentage)\nCPUQuota=80%\nCPUAccounting=true\n\n# Limit number of tasks/threads\nTasksMax=100\n</code></pre>"},{"location":"agent-guide/systemd-setup/#automatic-restart-configuration","title":"Automatic Restart Configuration","text":"<pre><code>[Service]\n# Restart on failure\nRestart=on-failure\nRestartSec=10\n\n# Or always restart (even on clean exit)\nRestart=always\nRestartSec=10\n\n# Limit restart attempts\nStartLimitInterval=600\nStartLimitBurst=5\n</code></pre>"},{"location":"agent-guide/systemd-setup/#gpu-access-for-system-services","title":"GPU Access for System Services","text":"<p>If running as a system service with GPU access:</p> <pre><code>[Service]\n# Add the service user to the video/render groups\nSupplementaryGroups=video render\n\n# Or for NVIDIA GPUs specifically\nSupplementaryGroups=video\n# May need to adjust device permissions\nDeviceAllow=/dev/nvidia* rw\nDeviceAllow=/dev/nvidiactl rw\nDeviceAllow=/dev/nvidia-uvm rw\n</code></pre>"},{"location":"agent-guide/systemd-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agent-guide/systemd-setup/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Service fails to start: Check logs with <code>journalctl --user -u krakenhashes-agent</code> (user) or <code>sudo journalctl -u krakenhashes-agent</code> (system)</p> </li> <li> <p>Permission denied errors: </p> </li> <li>User service: Ensure the agent binary is executable and in your home directory</li> <li> <p>System service: Check file ownership and permissions</p> </li> <li> <p>Agent can't find .env file:</p> </li> <li>Ensure WorkingDirectory is set correctly in the service file</li> <li> <p>Check that the .env file exists in that directory</p> </li> <li> <p>GPU not detected:</p> </li> <li>User service: Should work if you can access GPU normally</li> <li>System service: May need SupplementaryGroups configuration</li> </ol>"},{"location":"agent-guide/systemd-setup/#viewing-logs","title":"Viewing Logs","text":"<pre><code># User service - last 100 lines\njournalctl --user -u krakenhashes-agent -n 100\n\n# System service - last 100 lines  \nsudo journalctl -u krakenhashes-agent -n 100\n\n# Follow logs in real-time\njournalctl --user -u krakenhashes-agent -f  # User\nsudo journalctl -u krakenhashes-agent -f     # System\n\n# Logs from last boot\njournalctl --user -u krakenhashes-agent -b  # User\nsudo journalctl -u krakenhashes-agent -b     # System\n\n# Export logs to file\njournalctl --user -u krakenhashes-agent &gt; agent.log  # User\nsudo journalctl -u krakenhashes-agent &gt; agent.log     # System\n</code></pre>"},{"location":"agent-guide/systemd-setup/#service-status-commands","title":"Service Status Commands","text":"<pre><code># Check if service is active\nsystemctl --user is-active krakenhashes-agent   # User\nsudo systemctl is-active krakenhashes-agent      # System\n\n# Check if service is enabled\nsystemctl --user is-enabled krakenhashes-agent  # User  \nsudo systemctl is-enabled krakenhashes-agent     # System\n\n# Show service details\nsystemctl --user show krakenhashes-agent        # User\nsudo systemctl show krakenhashes-agent           # System\n</code></pre>"},{"location":"agent-guide/systemd-setup/#migration-between-service-types","title":"Migration Between Service Types","text":""},{"location":"agent-guide/systemd-setup/#from-manual-to-user-service","title":"From Manual to User Service","text":"<ol> <li>Stop the manual agent process</li> <li>Copy your existing <code>.env</code> file to the agent directory</li> <li>Follow the user service setup steps above</li> <li>Start the user service</li> </ol>"},{"location":"agent-guide/systemd-setup/#from-user-service-to-system-service","title":"From User Service to System Service","text":"<ol> <li>Stop the user service: <code>systemctl --user stop krakenhashes-agent</code></li> <li>Disable the user service: <code>systemctl --user disable krakenhashes-agent</code></li> <li>Copy your agent files to system location</li> <li>Follow the system service setup steps above</li> <li>Start the system service</li> </ol>"},{"location":"agent-guide/systemd-setup/#best-practices","title":"Best Practices","text":"<ol> <li>Use user services when possible - they're simpler and don't require root</li> <li>Keep the .env file in the same directory as specified in WorkingDirectory</li> <li>Set resource limits to prevent the agent from consuming too many resources</li> <li>Monitor logs regularly to catch issues early</li> <li>Use enable-linger for user services that should run without login</li> <li>Document your configuration including any custom paths or settings</li> </ol>"},{"location":"agent-guide/systemd-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Configure the agent</li> <li>Set up scheduling</li> <li>Monitor agent performance</li> </ul>"},{"location":"agent-guide/troubleshooting/","title":"Agent Troubleshooting Guide","text":"<p>This guide helps diagnose and resolve common issues with KrakenHashes agents. Use this reference when agents fail to connect, register, sync files, detect hardware, or execute jobs.</p>"},{"location":"agent-guide/troubleshooting/#quick-diagnostic-commands","title":"Quick Diagnostic Commands","text":"<p>Before diving into specific issues, run these commands to gather diagnostic information:</p> <pre><code># Check agent status\nsystemctl status krakenhashes-agent\n\n# View recent agent logs\njournalctl -u krakenhashes-agent -f --since \"5 minutes ago\"\n\n# Check agent configuration\n/path/to/krakenhashes-agent --version\ncat ~/.krakenhashes/agent/.env\n\n# Test connectivity to backend\ncurl -k https://your-backend:31337/api/health\n\n# Verify certificate files\nls -la ~/.krakenhashes/agent/config/\nopenssl x509 -in ~/.krakenhashes/agent/config/client.crt -text -noout\n</code></pre>"},{"location":"agent-guide/troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"agent-guide/troubleshooting/#agent-cannot-connect-to-backend","title":"Agent Cannot Connect to Backend","text":"<p>Symptoms: - Agent logs show \"failed to connect to WebSocket server\" - Repeated connection retry attempts - Certificate verification errors</p> <p>Common Causes:</p> <ol> <li> <p>Incorrect Backend URL Configuration <pre><code># Check agent configuration\ngrep -E \"KH_HOST|KH_PORT\" ~/.krakenhashes/agent/.env\n\n# Test backend accessibility\nping your-backend-host\ntelnet your-backend-host 31337\n</code></pre></p> </li> <li> <p>Certificate Issues <pre><code># Check certificate files exist\nls -la ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Verify certificate validity\nopenssl x509 -in ~/.krakenhashes/agent/config/client.crt -text -noout | grep -E \"Valid|Subject|Issuer\"\n</code></pre></p> </li> <li> <p>Network Firewall Blocking <pre><code># Test HTTPS connectivity\ncurl -k https://your-backend:31337/api/health\n\n# Test WebSocket connectivity (if nc available)\nnc -zv your-backend 31337\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Update Backend URL <pre><code># Edit agent configuration\nnano ~/.krakenhashes/agent/.env\n\n# Set correct values\nKH_HOST=your-backend-hostname\nKH_PORT=31337\nUSE_TLS=true\n\n# Restart agent\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> <li> <p>Renew Certificates <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Remove old certificates\nrm ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Start agent (will automatically renew certificates)\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> <li> <p>Fix Network/Firewall <pre><code># Check firewall rules\nsudo ufw status\nsudo iptables -L\n\n# Open required ports\nsudo ufw allow out 31337\nsudo ufw allow out 443\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#connection-drops-frequently","title":"Connection Drops Frequently","text":"<p>Symptoms: - Agent connects but disconnects after short periods - WebSocket ping/pong timeouts - Frequent reconnection attempts</p> <p>Causes and Solutions:</p> <ol> <li> <p>Network Instability <pre><code># Monitor network quality\nping -c 10 your-backend-host\n\n# Check for packet loss\nmtr your-backend-host\n</code></pre></p> </li> <li> <p>Backend Overload <pre><code># Check backend logs for resource issues\ndocker-compose -f docker-compose.dev-local.yml logs backend | grep -i \"error\\|timeout\\|overload\"\n</code></pre></p> </li> <li> <p>Aggressive Firewall/NAT <pre><code># Adjust WebSocket keepalive settings in agent config\necho \"KH_PING_PERIOD=30s\" &gt;&gt; ~/.krakenhashes/agent/.env\necho \"KH_PONG_WAIT=60s\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#registration-and-authentication-issues","title":"Registration and Authentication Issues","text":""},{"location":"agent-guide/troubleshooting/#agent-registration-fails","title":"Agent Registration Fails","text":"<p>Symptoms: - \"Registration failed\" errors - \"Invalid claim code\" messages - \"Registration request failed\" in logs</p> <p>Common Causes:</p> <ol> <li>Invalid or Expired Claim Code</li> <li>Check admin panel for active vouchers</li> <li> <p>Generate new voucher if expired</p> </li> <li> <p>Certificate Download Issues <pre><code># Test CA certificate download\ncurl -k https://your-backend:31337/ca.crt -o /tmp/ca.crt\nopenssl x509 -in /tmp/ca.crt -text -noout\n</code></pre></p> </li> <li> <p>Clock Synchronization Issues <pre><code># Check system time\ntimedatectl status\n\n# Sync time if needed\nsudo ntpdate -s time.nist.gov\n# or\nsudo chrony sources -v\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li>Get Valid Claim Code</li> <li>Access backend admin panel</li> <li>Go to Agent Management \u2192 Generate Voucher</li> <li> <p>Use the new claim code immediately</p> </li> <li> <p>Manual Registration <pre><code># Stop agent service\nsystemctl stop krakenhashes-agent\n\n# Register manually\n/path/to/krakenhashes-agent --register --claim-code YOUR_CLAIM_CODE --host your-backend:31337\n\n# Start service\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#authentication-errors-after-registration","title":"Authentication Errors After Registration","text":"<p>Symptoms: - \"Failed to load API key\" errors - \"Authentication failed\" messages - Agent connected but backend rejects requests</p> <p>Diagnostic Steps: <pre><code># Check credentials files\nls -la ~/.krakenhashes/agent/config/\ncat ~/.krakenhashes/agent/config/agent.key\n\n# Verify API key format (should be UUID)\ngrep -E '^[0-9a-f-]{36}:[0-9]+$' ~/.krakenhashes/agent/config/agent.key\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Regenerate Credentials <pre><code># Remove existing credentials\nrm ~/.krakenhashes/agent/config/agent.key\nrm ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Re-register\nsystemctl stop krakenhashes-agent\n/path/to/krakenhashes-agent --register --claim-code NEW_CLAIM_CODE --host your-backend:31337\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> <li> <p>Fix Permissions <pre><code># Set correct ownership and permissions\nchown -R $(whoami):$(whoami) ~/.krakenhashes/agent/\nchmod 700 ~/.krakenhashes/agent/config/\nchmod 600 ~/.krakenhashes/agent/config/agent.key\nchmod 600 ~/.krakenhashes/agent/config/client.key\nchmod 644 ~/.krakenhashes/agent/config/*.crt\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#hardware-detection-issues","title":"Hardware Detection Issues","text":""},{"location":"agent-guide/troubleshooting/#no-devices-detected","title":"No Devices Detected","text":"<p>Symptoms: - Agent shows \"0 devices detected\" - Missing GPU information in admin panel - Hashcat fails to find OpenCL/CUDA devices</p> <p>Diagnostic Steps: <pre><code># Check if hashcat binary exists\nls -la ~/.krakenhashes/agent/data/binaries/\n\n# Manually test hashcat device detection\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1 | xargs -I {} {} -I\n\n# Check for GPU drivers\nnvidia-smi           # NVIDIA\nrocm-smi             # AMD\nintel_gpu_top        # Intel\nlspci | grep -i vga  # General\n</code></pre></p> <p>Common Solutions:</p> <ol> <li> <p>Install GPU Drivers <pre><code># NVIDIA\nsudo apt update\nsudo apt install nvidia-driver-470  # or latest\n\n# AMD\nsudo apt install rocm-opencl-runtime\n\n# Intel\nsudo apt install intel-opencl-icd\n</code></pre></p> </li> <li> <p>Install OpenCL Runtime <pre><code># Install generic OpenCL\nsudo apt install ocl-icd-opencl-dev opencl-headers\n\n# Verify OpenCL installation\nclinfo  # if available\n</code></pre></p> </li> <li> <p>Fix Hashcat Binary Issues <pre><code># Check hashcat binary permissions\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs ls -la\n\n# Make executable if needed\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs chmod +x\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#devices-detected-but-not-usable","title":"Devices Detected But Not Usable","text":"<p>Symptoms: - Agent shows devices in hardware detection output - Devices appear in the admin panel - However, devices are not available for job execution - Jobs fail with \"no devices available\" errors</p> <p>Common Cause:</p> <p>Hashcat 7.x compatibility issues with older GPU drivers. Hashcat 7.x may detect devices but fail to initialize them for compute operations with certain driver versions.</p> <p>Solutions:</p> <ol> <li>Use Hashcat 6.x Binary (Recommended)</li> <li>Navigate to your Agent Details page in the web UI</li> <li>Enable \"Binary Override\" toggle</li> <li>Select a Hashcat 6.x version (e.g., 6.2.6 or 6.2.5) from the dropdown</li> <li>Click \"Save\"</li> <li>The agent will automatically download the binary</li> <li> <p>Device detection will re-run with the 6.x binary</p> </li> <li> <p>Update GPU Drivers <pre><code># NVIDIA - Use drivers 545.x or newer\nsudo apt update\nsudo apt install nvidia-driver-545  # or latest\nsudo reboot\n\n# AMD - Use ROCm 5.7 or newer / Adrenalin 23.12 or newer\n# Follow AMD's official ROCm installation guide\n\n# Verify driver installation\nnvidia-smi  # NVIDIA\nrocm-smi    # AMD\n</code></pre></p> </li> <li> <p>Verify Driver Compatibility <pre><code># For NVIDIA - check driver version\nnvidia-smi --query-gpu=driver_version --format=csv,noheader\n\n# For AMD - check ROCm version\nrocminfo | grep \"Agent\"\n\n# Manually test hashcat with specific binary\n~/.krakenhashes/agent/data/binaries/3/hashcat.bin -I  # Replace 3 with your binary version\n</code></pre></p> </li> <li> <p>Check Agent Logs <pre><code># Look for device initialization errors\njournalctl -u krakenhashes-agent -n 100 | grep -i \"device\\|gpu\\|opencl\\|cuda\"\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#partial-device-detection","title":"Partial Device Detection","text":"<p>Symptoms: - Some GPUs detected, others missing - Device count mismatch - Specific GPU types not showing</p> <p>Solutions:</p> <ol> <li> <p>Mixed GPU Environment <pre><code># Ensure all necessary drivers installed\nnvidia-smi &amp;&amp; rocm-smi &amp;&amp; intel_gpu_top --list\n\n# Check for driver conflicts\ndmesg | grep -i \"gpu\\|nvidia\\|amd\\|intel\" | tail -20\n</code></pre></p> </li> <li> <p>PCIe/Power Issues <pre><code># Check PCIe slot detection\nlspci | grep -i vga\nsudo lshw -c display\n\n# Check power management\ncat /sys/class/drm/card*/device/power_state\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#file-synchronization-problems","title":"File Synchronization Problems","text":""},{"location":"agent-guide/troubleshooting/#files-not-downloading","title":"Files Not Downloading","text":"<p>Symptoms: - Wordlists/rules not available for jobs - \"File not found\" errors during job execution - Sync requests timing out</p> <p>Diagnostic Steps: <pre><code># Check data directories\nls -la ~/.krakenhashes/agent/data/\nls ~/.krakenhashes/agent/data/wordlists/\nls ~/.krakenhashes/agent/data/rules/\nls ~/.krakenhashes/agent/data/binaries/\n\n# Test file download manually\ncurl -k -H \"X-API-Key: YOUR_API_KEY\" -H \"X-Agent-ID: YOUR_AGENT_ID\" \\\n     https://your-backend:31337/api/agent/files/wordlists/rockyou.txt \\\n     -o /tmp/test_download.txt\n</code></pre></p> <p>Common Solutions:</p> <ol> <li> <p>Fix Authentication <pre><code># Verify API key is valid\ngrep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1\n\n# Test API authentication\nAPI_KEY=$(grep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1)\nAGENT_ID=$(grep -o '[^:]*$' ~/.krakenhashes/agent/config/agent.key)\ncurl -k -H \"X-API-Key: $API_KEY\" -H \"X-Agent-ID: $AGENT_ID\" \\\n     https://your-backend:31337/api/agent/info\n</code></pre></p> </li> <li> <p>Fix Directory Permissions <pre><code># Ensure agent can write to data directories\nchown -R $(whoami):$(whoami) ~/.krakenhashes/agent/data/\nchmod -R 755 ~/.krakenhashes/agent/data/\n</code></pre></p> </li> <li> <p>Clear Corrupted Downloads <pre><code># Remove partial/corrupted files\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -delete\nfind ~/.krakenhashes/agent/data/ -size 0 -delete\n\n# Force re-sync\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#binary-extraction-failures","title":"Binary Extraction Failures","text":"<p>Symptoms: - Downloaded .7z files not extracted - Hashcat binary not executable - \"No such file or directory\" when running hashcat</p> <p>Solutions:</p> <ol> <li> <p>Install 7-Zip Support <pre><code>sudo apt install p7zip-full\n\n# Test extraction manually\ncd ~/.krakenhashes/agent/data/binaries/\nfind . -name \"*.7z\" | head -1 | xargs 7z t  # Test archive\n</code></pre></p> </li> <li> <p>Fix Extraction Permissions <pre><code># Ensure extraction destination is writable\nchmod 755 ~/.krakenhashes/agent/data/binaries/\n\n# Re-extract manually if needed\ncd ~/.krakenhashes/agent/data/binaries/\nfind . -name \"*.7z\" -exec 7z x {} \\;\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#job-execution-failures","title":"Job Execution Failures","text":""},{"location":"agent-guide/troubleshooting/#jobs-not-starting","title":"Jobs Not Starting","text":"<p>Symptoms: - Tasks assigned but never start - Agent shows as idle despite task assignment - \"No enabled devices\" errors</p> <p>Diagnostic Steps: <pre><code># Check agent task status\njournalctl -u krakenhashes-agent | grep -i \"task\\|job\" | tail -10\n\n# Verify enabled devices in backend\n# (Check admin panel Agent Details page)\n\n# Test hashcat manually\nHASHCAT=$(find ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1)\n$HASHCAT --help\n</code></pre></p> <p>Solutions:</p> <ol> <li>Enable Devices</li> <li>Go to backend Admin Panel</li> <li>Navigate to Agent Management</li> <li> <p>Select agent and enable required devices</p> </li> <li> <p>Fix Hashcat Path <pre><code># Ensure hashcat binary is executable\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs chmod +x\n\n# Create symlink if needed\nHASHCAT=$(find ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1)\nsudo ln -sf \"$HASHCAT\" /usr/local/bin/hashcat\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#jobs-crash-or-stop-unexpectedly","title":"Jobs Crash or Stop Unexpectedly","text":"<p>Symptoms: - Jobs start but terminate quickly - \"Process killed\" messages - Hashcat segmentation faults</p> <p>Diagnostic Steps: <pre><code># Check system resources\nfree -h\ndf -h ~/.krakenhashes/agent/data/\nps aux | grep hashcat\n\n# Check for OOM kills\ndmesg | grep -i \"killed process\\|out of memory\" | tail -5\njournalctl -f | grep -i \"oom\\|memory\"\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Resource Issues <pre><code># Check memory usage\nfree -h\n\n# Clear cache if needed\nsudo sync\necho 3 | sudo tee /proc/sys/vm/drop_caches\n\n# Check disk space\ndf -h ~/.krakenhashes/agent/data/\n\n# Clean old files if needed\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -mtime +7 -delete\n</code></pre></p> </li> <li> <p>Driver/Hardware Issues <pre><code># Check GPU status\nnvidia-smi  # Check temperature, power, utilization\n\n# Test memory stability\nnvidia-smi --query-gpu=memory.used,memory.free,temperature.gpu --format=csv -lms 1000\n\n# Check for hardware errors\ndmesg | grep -i \"error\\|fault\" | tail -10\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#job-progress-not-reporting","title":"Job Progress Not Reporting","text":"<p>Symptoms: - Jobs running but no progress updates - Backend shows tasks as \"running\" indefinitely - No crack notifications</p> <p>Solutions:</p> <ol> <li> <p>Check WebSocket Connection <pre><code># Verify agent is connected\njournalctl -u krakenhashes-agent | grep -i \"websocket\\|connected\" | tail -5\n\n# Look for progress send errors\njournalctl -u krakenhashes-agent | grep -i \"progress\\|send.*fail\" | tail -10\n</code></pre></p> </li> <li> <p>Restart Agent Connection <pre><code># Restart agent service\nsystemctl restart krakenhashes-agent\n\n# Monitor connection establishment\njournalctl -u krakenhashes-agent -f | grep -i \"connect\\|progress\"\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#agent-stability-and-connection-issues","title":"Agent Stability and Connection Issues","text":""},{"location":"agent-guide/troubleshooting/#agent-crashes-during-high-volume-cracking","title":"Agent Crashes During High-Volume Cracking","text":"<p>Symptoms: - Agent crashes when thousands of hashes crack rapidly - Panic errors related to closed channels - Connection drops during large password discoveries - \"send on closed channel\" errors</p> <p>Root Cause: High-volume cracking (e.g., 4,000+ cracks in seconds) can overwhelm the WebSocket message system if not properly buffered.</p> <p>Solutions Implemented (v1.2.1+):</p> <p>The system now includes automatic protections:</p> <ol> <li>Crack Batching System</li> <li>Cracks are batched in 500ms windows or 10,000-crack groups</li> <li>Reduces message volume by 100x (8,000 messages \u2192 80 messages)</li> <li> <p>See Crack Batching System for details</p> </li> <li> <p>Increased Channel Buffers <pre><code>// Agent outbound buffer increased from 256 \u2192 4,096 messages\n// Handles burst traffic during high-volume cracking\n</code></pre></p> </li> <li> <p>Channel Monitoring</p> </li> <li>Automatic warnings when buffer reaches 75% capacity</li> <li>Critical alerts at 90% capacity</li> <li>Graceful message dropping instead of crashes</li> </ol> <p>Monitoring for Issues: <pre><code># Check for channel fullness warnings\njournalctl -u krakenhashes-agent | grep -i \"channel.*full\\|fullness\"\n\n# Look for dropped messages (indicates overload)\njournalctl -u krakenhashes-agent | grep -i \"dropped message\"\n\n# Monitor batch sizes (should be 500-10000 cracks)\njournalctl -u krakenhashes-agent | grep -i \"flush.*batch\"\n</code></pre></p> <p>Expected Log Messages (Normal Operation): <pre><code>[INFO] Flushing crack batch for task abc-123: 2453 cracks\n[DEBUG] Crack batch sent successfully\n</code></pre></p> <p>Warning Signs: <pre><code>[WARNING] Outbound channel filling up (78.2%)\n[ERROR] Outbound channel critically full (92.5%)\n[ERROR] Dropped message - channel full (95.0%)\n</code></pre></p> <p>Recovery Actions:</p> <p>If you see persistent channel fullness warnings:</p> <ol> <li> <p>Check Backend Performance <pre><code># Verify backend is processing batches quickly\ndocker logs krakenhashes-backend | grep -i \"crack batch\\|processing.*cracks\"\n</code></pre></p> </li> <li> <p>Monitor Network Bandwidth <pre><code># Ensure adequate bandwidth for WebSocket traffic\niftop -i eth0  # or your network interface\n</code></pre></p> </li> <li> <p>Verify Database Performance <pre><code># Check for slow crack processing queries\n# (See backend logs for timing information)\ndocker logs krakenhashes-backend | grep -i \"processed.*cracks.*seconds\"\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#double-close-panic-prevention","title":"Double-Close Panic Prevention","text":"<p>Historical Issue (Fixed in v1.2.1): Agents could crash with \"close of closed channel\" panics during connection cleanup.</p> <p>Symptoms (if using older version): <pre><code>panic: close of closed channel\ngoroutine 123 [running]:\nagent/internal/agent.(*AgentConnection).cleanup()\n</code></pre></p> <p>Solution: Update to v1.2.1+ which includes: - Mutex-protected channel closing - Close-once semantics with sync.Once - Graceful shutdown during connection cleanup</p> <p>If Still Experiencing Issues: <pre><code># Verify agent version\n/path/to/krakenhashes-agent --version\n\n# Should show v1.2.1 or later\n# If older, update agent binary\n</code></pre></p>"},{"location":"agent-guide/troubleshooting/#channel-overflow-protection","title":"Channel Overflow Protection","text":"<p>How the System Protects You:</p> <ol> <li>Automatic Batching</li> <li>Individual cracks accumulated in memory</li> <li>Sent in bulk every 500ms or when 10k accumulated</li> <li> <p>Reduces network traffic and message count</p> </li> <li> <p>Buffer Monitoring</p> </li> <li>System tracks outbound channel capacity</li> <li>Warnings logged before critical levels reached</li> <li> <p>Allows proactive investigation</p> </li> <li> <p>Graceful Degradation</p> </li> <li>If channel is full, message is dropped (not crashed)</li> <li>Drop events are logged for investigation</li> <li>Agent remains operational</li> </ol> <p>Performance Tuning:</p> <p>For environments with extremely high crack rates:</p> <pre><code># Increase channel buffer (requires agent rebuild)\n# Edit agent/internal/agent/connection.go:\n# outbound: make(chan []byte, 8192)  // Double the default\n\n# Or reduce batch window for more frequent smaller batches\n# Edit agent/internal/jobs/hashcat_executor.go:\n# crackBatchInterval: 250 * time.Millisecond  // Half the window\n</code></pre> <p>\u26a0\ufe0f Warning: Custom tuning is rarely needed. The defaults handle &gt;99% of scenarios including extremely high-volume cracking.</p>"},{"location":"agent-guide/troubleshooting/#connection-stability-best-practices","title":"Connection Stability Best Practices","text":"<ol> <li> <p>Monitor Logs Proactively <pre><code># Set up log monitoring for early warning signs\njournalctl -u krakenhashes-agent -f | grep -E \"WARNING|ERROR|full|dropped\"\n</code></pre></p> </li> <li> <p>Network Quality</p> </li> <li>Ensure stable, low-latency connection to backend</li> <li>Avoid Wi-Fi for production agents (use wired connections)</li> <li> <p>Monitor for packet loss: <code>mtr your-backend-host</code></p> </li> <li> <p>Backend Capacity</p> </li> <li>Ensure backend can process batches quickly (&lt;5 seconds)</li> <li>Monitor backend CPU/memory during high-volume jobs</li> <li> <p>Scale backend resources if consistent warnings appear</p> </li> <li> <p>Update Regularly</p> </li> <li>Keep agent binary up to date for latest stability fixes</li> <li>Review release notes for performance improvements</li> <li>Test updates in dev environment first</li> </ol>"},{"location":"agent-guide/troubleshooting/#performance-problems","title":"Performance Problems","text":""},{"location":"agent-guide/troubleshooting/#slow-hash-rates","title":"Slow Hash Rates","text":"<p>Symptoms: - Lower than expected H/s rates - GPU underutilization - Benchmark speeds don't match job speeds</p> <p>Solutions:</p> <ol> <li> <p>GPU Optimization <pre><code># Check GPU power limits\nnvidia-smi -q -d POWER\n\n# Increase power limit (if supported)\nsudo nvidia-smi -pl 300  # 300W example\n\n# Set performance mode\nsudo nvidia-smi -pm 1\n</code></pre></p> </li> <li> <p>Cooling and Throttling <pre><code># Monitor temperatures\nwatch nvidia-smi\n\n# Check thermal throttling\nnvidia-smi --query-gpu=temperature.gpu,clocks_throttle_reasons.gpu_idle,clocks_throttle_reasons.applications_clocks_setting --format=csv -lms 1000\n</code></pre></p> </li> <li> <p>Hashcat Parameters <pre><code># Add optimization flags in agent config\necho \"HASHCAT_EXTRA_PARAMS=-O -w 4\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#high-system-load","title":"High System Load","text":"<p>Symptoms: - System becomes unresponsive - Other applications slow down - CPU usage constantly high</p> <p>Solutions:</p> <ol> <li> <p>Limit Resource Usage <pre><code># Limit hashcat workload\necho \"HASHCAT_EXTRA_PARAMS=-w 2\" &gt;&gt; ~/.krakenhashes/agent/.env\n\n# Set CPU affinity (example: use only cores 0-3)\nsystemctl edit krakenhashes-agent\n# Add:\n# [Service]\n# CPUAffinity=0-3\n</code></pre></p> </li> <li> <p>System Tuning <pre><code># Increase file descriptor limits\necho \"* soft nofile 65536\" | sudo tee -a /etc/security/limits.conf\necho \"* hard nofile 65536\" | sudo tee -a /etc/security/limits.conf\n\n# Optimize memory management\necho 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf\nsudo sysctl -p\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#error-message-reference","title":"Error Message Reference","text":""},{"location":"agent-guide/troubleshooting/#common-error-patterns","title":"Common Error Patterns","text":"Error Message Cause Solution <code>failed to connect to WebSocket server</code> Network/TLS issues Check connectivity, renew certificates <code>failed to load API key</code> Missing/corrupt credentials Re-register agent <code>registration failed</code> Invalid claim code Generate new voucher <code>failed to detect devices</code> Missing drivers/OpenCL Install GPU drivers <code>no enabled devices</code> Devices disabled in backend Enable devices in admin panel <code>file sync timeout</code> Network/authentication issues Check API credentials <code>hashcat not found</code> Missing/corrupt binary Re-download binaries <code>certificate verify failed</code> Expired/invalid certificates Renew certificates <code>connection refused</code> Backend not accessible Check backend status <code>permission denied</code> File/directory permissions Fix ownership/permissions"},{"location":"agent-guide/troubleshooting/#debug-logging","title":"Debug Logging","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code># Enable debug logging\necho \"DEBUG=true\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n\n# View detailed logs\njournalctl -u krakenhashes-agent -f\n\n# Disable debug logging after troubleshooting\nsed -i '/DEBUG=true/d' ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"agent-guide/troubleshooting/#complete-agent-reset","title":"Complete Agent Reset","text":"<p>When all else fails, completely reset the agent:</p> <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Backup current configuration\ncp -r ~/.krakenhashes/agent ~/.krakenhashes/agent.backup.$(date +%Y%m%d)\n\n# Remove all agent data\nrm -rf ~/.krakenhashes/agent/\n\n# Re-register with new claim code\n/path/to/krakenhashes-agent --register --claim-code NEW_CLAIM_CODE --host your-backend:31337\n\n# Start agent\nsystemctl start krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#emergency-job-cleanup","title":"Emergency Job Cleanup","text":"<p>Force cleanup of stuck hashcat processes:</p> <pre><code># Kill all hashcat processes\npkill -f hashcat\n\n# Clean temporary files\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -delete\nfind ~/.krakenhashes/agent/data/ -name \"*.restore\" -delete\n\n# Restart agent to reset job state\nsystemctl restart krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#certificate-recovery","title":"Certificate Recovery","text":"<p>Recover from certificate issues:</p> <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Download CA certificate manually\ncurl -k https://your-backend:31337/ca.crt -o ~/.krakenhashes/agent/config/ca.crt\n\n# Use API key to renew client certificates\nAPI_KEY=$(grep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1)\nAGENT_ID=$(grep -o '[^:]*$' ~/.krakenhashes/agent/config/agent.key)\ncurl -k -X POST -H \"X-API-Key: $API_KEY\" -H \"X-Agent-ID: $AGENT_ID\" \\\n     https://your-backend:31337/api/agent/renew-certificates\n\n# Start agent\nsystemctl start krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#when-to-restart-vs-reinstall","title":"When to Restart vs Reinstall","text":""},{"location":"agent-guide/troubleshooting/#restart-agent-service","title":"Restart Agent Service","text":"<ul> <li>Connection drops</li> <li>Configuration changes</li> <li>Minor authentication issues</li> <li>After enabling/disabling devices</li> </ul>"},{"location":"agent-guide/troubleshooting/#restart-system","title":"Restart System","text":"<ul> <li>GPU driver updates</li> <li>System resource exhaustion</li> <li>Hardware changes</li> <li>Kernel updates</li> </ul>"},{"location":"agent-guide/troubleshooting/#reinstall-agent","title":"Reinstall Agent","text":"<ul> <li>Corrupt binary files</li> <li>Persistent authentication failures after certificate renewal</li> <li>File system permission issues that can't be resolved</li> <li>Agent binary corruption</li> </ul>"},{"location":"agent-guide/troubleshooting/#complete-reset-last-resort","title":"Complete Reset (Last Resort)","text":"<ul> <li>Multiple interconnected issues</li> <li>System contamination from previous installations</li> <li>Unknown configuration corruption</li> <li>When restart and reinstall don't resolve issues</li> </ul> <p>Use the diagnostic commands at the beginning of this guide to determine the appropriate recovery level.</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/","title":"Implementation Summary: Benchmark-Based Job Assignment","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#overview","title":"Overview","text":"<p>Updated the backend's job scheduling service to wait for agent benchmarks before assigning work. This ensures accurate chunk calculations based on real-world performance metrics.</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#key-changes","title":"Key Changes","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#1-jobschedulingservice-backendinternalservicesjob_scheduling_servicego","title":"1. JobSchedulingService (<code>backend/internal/services/job_scheduling_service.go</code>)","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#updated-interface","title":"Updated Interface","text":"<pre><code>type JobWebSocketIntegration interface {\n    SendJobAssignment(ctx context.Context, task *models.JobTask, jobExecution *models.JobExecution) error\n    RequestAgentBenchmark(ctx context.Context, agentID int, jobExecution *models.JobExecution) error\n}\n</code></pre>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#modified-assignworktoagent-function","title":"Modified <code>assignWorkToAgent</code> Function","text":"<ul> <li>Added benchmark validation before calculating chunks</li> <li>Retrieves hashlist to get hash type for benchmark lookup</li> <li>Checks if agent has a valid benchmark for the attack mode/hash type combination</li> <li>Validates benchmark freshness using configurable cache duration</li> <li>If no valid benchmark exists:</li> <li>Sends benchmark request via WebSocket</li> <li>Returns without assigning work (deferred assignment)</li> <li>Agent remains available for next scheduling cycle</li> <li>If valid benchmark exists:</li> <li>Proceeds with normal chunk calculation and assignment</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#2-jobwebsocketintegration-backendinternalintegrationjob_websocket_integrationgo","title":"2. JobWebSocketIntegration (<code>backend/internal/integration/job_websocket_integration.go</code>)","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#new-method-requestagentbenchmark","title":"New Method: <code>RequestAgentBenchmark</code>","text":"<ul> <li>Implements the new interface method</li> <li>Retrieves full job configuration:</li> <li>Preset job details (binary version, wordlists, rules, mask)</li> <li>Hashlist details (hash type)</li> <li>Agent information</li> <li>Builds enhanced benchmark request with:</li> <li>Actual wordlist paths from the job</li> <li>Rule files if applicable</li> <li>Binary path for the specific version</li> <li>Hash type and attack mode</li> <li>Test duration (30 seconds for accuracy)</li> <li>Sends comprehensive benchmark request to agent</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#3-websocket-types-already-existed-enhanced-usage","title":"3. WebSocket Types (already existed, enhanced usage)","text":"<p>The <code>BenchmarkRequestPayload</code> now includes: - <code>HashlistID</code> and <code>HashlistPath</code> for real-world testing - <code>WordlistPaths</code> array for dictionary attacks - <code>RulePaths</code> array for rule-based attacks - <code>Mask</code> for brute force patterns - <code>TestDuration</code> for benchmark duration</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#workflow","title":"Workflow","text":"<ol> <li>Agent requests work \u2192 Scheduler finds pending job</li> <li>Benchmark check \u2192 System verifies agent has valid benchmark</li> <li>If no benchmark:</li> <li>Request benchmark with full job config</li> <li>Agent performs real-world speed test</li> <li>Reports results back</li> <li>Next scheduling cycle assigns work</li> <li>If valid benchmark exists:</li> <li>Calculate chunk based on known performance</li> <li>Assign work immediately</li> </ol>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#benefits","title":"Benefits","text":"<ul> <li>Accurate Performance: Benchmarks use actual job parameters</li> <li>Optimal Chunks: Prevents over/under-sized work assignments</li> <li>Reduced Failures: Avoids assigning impossible workloads</li> <li>Better Utilization: Maximizes agent efficiency</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#configuration","title":"Configuration","text":"<ul> <li><code>benchmark_cache_duration_hours</code>: Benchmark validity period (default: 168 hours)</li> <li><code>default_chunk_duration</code>: Target chunk duration in seconds (default: 1200)</li> <li><code>chunk_fluctuation_percentage</code>: Final chunk size tolerance (default: 20%)</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#testing","title":"Testing","text":"<p>Created unit tests demonstrating: - Benchmark request flow when no benchmark exists - Job assignment flow with valid benchmark - Mock WebSocket integration for testing</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#files-modified","title":"Files Modified","text":"<ol> <li><code>/backend/internal/services/job_scheduling_service.go</code></li> <li><code>/backend/internal/integration/job_websocket_integration.go</code></li> <li><code>/backend/internal/services/job_scheduling_benchmark_test.go</code> (new)</li> <li><code>/backend/BENCHMARK_WORKFLOW.md</code> (new documentation)</li> </ol>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#future-considerations","title":"Future Considerations","text":"<ul> <li>Benchmark history tracking</li> <li>Performance anomaly detection</li> <li>Multi-GPU per-device benchmarking</li> <li>Predictive performance modeling</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/","title":"Efficient Paginated Polling Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#overview","title":"Overview","text":"<p>This document describes the implementation of an efficient paginated polling system for the Jobs page, replacing the SSE (Server-Sent Events) approach with a more scalable solution.</p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#key-features","title":"Key Features","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#1-paginated-data-fetching","title":"1. Paginated Data Fetching","text":"<ul> <li>Only fetches data for the current page (e.g., 25 jobs per page)</li> <li>Reduces network traffic and server load</li> <li>Faster response times for large job lists</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#2-advanced-filtering","title":"2. Advanced Filtering","text":"<ul> <li>Status Filter: Filter by pending, running, completed, failed, or interrupted</li> <li>Priority Filter: Filter by priority levels (1-5)</li> <li>Search: Search in job names and hashlist names</li> <li>All filters work with pagination</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#3-smart-polling","title":"3. Smart Polling","text":"<ul> <li>Polls every 5 seconds by default</li> <li>Only fetches current page with active filters</li> <li>Can be toggled on/off by users</li> <li>Cancels in-flight requests when parameters change</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#4-status-counts","title":"4. Status Counts","text":"<ul> <li>Shows badge counts for each status</li> <li>Updates with each poll</li> <li>Helps users understand job distribution</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#backend-implementation","title":"Backend Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#repository-layer-job_execution_repository_extensiongo","title":"Repository Layer (<code>job_execution_repository_extension.go</code>)","text":"<pre><code>// New filter structure\ntype JobFilter struct {\n    Status   *string\n    Priority *int\n    Search   *string\n}\n\n// Key methods added:\n- ListWithFilters()      // Paginated list with filters\n- GetFilteredCount()     // Count matching filter criteria\n- GetStatusCounts()      // Get counts by status\n</code></pre>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#handler-layer-user_jobsgo","title":"Handler Layer (<code>user_jobs.go</code>)","text":"<p>The <code>/api/jobs</code> endpoint now supports: - <code>page</code> - Page number (default: 1) - <code>page_size</code> - Items per page (default: 25, max: 200) - <code>status</code> - Filter by job status - <code>priority</code> - Filter by priority (1-5) - <code>search</code> - Search in job/hashlist names</p> <p>Response includes: <pre><code>{\n  \"jobs\": [...],\n  \"pagination\": {\n    \"page\": 1,\n    \"page_size\": 25,\n    \"total\": 150,\n    \"total_pages\": 6\n  },\n  \"status_counts\": {\n    \"pending\": 10,\n    \"running\": 5,\n    \"completed\": 120,\n    \"failed\": 10,\n    \"interrupted\": 5\n  }\n}\n</code></pre></p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#frontend-implementation","title":"Frontend Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#jobs-page-frontendsrcpagesjobsindextsx","title":"Jobs Page (<code>/frontend/src/pages/Jobs/index.tsx</code>)","text":"<p>Key features: 1. State Management    - Separate state for pagination, filters, and data    - Maintains user's position during polls</p> <ol> <li>Efficient Polling</li> <li>Uses <code>setInterval</code> with 5-second intervals</li> <li>Cancels previous requests using <code>AbortController</code></li> <li> <p>Only shows loading on initial load or manual refresh</p> </li> <li> <p>User Controls</p> </li> <li>Toggle auto-refresh on/off</li> <li>Manual refresh button</li> <li>Page size selector</li> <li>Status filter buttons with counts</li> <li>Priority dropdown filter</li> <li> <p>Search field</p> </li> <li> <p>Performance Optimizations</p> </li> <li>Debounced search input</li> <li>Memoized query building</li> <li>Proper cleanup on unmount</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#benefits-over-sse","title":"Benefits Over SSE","text":"<ol> <li>Scalability</li> <li>No persistent connections</li> <li>Reduced server memory usage</li> <li> <p>Works better with load balancers</p> </li> <li> <p>Efficiency</p> </li> <li>Only fetches visible data</li> <li>Reduces bandwidth usage</li> <li> <p>Faster initial page loads</p> </li> <li> <p>User Experience</p> </li> <li>Maintains user's current view</li> <li>No sudden jumps or resets</li> <li> <p>Clear feedback on data freshness</p> </li> <li> <p>Reliability</p> </li> <li>No connection drops</li> <li>Works with all proxies</li> <li>Simpler error handling</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#usage-examples","title":"Usage Examples","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#basic-usage","title":"Basic Usage","text":"<ol> <li>Navigate to Jobs page</li> <li>Jobs auto-refresh every 5 seconds</li> <li>Use pagination to navigate large lists</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#filtering","title":"Filtering","text":"<ol> <li>Click status buttons to filter by status</li> <li>Select priority from dropdown</li> <li>Type in search box to search jobs</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#performance-tuning","title":"Performance Tuning","text":"<ol> <li>Increase page size for fewer requests</li> <li>Disable auto-refresh when not needed</li> <li>Use filters to reduce data volume</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#migration-notes","title":"Migration Notes","text":"<p>To migrate from SSE to polling: 1. Update backend handlers to support filtering 2. Replace SSE hooks with polling logic 3. Add filter UI components 4. Test with large datasets</p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Configurable Poll Interval: Allow users to set custom intervals</li> <li>Smart Polling: Slow down polling when no changes detected</li> <li>Batch Operations: Select multiple jobs for bulk actions</li> <li>Export Functionality: Export filtered job lists</li> <li>Advanced Filters: Date ranges, agent filters, etc.</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/","title":"Work Directory and Job Error Status Fixes","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#summary-of-changes","title":"Summary of Changes","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#1-removed-work-directory-references","title":"1. Removed Work Directory References","text":"<ul> <li>Agent HashcatExecutor: Removed all references to <code>workDirectory</code> since we're capturing output from stdout</li> <li>Agent JobManager: Updated to not pass work directory to HashcatExecutor</li> <li>Hashcat Command: Removed <code>cmd.Dir = e.workDirectory</code> to prevent \"chdir: no such file or directory\" error</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#2-fixed-hashcat-binary-permissions","title":"2. Fixed Hashcat Binary Permissions","text":"<ul> <li>File Sync: Enhanced binary extraction to set executable permissions (0755) for hashcat binaries</li> <li>Binary Detection: Improved detection of executable files during extraction (hashcat, hashcat.exe, hashcat.bin)</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#3-implemented-job-error-status-updates","title":"3. Implemented Job Error Status Updates","text":"<ul> <li>Agent JobProgress: Added <code>Status</code> and <code>ErrorMessage</code> fields to track task status</li> <li>Agent HashcatExecutor: Updated error handling to include error message in progress updates</li> <li>Backend JobProgress Model: Added matching <code>Status</code> and <code>ErrorMessage</code> fields</li> <li>Backend JobWebSocketIntegration: Added handling for failed status to update both task and job execution</li> <li>Backend JobExecutionRepository: Added <code>UpdateErrorMessage</code> method to store error messages</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#key-changes-by-file","title":"Key Changes by File","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#agent-changes","title":"Agent Changes","text":"<ol> <li><code>agent/internal/jobs/hashcat_executor.go</code></li> <li>Removed <code>workDirectory</code> field from struct</li> <li>Updated <code>NewHashcatExecutor</code> to not take work directory parameter</li> <li>Added status and error fields to JobProgress struct</li> <li> <p>Updated error handling to send proper error status</p> </li> <li> <p><code>agent/internal/jobs/jobs.go</code></p> </li> <li>Updated <code>NewJobManager</code> to not create work directory</li> <li> <p>Pass only data directory to HashcatExecutor</p> </li> <li> <p><code>agent/internal/sync/sync.go</code></p> </li> <li>Enhanced binary extraction to set executable permissions (0755)</li> <li>Improved detection of hashcat executables</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#backend-changes","title":"Backend Changes","text":"<ol> <li><code>backend/internal/models/jobs.go</code></li> <li> <p>Added <code>Status</code> and <code>ErrorMessage</code> fields to JobProgress struct</p> </li> <li> <p><code>backend/internal/integration/job_websocket_integration.go</code></p> </li> <li>Added handling for failed status in <code>HandleJobProgress</code></li> <li>Updates task status to failed with error message</li> <li> <p>Updates job execution status to failed</p> </li> <li> <p><code>backend/internal/repository/job_execution_repository.go</code></p> </li> <li>Added <code>UpdateErrorMessage</code> method to store error messages</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#result","title":"Result","text":"<ul> <li>Agent no longer tries to use non-existent work directory</li> <li>Hashcat binaries have proper execute permissions after extraction</li> <li>Failed jobs now properly show error status instead of remaining as pending</li> <li>Error messages are captured and displayed in the UI</li> </ul>"},{"location":"archive/development-notes/docker-initialization/","title":"Docker Initialization Guide","text":""},{"location":"archive/development-notes/docker-initialization/#overview","title":"Overview","text":"<p>This guide explains how to initialize and run KrakenHashes using Docker. KrakenHashes runs as a single container that includes the frontend, backend, and PostgreSQL database.</p>"},{"location":"archive/development-notes/docker-initialization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10.0 or higher</li> <li>Docker Compose v2.0.0 or higher</li> <li>At least 4GB of free disk space</li> </ul>"},{"location":"archive/development-notes/docker-initialization/#environment-configuration","title":"Environment Configuration","text":"<p>The application uses a single <code>.env</code> file for all configuration. Copy the <code>.env.example</code> file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"archive/development-notes/docker-initialization/#important-environment-variables","title":"Important Environment Variables","text":""},{"location":"archive/development-notes/docker-initialization/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>DEBUG</code>: Set to 'true' or '1' to enable debug output</li> <li><code>LOG_LEVEL</code>: Controls message verbosity (DEBUG, INFO, WARNING, ERROR)</li> <li><code>DEBUG_SQL</code>: Enable SQL query logging</li> <li><code>DEBUG_HTTP</code>: Enable HTTP request/response logging</li> <li><code>DEBUG_WEBSOCKET</code>: Enable WebSocket message logging</li> <li><code>DEBUG_AUTH</code>: Enable authentication debugging</li> <li><code>DEBUG_JOBS</code>: Enable job processing debugging</li> </ul>"},{"location":"archive/development-notes/docker-initialization/#log-file-locations","title":"Log File Locations","text":"<p>All logs are written to both stdout/stderr and files within the container: - <code>/var/log/krakenhashes/</code>: Base log directory mounted from host   - <code>backend/</code>: Backend service logs   - <code>frontend/</code>: Frontend service logs   - <code>nginx/</code>: Nginx access and error logs   - <code>postgres/</code>: PostgreSQL logs</p>"},{"location":"archive/development-notes/docker-initialization/#starting-the-service","title":"Starting the Service","text":"<ol> <li> <p>Build and start the service:    <pre><code>docker-compose up --build\n</code></pre></p> </li> <li> <p>Verify the service is running:    <pre><code>docker-compose ps\n</code></pre></p> </li> </ol>"},{"location":"archive/development-notes/docker-initialization/#debugging","title":"Debugging","text":""},{"location":"archive/development-notes/docker-initialization/#viewing-logs","title":"Viewing Logs","text":"<ol> <li> <p>Real-time container logs:    <pre><code>docker-compose logs -f\n</code></pre></p> </li> <li> <p>Access log files directly from host machine:    <pre><code>ls /var/log/krakenhashes/\n</code></pre></p> </li> </ol>"},{"location":"archive/development-notes/docker-initialization/#common-issues","title":"Common Issues","text":"<ol> <li>Database Connection Issues</li> <li>Check PostgreSQL logs in <code>/var/log/krakenhashes/postgres/</code></li> <li>Verify database credentials in .env</li> <li> <p>Ensure database port is not in use</p> </li> <li> <p>Certificate Issues</p> </li> <li>Verify TLS configuration in .env</li> <li>Check certificate paths</li> <li>Ensure proper permissions on certificate files</li> </ol>"},{"location":"archive/development-notes/docker-initialization/#maintenance","title":"Maintenance","text":""},{"location":"archive/development-notes/docker-initialization/#database-backups","title":"Database Backups","text":"<p>PostgreSQL data is persisted in a named volume. To backup: <pre><code>docker-compose exec krakenhashes pg_dump -U krakenhashes &gt; backup.sql\n</code></pre></p>"},{"location":"archive/development-notes/docker-initialization/#log-rotation","title":"Log Rotation","text":"<p>Logs are automatically rotated using logrotate with the following policy: - Maximum size: 100MB - Retention: 30 days - Compression: enabled</p>"},{"location":"archive/development-notes/docker-initialization/#security-notes","title":"Security Notes","text":"<ol> <li>Change default passwords in .env</li> <li>Secure the log directory permissions</li> <li>Regular security updates</li> <li>Monitor log files for suspicious activity </li> </ol>"},{"location":"archive/development-notes/hashlists/","title":"Hashlists","text":"<p>Hashlists are fundamental to KrakenHashes, representing collections of hashes uploaded for cracking jobs or analysis. This document outlines how hashlists are uploaded, processed, and managed within the system.</p>"},{"location":"archive/development-notes/hashlists/#overview","title":"Overview","text":"<ul> <li>Definition: A hashlist is a file containing multiple lines, where each line typically represents a single hash (and potentially its cracked password).</li> <li>Association: Each hashlist is associated with a specific Hash Type (e.g., NTLM, SHA1) and can optionally be linked to a Client/Engagement.</li> <li>Processing: Uploaded hashlists undergo an asynchronous background processing workflow to ingest the hashes into the central database.</li> <li>Storage: Hashlist files are stored on the backend server in a configured directory.</li> </ul>"},{"location":"archive/development-notes/hashlists/#uploading-hashlists","title":"Uploading Hashlists","text":"<p>Hashlists are typically uploaded through the frontend UI.</p> <ol> <li>Navigate: Go to the \"Hashlists\" section of the dashboard.</li> <li>Initiate Upload: Click the \"Upload Hashlist\" button.</li> <li>Fill Details: In the dialog, provide:<ul> <li>Name: A descriptive name for the hashlist.</li> <li>Hash Type: Select the correct hash type from the dropdown. This list is populated from the <code>hash_types</code> table in the database (see Hash Types below). The format displayed is <code>ID - Name</code> (e.g., <code>1000 - NTLM</code>).</li> <li>Client: (Optional) Select an existing client to associate this hashlist with, or create a new one on the fly.</li> <li>File: Choose the hashlist file from your local machine.</li> </ul> </li> <li>Submit: Click the upload button in the dialog.</li> </ol>"},{"location":"archive/development-notes/hashlists/#api-endpoint","title":"API Endpoint","text":"<p>The frontend interacts with the <code>POST /api/hashlists</code> endpoint. This endpoint expects a <code>multipart/form-data</code> request containing the fields mentioned above (name, hash_type_id, client_id) and the hashlist file itself.</p>"},{"location":"archive/development-notes/hashlists/#file-storage","title":"File Storage","text":"<ul> <li>Uploaded hashlist files are stored on the backend server.</li> <li>The base directory for uploads is configured via the <code>KH_DATA_DIR</code> environment variable.</li> <li>Within the data directory, hashlists are stored in a specific subdirectory, typically <code>hashlist_uploads</code>, but configurable via <code>KH_HASH_UPLOAD_DIR</code>.</li> <li>The maximum allowed upload size is determined by the <code>KH_MAX_UPLOAD_SIZE_MB</code> environment variable (default: 32 MiB).</li> </ul>"},{"location":"archive/development-notes/hashlists/#hashlist-processing","title":"Hashlist Processing","text":"<p>Once a hashlist file is uploaded and initial metadata is saved, it enters an asynchronous processing queue.</p>"},{"location":"archive/development-notes/hashlists/#status-workflow","title":"Status Workflow","text":"<p>A hashlist progresses through the following statuses:</p> <ol> <li><code>uploading</code>: Initial state when the upload request is received.</li> <li><code>processing</code>: The backend worker has picked up the hashlist and is actively reading the file and ingesting hashes.</li> <li><code>ready</code>: Processing completed successfully. All valid lines have been processed and stored. The hashlist is now available for use in cracking jobs.</li> <li><code>ready_with_errors</code>: Processing finished, but one or more lines in the file could not be processed correctly (e.g., invalid format for the selected hash type). Valid lines were still ingested. Check backend logs for details on specific line errors. (Not fully implemented)    `</li> <li><code>error</code>: A fatal error occurred during processing (e.g., file unreadable, database error during batch insert). The <code>error_message</code> field on the hashlist provides a general reason. Check backend logs for more details.</li> </ol>"},{"location":"archive/development-notes/hashlists/#processing-steps","title":"Processing Steps","text":"<p>The backend processor performs the following steps:</p> <ol> <li>Fetch Details: Retrieves the hashlist metadata (ID, file path, hash type ID) from the database.</li> <li>Open File: Opens the stored hashlist file.</li> <li>Scan Line by Line: Reads the file line by line.<ul> <li>Empty lines are skipped.</li> <li>Lines starting with <code>#</code> are treated as comments and skipped.</li> </ul> </li> <li>Extract Hash/Password:<ul> <li>Default: Checks for a colon (<code>:</code>) separator. If found, the part before the colon is treated as the hash, and the part after is treated as the pre-cracked password (<code>is_cracked</code> = true). If no colon is found, the entire line is treated as the hash (<code>is_cracked</code> = false).</li> <li>Type-Specific Processing: For certain hash types (e.g., <code>1000 - NTLM</code>), specific processing logic might be applied to extract the canonical hash format from more complex lines (like <code>user:sid:LM:NT:::</code>). This logic is determined by the <code>needs_processing</code> flag and potentially the <code>processing_logic</code> field in the <code>hash_types</code> table.</li> </ul> </li> <li>Batching: Hashes are collected into batches (size configured by <code>KH_HASHLIST_BATCH_SIZE</code>, default: 1000).</li> <li>Database Insertion: Each batch is processed:<ul> <li>The system checks if any hashes in the batch already exist in the central <code>hashes</code> table (based on hash value and hash type ID).</li> <li>New, unique hashes are inserted into the <code>hashes</code> table.</li> <li>Entries are created in the <code>hashlist_hashes</code> join table to link both new and existing hashes from the batch to the current hashlist.</li> <li>If a hash being added includes a pre-cracked password, the corresponding record in the <code>hashes</code> table is updated (<code>is_cracked</code>=true, <code>password</code>=...).</li> </ul> </li> <li>Update Status: Once the entire file is processed, the hashlist status is updated to <code>ready</code>, <code>ready_with_errors</code>, or <code>error</code>, along with the final <code>total_hashes</code> and <code>cracked_hashes</code> counts.</li> </ol>"},{"location":"archive/development-notes/hashlists/#supported-input-formats","title":"Supported Input Formats","text":"<p>The processor primarily expects:</p> <ul> <li>One hash per line.</li> <li>Optional: <code>hash:password</code> format for lines containing already cracked hashes.</li> <li>Lines starting with <code>#</code> are ignored.</li> <li>Empty lines are ignored.</li> <li>Specific formats handled by type-specific processors (e.g., NTLM).</li> </ul>"},{"location":"archive/development-notes/hashlists/#hash-types","title":"Hash Types","text":"<ul> <li>Supported hash types are defined in the <code>hash_types</code> database table.</li> <li>This table is populated by a database migration (<code>000016_add_hashcat_hash_types.up.sql</code>) which includes common types and examples sourced from the Hashcat wiki.</li> <li>Each type has an ID (corresponding to the Hashcat mode), Name, Description (optional), Example (optional), and flags indicating if it needs special processing (<code>needs_processing</code>, <code>processing_logic</code>) or is enabled (<code>is_enabled</code>).</li> <li>The frontend uses the <code>GET /api/hashtypes</code> endpoint (filtered by <code>is_enabled=true</code> by default) to populate the selection dropdown during hashlist upload.</li> </ul>"},{"location":"archive/development-notes/hashlists/#managing-hashlists","title":"Managing Hashlists","text":"<ul> <li>Viewing: The \"Hashlists\" dashboard provides a sortable and filterable view of all accessible hashlists, showing Name, Client, Status, Progress (% Cracked), and Creation Date.</li> <li>Downloading: Use the download icon on the dashboard or the <code>GET /api/hashlists/{id}/download</code> endpoint to retrieve the original uploaded hashlist file.</li> <li>Deleting: Use the delete icon on the dashboard or the <code>DELETE /api/hashlists/{id}</code> endpoint. Deleting a hashlist removes its entry from the <code>hashlists</code> table, removes associated entries from the <code>hashlist_hashes</code> table, and deletes the original hashlist file from the backend storage. It does not delete the individual hashes from the central <code>hashes</code> table, as they might be referenced by other hashlists.</li> </ul>"},{"location":"archive/development-notes/hashlists/#data-retention","title":"Data Retention","text":"<p>Uploaded hashlists and their associated data are subject to the system's data retention policies. Old hashlists may be automatically purged based on client-specific or default retention settings configured by an administrator. See Admin Settings documentation for details. </p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>Complete guide for deploying KrakenHashes in various environments.</p>"},{"location":"deployment/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Docker Deployment</p> <p>Deploy using Docker containers with initialization</p> </li> <li> <p> Docker Compose</p> <p>Multi-container deployment with orchestration</p> </li> <li> <p> Production Best Practices</p> <p>Security, performance, and reliability guidelines</p> </li> <li> <p> Update Procedures</p> <p>Safely updating KrakenHashes components</p> </li> </ul>"},{"location":"deployment/#deployment-options","title":"Deployment Options","text":""},{"location":"deployment/#quick-deployment","title":"Quick Deployment","text":"<p>For testing or small deployments, use our pre-configured Docker Compose: <pre><code>docker-compose up -d\n</code></pre></p>"},{"location":"deployment/#production-deployment","title":"Production Deployment","text":"<p>For production environments, follow our Production Best Practices guide for: - High availability setup - Security hardening - Performance optimization - Monitoring configuration</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: Linux (recommended), Windows Server 2019+, or macOS</li> <li>CPU: 4+ cores recommended</li> <li>RAM: 8GB minimum, 16GB+ recommended</li> <li>Storage: 50GB+ for application and data</li> <li>Network: Stable internet connection for updates</li> </ul>"},{"location":"deployment/#software-requirements","title":"Software Requirements","text":"<ul> <li>Docker 20.10+ and Docker Compose 2.0+</li> <li>PostgreSQL 15+ (or use included container)</li> <li>Valid SSL/TLS certificates for production</li> </ul>"},{"location":"deployment/#deployment-checklist","title":"Deployment Checklist","text":"<p>Before You Deploy</p> <ul> <li> Review system requirements</li> <li> Plan network architecture</li> <li> Configure firewall rules</li> <li> Prepare SSL/TLS certificates</li> <li> Set up backup storage</li> <li> Plan monitoring strategy</li> <li> Review security guidelines</li> </ul>"},{"location":"deployment/#support-matrix","title":"Support Matrix","text":"Component Docker Bare Metal Kubernetes Cloud Backend \u2705 Full \u26a0\ufe0f Manual \ud83d\udea7 Planned \u2705 Yes Frontend \u2705 Full \u2705 Full \ud83d\udea7 Planned \u2705 Yes Database \u2705 Full \u2705 Full \ud83d\udea7 Planned \u2705 Yes Agent \u2705 Full \u2705 Full \u274c No \u26a0\ufe0f Limited"},{"location":"deployment/#security-considerations","title":"Security Considerations","text":"<p>Production Security</p> <p>Always follow these security practices:</p> <ul> <li>Enable TLS/SSL for all connections</li> <li>Use strong passwords and rotate regularly</li> <li>Enable MFA for all admin accounts</li> <li>Restrict network access with firewalls</li> <li>Regular security updates</li> <li>Monitor logs for suspicious activity</li> </ul>"},{"location":"deployment/#getting-help","title":"Getting Help","text":"<ul> <li>Check our Troubleshooting Guide</li> <li>Join our Discord Community</li> <li>Review GitHub Issues</li> </ul>"},{"location":"deployment/docker-compose/","title":"Docker Compose Deployment Guide","text":"<p>\u26a0\ufe0f Version Requirement: Docker Compose v2.0+ is required. Use <code>docker compose</code> (space), not <code>docker-compose</code> (hyphen).</p> <p>This guide covers deploying KrakenHashes using Docker Compose, including configuration options, network setup, volume management, and common customizations.</p>"},{"location":"deployment/docker-compose/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Configuration Options</li> <li>Network Setup</li> <li>Volume Management</li> <li>Deployment Steps</li> <li>Scaling Considerations</li> <li>Common Customizations</li> <li>Troubleshooting</li> <li>Security Considerations</li> </ul>"},{"location":"deployment/docker-compose/#overview","title":"Overview","text":"<p>KrakenHashes uses Docker Compose to orchestrate multiple services:</p> <ul> <li>PostgreSQL: Database backend for storing hashlists, jobs, and system data</li> <li>KrakenHashes App: Combined backend API and frontend served through nginx</li> </ul> <p>The Docker Compose setup provides: - Automatic service dependency management - Health checks for service readiness - Persistent data storage through Docker volumes - Environment-based configuration - Isolated networking</p>"},{"location":"deployment/docker-compose/#prerequisites","title":"Prerequisites","text":"<p>Before deploying with Docker Compose:</p> <ol> <li>Docker Engine: Version 20.10 or higher</li> <li>Docker Compose: Version 2.0 or higher (included with Docker Desktop)</li> <li>System Requirements:</li> <li>4GB RAM minimum (8GB recommended)</li> <li>10GB free disk space</li> <li> <p>Linux, macOS, or Windows with WSL2</p> </li> <li> <p>Network Ports:</p> </li> <li>443 (HTTPS frontend)</li> <li>1337 (HTTP API)</li> <li>31337 (HTTPS API)</li> <li>5432 (PostgreSQL - optional, can be internal only)</li> </ol>"},{"location":"deployment/docker-compose/#configuration-options","title":"Configuration Options","text":""},{"location":"deployment/docker-compose/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root with the following variables:</p> <pre><code># Database Configuration\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\n\n# Port Configuration\nFRONTEND_PORT=443\nKH_PORT=1337\nKH_HTTPS_PORT=31337\n\n# Directory Configuration\nLOG_DIR=/var/log/krakenhashes\nKH_CONFIG_DIR_HOST=/etc/krakenhashes\nKH_DATA_DIR_HOST=/var/lib/krakenhashes\n\n# User/Group IDs (for file permissions)\nPUID=1000\nPGID=1000\n\n# TLS Configuration\nKH_TLS_MODE=self-signed\nKH_CERT_KEY_SIZE=4096\nKH_CERT_VALIDITY_DAYS=365\nKH_CA_VALIDITY_DAYS=3650\n</code></pre>"},{"location":"deployment/docker-compose/#service-specific-configuration","title":"Service-Specific Configuration","text":""},{"location":"deployment/docker-compose/#postgresql-service","title":"PostgreSQL Service","text":"<p>The PostgreSQL service is configured with: - Alpine-based image for smaller footprint - Health checks using <code>pg_isready</code> - Persistent data storage in Docker volume - Configurable credentials via environment variables</p>"},{"location":"deployment/docker-compose/#krakenhashes-application","title":"KrakenHashes Application","text":"<p>The main application container includes: - Multi-stage build for optimized image size - Combined backend and frontend services - nginx reverse proxy for frontend - TLS/SSL support with multiple modes - File storage for binaries, wordlists, and hashlists</p>"},{"location":"deployment/docker-compose/#network-setup","title":"Network Setup","text":""},{"location":"deployment/docker-compose/#default-network","title":"Default Network","text":"<p>Docker Compose creates an isolated bridge network <code>krakenhashes-net</code>:</p> <pre><code>networks:\n  krakenhashes-net:\n    driver: bridge\n</code></pre> <p>This provides: - Service discovery by container name - Isolation from other Docker networks - Internal DNS resolution</p>"},{"location":"deployment/docker-compose/#service-communication","title":"Service Communication","text":"<ul> <li>Backend connects to PostgreSQL using hostname <code>postgres</code></li> <li>All services communicate over the internal network</li> <li>Only required ports are exposed to the host</li> </ul>"},{"location":"deployment/docker-compose/#custom-network-configuration","title":"Custom Network Configuration","text":"<p>To use an existing network or customize settings:</p> <pre><code>networks:\n  krakenhashes-net:\n    external: true\n    name: my-existing-network\n</code></pre> <p>Or with custom subnet:</p> <pre><code>networks:\n  krakenhashes-net:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n          gateway: 172.20.0.1\n</code></pre>"},{"location":"deployment/docker-compose/#volume-management","title":"Volume Management","text":""},{"location":"deployment/docker-compose/#persistent-volumes","title":"Persistent Volumes","text":"<p>KrakenHashes uses named volumes for data persistence:</p> <ol> <li>postgres_data: PostgreSQL database files</li> <li>krakenhashes_data: Application data (wordlists, rules, hashlists)</li> </ol>"},{"location":"deployment/docker-compose/#volume-locations","title":"Volume Locations","text":"<p>Default volume storage locations: - Docker managed: <code>/var/lib/docker/volumes/</code> - Named volumes:   - <code>krakenhashes_postgres_data</code>   - <code>krakenhashes_app_data</code></p>"},{"location":"deployment/docker-compose/#bind-mounts","title":"Bind Mounts","text":"<p>The compose file uses bind mounts for: - Logs: <code>${LOG_DIR:-/var/log/krakenhashes}</code> - Config: <code>${KH_CONFIG_DIR_HOST:-/etc/krakenhashes}</code> - Data: <code>${KH_DATA_DIR_HOST:-/var/lib/krakenhashes}</code></p>"},{"location":"deployment/docker-compose/#backup-strategy","title":"Backup Strategy","text":"<p>To backup volumes:</p> <pre><code># Backup PostgreSQL data\ndocker run --rm -v krakenhashes_postgres_data:/data \\\n  -v $(pwd):/backup alpine tar czf /backup/postgres-backup.tar.gz -C /data .\n\n# Backup application data\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd):/backup alpine tar czf /backup/app-backup.tar.gz -C /data .\n</code></pre>"},{"location":"deployment/docker-compose/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/docker-compose/#initial-deployment","title":"Initial Deployment","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Create environment file:    <pre><code>cp .env.example .env\n# Edit .env with your configuration\n</code></pre></p> </li> <li> <p>Create required directories:    <pre><code>sudo mkdir -p /var/log/krakenhashes\nsudo mkdir -p /etc/krakenhashes\nsudo mkdir -p /var/lib/krakenhashes\nsudo chown -R 1000:1000 /var/log/krakenhashes\nsudo chown -R 1000:1000 /etc/krakenhashes\nsudo chown -R 1000:1000 /var/lib/krakenhashes\n</code></pre></p> </li> <li> <p>Build and start services:    <pre><code>docker-compose up -d --build\n</code></pre></p> </li> <li> <p>Verify deployment:    <pre><code>docker-compose ps\ndocker-compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#updating-deployment","title":"Updating Deployment","text":"<ol> <li> <p>Pull latest changes:    <pre><code>git pull origin main\n</code></pre></p> </li> <li> <p>Rebuild and restart:    <pre><code>docker-compose down\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Check migration status:    <pre><code>docker-compose logs krakenhashes | grep -i migration\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"deployment/docker-compose/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>While the current setup runs as a single instance, you can prepare for scaling:</p> <ol> <li>Database Scaling:</li> <li>Use external PostgreSQL for production</li> <li>Consider connection pooling with PgBouncer</li> <li> <p>Implement read replicas for reporting</p> </li> <li> <p>Application Scaling:</p> </li> <li>Use external load balancer (nginx, HAProxy)</li> <li>Share file storage (NFS, S3-compatible)</li> <li>Implement Redis for session storage</li> </ol>"},{"location":"deployment/docker-compose/#resource-limits","title":"Resource Limits","text":"<p>Add resource constraints to prevent container resource exhaustion:</p> <pre><code>services:\n  krakenhashes:\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n        reservations:\n          cpus: '1'\n          memory: 2G\n</code></pre>"},{"location":"deployment/docker-compose/#common-customizations","title":"Common Customizations","text":""},{"location":"deployment/docker-compose/#development-mode","title":"Development Mode","text":"<p>For development, uncomment restart policies and expose additional ports:</p> <pre><code>services:\n  postgres:\n    restart: unless-stopped\n    ports:\n      - \"5432:5432\"  # Direct database access\n\n  krakenhashes:\n    restart: unless-stopped\n    environment:\n      - DEBUG=true\n      - LOG_LEVEL=debug\n</code></pre>"},{"location":"deployment/docker-compose/#production-optimizations","title":"Production Optimizations","text":"<ol> <li> <p>Remove unnecessary port exposures:    <pre><code>services:\n  postgres:\n    # Remove ports section for internal-only access\n</code></pre></p> </li> <li> <p>Enable restart policies:    <pre><code>restart: always\n</code></pre></p> </li> <li> <p>Use specific image tags:    <pre><code>image: postgres:15.4-alpine\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#custom-tls-certificates","title":"Custom TLS Certificates","text":"<p>To use your own certificates:</p> <ol> <li>Place certificates in <code>/etc/krakenhashes/certs/</code></li> <li>Set environment variables:    <pre><code>KH_TLS_MODE=provided\nKH_TLS_CERT_PATH=/etc/krakenhashes/certs/server.crt\nKH_TLS_KEY_PATH=/etc/krakenhashes/certs/server.key\n</code></pre></li> </ol>"},{"location":"deployment/docker-compose/#external-database","title":"External Database","text":"<p>To use an external PostgreSQL instance:</p> <ol> <li>Remove the postgres service from docker-compose.yml</li> <li>Update environment variables:    <pre><code>DB_HOST=your-database-host.com\nDB_PORT=5432\nDB_NAME=krakenhashes\nDB_USER=krakenhashes\nDB_PASSWORD=your-password\n</code></pre></li> </ol>"},{"location":"deployment/docker-compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/docker-compose/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Container fails to start:    <pre><code># Check logs\ndocker-compose logs krakenhashes\n\n# Check health status\ndocker-compose ps\n</code></pre></p> </li> <li> <p>Database connection errors:    <pre><code># Test database connectivity\ndocker-compose exec krakenhashes pg_isready -h postgres -U krakenhashes\n\n# Check PostgreSQL logs\ndocker-compose logs postgres\n</code></pre></p> </li> <li> <p>Permission issues:    <pre><code># Fix ownership\nsudo chown -R 1000:1000 /var/lib/krakenhashes\nsudo chown -R 1000:1000 /var/log/krakenhashes\n</code></pre></p> </li> <li> <p>Port conflicts:    <pre><code># Check port usage\nsudo netstat -tlnp | grep -E '(443|1337|31337|5432)'\n\n# Change ports in .env file\nFRONTEND_PORT=8443\nKH_PORT=8337\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code># In .env file\nLOG_LEVEL=debug\nDEBUG=true\n\n# Restart services\ndocker-compose restart krakenhashes\n</code></pre>"},{"location":"deployment/docker-compose/#health-checks","title":"Health Checks","text":"<p>Monitor service health:</p> <pre><code># Check all services\ndocker-compose ps\n\n# Detailed health info\ndocker inspect krakenhashes-postgres | jq '.[0].State.Health'\n</code></pre>"},{"location":"deployment/docker-compose/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/docker-compose/#network-security","title":"Network Security","text":"<ol> <li>Firewall Rules:</li> <li>Only expose necessary ports</li> <li>Use firewall to restrict access</li> <li> <p>Consider VPN for administrative access</p> </li> <li> <p>TLS/SSL:</p> </li> <li>Always use HTTPS in production</li> <li>Regularly update certificates</li> <li>Use strong cipher suites</li> </ol>"},{"location":"deployment/docker-compose/#container-security","title":"Container Security","text":"<ol> <li>Run as non-root:</li> <li>Containers use UID/GID 1000 by default</li> <li> <p>Avoid running as root user</p> </li> <li> <p>Image Security:    <pre><code># Scan for vulnerabilities\ndocker scan krakenhashes:latest\n</code></pre></p> </li> <li> <p>Secrets Management:</p> </li> <li>Use Docker secrets for sensitive data</li> <li>Rotate database passwords regularly</li> <li>Never commit .env files to version control</li> </ol>"},{"location":"deployment/docker-compose/#backup-and-recovery","title":"Backup and Recovery","text":"<ol> <li> <p>Regular Backups:    <pre><code># Automated backup script\n#!/bin/bash\nBACKUP_DIR=\"/backup/krakenhashes/$(date +%Y%m%d)\"\nmkdir -p $BACKUP_DIR\n\n# Backup database\ndocker-compose exec -T postgres pg_dump -U krakenhashes &gt; $BACKUP_DIR/database.sql\n\n# Backup volumes\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $BACKUP_DIR:/backup alpine \\\n  tar czf /backup/app-data.tar.gz -C /data .\n</code></pre></p> </li> <li> <p>Test Recovery:</p> </li> <li>Regularly test backup restoration</li> <li>Document recovery procedures</li> <li>Keep multiple backup generations</li> </ol>"},{"location":"deployment/docker-compose/#monitoring","title":"Monitoring","text":"<p>Implement monitoring for: - Container health and restarts - Resource usage (CPU, memory, disk) - Application logs and errors - Database performance - TLS certificate expiration</p> <p>Consider using: - Prometheus + Grafana for metrics - ELK stack for log aggregation - Uptime monitoring services</p>"},{"location":"deployment/docker/","title":"Docker Initialization Guide","text":""},{"location":"deployment/docker/#overview","title":"Overview","text":"<p>This guide explains how to initialize and run KrakenHashes using Docker. KrakenHashes runs as a single container that includes the frontend, backend, and PostgreSQL database.</p>"},{"location":"deployment/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10.0 or higher</li> <li>Docker Compose v2.0.0 or higher</li> <li>At least 4GB of free disk space</li> </ul>"},{"location":"deployment/docker/#environment-configuration","title":"Environment Configuration","text":"<p>The application uses a single <code>.env</code> file for all configuration. Copy the <code>.env.example</code> file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"deployment/docker/#important-environment-variables","title":"Important Environment Variables","text":""},{"location":"deployment/docker/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>DEBUG</code>: Set to 'true' or '1' to enable debug output</li> <li><code>LOG_LEVEL</code>: Controls message verbosity (DEBUG, INFO, WARNING, ERROR)</li> <li><code>DEBUG_SQL</code>: Enable SQL query logging</li> <li><code>DEBUG_HTTP</code>: Enable HTTP request/response logging</li> <li><code>DEBUG_WEBSOCKET</code>: Enable WebSocket message logging</li> <li><code>DEBUG_AUTH</code>: Enable authentication debugging</li> <li><code>DEBUG_JOBS</code>: Enable job processing debugging</li> </ul>"},{"location":"deployment/docker/#log-file-locations","title":"Log File Locations","text":"<p>All logs are written to both stdout/stderr and files within the container: - <code>/var/log/krakenhashes/</code>: Base log directory mounted from host   - <code>backend/</code>: Backend service logs   - <code>frontend/</code>: Frontend service logs   - <code>nginx/</code>: Nginx access and error logs   - <code>postgres/</code>: PostgreSQL logs</p>"},{"location":"deployment/docker/#starting-the-service","title":"Starting the Service","text":"<ol> <li> <p>Build and start the service:    <pre><code>docker-compose up --build\n</code></pre></p> </li> <li> <p>Verify the service is running:    <pre><code>docker-compose ps\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#debugging","title":"Debugging","text":""},{"location":"deployment/docker/#viewing-logs","title":"Viewing Logs","text":"<ol> <li> <p>Real-time container logs:    <pre><code>docker-compose logs -f\n</code></pre></p> </li> <li> <p>Access log files directly from host machine:    <pre><code>ls /var/log/krakenhashes/\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#common-issues","title":"Common Issues","text":"<ol> <li>Database Connection Issues</li> <li>Check PostgreSQL logs in <code>/var/log/krakenhashes/postgres/</code></li> <li>Verify database credentials in .env</li> <li> <p>Ensure database port is not in use</p> </li> <li> <p>Certificate Issues</p> </li> <li>Verify TLS configuration in .env</li> <li>Check certificate paths</li> <li>Ensure proper permissions on certificate files</li> </ol>"},{"location":"deployment/docker/#maintenance","title":"Maintenance","text":""},{"location":"deployment/docker/#database-backups","title":"Database Backups","text":"<p>PostgreSQL data is persisted in a named volume. To backup: <pre><code>docker-compose exec krakenhashes pg_dump -U krakenhashes &gt; backup.sql\n</code></pre></p>"},{"location":"deployment/docker/#log-rotation","title":"Log Rotation","text":"<p>Logs are automatically rotated using logrotate with the following policy: - Maximum size: 100MB - Retention: 30 days - Compression: enabled</p>"},{"location":"deployment/docker/#security-notes","title":"Security Notes","text":"<ol> <li>Change default passwords in .env</li> <li>Secure the log directory permissions</li> <li>Regular security updates</li> <li>Monitor log files for suspicious activity </li> </ol>"},{"location":"deployment/production/","title":"Production Best Practices Guide","text":"<p>This guide provides comprehensive recommendations for deploying and operating KrakenHashes in production environments. Following these practices ensures security, reliability, performance, and compliance for your password cracking infrastructure.</p>"},{"location":"deployment/production/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Infrastructure Requirements and Recommendations</li> <li>Security Hardening Checklist</li> <li>Performance Optimization</li> <li>High Availability Setup</li> <li>Monitoring and Alerting</li> <li>Backup and Disaster Recovery</li> <li>Compliance Considerations</li> </ol>"},{"location":"deployment/production/#infrastructure-requirements-and-recommendations","title":"Infrastructure Requirements and Recommendations","text":""},{"location":"deployment/production/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"deployment/production/#backend-server-minimum","title":"Backend Server (Minimum)","text":"<ul> <li>CPU: 8 cores (16+ recommended for large deployments)</li> <li>RAM: 16GB (32GB+ recommended)</li> <li>Storage: </li> <li>System: 50GB SSD</li> <li>Data: 500GB+ SSD (scales with hashlist/wordlist size)</li> <li>Database: 100GB+ SSD with high IOPS</li> <li>Network: 1Gbps connection minimum</li> </ul>"},{"location":"deployment/production/#backend-server-recommended","title":"Backend Server (Recommended)","text":"<ul> <li>CPU: 16-32 cores (AMD EPYC or Intel Xeon)</li> <li>RAM: 64GB ECC memory</li> <li>Storage:</li> <li>System: 2x 250GB SSD in RAID 1</li> <li>Data: 2TB+ NVMe SSD array (RAID 10)</li> <li>Database: Dedicated 500GB+ enterprise SSD</li> <li>Network: 10Gbps connection for agent communication</li> </ul>"},{"location":"deployment/production/#agent-hardware-per-agent","title":"Agent Hardware (Per Agent)","text":"<ul> <li>CPU: 8+ cores for hashcat coordination</li> <li>RAM: 16GB minimum (32GB+ for large wordlists)</li> <li>GPU: NVIDIA RTX 3090/4090 or better</li> <li>Storage: 250GB SSD for local caching</li> <li>Network: 1Gbps stable connection</li> </ul>"},{"location":"deployment/production/#network-architecture","title":"Network Architecture","text":"<pre><code>Internet\n    \u2502\n    \u251c\u2500\u2500\u2500 Firewall/WAF\n    \u2502         \u2502\n    \u2502    Load Balancer (Optional for HA)\n    \u2502         \u2502\n    \u2502    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502    \u2502 Backend \u2502 \u2190\u2500\u2500 Port 31337 (HTTPS API)\n    \u2502    \u2502 Server  \u2502 \u2190\u2500\u2500 Port 1337 (HTTP CA cert)\n    \u2502    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502         \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500 PostgreSQL (Port 5432)\n    \u2502\n    \u2514\u2500\u2500\u2500 Agent Network \u2190\u2500\u2500 WebSocket connections\n</code></pre>"},{"location":"deployment/production/#firewall-rules","title":"Firewall Rules","text":""},{"location":"deployment/production/#inbound-rules","title":"Inbound Rules","text":"<pre><code># Public Access\n443/tcp  \u2192 Load Balancer       # HTTPS (if using reverse proxy)\n31337/tcp \u2192 Backend Server      # HTTPS API\n1337/tcp  \u2192 Backend Server      # CA Certificate endpoint\n\n# Internal Only\n5432/tcp  \u2192 PostgreSQL          # Database (restrict to backend only)\n22/tcp    \u2192 All Servers         # SSH (restrict source IPs)\n</code></pre>"},{"location":"deployment/production/#outbound-rules","title":"Outbound Rules","text":"<pre><code>443/tcp   \u2192 Internet            # Updates, external services\n80/tcp    \u2192 Internet            # Package updates\n53/udp    \u2192 DNS Servers         # DNS resolution\n123/udp   \u2192 NTP Servers         # Time synchronization\n</code></pre>"},{"location":"deployment/production/#storage-recommendations","title":"Storage Recommendations","text":""},{"location":"deployment/production/#file-system-layout","title":"File System Layout","text":"<pre><code>/var/lib/krakenhashes/          # Main data directory\n\u251c\u2500\u2500 binaries/                   # Hashcat binaries (10GB)\n\u251c\u2500\u2500 wordlists/                  # Wordlist storage (100GB+)\n\u2502   \u251c\u2500\u2500 general/               \n\u2502   \u251c\u2500\u2500 specialized/           \n\u2502   \u251c\u2500\u2500 targeted/              \n\u2502   \u2514\u2500\u2500 custom/                \n\u251c\u2500\u2500 rules/                      # Rule files (10GB)\n\u2502   \u251c\u2500\u2500 hashcat/               \n\u2502   \u251c\u2500\u2500 john/                  \n\u2502   \u2514\u2500\u2500 custom/                \n\u251c\u2500\u2500 hashlists/                  # User hashlists (100GB+)\n\u2514\u2500\u2500 hashlist_uploads/           # Temporary uploads (50GB)\n\n/var/lib/postgresql/            # Database files (separate disk)\n/var/log/krakenhashes/          # Application logs\n/backup/krakenhashes/           # Backup storage (separate disk/NAS)\n</code></pre>"},{"location":"deployment/production/#storage-best-practices","title":"Storage Best Practices","text":"<ul> <li>Use separate disks/volumes for database, data, and backups</li> <li>Implement RAID for redundancy (RAID 10 recommended)</li> <li>Monitor disk usage and set alerts at 80% capacity</li> <li>Use SSD/NVMe for database and frequently accessed data</li> <li>Consider object storage (S3-compatible) for large wordlists</li> </ul>"},{"location":"deployment/production/#security-hardening-checklist","title":"Security Hardening Checklist","text":""},{"location":"deployment/production/#authentication-and-access-control","title":"Authentication and Access Control","text":""},{"location":"deployment/production/#1-strong-authentication-configuration","title":"1. Strong Authentication Configuration","text":"<pre><code># Configure in Admin Panel \u2192 Authentication Settings\n\n\u2713 Minimum Password Length: 20 characters\n\u2713 Require Uppercase: Enabled\n\u2713 Require Lowercase: Enabled\n\u2713 Require Numbers: Enabled\n\u2713 Require Special Characters: Enabled\n\u2713 Maximum Failed Login Attempts: 3\n\u2713 Account Lockout Duration: 30 minutes\n\u2713 JWT Token Expiry: 15 minutes (production)\n</code></pre>"},{"location":"deployment/production/#2-multi-factor-authentication-mfa","title":"2. Multi-Factor Authentication (MFA)","text":"<pre><code>\u2713 Require MFA for All Users: Enabled\n\u2713 Allowed Methods:\n  - Email Authentication: Enabled (backup only)\n  - Authenticator Apps: Enabled (primary)\n\u2713 Email Code Validity: 5 minutes\n\u2713 Code Cooldown Period: 2 minutes\n\u2713 Maximum Code Attempts: 3\n\u2713 Number of Backup Codes: 10\n</code></pre>"},{"location":"deployment/production/#3-user-account-security","title":"3. User Account Security","text":"<ul> <li>Enforce unique usernames (no shared accounts)</li> <li>Implement role-based access control (RBAC)</li> <li>Regular access reviews (quarterly)</li> <li>Disable default accounts</li> <li>Audit privileged account usage</li> </ul>"},{"location":"deployment/production/#network-security","title":"Network Security","text":""},{"location":"deployment/production/#1-tlsssl-configuration","title":"1. TLS/SSL Configuration","text":"<pre><code># Production TLS Setup (use provided certificates)\nexport KH_TLS_MODE=provided\nexport KH_TLS_CERT_PATH=/etc/krakenhashes/certs/server.crt\nexport KH_TLS_KEY_PATH=/etc/krakenhashes/certs/server.key\nexport KH_TLS_CA_PATH=/etc/krakenhashes/certs/ca.crt\n\n# Or use Let's Encrypt\nexport KH_TLS_MODE=certbot\nexport KH_CERTBOT_DOMAIN=krakenhashes.example.com\nexport KH_CERTBOT_EMAIL=admin@example.com\n</code></pre>"},{"location":"deployment/production/#2-network-hardening","title":"2. Network Hardening","text":"<pre><code># Disable unnecessary services\nsystemctl disable avahi-daemon\nsystemctl disable cups\nsystemctl disable bluetooth\n\n# Configure iptables/firewalld\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --permanent --add-port=31337/tcp\nfirewall-cmd --permanent --add-port=1337/tcp\nfirewall-cmd --permanent --remove-service=ssh # Use specific source IPs\nfirewall-cmd --reload\n\n# Enable fail2ban for SSH and API endpoints\napt-get install fail2ban\n</code></pre>"},{"location":"deployment/production/#3-api-security","title":"3. API Security","text":"<ul> <li>Implement rate limiting (100 requests/minute per IP)</li> <li>Enable CORS with specific allowed origins</li> <li>Validate all input data</li> <li>Use prepared statements for database queries</li> <li>Implement request signing for agent communication</li> </ul>"},{"location":"deployment/production/#database-security","title":"Database Security","text":""},{"location":"deployment/production/#1-postgresql-hardening","title":"1. PostgreSQL Hardening","text":"<pre><code>-- Restrict connections\nALTER SYSTEM SET listen_addresses = 'localhost';\nALTER SYSTEM SET max_connections = 200;\n\n-- Enable SSL\nALTER SYSTEM SET ssl = on;\nALTER SYSTEM SET ssl_cert_file = '/etc/postgresql/server.crt';\nALTER SYSTEM SET ssl_key_file = '/etc/postgresql/server.key';\n\n-- Configure authentication\n-- Edit pg_hba.conf\nhostssl krakenhashes krakenhashes 127.0.0.1/32 scram-sha-256\nhostssl krakenhashes krakenhashes ::1/128 scram-sha-256\n\n-- Set strong passwords\nALTER USER krakenhashes WITH PASSWORD 'use-a-very-strong-password-here';\nALTER USER postgres WITH PASSWORD 'another-very-strong-password';\n\n-- Revoke unnecessary permissions\nREVOKE CREATE ON SCHEMA public FROM PUBLIC;\n</code></pre>"},{"location":"deployment/production/#2-database-access-control","title":"2. Database Access Control","text":"<ul> <li>Use application-specific database users</li> <li>Implement least privilege principle</li> <li>Regular password rotation (90 days)</li> <li>Audit database access logs</li> <li>Encrypt sensitive columns (future feature)</li> </ul>"},{"location":"deployment/production/#file-system-security","title":"File System Security","text":""},{"location":"deployment/production/#1-directory-permissions","title":"1. Directory Permissions","text":"<pre><code># Set proper ownership\nchown -R krakenhashes:krakenhashes /var/lib/krakenhashes\nchown -R postgres:postgres /var/lib/postgresql\n\n# Set restrictive permissions\nchmod 750 /var/lib/krakenhashes\nchmod 750 /var/lib/krakenhashes/binaries\nchmod 750 /var/lib/krakenhashes/wordlists\nchmod 750 /var/lib/krakenhashes/rules\nchmod 750 /var/lib/krakenhashes/hashlists\nchmod 1777 /var/lib/krakenhashes/hashlist_uploads  # Sticky bit for uploads\n\n# Protect configuration\nchmod 600 /etc/krakenhashes/config.env\nchmod 700 /etc/krakenhashes/certs\nchmod 600 /etc/krakenhashes/certs/*\n</code></pre>"},{"location":"deployment/production/#2-file-integrity-monitoring","title":"2. File Integrity Monitoring","text":"<pre><code># Install AIDE or similar\napt-get install aide\naide --init\naide --check\n\n# Monitor critical files\n/usr/local/bin/krakenhashes\n/etc/krakenhashes/*\n/var/lib/krakenhashes/binaries/*\n</code></pre>"},{"location":"deployment/production/#container-security-docker-deployments","title":"Container Security (Docker Deployments)","text":""},{"location":"deployment/production/#1-docker-hardening","title":"1. Docker Hardening","text":"<pre><code># docker-compose.yml security additions\nservices:\n  backend:\n    security_opt:\n      - no-new-privileges:true\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /var/run\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n    user: \"1000:1000\"\n</code></pre>"},{"location":"deployment/production/#2-image-security","title":"2. Image Security","text":"<pre><code># Scan images for vulnerabilities\ndocker scan krakenhashes/backend:latest\n\n# Use specific versions, not 'latest'\nimage: krakenhashes/backend:v0.1.0-alpha\n\n# Sign images\nexport DOCKER_CONTENT_TRUST=1\n</code></pre>"},{"location":"deployment/production/#performance-optimization","title":"Performance Optimization","text":""},{"location":"deployment/production/#database-optimization","title":"Database Optimization","text":""},{"location":"deployment/production/#1-postgresql-tuning","title":"1. PostgreSQL Tuning","text":"<pre><code>-- Memory settings (for 64GB RAM server)\nALTER SYSTEM SET shared_buffers = '16GB';\nALTER SYSTEM SET effective_cache_size = '48GB';\nALTER SYSTEM SET maintenance_work_mem = '2GB';\nALTER SYSTEM SET work_mem = '256MB';\n\n-- Connection pooling\nALTER SYSTEM SET max_connections = 200;\nALTER SYSTEM SET max_prepared_transactions = 200;\n\n-- Write performance\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET wal_buffers = '16MB';\nALTER SYSTEM SET default_statistics_target = 100;\n\n-- Query optimization\nALTER SYSTEM SET random_page_cost = 1.1;  -- For SSD\nALTER SYSTEM SET effective_io_concurrency = 200;  -- For SSD\n</code></pre>"},{"location":"deployment/production/#2-index-optimization","title":"2. Index Optimization","text":"<pre><code>-- Analyze query patterns and create appropriate indexes\nCREATE INDEX CONCURRENTLY idx_hashes_hashlist_cracked \n    ON hashes(hashlist_id, is_cracked);\n\nCREATE INDEX CONCURRENTLY idx_job_tasks_status \n    ON job_tasks(job_execution_id, status);\n\nCREATE INDEX CONCURRENTLY idx_agent_performance_metrics_lookup \n    ON agent_performance_metrics(agent_id, metric_type, timestamp);\n\n-- Regular maintenance\nVACUUM ANALYZE;\nREINDEX CONCURRENTLY;\n</code></pre>"},{"location":"deployment/production/#application-performance","title":"Application Performance","text":""},{"location":"deployment/production/#1-backend-optimization","title":"1. Backend Optimization","text":"<pre><code># Environment variables for performance\nexport GOMAXPROCS=16                    # Match CPU cores\nexport KH_HASHLIST_BATCH_SIZE=5000      # Increase for better throughput\nexport KH_DB_MAX_OPEN_CONNS=50          # Database connection pool\nexport KH_DB_MAX_IDLE_CONNS=25\nexport KH_WEBSOCKET_BUFFER_SIZE=8192    # Larger WebSocket buffers\n</code></pre>"},{"location":"deployment/production/#2-caching-strategy","title":"2. Caching Strategy","text":"<ul> <li>Implement Redis for session caching</li> <li>Cache agent benchmark results (24 hours)</li> <li>Cache frequently accessed wordlist metadata</li> <li>Use CDN for static assets</li> </ul>"},{"location":"deployment/production/#3-job-processing-optimization","title":"3. Job Processing Optimization","text":"<pre><code># Configure job chunking for optimal performance\nRule Splitting Threshold: 10000         # Split large rule files\nKeyspace Chunk Size: 1 hour            # Balanced chunk duration\nMaximum Concurrent Jobs: 100            # Per-agent limit\nTask Dispatch Batch Size: 10            # Parallel task dispatch\n</code></pre>"},{"location":"deployment/production/#network-optimization","title":"Network Optimization","text":""},{"location":"deployment/production/#1-websocket-tuning","title":"1. WebSocket Tuning","text":"<pre><code># Nginx configuration for WebSocket\nlocation /ws {\n    proxy_pass https://backend:31337;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_read_timeout 3600s;\n    proxy_send_timeout 3600s;\n    proxy_buffer_size 64k;\n    proxy_buffers 16 32k;\n}\n</code></pre>"},{"location":"deployment/production/#2-load-balancing","title":"2. Load Balancing","text":"<pre><code>upstream krakenhashes_backend {\n    least_conn;\n    server backend1:31337 max_fails=3 fail_timeout=30s;\n    server backend2:31337 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n</code></pre>"},{"location":"deployment/production/#high-availability-setup","title":"High Availability Setup","text":""},{"location":"deployment/production/#architecture-overview","title":"Architecture Overview","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Load Balancer\u2502\n                    \u2502   (HAProxy)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Backend 1  \u2502      \u2502  Backend 2  \u2502\n         \u2502  (Active)   \u2502      \u2502  (Active)   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502     PostgreSQL Cluster          \u2502\n         \u2502   (Primary + Replica)           \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/production/#database-high-availability","title":"Database High Availability","text":""},{"location":"deployment/production/#1-postgresql-replication-setup","title":"1. PostgreSQL Replication Setup","text":"<pre><code># On Primary\npostgresql.conf:\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_segments = 64\nsynchronous_commit = on\nsynchronous_standby_names = 'replica1'\n\n# On Replica\nrecovery.conf:\nstandby_mode = 'on'\nprimary_conninfo = 'host=primary port=5432 user=replicator'\ntrigger_file = '/tmp/postgresql.trigger'\n</code></pre>"},{"location":"deployment/production/#2-connection-pooling-with-pgbouncer","title":"2. Connection Pooling with PgBouncer","text":"<pre><code># pgbouncer.ini\n[databases]\nkrakenhashes = host=primary port=5432 dbname=krakenhashes\n\n[pgbouncer]\npool_mode = transaction\nmax_client_conn = 1000\ndefault_pool_size = 50\nmin_pool_size = 10\nreserve_pool_size = 5\nreserve_pool_timeout = 3\nserver_lifetime = 3600\nserver_idle_timeout = 600\n</code></pre>"},{"location":"deployment/production/#application-high-availability","title":"Application High Availability","text":""},{"location":"deployment/production/#1-backend-clustering","title":"1. Backend Clustering","text":"<pre><code># docker-compose-ha.yml\nservices:\n  backend1:\n    image: krakenhashes/backend:v0.1.0\n    environment:\n      - KH_CLUSTER_MODE=true\n      - KH_NODE_ID=backend1\n      - KH_CLUSTER_PEERS=backend2:31337\n    volumes:\n      - shared-data:/var/lib/krakenhashes\n\n  backend2:\n    image: krakenhashes/backend:v0.1.0\n    environment:\n      - KH_CLUSTER_MODE=true\n      - KH_NODE_ID=backend2\n      - KH_CLUSTER_PEERS=backend1:31337\n    volumes:\n      - shared-data:/var/lib/krakenhashes\n</code></pre>"},{"location":"deployment/production/#2-load-balancer-configuration","title":"2. Load Balancer Configuration","text":"<pre><code># haproxy.cfg\nglobal\n    maxconn 4096\n    log stdout local0\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n    option httplog\n\nfrontend krakenhashes_front\n    bind *:443 ssl crt /etc/ssl/krakenhashes.pem\n    default_backend krakenhashes_back\n\nbackend krakenhashes_back\n    balance leastconn\n    option httpchk GET /api/health\n    server backend1 backend1:31337 check ssl verify none\n    server backend2 backend2:31337 check ssl verify none\n</code></pre>"},{"location":"deployment/production/#storage-high-availability","title":"Storage High Availability","text":""},{"location":"deployment/production/#1-distributed-file-system","title":"1. Distributed File System","text":"<pre><code># GlusterFS setup for shared storage\ngluster volume create krakenhashes-data replica 2 \\\n    server1:/data/gluster/krakenhashes \\\n    server2:/data/gluster/krakenhashes\n\ngluster volume start krakenhashes-data\nmount -t glusterfs server1:/krakenhashes-data /var/lib/krakenhashes\n</code></pre>"},{"location":"deployment/production/#2-object-storage-integration","title":"2. Object Storage Integration","text":"<pre><code># S3-compatible storage for large files\nexport KH_STORAGE_TYPE=s3\nexport KH_S3_ENDPOINT=https://s3.example.com\nexport KH_S3_BUCKET=krakenhashes-data\nexport KH_S3_ACCESS_KEY=your-access-key\nexport KH_S3_SECRET_KEY=your-secret-key\n</code></pre>"},{"location":"deployment/production/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"deployment/production/#metrics-collection","title":"Metrics Collection","text":""},{"location":"deployment/production/#1-prometheus-integration","title":"1. Prometheus Integration","text":"<pre><code># prometheus.yml\nscrape_configs:\n  - job_name: 'krakenhashes'\n    static_configs:\n      - targets: ['backend1:31337', 'backend2:31337']\n    metrics_path: '/metrics'\n    scheme: 'https'\n    tls_config:\n      insecure_skip_verify: true\n</code></pre>"},{"location":"deployment/production/#2-key-metrics-to-monitor","title":"2. Key Metrics to Monitor","text":"<pre><code># System Metrics\n- CPU usage per core\n- Memory usage and available\n- Disk I/O and latency\n- Network throughput and errors\n\n# Application Metrics\n- Request rate and latency\n- Error rate by endpoint\n- Active WebSocket connections\n- Database connection pool usage\n\n# Business Metrics\n- Jobs queued/running/completed\n- Hash crack rate\n- Agent utilization\n- Storage usage growth\n</code></pre>"},{"location":"deployment/production/#alerting-configuration","title":"Alerting Configuration","text":""},{"location":"deployment/production/#1-critical-alerts-immediate-response","title":"1. Critical Alerts (Immediate Response)","text":"<pre><code>groups:\n  - name: critical\n    rules:\n      - alert: ServiceDown\n        expr: up{job=\"krakenhashes\"} == 0\n        for: 2m\n\n      - alert: DatabaseDown\n        expr: pg_up == 0\n        for: 1m\n\n      - alert: HighErrorRate\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) &gt; 0.05\n        for: 5m\n\n      - alert: DiskSpaceCritical\n        expr: disk_free_percentage &lt; 10\n        for: 5m\n</code></pre>"},{"location":"deployment/production/#2-warning-alerts-business-hours","title":"2. Warning Alerts (Business Hours)","text":"<pre><code>groups:\n  - name: warnings\n    rules:\n      - alert: HighCPUUsage\n        expr: cpu_usage_percentage &gt; 80\n        for: 15m\n\n      - alert: MemoryPressure\n        expr: memory_available_percentage &lt; 20\n        for: 10m\n\n      - alert: SlowQueries\n        expr: pg_slow_queries_rate &gt; 10\n        for: 10m\n\n      - alert: AgentDisconnections\n        expr: rate(agent_disconnections[5m]) &gt; 5\n        for: 5m\n</code></pre>"},{"location":"deployment/production/#logging-strategy","title":"Logging Strategy","text":""},{"location":"deployment/production/#1-centralized-logging","title":"1. Centralized Logging","text":"<pre><code># Fluentd configuration\n&lt;source&gt;\n  @type tail\n  path /var/log/krakenhashes/backend/*.log\n  pos_file /var/log/fluentd/krakenhashes.pos\n  tag krakenhashes.backend\n  &lt;parse&gt;\n    @type json\n  &lt;/parse&gt;\n&lt;/source&gt;\n\n&lt;match krakenhashes.**&gt;\n  @type elasticsearch\n  host elasticsearch.example.com\n  port 9200\n  logstash_format true\n  logstash_prefix krakenhashes\n&lt;/match&gt;\n</code></pre>"},{"location":"deployment/production/#2-log-retention-policy","title":"2. Log Retention Policy","text":"<ul> <li>Application logs: 30 days</li> <li>Security logs: 90 days</li> <li>Audit logs: 365 days</li> <li>Performance metrics: 90 days aggregated</li> </ul>"},{"location":"deployment/production/#dashboard-setup","title":"Dashboard Setup","text":""},{"location":"deployment/production/#1-grafana-dashboards","title":"1. Grafana Dashboards","text":"<ul> <li>System Overview Dashboard</li> <li>Job Performance Dashboard</li> <li>Agent Monitoring Dashboard</li> <li>Security Events Dashboard</li> <li>Database Performance Dashboard</li> </ul>"},{"location":"deployment/production/#2-real-time-monitoring","title":"2. Real-time Monitoring","text":"<pre><code># Custom monitoring endpoints\nGET /api/admin/metrics/realtime\nGET /api/admin/health/detailed\nGET /api/admin/agents/status\nGET /api/admin/jobs/statistics\n</code></pre>"},{"location":"deployment/production/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":""},{"location":"deployment/production/#backup-strategy","title":"Backup Strategy","text":""},{"location":"deployment/production/#1-automated-backup-schedule","title":"1. Automated Backup Schedule","text":"<pre><code># Database backups - every 4 hours\n0 */4 * * * /usr/local/bin/krakenhashes-db-backup.sh\n\n# File system backups - daily at 2 AM\n0 2 * * * /usr/local/bin/krakenhashes-file-backup.sh\n\n# Configuration backups - weekly\n0 3 * * 0 /usr/local/bin/krakenhashes-config-backup.sh\n\n# Off-site sync - daily at 4 AM\n0 4 * * * /usr/local/bin/krakenhashes-offsite-sync.sh\n</code></pre>"},{"location":"deployment/production/#2-backup-verification","title":"2. Backup Verification","text":"<pre><code>#!/bin/bash\n# Automated backup verification\nBACKUP_DIR=\"/backup/krakenhashes\"\nLOG_FILE=\"/var/log/krakenhashes-backup-verify.log\"\n\n# Verify latest backups\nfor backup_type in postgres files config; do\n    latest=$(find $BACKUP_DIR/$backup_type -name \"*.gz\" -mtime -1 | head -1)\n    if [ -z \"$latest\" ]; then\n        echo \"ERROR: No recent $backup_type backup found\" &gt;&gt; $LOG_FILE\n        # Send alert\n    fi\ndone\n</code></pre>"},{"location":"deployment/production/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":""},{"location":"deployment/production/#1-rtorpo-targets","title":"1. RTO/RPO Targets","text":"<ul> <li>Database RTO: 30 minutes</li> <li>Database RPO: 4 hours</li> <li>Full System RTO: 2 hours</li> <li>Full System RPO: 24 hours</li> </ul>"},{"location":"deployment/production/#2-recovery-procedures","title":"2. Recovery Procedures","text":"<pre><code># Quick recovery checklist\n1. Assess damage and determine recovery scope\n2. Provision replacement infrastructure\n3. Restore database from latest backup\n4. Restore file system data\n5. Update DNS/load balancer configuration\n6. Verify system functionality\n7. Communicate with users\n</code></pre>"},{"location":"deployment/production/#3-disaster-recovery-testing","title":"3. Disaster Recovery Testing","text":"<ul> <li>Monthly backup restoration tests</li> <li>Quarterly full DR drills</li> <li>Annual infrastructure failover test</li> <li>Document lessons learned</li> </ul>"},{"location":"deployment/production/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"deployment/production/#data-protection","title":"Data Protection","text":""},{"location":"deployment/production/#1-gdpr-compliance","title":"1. GDPR Compliance","text":"<ul> <li>Implement right to erasure (data deletion)</li> <li>Maintain data processing records</li> <li>Encrypt personal data at rest and in transit</li> <li>Implement data retention policies</li> <li>Regular privacy impact assessments</li> </ul>"},{"location":"deployment/production/#2-data-retention-policy","title":"2. Data Retention Policy","text":"<pre><code>-- Automated data retention\nDELETE FROM hashes \nWHERE is_cracked = true \n  AND cracked_at &lt; NOW() - INTERVAL '90 days'\n  AND hashlist_id IN (\n    SELECT id FROM hashlists \n    WHERE retention_days = 90\n  );\n\nDELETE FROM job_executions \nWHERE completed_at &lt; NOW() - INTERVAL '180 days';\n\nDELETE FROM agent_performance_metrics \nWHERE timestamp &lt; NOW() - INTERVAL '90 days';\n</code></pre>"},{"location":"deployment/production/#security-compliance","title":"Security Compliance","text":""},{"location":"deployment/production/#1-access-control-compliance","title":"1. Access Control Compliance","text":"<ul> <li>Implement least privilege access</li> <li>Regular access reviews</li> <li>Multi-factor authentication</li> <li>Session management controls</li> <li>Audit trail for all admin actions</li> </ul>"},{"location":"deployment/production/#2-audit-logging","title":"2. Audit Logging","text":"<pre><code>-- Audit table structure\nCREATE TABLE audit_logs (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER,\n    action VARCHAR(100),\n    resource_type VARCHAR(50),\n    resource_id INTEGER,\n    details JSONB,\n    ip_address INET,\n    user_agent TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Index for efficient querying\nCREATE INDEX idx_audit_logs_user_action \n    ON audit_logs(user_id, action, created_at);\n</code></pre>"},{"location":"deployment/production/#industry-standards","title":"Industry Standards","text":""},{"location":"deployment/production/#1-password-handling","title":"1. Password Handling","text":"<ul> <li>Never store plaintext passwords</li> <li>Use bcrypt with cost factor 12+</li> <li>Implement password history</li> <li>Enforce password complexity</li> <li>Regular password policy reviews</li> </ul>"},{"location":"deployment/production/#2-cryptographic-standards","title":"2. Cryptographic Standards","text":"<ul> <li>TLS 1.2 minimum (prefer TLS 1.3)</li> <li>Strong cipher suites only</li> <li>2048-bit RSA minimum (prefer 4096-bit)</li> <li>Regular certificate rotation</li> <li>Hardware security module (HSM) for keys</li> </ul>"},{"location":"deployment/production/#compliance-reporting","title":"Compliance Reporting","text":""},{"location":"deployment/production/#1-regular-reports","title":"1. Regular Reports","text":"<ul> <li>Monthly security metrics</li> <li>Quarterly compliance audits</li> <li>Annual penetration testing</li> <li>Incident response reports</li> <li>User access reviews</li> </ul>"},{"location":"deployment/production/#2-documentation-requirements","title":"2. Documentation Requirements","text":"<ul> <li>System architecture diagrams</li> <li>Data flow documentation</li> <li>Security policies and procedures</li> <li>Incident response plan</li> <li>Business continuity plan</li> </ul>"},{"location":"deployment/production/#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"deployment/production/#change-management","title":"Change Management","text":""},{"location":"deployment/production/#1-deployment-process","title":"1. Deployment Process","text":"<pre><code># Pre-deployment checklist\n- [ ] Code review completed\n- [ ] Security scan passed\n- [ ] Performance testing done\n- [ ] Backup taken\n- [ ] Rollback plan ready\n- [ ] Maintenance window scheduled\n- [ ] User notification sent\n</code></pre>"},{"location":"deployment/production/#2-version-control","title":"2. Version Control","text":"<ul> <li>Tag all production releases</li> <li>Maintain detailed changelogs</li> <li>Document breaking changes</li> <li>Test upgrade paths</li> <li>Keep rollback scripts ready</li> </ul>"},{"location":"deployment/production/#capacity-planning","title":"Capacity Planning","text":""},{"location":"deployment/production/#1-growth-monitoring","title":"1. Growth Monitoring","text":"<pre><code>-- Monitor growth trends\nSELECT \n    DATE_TRUNC('month', created_at) as month,\n    COUNT(*) as hashlists_created,\n    SUM(total_hashes) as total_hashes_added\nFROM hashlists\nGROUP BY month\nORDER BY month;\n</code></pre>"},{"location":"deployment/production/#2-scaling-triggers","title":"2. Scaling Triggers","text":"<ul> <li>CPU usage &gt; 70% sustained</li> <li>Memory usage &gt; 80%</li> <li>Storage usage &gt; 75%</li> <li>Response time &gt; 2 seconds</li> <li>Queue depth &gt; 1000 jobs</li> </ul>"},{"location":"deployment/production/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"deployment/production/#1-scheduled-maintenance","title":"1. Scheduled Maintenance","text":"<ul> <li>Weekly: Log rotation, temp file cleanup</li> <li>Monthly: Database optimization, index rebuilds</li> <li>Quarterly: Security updates, certificate renewal</li> <li>Annually: Major version upgrades</li> </ul>"},{"location":"deployment/production/#2-emergency-maintenance","title":"2. Emergency Maintenance","text":"<ul> <li>Critical security patches: Immediate</li> <li>Data corruption: Immediate</li> <li>Performance degradation: Within 4 hours</li> <li>Non-critical bugs: Next maintenance window</li> </ul>"},{"location":"deployment/production/#summary","title":"Summary","text":"<p>This production deployment guide provides a comprehensive framework for operating KrakenHashes at scale. Key takeaways:</p> <ol> <li>Security First: Implement defense in depth with multiple security layers</li> <li>High Availability: Design for failure with redundancy at every level</li> <li>Performance: Optimize continuously based on monitoring data</li> <li>Compliance: Maintain audit trails and follow data protection regulations</li> <li>Automation: Automate routine tasks to reduce human error</li> <li>Documentation: Keep all procedures documented and up to date</li> </ol> <p>Regular review and updates of these practices ensure your KrakenHashes deployment remains secure, performant, and reliable as your organization's needs evolve.</p>"},{"location":"deployment/updates/","title":"Update Procedures Guide","text":"<p>This guide covers the procedures for updating KrakenHashes deployments, including pre-update checks, update processes, and rollback procedures.</p> <p>\u26a0\ufe0f IMPORTANT: KrakenHashes is currently in v0.1.0-alpha. Breaking changes are expected between versions. Always review release notes and test updates in a non-production environment first.</p>"},{"location":"deployment/updates/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Pre-Update Checklist</li> <li>Updating Docker Deployments</li> <li>Database Migration Procedures</li> <li>Agent Update Process</li> <li>Rollback Procedures</li> <li>Version Compatibility</li> <li>Post-Update Verification</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/updates/#pre-update-checklist","title":"Pre-Update Checklist","text":"<p>Before beginning any update, complete the following checklist:</p>"},{"location":"deployment/updates/#1-review-release-notes","title":"1. Review Release Notes","text":"<ul> <li> Check the release notes for breaking changes</li> <li> Review migration scripts included in the release</li> <li> Identify any configuration changes required</li> <li> Note any new environment variables or removed features</li> </ul>"},{"location":"deployment/updates/#2-backup-current-system","title":"2. Backup Current System","text":"<pre><code># Backup database\ndocker-compose exec postgres pg_dump -U krakenhashes krakenhashes &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# Backup configuration files\ncp -r /home/zerkereod/Programming/passwordCracking/krakenhashes/.env backup/.env.$(date +%Y%m%d_%H%M%S)\ncp -r /home/zerkereod/Programming/passwordCracking/kh-backend/config backup/config_$(date +%Y%m%d_%H%M%S)\n\n# Backup data directory\ntar -czf backup/data_$(date +%Y%m%d_%H%M%S).tar.gz /home/zerkereod/Programming/passwordCracking/kh-backend/data\n</code></pre>"},{"location":"deployment/updates/#3-check-system-health","title":"3. Check System Health","text":"<pre><code># Check service status\ndocker-compose ps\n\n# Verify no active jobs\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/health\n\n# Check agent connections\ndocker-compose logs backend | grep -i \"agent.*connected\" | tail -n 20\n</code></pre>"},{"location":"deployment/updates/#4-document-current-version","title":"4. Document Current Version","text":"<pre><code># Record current versions\ndocker-compose exec backend /app/krakenhashes --version &gt; current_version.txt\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1;\"\n</code></pre>"},{"location":"deployment/updates/#5-plan-maintenance-window","title":"5. Plan Maintenance Window","text":"<ul> <li> Notify users of planned downtime</li> <li> Schedule update during low-activity period</li> <li> Prepare rollback plan</li> <li> Assign responsible personnel</li> </ul>"},{"location":"deployment/updates/#updating-docker-deployments","title":"Updating Docker Deployments","text":""},{"location":"deployment/updates/#standard-update-process","title":"Standard Update Process","text":"<ol> <li> <p>Stop Current Services <pre><code>cd /home/zerkereod/Programming/passwordCracking/krakenhashes\ndocker-compose down\n</code></pre></p> </li> <li> <p>Pull Latest Code <pre><code>git fetch origin\ngit checkout tags/v0.2.0  # Replace with target version\n# OR for latest development\ngit checkout master\ngit pull origin master\n</code></pre></p> </li> <li> <p>Review Configuration Changes <pre><code># Check for new environment variables\ndiff .env.example .env\n\n# Apply any new required variables\nnano .env\n</code></pre></p> </li> <li> <p>Build and Start Services <pre><code># Build new images\ndocker-compose build --no-cache\n\n# Start services\ndocker-compose up -d\n\n# Monitor startup\ndocker-compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#incremental-updates-development","title":"Incremental Updates (Development)","text":"<p>For development environments with frequent updates:</p> <pre><code># Quick rebuild and restart\ndocker-compose down\ngit pull origin master\ndocker-compose up -d --build backend\ndocker-compose logs -f backend\n</code></pre>"},{"location":"deployment/updates/#database-migration-procedures","title":"Database Migration Procedures","text":""},{"location":"deployment/updates/#automatic-migrations","title":"Automatic Migrations","text":"<p>Migrations are automatically applied on backend startup. Monitor the process:</p> <pre><code># Watch migration logs\ndocker-compose logs backend | grep -i migration\n\n# Verify migration status\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT version, dirty FROM schema_migrations ORDER BY version DESC LIMIT 5;\"\n</code></pre>"},{"location":"deployment/updates/#manual-migration-control","title":"Manual Migration Control","text":"<p>For production environments requiring manual migration control:</p> <ol> <li> <p>Disable Auto-Migration <pre><code># In .env, set:\nAUTO_MIGRATE=false\n</code></pre></p> </li> <li> <p>Apply Migrations Manually <pre><code>cd backend\n\n# View pending migrations\nmake migrate-status\n\n# Apply all pending migrations\nmake migrate-up\n\n# Apply specific version\nmigrate -path db/migrations -database \"$DATABASE_URL\" goto 20240115120000\n</code></pre></p> </li> <li> <p>Verify Migration Success <pre><code># Check migration history\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM schema_migrations;\"\n\n# Test critical tables\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"\\dt\"\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#handling-failed-migrations","title":"Handling Failed Migrations","text":"<p>If a migration fails:</p> <ol> <li> <p>Check Migration Status <pre><code># Check if migration is dirty\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM schema_migrations WHERE dirty = true;\"\n</code></pre></p> </li> <li> <p>Fix Dirty Migration <pre><code># Force version (use with caution)\ncd backend\nmigrate -path db/migrations -database \"$DATABASE_URL\" force 20240115120000\n\n# Then retry\nmake migrate-up\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#agent-update-process","title":"Agent Update Process","text":""},{"location":"deployment/updates/#coordinated-agent-updates","title":"Coordinated Agent Updates","text":"<ol> <li> <p>Prepare New Agent Binary <pre><code># New agent binaries are typically included in backend updates\n# Verify new version is available\ndocker-compose exec backend ls -la /data/krakenhashes/binaries/\n</code></pre></p> </li> <li> <p>Notify Connected Agents <pre><code># Agents will receive update notifications via WebSocket\n# Monitor agent update status in backend logs\ndocker-compose logs -f backend | grep -i \"agent.*update\"\n</code></pre></p> </li> <li> <p>Manual Agent Update (if auto-update fails) <pre><code># On each agent machine\ncd /path/to/agent\n./update.sh  # If provided\n\n# Or manually:\nsystemctl stop krakenhashes-agent\nwget https://your-server/api/v1/binaries/agent/latest -O krakenhashes-agent\nchmod +x krakenhashes-agent\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#agent-compatibility-check","title":"Agent Compatibility Check","text":"<p>Before updating: <pre><code># Check agent versions\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/agents | jq '.[] | {id, version, last_seen}'\n\n# Verify compatibility matrix in release notes\n</code></pre></p>"},{"location":"deployment/updates/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"deployment/updates/#quick-rollback-docker","title":"Quick Rollback (Docker)","text":"<ol> <li> <p>Stop Current Services <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>Restore Previous Version <pre><code># Checkout previous version\ngit checkout tags/v0.1.0  # Previous version\n\n# Restore configuration\ncp backup/.env.20240115_120000 .env\n\n# Rebuild and start\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Restore Database (if schema changed) <pre><code># Stop backend to prevent connections\ndocker-compose stop backend\n\n# Restore database\ndocker-compose exec -T postgres psql -U krakenhashes -d krakenhashes &lt; backup_20240115_120000.sql\n\n# Restart backend\ndocker-compose start backend\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#rollback-with-data-preservation","title":"Rollback with Data Preservation","text":"<p>For rollbacks that need to preserve new data:</p> <ol> <li> <p>Export New Data <pre><code># Export specific tables with new data\ndocker-compose exec postgres pg_dump -U krakenhashes -t job_executions -t hashes --data-only krakenhashes &gt; new_data.sql\n</code></pre></p> </li> <li> <p>Perform Rollback Follow standard rollback procedure</p> </li> <li> <p>Reimport Preserved Data <pre><code># Carefully reimport compatible data\ndocker-compose exec -T postgres psql -U krakenhashes -d krakenhashes &lt; new_data.sql\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#version-compatibility","title":"Version Compatibility","text":""},{"location":"deployment/updates/#compatibility-matrix","title":"Compatibility Matrix","text":"Component Backend Agent Frontend Database Schema v0.1.0 0.1.0 0.1.0 0.1.0 19 v0.2.0 0.2.0 0.1.0-0.2.0 0.2.0 22 v1.0.0 1.0.0 1.0.0 1.0.0 30 <p>Note: During alpha, assume all components must be updated together unless release notes specify otherwise.</p>"},{"location":"deployment/updates/#checking-compatibility","title":"Checking Compatibility","text":"<pre><code># Check all component versions\ndocker-compose exec backend /app/krakenhashes --version\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/system/info\n\n# Check schema version\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT MAX(version) FROM schema_migrations;\"\n</code></pre>"},{"location":"deployment/updates/#post-update-verification","title":"Post-Update Verification","text":""},{"location":"deployment/updates/#1-system-health-checks","title":"1. System Health Checks","text":"<pre><code># Backend health\ncurl -s https://localhost:8443/api/v1/health | jq .\n\n# Database connectivity\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/system/db-check\n\n# Frontend accessibility\ncurl -I https://localhost:8443\n</code></pre>"},{"location":"deployment/updates/#2-functional-verification","title":"2. Functional Verification","text":"<pre><code># Test authentication\ncurl -X POST https://localhost:8443/api/v1/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\":\"admin\",\"password\":\"your-password\"}'\n\n# Check agent connectivity\ndocker-compose logs backend | grep -i \"websocket.*agent\" | tail -n 10\n\n# Verify job creation (with auth token)\ncurl -X GET https://localhost:8443/api/v1/jobs \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"deployment/updates/#3-data-integrity-checks","title":"3. Data Integrity Checks","text":"<pre><code>-- Connect to database\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes\n\n-- Check critical tables\nSELECT COUNT(*) FROM users;\nSELECT COUNT(*) FROM agents;\nSELECT COUNT(*) FROM hashlists;\nSELECT COUNT(*) FROM job_executions WHERE status = 'running';\n\n-- Verify migrations\nSELECT * FROM schema_migrations ORDER BY version DESC LIMIT 5;\n</code></pre>"},{"location":"deployment/updates/#4-performance-verification","title":"4. Performance Verification","text":"<pre><code># Check resource usage\ndocker stats --no-stream\n\n# Monitor logs for errors\ndocker-compose logs --tail=100 backend | grep -i error\n\n# Check response times\ntime curl -s https://localhost:8443/api/v1/health\n</code></pre>"},{"location":"deployment/updates/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/updates/#common-update-issues","title":"Common Update Issues","text":""},{"location":"deployment/updates/#1-migration-failures","title":"1. Migration Failures","text":"<pre><code># Check migration logs\ndocker-compose logs backend | grep -E \"(migration|migrate)\"\n\n# Reset dirty migration\ncd backend\nmigrate -path db/migrations -database \"$DATABASE_URL\" force VERSION_NUMBER\n</code></pre>"},{"location":"deployment/updates/#2-container-start-failures","title":"2. Container Start Failures","text":"<pre><code># Check detailed logs\ndocker-compose logs backend\ndocker-compose logs postgres\n\n# Verify file permissions\nls -la /home/zerkereod/Programming/passwordCracking/kh-backend/data\n\n# Check disk space\ndf -h\n</code></pre>"},{"location":"deployment/updates/#3-agent-connection-issues","title":"3. Agent Connection Issues","text":"<pre><code># Restart agent connections\ndocker-compose restart backend\n\n# Check WebSocket logs\ndocker-compose logs backend | grep -i websocket\n\n# Verify agent API keys are still valid\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT * FROM agents WHERE active = true;\"\n</code></pre>"},{"location":"deployment/updates/#4-frontend-loading-issues","title":"4. Frontend Loading Issues","text":"<pre><code># Clear browser cache and cookies\n# Rebuild frontend\ndocker-compose up -d --build app\n\n# Check nginx logs\ndocker-compose logs app\n</code></pre>"},{"location":"deployment/updates/#emergency-procedures","title":"Emergency Procedures","text":"<p>If the system becomes unresponsive:</p> <ol> <li> <p>Preserve Logs <pre><code>docker-compose logs &gt; emergency_logs_$(date +%Y%m%d_%H%M%S).txt\n</code></pre></p> </li> <li> <p>Force Stop <pre><code>docker-compose down -v\n</code></pre></p> </li> <li> <p>Clean Start <pre><code>docker system prune -f\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Contact Support</p> </li> <li>Provide emergency logs</li> <li>Document steps leading to failure</li> <li>Note any error messages</li> </ol>"},{"location":"deployment/updates/#best-practices","title":"Best Practices","text":"<ol> <li>Always Test Updates</li> <li>Use staging environment</li> <li>Test core functionality</li> <li> <p>Verify agent connectivity</p> </li> <li> <p>Schedule Wisely</p> </li> <li>Update during maintenance windows</li> <li>Avoid updates during active cracking jobs</li> <li> <p>Coordinate with users</p> </li> <li> <p>Document Everything</p> </li> <li>Record version changes</li> <li>Note configuration modifications</li> <li> <p>Log any issues encountered</p> </li> <li> <p>Monitor Post-Update</p> </li> <li>Watch logs for 24 hours</li> <li>Check performance metrics</li> <li> <p>Gather user feedback</p> </li> <li> <p>Maintain Backups</p> </li> <li>Keep multiple backup versions</li> <li>Test restore procedures regularly</li> <li>Store backups securely</li> </ol>"},{"location":"deployment/updates/#conclusion","title":"Conclusion","text":"<p>Updating KrakenHashes requires careful planning and execution, especially during the alpha phase. Always prioritize data safety and system availability. When in doubt, test in a non-production environment first.</p> <p>For additional support or questions about specific update scenarios, consult the documentation or contact the development team.</p>"},{"location":"developer/","title":"Developer Guide","text":"<p>Documentation for developers working on or extending KrakenHashes.</p>"},{"location":"developer/#in-this-section","title":"In This Section","text":"<ul> <li> <p>:material-architecture:{ .lg .middle } System Architecture</p> <p>Understanding the overall system design and components</p> </li> <li> <p> Backend Development</p> <p>Working with the Go backend, APIs, and services</p> </li> <li> <p> Frontend Development</p> <p>React frontend, Material-UI components, and state management</p> </li> <li> <p> Agent Development</p> <p>Agent architecture, hardware detection, and job execution</p> </li> </ul>"},{"location":"developer/#getting-started","title":"Getting Started","text":""},{"location":"developer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21+ for backend development</li> <li>Node.js 18+ for frontend development  </li> <li>Docker and Docker Compose for testing</li> <li>PostgreSQL 15+ for database</li> <li>Git for version control</li> </ul>"},{"location":"developer/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/ZerkerEOD/krakenhashes.git\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Set up development environment</p> </li> <li>Copy <code>.env.example</code> to <code>.env</code></li> <li>Configure database connection</li> <li> <p>Set development flags</p> </li> <li> <p>Start services <pre><code>docker-compose up -d\n</code></pre></p> </li> </ol>"},{"location":"developer/#development-workflow","title":"Development Workflow","text":""},{"location":"developer/#code-organization","title":"Code Organization","text":"<pre><code>krakenhashes/\n\u251c\u2500\u2500 backend/         # Go backend service\n\u251c\u2500\u2500 frontend/        # React frontend\n\u251c\u2500\u2500 agent/          # Go agent system\n\u251c\u2500\u2500 docs/           # Documentation\n\u2514\u2500\u2500 scripts/        # Utility scripts\n</code></pre>"},{"location":"developer/#key-technologies","title":"Key Technologies","text":"<ul> <li>Backend: Go, Gin, GORM, JWT, WebSocket</li> <li>Frontend: React, TypeScript, Material-UI, React Query</li> <li>Database: PostgreSQL with migrations</li> <li>Agent: Go, Hashcat integration, Hardware detection</li> <li>Communication: REST API, WebSocket, TLS</li> </ul>"},{"location":"developer/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Pre-v1.0 Status</p> <p>External contributions are not being accepted until v1.0 release. This documentation is for understanding the codebase structure.</p>"},{"location":"developer/#code-standards","title":"Code Standards","text":"<ul> <li>Follow Go conventions for backend code</li> <li>Use TypeScript strictly in frontend</li> <li>Write tests for new functionality</li> <li>Document complex algorithms</li> <li>Keep security in mind</li> </ul>"},{"location":"developer/#testing","title":"Testing","text":"<ul> <li>Unit tests alongside code (<code>*_test.go</code>)</li> <li>Integration tests in <code>integration_test/</code></li> <li>Frontend tests with React Testing Library</li> <li>Manual testing with Docker environment</li> </ul>"},{"location":"developer/#architecture-principles","title":"Architecture Principles","text":"<ol> <li>Separation of Concerns</li> <li>Clear boundaries between layers</li> <li>Repository pattern for data access</li> <li> <p>Service layer for business logic</p> </li> <li> <p>Security First</p> </li> <li>Authentication on all endpoints</li> <li>Input validation and sanitization</li> <li> <p>Secure communication channels</p> </li> <li> <p>Scalability</p> </li> <li>Distributed agent architecture</li> <li>Efficient job scheduling</li> <li> <p>Resource pooling</p> </li> <li> <p>Maintainability</p> </li> <li>Clear code organization</li> <li>Comprehensive error handling</li> <li>Extensive logging</li> </ol>"},{"location":"developer/#need-help","title":"Need Help?","text":"<ul> <li>Check existing code patterns</li> <li>Review test files for examples</li> <li>Join Discord development channel</li> </ul>"},{"location":"developer/agent/","title":"KrakenHashes Agent Development Guide","text":""},{"location":"developer/agent/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Agent Architecture Overview</li> <li>Hardware Detection System</li> <li>Job Execution Flow</li> <li>WebSocket Communication</li> <li>File Synchronization</li> <li>Metrics Collection</li> <li>Adding New Features</li> <li>Testing Agents</li> <li>Mock Agent Mode</li> <li>Best Practices</li> <li>Troubleshooting</li> </ol>"},{"location":"developer/agent/#agent-architecture-overview","title":"Agent Architecture Overview","text":"<p>The KrakenHashes agent is a distributed compute node that executes password cracking jobs using hashcat. It's built with Go and designed to be cross-platform, supporting various GPU and CPU configurations.</p>"},{"location":"developer/agent/#core-components","title":"Core Components","text":"<pre><code>agent/\n\u251c\u2500\u2500 cmd/agent/          # Entry point and initialization\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 agent/         # Core agent logic and WebSocket connection\n\u2502   \u251c\u2500\u2500 auth/          # API key and certificate management\n\u2502   \u251c\u2500\u2500 config/        # Configuration management\n\u2502   \u251c\u2500\u2500 hardware/      # GPU/CPU detection using hashcat\n\u2502   \u251c\u2500\u2500 jobs/          # Job execution and hashcat management\n\u2502   \u251c\u2500\u2500 metrics/       # System metrics collection\n\u2502   \u251c\u2500\u2500 sync/          # File synchronization with backend\n\u2502   \u2514\u2500\u2500 status/        # Agent status management\n\u2514\u2500\u2500 pkg/debug/         # Debug logging utilities\n</code></pre>"},{"location":"developer/agent/#agent-lifecycle","title":"Agent Lifecycle","text":"<pre><code>// From cmd/agent/main.go\nfunc main() {\n    // 1. Initialize debug logging\n    debug.Reinitialize()\n\n    // 2. Load configuration from .env\n    cfg := loadConfig()\n\n    // 3. Initialize data directories\n    dataDirs, err := config.GetDataDirs()\n\n    // 4. Create metrics collector\n    collector, err := metrics.New(metrics.Config{\n        CollectionInterval: time.Duration(cfg.heartbeatInterval) * time.Second,\n        EnableGPU:          true,\n    })\n\n    // 5. Load or register agent credentials\n    agentID, cert, err := agent.LoadCredentials()\n\n    // 6. Create WebSocket connection\n    conn, err := agent.NewConnection(urlConfig)\n\n    // 7. Create job manager with hardware monitor\n    hwMonitor := conn.GetHardwareMonitor()\n    jobManager = jobs.NewJobManager(agentConfig, nil, hwMonitor)\n\n    // 8. Start connection and maintenance\n    conn.Start()\n\n    // 9. Detect and send device information\n    conn.DetectAndSendDevices()\n}\n</code></pre>"},{"location":"developer/agent/#hardware-detection-system","title":"Hardware Detection System","text":"<p>The agent uses hashcat's built-in device detection to identify available compute devices (GPUs and CPUs).</p>"},{"location":"developer/agent/#device-detection-implementation","title":"Device Detection Implementation","text":"<pre><code>// From internal/hardware/hashcat_detector.go\ntype HashcatDetector struct {\n    binaryPath     string\n    dataDirectory  string\n}\n\nfunc (d *HashcatDetector) DetectDevices() (*types.DeviceDetectionResult, error) {\n    // Build hashcat command with device info flags\n    args := []string{\"-I\", \"--machine-readable\", \"--quiet\"}\n\n    cmd := exec.CommandContext(ctx, d.binaryPath, args...)\n    output, err := cmd.Output()\n\n    // Parse hashcat output\n    devices, backends, err := d.parseHashcatOutput(string(output))\n\n    return &amp;types.DeviceDetectionResult{\n        Devices:          devices,\n        OpenCLBackends:   backends,\n        DetectedAt:       time.Now(),\n        HashcatVersion:   d.getHashcatVersion(),\n    }, nil\n}\n</code></pre>"},{"location":"developer/agent/#device-structure","title":"Device Structure","text":"<pre><code>// From internal/hardware/types/device.go\ntype Device struct {\n    ID          int    `json:\"id\"`\n    Type        string `json:\"type\"`        // \"gpu\" or \"cpu\"\n    Brand       string `json:\"brand\"`       // \"nvidia\", \"amd\", \"intel\"\n    Name        string `json:\"name\"`\n    Processor   string `json:\"processor\"`\n    Memory      int64  `json:\"memory\"`      // In bytes\n    DriverVersion string `json:\"driver_version\"`\n\n    // Performance characteristics\n    PCIeBus     string `json:\"pcie_bus\"`\n    CoreCount   int    `json:\"core_count\"`\n    ClockSpeed  int    `json:\"clock_speed\"` // In MHz\n\n    // Runtime state\n    Enabled     bool   `json:\"enabled\"`\n}\n</code></pre>"},{"location":"developer/agent/#hardware-monitor","title":"Hardware Monitor","text":"<pre><code>// From internal/hardware/monitor.go\ntype Monitor struct {\n    mu             sync.RWMutex\n    devices        []types.Device\n    hashcatDetector *HashcatDetector\n}\n\n// Detect devices using hashcat\nfunc (m *Monitor) DetectDevices() (*types.DeviceDetectionResult, error) {\n    result, err := m.hashcatDetector.DetectDevices()\n    if err != nil {\n        return nil, err\n    }\n\n    // Store devices in monitor\n    m.mu.Lock()\n    m.devices = result.Devices\n    m.mu.Unlock()\n\n    return result, nil\n}\n\n// Get enabled device flags for hashcat (-d parameter)\nfunc (m *Monitor) GetEnabledDeviceFlags() string {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n\n    return BuildDeviceFlags(m.devices)\n}\n</code></pre>"},{"location":"developer/agent/#job-execution-flow","title":"Job Execution Flow","text":"<p>The agent executes hashcat jobs based on task assignments from the backend.</p>"},{"location":"developer/agent/#job-manager","title":"Job Manager","text":"<pre><code>// From internal/jobs/jobs.go\ntype JobManager struct {\n    executor         *HashcatExecutor\n    config           *config.Config\n    progressCallback func(*JobProgress)\n    outputCallback   func(taskID string, output string, isError bool)\n    fileSync         *filesync.FileSync\n    hwMonitor        HardwareMonitor\n\n    mutex           sync.RWMutex\n    activeJobs      map[string]*JobExecution\n}\n\n// Process a job assignment from the backend\nfunc (jm *JobManager) ProcessJobAssignment(ctx context.Context, assignmentData []byte) error {\n    var assignment JobTaskAssignment\n    err := json.Unmarshal(assignmentData, &amp;assignment)\n\n    // 1. Ensure hashlist is available\n    err = jm.ensureHashlist(ctx, &amp;assignment)\n\n    // 2. Ensure rule chunks are available if needed\n    err = jm.ensureRuleChunks(ctx, &amp;assignment)\n\n    // 3. Start job execution\n    process, err := jm.executor.ExecuteTask(ctx, &amp;assignment)\n\n    // 4. Monitor job progress\n    go jm.monitorJobProgress(ctx, jobExecution)\n\n    return nil\n}\n</code></pre>"},{"location":"developer/agent/#hashcat-executor","title":"Hashcat Executor","text":"<pre><code>// From internal/jobs/hashcat_executor.go\ntype HashcatExecutor struct {\n    dataDirectory      string\n    activeProcesses    map[string]*HashcatProcess\n    mu                 sync.RWMutex\n    agentExtraParams   string\n    deviceFlagsCallback func() string\n}\n\n// Execute a hashcat task\nfunc (e *HashcatExecutor) ExecuteTask(ctx context.Context, assignment *JobTaskAssignment) (*HashcatProcess, error) {\n    // 1. Build hashcat command\n    args := e.buildHashcatCommand(assignment)\n\n    // 2. Create process with output capture\n    cmd := exec.CommandContext(ctx, binaryPath, args...)\n\n    // 3. Set up output pipes\n    stdout, _ := cmd.StdoutPipe()\n    stderr, _ := cmd.StderrPipe()\n\n    // 4. Start the process\n    err := cmd.Start()\n\n    // 5. Monitor output and parse progress\n    go e.monitorOutput(process, stdout, stderr)\n\n    return process, nil\n}\n\n// Build hashcat command with all parameters\nfunc (e *HashcatExecutor) buildHashcatCommand(assignment *JobTaskAssignment) []string {\n    args := []string{\n        \"-m\", strconv.Itoa(assignment.HashType),\n        \"-a\", strconv.Itoa(assignment.AttackMode),\n        \"--status\",\n        \"--status-json\",\n        \"--status-timer\", strconv.Itoa(assignment.ReportInterval),\n    }\n\n    // Add device flags if callback is set\n    if e.deviceFlagsCallback != nil {\n        deviceFlags := e.deviceFlagsCallback()\n        if deviceFlags != \"\" {\n            args = append(args, \"-d\", deviceFlags)\n        }\n    }\n\n    // Add skip and limit for keyspace splitting\n    if assignment.KeyspaceStart &gt; 0 {\n        args = append(args, \"-s\", strconv.FormatInt(assignment.KeyspaceStart, 10))\n    }\n    if assignment.KeyspaceEnd &gt; assignment.KeyspaceStart {\n        limit := assignment.KeyspaceEnd - assignment.KeyspaceStart\n        args = append(args, \"-l\", strconv.FormatInt(limit, 10))\n    }\n\n    return args\n}\n</code></pre>"},{"location":"developer/agent/#job-progress-tracking","title":"Job Progress Tracking","text":"<pre><code>// From internal/jobs/types.go\ntype JobProgress struct {\n    TaskID            string         `json:\"task_id\"`\n    Status            string         `json:\"status\"`\n    Progress          float64        `json:\"progress\"`\n    Speed             int64          `json:\"speed\"`\n    DeviceSpeeds      []DeviceSpeed  `json:\"device_speeds\"`\n    TimeRemaining     int            `json:\"time_remaining\"`\n    KeyspaceProcessed int64          `json:\"keyspace_processed\"`\n    CrackedCount      int            `json:\"cracked_count\"`\n    CrackedHashes     []CrackedHash  `json:\"cracked_hashes,omitempty\"`\n    ErrorMessage      string         `json:\"error_message,omitempty\"`\n}\n</code></pre>"},{"location":"developer/agent/#websocket-communication","title":"WebSocket Communication","text":"<p>The agent maintains a persistent WebSocket connection with the backend for real-time communication.</p>"},{"location":"developer/agent/#connection-management","title":"Connection Management","text":"<pre><code>// From internal/agent/connection.go\ntype Connection struct {\n    ws              *websocket.Conn\n    urlConfig       *config.URLConfig\n    hwMonitor       *hardware.Monitor\n    outbound        chan *WSMessage\n    done            chan struct{}\n    isConnected     atomic.Bool\n    tlsConfig       *tls.Config\n    fileSync        *filesync.FileSync\n    jobManager      JobManager\n}\n\n// WebSocket message types\ntype WSMessageType string\n\nconst (\n    WSTypeHardwareInfo         WSMessageType = \"hardware_info\"\n    WSTypeMetrics              WSMessageType = \"metrics\"\n    WSTypeHeartbeat            WSMessageType = \"heartbeat\"\n    WSTypeAgentStatus          WSMessageType = \"agent_status\"\n    WSTypeFileSyncRequest      WSMessageType = \"file_sync_request\"\n    WSTypeFileSyncResponse     WSMessageType = \"file_sync_response\"\n    WSTypeTaskAssignment       WSMessageType = \"task_assignment\"\n    WSTypeJobProgress          WSMessageType = \"job_progress\"\n    WSTypeJobStop              WSMessageType = \"job_stop\"\n    WSTypeBenchmarkRequest     WSMessageType = \"benchmark_request\"\n    WSTypeBenchmarkResult      WSMessageType = \"benchmark_result\"\n    WSTypeHashcatOutput        WSMessageType = \"hashcat_output\"\n    WSTypeDeviceDetection      WSMessageType = \"device_detection\"\n    WSTypeDeviceUpdate         WSMessageType = \"device_update\"\n    WSTypeCrackBatch           WSMessageType = \"crack_batch\"\n    WSTypeCrackBatchesComplete WSMessageType = \"crack_batches_complete\"  // NEW in v1.3.1\n)\n</code></pre>"},{"location":"developer/agent/#message-handling","title":"Message Handling","text":"<pre><code>// From internal/agent/connection.go - readPump method\nfunc (c *Connection) readPump() {\n    for {\n        var msg WSMessage\n        err := c.ws.ReadJSON(&amp;msg)\n\n        switch msg.Type {\n        case WSTypeTaskAssignment:\n            // Process job assignment\n            if err := c.jobManager.ProcessJobAssignment(ctx, msg.Payload); err != nil {\n                debug.Error(\"Failed to process job assignment: %v\", err)\n            }\n\n        case WSTypeFileSyncRequest:\n            // Handle file sync request\n            go c.handleFileSyncAsync(requestPayload)\n\n        case WSTypeBenchmarkRequest:\n            // Run speed test\n            go func() {\n                totalSpeed, deviceSpeeds, err := executor.RunSpeedTest(ctx, assignment, testDuration)\n                // Send results back\n            }()\n\n        case WSTypeDeviceUpdate:\n            // Update device enabled/disabled state\n            c.hwMonitor.UpdateDeviceStatus(updatePayload.DeviceID, updatePayload.Enabled)\n        }\n    }\n}\n</code></pre>"},{"location":"developer/agent/#sending-updates","title":"Sending Updates","text":"<pre><code>// Send job progress to backend\nfunc (c *Connection) SendJobProgress(progress *jobs.JobProgress) error {\n    progressJSON, err := json.Marshal(progress)\n\n    msg := &amp;WSMessage{\n        Type:      WSTypeJobProgress,\n        Payload:   progressJSON,\n        Timestamp: time.Now(),\n    }\n\n    select {\n    case c.outbound &lt;- msg:\n        return nil\n    case &lt;-time.After(5 * time.Second):\n        return fmt.Errorf(\"failed to queue job progress: channel blocked\")\n    }\n}\n</code></pre>"},{"location":"developer/agent/#crack-batch-workflow","title":"Crack Batch Workflow","text":"<p>The agent sends cracked passwords in batches and signals completion to enable the backend's processing status workflow.</p> <p>Message Flow:</p> <ol> <li>Task Completes: Agent sends final <code>job_progress</code> with <code>Status=\"completed\"</code> and <code>CrackedCount</code> field</li> <li>Send Crack Batches: Agent sends one or more <code>crack_batch</code> messages with cracked passwords</li> <li>Signal Completion: Agent sends <code>crack_batches_complete</code> to signal all batches sent</li> <li>Agent Available: Agent is immediately available for new work</li> </ol> <p>Implementation:</p> <pre><code>// Send final progress with crack count\nprogress := &amp;JobProgress{\n    TaskID:          taskID,\n    Status:          \"completed\",\n    ProgressPercent: 100.0,\n    CrackedCount:    totalCracks,  // Expected number of cracks\n}\nc.SendJobProgress(progress)\n\n// Send crack batches (batched during execution)\nfor _, batch := range crackBatches {\n    crackBatch := &amp;CrackBatch{\n        TaskID:        taskID,\n        CrackedHashes: batch,\n    }\n    c.SendCrackBatch(crackBatch)\n}\n\n// Signal all batches sent\ncompletion := &amp;CrackBatchesComplete{\n    TaskID: taskID,\n}\nc.SendCrackBatchesComplete(completion)\n</code></pre> <p>Message Structures:</p> <pre><code>// Final progress message\ntype JobProgress struct {\n    TaskID          string  `json:\"task_id\"`\n    Status          string  `json:\"status\"`           // \"completed\"\n    ProgressPercent float64 `json:\"progress_percent\"` // 100.0\n    CrackedCount    int     `json:\"cracked_count\"`    // Expected cracks\n    // ... other fields\n}\n\n// Crack batch message\ntype CrackBatch struct {\n    TaskID        string         `json:\"task_id\"`\n    CrackedHashes []CrackedHash  `json:\"cracked_hashes\"`\n}\n\ntype CrackedHash struct {\n    Hash         string `json:\"hash\"`\n    Plaintext    string `json:\"plaintext\"`\n    OriginalLine string `json:\"original_line\"`\n}\n\n// Completion signal message\ntype CrackBatchesComplete struct {\n    TaskID string `json:\"task_id\"`\n}\n</code></pre> <p>Backend Processing:</p> <p>When backend receives <code>crack_batches_complete</code>: 1. Task transitions from <code>processing</code> to <code>completed</code> 2. Backend verifies: <code>received_crack_count &gt;= expected_crack_count</code> 3. Agent busy status cleared 4. Agent can immediately accept new tasks</p> <p>See Crack Batching System for full details on batching logic and performance optimization.</p>"},{"location":"developer/agent/#file-synchronization","title":"File Synchronization","text":"<p>The agent synchronizes wordlists, rules, and binaries with the backend.</p>"},{"location":"developer/agent/#file-sync-implementation","title":"File Sync Implementation","text":"<pre><code>// From internal/sync/sync.go\ntype FileSync struct {\n    client     *http.Client\n    urlConfig  *config.URLConfig\n    dataDirs   *config.DataDirs\n    sem        chan struct{} // Semaphore for concurrent downloads\n    maxRetries int\n    apiKey     string\n    agentID    string\n}\n\n// Download a file from the backend\nfunc (fs *FileSync) DownloadFileFromInfo(ctx context.Context, fileInfo *FileInfo) error {\n    // 1. Check if file already exists with correct hash\n    if fs.fileExistsWithHash(fileInfo) {\n        return nil\n    }\n\n    // 2. Build download URL\n    url := fs.buildDownloadURL(fileInfo)\n\n    // 3. Create authenticated request\n    req, _ := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n    req.Header.Set(\"X-API-Key\", fs.apiKey)\n    req.Header.Set(\"X-Agent-ID\", fs.agentID)\n\n    // 4. Download to temporary file\n    tempPath := finalPath + \".tmp\"\n    err := fs.downloadToFile(req, tempPath)\n\n    // 5. Verify hash\n    if !fs.verifyFileHash(tempPath, fileInfo.MD5Hash) {\n        return fmt.Errorf(\"hash mismatch\")\n    }\n\n    // 6. Move to final location\n    os.Rename(tempPath, finalPath)\n\n    // 7. Extract if it's a binary archive\n    if fileInfo.FileType == \"binary\" &amp;&amp; strings.HasSuffix(fileInfo.Name, \".7z\") {\n        fs.ExtractBinary7z(finalPath, targetDir)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/agent/#directory-structure","title":"Directory Structure","text":"<pre><code>// From internal/config/dirs.go\ntype DataDirs struct {\n    Binaries   string // /path/to/data/binaries\n    Wordlists  string // /path/to/data/wordlists\n    Rules      string // /path/to/data/rules\n    Hashlists  string // /path/to/data/hashlists\n}\n\n// Wordlist categories:\n// - general/     # Common wordlists\n// - specialized/ # Domain-specific lists\n// - targeted/    # Custom lists for specific targets\n// - custom/      # User-uploaded lists\n\n// Rule categories:\n// - hashcat/     # Hashcat rule files\n// - john/        # John the Ripper rules\n// - custom/      # User-created rules\n// - chunks/      # Split rule files for distributed processing\n</code></pre>"},{"location":"developer/agent/#metrics-collection","title":"Metrics Collection","text":"<p>The agent collects system metrics for monitoring and optimization.</p>"},{"location":"developer/agent/#metrics-collector","title":"Metrics Collector","text":"<pre><code>// From internal/metrics/collector.go\ntype Collector struct {\n    interval   time.Duration\n    gpuEnabled bool\n}\n\n// Collect system metrics\nfunc (c *Collector) Collect() (*SystemMetrics, error) {\n    metrics := &amp;SystemMetrics{}\n\n    // CPU metrics using gopsutil\n    percentage, _ := cpu.Percent(time.Second, false)\n    metrics.CPUUsage = percentage[0]\n\n    // Memory metrics\n    vmem, _ := mem.VirtualMemory()\n    metrics.MemoryUsage = vmem.UsedPercent\n\n    // GPU metrics come from hashcat JSON status\n    // during job execution\n\n    return metrics, nil\n}\n</code></pre>"},{"location":"developer/agent/#metrics-data-structure","title":"Metrics Data Structure","text":"<pre><code>// From internal/agent/connection.go\ntype MetricsData struct {\n    AgentID     int               `json:\"agent_id\"`\n    CollectedAt time.Time         `json:\"collected_at\"`\n    CPUs        []CPUMetrics      `json:\"cpus\"`\n    GPUs        []GPUMetrics      `json:\"gpus\"`\n    Memory      MemoryMetrics     `json:\"memory\"`\n    Disk        []DiskMetrics     `json:\"disk\"`\n    Network     []NetworkMetrics  `json:\"network\"`\n    Process     []ProcessMetrics  `json:\"process\"`\n}\n</code></pre>"},{"location":"developer/agent/#adding-new-features","title":"Adding New Features","text":""},{"location":"developer/agent/#1-adding-a-new-websocket-message-type","title":"1. Adding a New WebSocket Message Type","text":"<pre><code>// 1. Define the message type in connection.go\nconst WSTypeNewFeature WSMessageType = \"new_feature\"\n\n// 2. Create payload structures\ntype NewFeatureRequest struct {\n    Parameter1 string `json:\"parameter1\"`\n    Parameter2 int    `json:\"parameter2\"`\n}\n\n// 3. Add handler in readPump\ncase WSTypeNewFeature:\n    var payload NewFeatureRequest\n    if err := json.Unmarshal(msg.Payload, &amp;payload); err != nil {\n        debug.Error(\"Failed to parse new feature: %v\", err)\n        continue\n    }\n\n    // Handle the feature\n    go c.handleNewFeature(payload)\n\n// 4. Implement the handler\nfunc (c *Connection) handleNewFeature(payload NewFeatureRequest) {\n    // Implementation\n}\n</code></pre>"},{"location":"developer/agent/#2-adding-hardware-support","title":"2. Adding Hardware Support","text":"<pre><code>// 1. Update device detection in hashcat_detector.go\nfunc (d *HashcatDetector) parseHashcatOutput(output string) ([]types.Device, []types.OpenCLBackend, error) {\n    // Add parsing for new hardware types\n\n    // Example: Detect new GPU vendor\n    if strings.Contains(line, \"NewVendor\") {\n        device.Brand = \"newvendor\"\n        device.Type = \"gpu\"\n    }\n}\n\n// 2. Add device-specific handling\nfunc BuildDeviceFlags(devices []types.Device) string {\n    // Add logic for new device types\n}\n</code></pre>"},{"location":"developer/agent/#3-adding-job-features","title":"3. Adding Job Features","text":"<pre><code>// 1. Update JobTaskAssignment structure\ntype JobTaskAssignment struct {\n    // Existing fields...\n\n    NewFeature string `json:\"new_feature\"`\n}\n\n// 2. Update hashcat command building\nfunc (e *HashcatExecutor) buildHashcatCommand(assignment *JobTaskAssignment) []string {\n    // Add new hashcat parameters\n    if assignment.NewFeature != \"\" {\n        args = append(args, \"--new-feature\", assignment.NewFeature)\n    }\n}\n\n// 3. Update progress monitoring if needed\nfunc (e *HashcatExecutor) parseHashcatStatus(status map[string]interface{}) *JobProgress {\n    // Parse new status fields\n}\n</code></pre>"},{"location":"developer/agent/#testing-agents","title":"Testing Agents","text":""},{"location":"developer/agent/#unit-testing","title":"Unit Testing","text":"<pre><code>// Example from agent_test.go\nfunc TestGetAgentID(t *testing.T) {\n    tests := []struct {\n        name        string\n        setupFunc   func(configDir string) error\n        expectedID  int\n        wantErr     bool\n    }{\n        {\n            name: \"successful ID retrieval\",\n            setupFunc: func(configDir string) error {\n                return auth.SaveAgentKey(configDir, \"test-api-key\", \"456\")\n            },\n            expectedID: 456,\n            wantErr:    false,\n        },\n    }\n\n    for _, tt := range tests {\n        t.Run(tt.name, func(t *testing.T) {\n            tempDir := t.TempDir()\n            t.Setenv(\"KH_CONFIG_DIR\", tempDir)\n\n            if tt.setupFunc != nil {\n                err := tt.setupFunc(tempDir)\n                require.NoError(t, err)\n            }\n\n            id, err := GetAgentID()\n\n            if tt.wantErr {\n                assert.Error(t, err)\n            } else {\n                assert.NoError(t, err)\n                assert.Equal(t, tt.expectedID, id)\n            }\n        })\n    }\n}\n</code></pre>"},{"location":"developer/agent/#integration-testing","title":"Integration Testing","text":"<pre><code>// Test WebSocket connection\nfunc TestWebSocketConnection(t *testing.T) {\n    // Create test server\n    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        upgrader := websocket.Upgrader{}\n        conn, _ := upgrader.Upgrade(w, r, nil)\n        defer conn.Close()\n\n        // Test message exchange\n        var msg WSMessage\n        conn.ReadJSON(&amp;msg)\n        assert.Equal(t, WSTypeHeartbeat, msg.Type)\n\n        // Send response\n        conn.WriteJSON(WSMessage{\n            Type: WSTypeHeartbeat,\n            Timestamp: time.Now(),\n        })\n    }))\n    defer server.Close()\n\n    // Test connection\n    conn, err := NewConnection(urlConfig)\n    assert.NoError(t, err)\n\n    err = conn.Start()\n    assert.NoError(t, err)\n}\n</code></pre>"},{"location":"developer/agent/#mock-testing","title":"Mock Testing","text":"<pre><code>// From internal/mocks/\ntype MockJobManager struct {\n    mock.Mock\n}\n\nfunc (m *MockJobManager) ProcessJobAssignment(ctx context.Context, data []byte) error {\n    args := m.Called(ctx, data)\n    return args.Error(0)\n}\n\n// Use in tests\nfunc TestJobProcessing(t *testing.T) {\n    mockJM := new(MockJobManager)\n    mockJM.On(\"ProcessJobAssignment\", mock.Anything, mock.Anything).Return(nil)\n\n    conn := &amp;Connection{\n        jobManager: mockJM,\n    }\n\n    // Test job processing\n    msg := WSMessage{\n        Type: WSTypeTaskAssignment,\n        Payload: json.RawMessage(`{\"task_id\": \"test-123\"}`),\n    }\n\n    // Process message\n    // Assert expectations\n    mockJM.AssertExpectations(t)\n}\n</code></pre>"},{"location":"developer/agent/#performance-testing","title":"Performance Testing","text":"<pre><code>func BenchmarkHashcatCommandBuilding(b *testing.B) {\n    executor := NewHashcatExecutor(\"/data\")\n    assignment := &amp;JobTaskAssignment{\n        TaskID:     \"bench-task\",\n        HashType:   0,\n        AttackMode: 0,\n        WordlistPaths: []string{\"wordlist1.txt\", \"wordlist2.txt\"},\n        RulePaths:     []string{\"rule1.rule\", \"rule2.rule\"},\n    }\n\n    b.ResetTimer()\n    for i := 0; i &lt; b.N; i++ {\n        _ = executor.buildHashcatCommand(assignment)\n    }\n}\n</code></pre>"},{"location":"developer/agent/#manual-testing","title":"Manual Testing","text":"<pre><code># Build and run agent locally\ncd agent\ngo build -o krakenhashes-agent cmd/agent/main.go\n\n# Run with test configuration\n./krakenhashes-agent -host localhost:8080 -claim TEST-CLAIM-CODE -debug\n\n# Test specific components\n# Device detection\n./krakenhashes-agent -test-devices\n\n# File sync\n./krakenhashes-agent -test-sync wordlists\n\n# Benchmark\n./krakenhashes-agent -test-benchmark -m 0 -a 0\n</code></pre>"},{"location":"developer/agent/#mock-agent-mode","title":"Mock Agent Mode","text":"<p>Mock agents simulate GPU work without requiring real hardware, enabling testing of the scheduling system on development machines.</p>"},{"location":"developer/agent/#enabling-mock-mode","title":"Enabling Mock Mode","text":"<p>Use the <code>--test-mode</code> flag or <code>TEST_MODE=true</code> environment variable:</p> <pre><code># Via command line\n./krakenhashes-agent --host localhost:31337 --claim VOUCHER_CODE --test-mode\n\n# Via environment variable\nexport TEST_MODE=true\n./krakenhashes-agent --host localhost:31337 --claim VOUCHER_CODE\n</code></pre>"},{"location":"developer/agent/#mock-agent-configuration","title":"Mock Agent Configuration","text":"<p>Configure mock agent behavior via environment variables:</p> <pre><code>MOCK_PROGRESS_SPEED=120    # Seconds to complete a task (default: 120)\nMOCK_CRACK_RATE=0.05       # Percentage of hashes to crack (0.05 = 5%)\nMOCK_HASH_RATE=1000000000  # Simulated hash rate in H/s\nMOCK_GPU_COUNT=2           # Number of fake GPUs to report\nMOCK_GPU_VENDOR=nvidia     # GPU vendor: nvidia, amd, intel\nMOCK_GPU_MODEL=\"RTX 4090\"  # GPU model name\nMOCK_GPU_MEMORY_MB=24576   # GPU memory in MB\n</code></pre>"},{"location":"developer/agent/#mock-agent-limitations","title":"\u26a0\ufe0f Mock Agent Limitations","text":"<p>Important</p> <p>Mock agents are designed for testing scheduling algorithms, NOT for accurate job progress simulation.</p> <p>Mock agents cannot accurately simulate job progress because:</p> Aspect Mock Agent Real Hashcat Progress values Reports BASE keyspace as progress Reports EFFECTIVE keyspace (<code>progress[0]</code>/<code>progress[1]</code>) Job progress % Shows very low % (e.g., 0.28%) Shows accurate % based on actual candidates Keyspace calculation Uses <code>--skip</code>/<code>--limit</code> values directly Reports <code>candidates processed</code> \u00d7 <code>rules applied</code> <p>Why this happens:</p> <ol> <li><code>JobTaskAssignment</code> only includes <code>KeyspaceStart</code>/<code>KeyspaceEnd</code> (BASE keyspace units)</li> <li>Mock agents don't receive <code>EffectiveKeyspaceStart/End</code> values</li> <li>Real hashcat calculates and reports effective candidates internally via <code>progress[0]</code> and <code>progress[1]</code></li> </ol>"},{"location":"developer/agent/#what-mock-agents-are-good-for","title":"What Mock Agents ARE Good For","text":"<ul> <li>\u2705 Testing agent registration, connection, and heartbeat</li> <li>\u2705 Testing priority-based scheduling and agent allocation</li> <li>\u2705 Testing task assignment and distribution across agents</li> <li>\u2705 Testing job start/stop/pause workflows</li> <li>\u2705 Verifying keyspace/rule splitting logic (no overlaps)</li> <li>\u2705 Testing file synchronization</li> <li>\u2705 Testing device enable/disable functionality</li> </ul>"},{"location":"developer/agent/#what-requires-real-agents","title":"What Requires Real Agents","text":"<ul> <li>\u274c Accurate job progress percentage tracking</li> <li>\u274c Accurate ETA calculations</li> <li>\u274c Realistic crack rate statistics</li> <li>\u274c Actual password recovery testing</li> <li>\u274c Hashcat error handling verification</li> <li>\u274c GPU memory and performance testing</li> </ul>"},{"location":"developer/agent/#example-testing-scheduling-with-mock-agents","title":"Example: Testing Scheduling with Mock Agents","text":"<pre><code># Start 5 mock agents with different GPU configurations\nMOCK_GPU_COUNT=2 MOCK_HASH_RATE=1000000000 ./agent --test-mode --claim CODE1 &amp;\nMOCK_GPU_COUNT=4 MOCK_HASH_RATE=2000000000 ./agent --test-mode --claim CODE2 &amp;\nMOCK_GPU_COUNT=1 MOCK_HASH_RATE=500000000  ./agent --test-mode --claim CODE3 &amp;\nMOCK_GPU_COUNT=2 MOCK_HASH_RATE=1500000000 ./agent --test-mode --claim CODE4 &amp;\nMOCK_GPU_COUNT=3 MOCK_HASH_RATE=1800000000 ./agent --test-mode --claim CODE5 &amp;\n\n# Create jobs and observe:\n# - Agent allocation based on priority\n# - Task distribution with no keyspace overlap\n# - Rule splitting with sequential rule ranges\n# - Job start/stop behavior\n\n# Note: Job progress % will NOT be accurate with mock agents\n</code></pre>"},{"location":"developer/agent/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Error Handling: Always use the debug package for logging    <pre><code>debug.Error(\"Operation failed: %v\", err)\ndebug.Info(\"Operation completed successfully\")\n</code></pre></p> </li> <li> <p>Resource Management: Always clean up resources    <pre><code>defer func() {\n  if conn != nil {\n      conn.Close()\n  }\n}()\n</code></pre></p> </li> <li> <p>Concurrent Operations: Use proper synchronization    <pre><code>type SafeMap struct {\n    mu   sync.RWMutex\n    data map[string]interface{}\n}\n</code></pre></p> </li> <li> <p>Context Usage: Respect context cancellation    <pre><code>select {\ncase &lt;-ctx.Done():\n    return ctx.Err()\ncase result := &lt;-resultChan:\n    return result\n}\n</code></pre></p> </li> <li> <p>Configuration: Use environment variables    <pre><code>value := os.Getenv(\"KH_SETTING\")\nif value == \"\" {\n    value = \"default\"\n}\n</code></pre></p> </li> </ol>"},{"location":"developer/agent/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Certificate Errors: Agent will attempt to renew certificates automatically</li> <li>Connection Drops: Automatic reconnection with exponential backoff</li> <li>File Sync Failures: Automatic retry with hash verification</li> <li>Hashcat Errors: Check device permissions and driver installation</li> <li>Memory Issues: Monitor system resources during large jobs</li> </ol> <p>For debugging, enable debug mode: <pre><code>export DEBUG=true\nexport LOG_LEVEL=DEBUG\n</code></pre></p>"},{"location":"developer/architecture/","title":"KrakenHashes System Architecture","text":""},{"location":"developer/architecture/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>High-Level Architecture</li> <li>Backend Architecture</li> <li>Frontend Architecture</li> <li>Agent Architecture</li> <li>Communication Protocols</li> <li>Database Schema</li> <li>Security Architecture</li> <li>File Storage Architecture</li> <li>Deployment Architecture</li> </ol>"},{"location":"developer/architecture/#overview","title":"Overview","text":"<p>KrakenHashes is a distributed password cracking management system designed to orchestrate and manage hashcat operations across multiple compute agents. The system follows a client-server architecture with a centralized backend, web-based frontend, and distributed agent nodes.</p>"},{"location":"developer/architecture/#key-components","title":"Key Components","text":"<ul> <li>Backend Server (Go): REST API server managing job orchestration, user authentication, and agent coordination</li> <li>Frontend (React/TypeScript): Web UI for system management and monitoring</li> <li>Agent (Go): Distributed compute nodes executing hashcat jobs</li> <li>PostgreSQL Database: Persistent storage for system data</li> <li>File Storage: Centralized storage for binaries, wordlists, rules, and hashlists</li> </ul>"},{"location":"developer/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            Frontend (React)                              \u2502\n\u2502                         Material-UI Components                           \u2502\n\u2502                      React Query + TypeScript                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 HTTPS/REST API\n                                 \u2502 WebSocket\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Backend Server (Go)                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Handlers   \u2502  \u2502   Services   \u2502  \u2502 Repositories \u2502  \u2502 Middleware \u2502 \u2502\n\u2502  \u2502  (HTTP/WS)   \u2502  \u2502 (Business)   \u2502  \u2502   (Data)     \u2502  \u2502   (Auth)   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 SQL\n                                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PostgreSQL Database                              \u2502\n\u2502                    (Users, Agents, Jobs, Hashlists)                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Agent Nodes (Go)                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Hardware   \u2502  \u2502     Job      \u2502  \u2502     Sync     \u2502  \u2502 Heartbeat  \u2502 \u2502\n\u2502  \u2502  Detection   \u2502  \u2502  Execution   \u2502  \u2502   Manager    \u2502  \u2502  Manager   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/architecture/#backend-architecture","title":"Backend Architecture","text":""},{"location":"developer/architecture/#layered-architecture","title":"Layered Architecture","text":"<p>The backend follows a clean layered architecture with clear separation of concerns:</p>"},{"location":"developer/architecture/#1-presentation-layer-internalhandlers","title":"1. Presentation Layer (<code>internal/handlers/</code>)","text":"<ul> <li>HTTP request handlers organized by domain</li> <li>WebSocket handlers for real-time communication</li> <li>Request validation and response formatting</li> </ul> <p>Key Packages: - <code>admin/</code> - Administrative functions (users, clients, settings) - <code>agent/</code> - Agent management and registration - <code>auth/</code> - Authentication and authorization - <code>hashlist/</code> - Hashlist management - <code>jobs/</code> - Job execution and monitoring - <code>websocket/</code> - WebSocket connection handling</p>"},{"location":"developer/architecture/#2-service-layer-internalservices","title":"2. Service Layer (<code>internal/services/</code>)","text":"<ul> <li>Business logic implementation</li> <li>Transaction management</li> <li>Cross-cutting concerns (scheduling, monitoring)</li> </ul> <p>Key Services: - <code>AgentService</code> - Agent lifecycle management - <code>JobExecutionService</code> - Job orchestration - <code>JobSchedulingService</code> - Task distribution with modular architecture (v1.1+)   - Modular Components:     - <code>job_scheduling_service.go</code> (600+ lines) - Main scheduling coordinator     - <code>job_scheduling_benchmark_planning.go</code> (684 lines) - Parallel benchmark planning and execution     - <code>job_scheduling_task_assignment.go</code> (710 lines) - Parallel task assignment with two-phase execution   - Performance Characteristics:     - Parallel benchmarking: 96% improvement (15 agents: 450s \u2192 12s)     - Parallel task assignment: 95% improvement (15 agents: 450s \u2192 20s)     - Goroutine-based concurrent operations for scalability   - Key Features:     - Priority-based allocation with overflow control (FIFO/round-robin modes)     - Round-robin benchmark distribution     - Two-phase task planning (sequential planning, parallel execution)     - Forced benchmark agent prioritization - <code>JobProgressCalculationService</code> - Polling-based progress calculation   - Recalculates job progress every 2 seconds from task data   - Ensures accurate processed/dispatched keyspace tracking - <code>ClientService</code> - Customer management - <code>RetentionService</code> - Automated data purging with secure deletion - <code>WebSocketService</code> - Real-time communication hub - <code>HashlistSyncService</code> - File synchronization to agents - <code>MetricsCleanupService</code> - Agent metrics pruning</p>"},{"location":"developer/architecture/#3-repository-layer-internalrepository","title":"3. Repository Layer (<code>internal/repository/</code>)","text":"<ul> <li>Database access abstraction</li> <li>SQL query execution</li> <li>Data mapping</li> </ul> <p>Key Repositories: - <code>UserRepository</code> - User account management - <code>AgentRepository</code> - Agent registration and status - <code>HashlistRepository</code> - Hashlist storage - <code>JobExecutionRepository</code> - Job tracking - <code>JobTaskRepository</code> - Task management</p>"},{"location":"developer/architecture/#4-infrastructure-layer","title":"4. Infrastructure Layer","text":"<ul> <li>Database connections (<code>internal/database/</code>)</li> <li>File storage (<code>internal/binary/</code>, <code>internal/wordlist/</code>, <code>internal/rule/</code>)</li> <li>External integrations (email providers)</li> <li>TLS/SSL management (<code>internal/tls/</code>)</li> </ul>"},{"location":"developer/architecture/#design-patterns","title":"Design Patterns","text":"<ol> <li>Repository Pattern: All database operations go through repository interfaces</li> <li>Service Layer Pattern: Business logic separated from data access</li> <li>Middleware Pattern: Cross-cutting concerns (auth, logging, CORS)</li> <li>Hub Pattern: Central WebSocket hub for agent connections</li> <li>Factory Pattern: TLS provider creation, GPU detector creation</li> </ol>"},{"location":"developer/architecture/#key-backend-features","title":"Key Backend Features","text":"<ul> <li>JWT Authentication: Access/refresh token pattern</li> <li>Multi-Factor Authentication: TOTP, email, backup codes</li> <li>Role-Based Access Control: user, admin, agent, system roles</li> <li>Job Scheduling: Dynamic task distribution with chunking</li> <li>File Synchronization: Agent-backend file sync</li> <li>Monitoring: System metrics and heartbeat management</li> <li>Data Retention: Configurable retention policies</li> <li>Accurate Keyspace Tracking: Captures real keyspace from hashcat <code>progress[1]</code> values and recalculates progress every 2 seconds for precise, self-healing progress reporting</li> </ul>"},{"location":"developer/architecture/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"developer/architecture/#component-structure","title":"Component Structure","text":"<p>The frontend uses React with TypeScript and follows a component-based architecture:</p>"},{"location":"developer/architecture/#1-pages-srcpages","title":"1. Pages (<code>src/pages/</code>)","text":"<ul> <li>Top-level route components</li> <li>Page-specific business logic</li> <li>Component composition</li> </ul> <p>Key Pages: - <code>Dashboard</code> - System overview - <code>AgentManagement</code> - Agent monitoring - <code>Jobs/</code> - Job execution interface - <code>AdminSettings/</code> - System configuration - <code>Login</code> - Authentication</p>"},{"location":"developer/architecture/#2-components-srccomponents","title":"2. Components (<code>src/components/</code>)","text":"<ul> <li>Reusable UI components</li> <li>Domain-specific components</li> <li>Common UI patterns</li> </ul> <p>Component Categories: - <code>admin/</code> - Administrative UI components - <code>agent/</code> - Agent-related components - <code>auth/</code> - Authentication components - <code>common/</code> - Shared components - <code>hashlist/</code> - Hashlist management UI</p>"},{"location":"developer/architecture/#3-services-srcservices","title":"3. Services (<code>src/services/</code>)","text":"<ul> <li>API communication layer</li> <li>HTTP request handling</li> <li>Response transformation</li> </ul> <p>Key Services: - <code>api.ts</code> - Base API configuration - <code>auth.ts</code> - Authentication API - <code>jobSettings.ts</code> - Job configuration - <code>systemSettings.ts</code> - System settings</p>"},{"location":"developer/architecture/#4-state-management","title":"4. State Management","text":"<ul> <li>React Context: Authentication state (<code>AuthContext</code>)</li> <li>React Query: Server state management with caching</li> <li>Local State: Component-specific state with hooks</li> </ul>"},{"location":"developer/architecture/#5-type-system-srctypes","title":"5. Type System (<code>src/types/</code>)","text":"<ul> <li>TypeScript interfaces and types</li> <li>API response types</li> <li>Domain models</li> </ul>"},{"location":"developer/architecture/#frontend-technologies","title":"Frontend Technologies","text":"<ul> <li>React 18: Component framework</li> <li>TypeScript: Type safety</li> <li>Material-UI: Component library</li> <li>React Query: Data fetching and caching</li> <li>React Router: Client-side routing</li> <li>Axios: HTTP client</li> </ul>"},{"location":"developer/architecture/#agent-architecture","title":"Agent Architecture","text":""},{"location":"developer/architecture/#core-modules","title":"Core Modules","text":""},{"location":"developer/architecture/#1-agent-core-internalagent","title":"1. Agent Core (<code>internal/agent/</code>)","text":"<ul> <li>WebSocket connection management</li> <li>Registration with claim codes</li> <li>Heartbeat maintenance</li> <li>Message routing</li> </ul>"},{"location":"developer/architecture/#2-hardware-detection-internalhardware","title":"2. Hardware Detection (<code>internal/hardware/</code>)","text":"<ul> <li>GPU detection (NVIDIA, AMD, Intel)</li> <li>System resource monitoring</li> <li>Hashcat availability checking</li> <li>Device capability reporting</li> </ul> <p>GPU Detectors: - <code>gpu/nvidia.go</code> - NVIDIA GPU detection - <code>gpu/amd.go</code> - AMD GPU detection - <code>gpu/intel.go</code> - Intel GPU detection - <code>gpu/detector.go</code> - Detection orchestration</p>"},{"location":"developer/architecture/#3-job-execution-internaljobs","title":"3. Job Execution (<code>internal/jobs/</code>)","text":"<ul> <li>Hashcat process management</li> <li>Job progress tracking</li> <li>Output parsing</li> <li>Error handling</li> </ul>"},{"location":"developer/architecture/#4-file-synchronization-internalsync","title":"4. File Synchronization (<code>internal/sync/</code>)","text":"<ul> <li>Binary synchronization</li> <li>Wordlist management</li> <li>Rule file handling</li> <li>Hashlist retrieval</li> </ul>"},{"location":"developer/architecture/#5-metrics-collection-internalmetrics","title":"5. Metrics Collection (<code>internal/metrics/</code>)","text":"<ul> <li>System resource monitoring</li> <li>GPU utilization tracking</li> <li>Performance metrics reporting</li> </ul>"},{"location":"developer/architecture/#agent-lifecycle","title":"Agent Lifecycle","text":"<ol> <li>Registration Phase</li> <li>Claim code validation</li> <li>API key generation</li> <li>Certificate exchange</li> <li> <p>Initial synchronization</p> </li> <li> <p>Active Phase</p> </li> <li>Heartbeat maintenance</li> <li>Job reception and execution</li> <li>Progress reporting</li> <li> <p>File synchronization</p> </li> <li> <p>Execution Phase</p> </li> <li>Task assignment reception</li> <li>Hashcat process spawning</li> <li>Progress monitoring</li> <li>Result reporting</li> </ol>"},{"location":"developer/architecture/#communication-protocols","title":"Communication Protocols","text":""},{"location":"developer/architecture/#rest-api","title":"REST API","text":"<p>The system uses RESTful APIs for standard CRUD operations:</p> <p>Endpoint Structure: <pre><code>/api/v1/auth/*         - Authentication endpoints\n/api/v1/admin/*        - Administrative functions\n/api/v1/agents/*       - Agent management\n/api/v1/hashlists/*    - Hashlist operations\n/api/v1/jobs/*         - Job management\n/api/v1/wordlists/*    - Wordlist management\n/api/v1/rules/*        - Rule file management\n</code></pre></p> <p>Authentication: - JWT Bearer tokens - API key authentication (agents) - Refresh token rotation</p>"},{"location":"developer/architecture/#websocket-protocol","title":"WebSocket Protocol","text":"<p>Real-time communication uses WebSocket with JSON message format:</p> <p>Message Structure: <pre><code>{\n  \"type\": \"message_type\",\n  \"payload\": { ... },\n  \"timestamp\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre></p> <p>Agent \u2192 Server Messages: - <code>heartbeat</code> - Keep-alive signal - <code>task_status</code> - Task execution status - <code>job_progress</code> - Job progress updates - <code>benchmark_result</code> - Benchmark results - <code>hardware_info</code> - Hardware capabilities - <code>hashcat_output</code> - Hashcat output streams - <code>device_update</code> - Device status changes</p> <p>Server \u2192 Agent Messages: - <code>task_assignment</code> - New task assignment - <code>job_stop</code> - Stop job execution - <code>benchmark_request</code> - Request benchmark - <code>config_update</code> - Configuration changes - <code>file_sync_request</code> - File sync command - <code>force_cleanup</code> - Force cleanup command</p>"},{"location":"developer/architecture/#file-transfer-protocol","title":"File Transfer Protocol","text":"<p>File synchronization uses HTTP(S) with the following endpoints:</p> <ul> <li><code>GET /api/v1/sync/binaries/:name</code> - Download binaries</li> <li><code>GET /api/v1/sync/wordlists/:id</code> - Download wordlists</li> <li><code>GET /api/v1/sync/rules/:id</code> - Download rules</li> <li><code>GET /api/v1/sync/hashlists/:id</code> - Download hashlists</li> </ul>"},{"location":"developer/architecture/#database-schema","title":"Database Schema","text":""},{"location":"developer/architecture/#core-tables","title":"Core Tables","text":""},{"location":"developer/architecture/#user-management","title":"User Management","text":"<ul> <li><code>users</code> - User accounts with roles and preferences</li> <li><code>auth_tokens</code> - JWT refresh tokens</li> <li><code>mfa_methods</code> - Multi-factor authentication settings</li> <li><code>mfa_backup_codes</code> - MFA recovery codes</li> </ul>"},{"location":"developer/architecture/#agent-management","title":"Agent Management","text":"<ul> <li><code>agents</code> - Registered compute agents</li> <li><code>agent_devices</code> - GPU/compute devices per agent</li> <li><code>agent_schedules</code> - Agent availability schedules</li> <li><code>agent_hashlists</code> - Agent-hashlist assignments</li> </ul>"},{"location":"developer/architecture/#job-management","title":"Job Management","text":"<ul> <li><code>job_workflows</code> - Attack strategy definitions</li> <li><code>preset_jobs</code> - Predefined job templates</li> <li><code>job_executions</code> - Active job instances</li> <li><code>job_tasks</code> - Individual task assignments</li> <li><code>performance_metrics</code> - Task performance data</li> </ul>"},{"location":"developer/architecture/#data-management","title":"Data Management","text":"<ul> <li><code>hashlists</code> - Password hash collections</li> <li><code>hashes</code> - Individual password hashes</li> <li><code>clients</code> - Customer/engagement tracking</li> <li><code>wordlists</code> - Dictionary files</li> <li><code>rules</code> - Rule files for mutations</li> </ul>"},{"location":"developer/architecture/#system-management","title":"System Management","text":"<ul> <li><code>vouchers</code> - Agent registration codes</li> <li><code>binary_versions</code> - Hashcat binary versions</li> <li><code>system_settings</code> - Global configuration</li> <li><code>client_settings</code> - Per-client settings</li> </ul>"},{"location":"developer/architecture/#key-relationships","title":"Key Relationships","text":"<pre><code>users \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 agents (owner_id)\n               \u251c\u2500\u2500\u2500\u2500 hashlists (created_by)\n               \u2514\u2500\u2500\u2500\u2500 job_executions (created_by)\n\nagents \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 agent_devices\n               \u251c\u2500\u2500\u2500\u2500 agent_schedules\n               \u2514\u2500\u2500\u2500\u2500 job_tasks\n\nhashlists \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 hashes\n               \u251c\u2500\u2500\u2500\u2500 job_executions\n               \u2514\u2500\u2500\u2500\u2500 clients\n\njob_workflows \u2500\u2500\u252c\u2500\u2500\u2500 preset_jobs\n                \u2514\u2500\u2500\u2500 job_executions \u2500\u2500\u2500\u2500 job_tasks\n</code></pre>"},{"location":"developer/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"developer/architecture/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"developer/architecture/#multi-layer-authentication","title":"Multi-Layer Authentication","text":"<ol> <li>User Authentication</li> <li>Username/password with bcrypt hashing</li> <li>JWT access/refresh token pattern</li> <li>Session management with token rotation</li> <li> <p>Session-token binding with CASCADE delete for security</p> </li> <li> <p>Multi-Factor Authentication</p> </li> <li>TOTP (Time-based One-Time Passwords)</li> <li>Email-based verification</li> <li>Backup codes for recovery</li> <li> <p>Configurable MFA policies</p> </li> <li> <p>Agent Authentication</p> </li> <li>Claim code registration</li> <li>API key authentication</li> <li>Certificate-based trust</li> </ol>"},{"location":"developer/architecture/#session-security-architecture","title":"Session Security Architecture","text":"<p>KrakenHashes implements a database-backed session-token relationship to ensure true session termination:</p> <ul> <li>Foreign Key Binding: <code>active_sessions.token_id</code> \u2192 <code>tokens.id</code> with ON DELETE CASCADE</li> <li>Atomic Revocation: Deleting a token automatically removes associated sessions</li> <li>True Logout: Session termination immediately invalidates JWT authentication</li> <li>Security Enforcement: Auth middleware validates both token existence and session validity</li> <li>No Orphaned Tokens: Database constraints prevent tokens from persisting after logout</li> </ul> <p>This architecture prevents a critical vulnerability where session termination would only remove UI state but leave JWT tokens active in the database.</p>"},{"location":"developer/architecture/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Roles: - <code>user</code> - Standard user access - <code>admin</code> - Administrative privileges - <code>agent</code> - Agent-specific operations - <code>system</code> - System-level operations</p> <p>Middleware Chain: <pre><code>AuthMiddleware \u2192 RoleMiddleware \u2192 ResourceMiddleware \u2192 Handler\n</code></pre></p>"},{"location":"developer/architecture/#transport-security","title":"Transport Security","text":""},{"location":"developer/architecture/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<p>Supported Modes: 1. Self-Signed Certificates    - Automatic generation with CA    - Configurable validity periods    - SAN extension support</p> <ol> <li>Provided Certificates</li> <li>Custom certificate installation</li> <li> <p>Certificate chain validation</p> </li> <li> <p>Let's Encrypt (Certbot)</p> </li> <li>Automatic certificate renewal</li> <li>ACME protocol support</li> </ol> <p>Certificate Features: - RSA 2048/4096 bit keys - Multiple DNS names and IP addresses - Proper certificate chain delivery - Browser-compatible extensions</p>"},{"location":"developer/architecture/#data-security","title":"Data Security","text":"<ol> <li>Password Storage</li> <li>bcrypt with configurable cost factor</li> <li> <p>No plaintext storage</p> </li> <li> <p>Token Security</p> </li> <li>Short-lived access tokens (15 minutes)</li> <li>Refresh token rotation</li> <li> <p>Secure token storage</p> </li> <li> <p>File Access Control</p> </li> <li>Path sanitization</li> <li>Directory restrictions</li> <li> <p>User-based access control</p> </li> <li> <p>API Security</p> </li> <li>Rate limiting</li> <li>Request validation</li> <li>CORS configuration</li> </ol>"},{"location":"developer/architecture/#file-storage-architecture","title":"File Storage Architecture","text":""},{"location":"developer/architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/         # Hashcat binaries\n\u2502   \u251c\u2500\u2500 hashcat-linux-x64/\n\u2502   \u251c\u2500\u2500 hashcat-windows-x64/\n\u2502   \u2514\u2500\u2500 hashcat-darwin-x64/\n\u251c\u2500\u2500 wordlists/        # Dictionary files\n\u2502   \u251c\u2500\u2500 general/      # Common wordlists\n\u2502   \u251c\u2500\u2500 specialized/  # Domain-specific\n\u2502   \u251c\u2500\u2500 targeted/     # Custom lists\n\u2502   \u2514\u2500\u2500 custom/       # User uploads\n\u251c\u2500\u2500 rules/            # Mutation rules\n\u2502   \u251c\u2500\u2500 hashcat/      # Hashcat rules\n\u2502   \u251c\u2500\u2500 john/         # John rules\n\u2502   \u2514\u2500\u2500 custom/       # Custom rules\n\u2514\u2500\u2500 hashlists/        # Hash files\n    \u2514\u2500\u2500 {client_id}/  # Per-client storage\n</code></pre>"},{"location":"developer/architecture/#storage-management","title":"Storage Management","text":"<ul> <li>Upload Processing: Files are uploaded to temporary storage, processed, then moved to permanent locations</li> <li>Deduplication: Files are tracked by MD5 hash to prevent duplicates</li> <li>Synchronization: Agent sync service ensures agents have required files</li> <li>Cleanup: Automated retention policies remove expired data</li> </ul>"},{"location":"developer/architecture/#data-lifecycle-management","title":"Data Lifecycle Management","text":""},{"location":"developer/architecture/#retention-system","title":"Retention System","text":"<p>The system implements comprehensive data lifecycle management with automated retention policies:</p>"},{"location":"developer/architecture/#backend-retention-service","title":"Backend Retention Service","text":"<ul> <li>Automatic Purging: Runs daily at midnight and on startup</li> <li>Client-Specific Policies: Each client can have custom retention periods</li> <li>Secure Deletion Process:</li> <li>Transaction-based database cleanup</li> <li>Secure file overwriting with random data</li> <li>PostgreSQL VACUUM to prevent recovery</li> <li>Comprehensive audit logging</li> </ul>"},{"location":"developer/architecture/#agent-cleanup-service","title":"Agent Cleanup Service","text":"<ul> <li>3-Day Retention: Temporary files removed after 3 days</li> <li>Automatic Cleanup: Runs every 6 hours</li> <li>File Types Managed:</li> <li>Hashlist files (after inactivity)</li> <li>Rule chunks (temporary segments)</li> <li>Chunk ID tracking files</li> <li>Preserved Files: Base rules, wordlists, and binaries</li> </ul>"},{"location":"developer/architecture/#potfile-exclusion","title":"Potfile Exclusion","text":"<p>Important</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) containing plaintext passwords is NOT managed by the retention system. It requires separate manual management for compliance with data protection regulations.</p>"},{"location":"developer/architecture/#data-security_1","title":"Data Security","text":""},{"location":"developer/architecture/#secure-deletion","title":"Secure Deletion","text":"<ul> <li>Files overwritten with random data before removal</li> <li>VACUUM ANALYZE on PostgreSQL tables</li> <li>Prevention of WAL (Write-Ahead Log) recovery</li> <li>Transaction safety for atomic operations</li> </ul>"},{"location":"developer/architecture/#audit-trail","title":"Audit Trail","text":"<ul> <li>All deletion operations logged</li> <li>Retention compliance tracking</li> <li> <p>Last purge timestamp recording</p> </li> <li> <p>File Organization</p> </li> <li>Client-based isolation</li> <li>Category-based grouping</li> <li> <p>Version tracking</p> </li> <li> <p>Synchronization</p> </li> <li>Delta-based updates</li> <li>Checksum verification</li> <li> <p>Compression support</p> </li> <li> <p>Retention Policies</p> </li> <li>Configurable retention periods</li> <li>Automatic cleanup</li> <li>Archive support</li> </ul>"},{"location":"developer/architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"developer/architecture/#docker-based-deployment","title":"Docker-Based Deployment","text":"<pre><code>Services:\n- backend    # Go backend server\n- postgres   # PostgreSQL database\n- app        # Nginx + React frontend\n\nNetworks:\n- krakenhashes_default # Internal network\n\nVolumes:\n- postgres_data     # Database persistence\n- kh_config        # Configuration files\n- kh_data          # Application data\n- kh_logs          # Log files\n</code></pre>"},{"location":"developer/architecture/#production-considerations","title":"Production Considerations","text":"<ol> <li>Scalability</li> <li>Horizontal agent scaling</li> <li>Database connection pooling</li> <li> <p>Load balancer ready</p> </li> <li> <p>Monitoring</p> </li> <li>Health check endpoints</li> <li>Metrics collection</li> <li> <p>Log aggregation</p> </li> <li> <p>Backup &amp; Recovery</p> </li> <li>Database backups</li> <li>File system snapshots</li> <li> <p>Configuration backup</p> </li> <li> <p>High Availability</p> </li> <li>Database replication support</li> <li>Stateless backend design</li> <li>Agent failover handling</li> </ol>"},{"location":"developer/architecture/#environment-configuration","title":"Environment Configuration","text":"<p>Key Environment Variables: <pre><code># Database\nDB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME\n\n# Security\nJWT_SECRET, JWT_REFRESH_SECRET\n\n# TLS/SSL\nKH_TLS_MODE, KH_CERT_KEY_SIZE\n\n# Directories\nKH_CONFIG_DIR, KH_DATA_DIR\n\n# Ports\nKH_HTTP_PORT, KH_HTTPS_PORT\n</code></pre></p>"},{"location":"developer/architecture/#conclusion","title":"Conclusion","text":"<p>KrakenHashes implements a robust distributed architecture designed for scalability, security, and maintainability. The system's modular design allows for independent scaling of components while maintaining clear separation of concerns throughout the stack.</p>"},{"location":"developer/backend/","title":"Backend Development Guide","text":"<p>This guide covers the KrakenHashes backend development, including environment setup, architecture, coding patterns, and common development tasks.</p>"},{"location":"developer/backend/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Development Environment Setup</li> <li>Code Structure and Architecture</li> <li>Core Conventions and Patterns</li> <li>Adding New Endpoints</li> <li>Database Operations</li> <li>Authentication and Authorization</li> <li>WebSocket Development</li> <li>Testing Strategies</li> <li>Common Patterns and Utilities</li> <li>Debugging and Logging</li> </ol>"},{"location":"developer/backend/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"developer/backend/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose (primary development method)</li> <li>Go 1.21+ (for IDE support and running tests locally)</li> <li>PostgreSQL client tools (optional, for database inspection)</li> <li>Make (for running build commands)</li> </ul>"},{"location":"developer/backend/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone &lt;repository-url&gt;\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Set up environment variables <pre><code># Copy the example environment file\ncp .env.example .env\n\n# Edit .env with your configuration\n# Required variables:\nDB_HOST=postgres\nDB_PORT=5432\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\nJWT_SECRET=your-jwt-secret\nKH_TLS_MODE=self-signed\n</code></pre></p> </li> <li> <p>Start the development environment <pre><code># Build and start all services\ndocker-compose down &amp;&amp; docker-compose up -d --build\n\n# View logs\ndocker-compose logs -f backend\n</code></pre></p> </li> <li> <p>Verify the setup <pre><code># Check backend health\ncurl -k https://localhost:8443/api/status\n\n# Check database migrations\ndocker-compose exec backend ls -la /app/db/migrations\n</code></pre></p> </li> </ol>"},{"location":"developer/backend/#docker-development-workflow","title":"Docker Development Workflow","text":"<p>Important: Always use Docker for building and testing. Never use <code>go build</code> directly as it creates binaries in the project directory.</p> <pre><code># Rebuild backend only\ndocker-compose up -d --build backend\n\n# Run database migrations\ncd backend &amp;&amp; make migrate-up\n\n# View structured logs\ndocker-compose logs backend | grep -E \"ERROR|WARNING|INFO\"\n\n# Access backend container\ndocker-compose exec backend sh\n</code></pre>"},{"location":"developer/backend/#code-structure-and-architecture","title":"Code Structure and Architecture","text":"<p>The backend follows a layered architecture with clear separation of concerns:</p> <pre><code>backend/\n\u251c\u2500\u2500 cmd/\n\u2502   \u251c\u2500\u2500 server/          # Main application entry point\n\u2502   \u2514\u2500\u2500 migrate/         # Database migration tool\n\u251c\u2500\u2500 internal/            # Private application code\n\u2502   \u251c\u2500\u2500 config/          # Configuration management\n\u2502   \u251c\u2500\u2500 db/              # Database wrapper and utilities\n\u2502   \u251c\u2500\u2500 handlers/        # HTTP request handlers (controllers)\n\u2502   \u251c\u2500\u2500 middleware/      # HTTP middleware\n\u2502   \u251c\u2500\u2500 models/          # Domain models and types\n\u2502   \u251c\u2500\u2500 repository/      # Data access layer\n\u2502   \u251c\u2500\u2500 services/        # Business logic layer\n\u2502   \u251c\u2500\u2500 websocket/       # WebSocket handlers\n\u2502   \u2514\u2500\u2500 routes/          # Route configuration\n\u251c\u2500\u2500 pkg/                 # Public packages\n\u2502   \u251c\u2500\u2500 debug/           # Debug logging utilities\n\u2502   \u251c\u2500\u2500 jwt/             # JWT token handling\n\u2502   \u2514\u2500\u2500 httputil/        # HTTP utilities\n\u2514\u2500\u2500 db/\n    \u2514\u2500\u2500 migrations/      # SQL migration files\n</code></pre>"},{"location":"developer/backend/#key-architecture-patterns","title":"Key Architecture Patterns","text":"<ol> <li>Repository Pattern: All database access through repositories</li> <li>Service Layer: Business logic separated from handlers</li> <li>Dependency Injection: Dependencies passed through constructors</li> <li>Middleware Chain: Composable middleware for cross-cutting concerns</li> <li>Context Propagation: Request context flows through all layers</li> </ol>"},{"location":"developer/backend/#core-conventions-and-patterns","title":"Core Conventions and Patterns","text":""},{"location":"developer/backend/#database-access-pattern","title":"Database Access Pattern","text":"<p>The backend uses a custom DB wrapper instead of sqlx directly:</p> <pre><code>// internal/db/db.go\ntype DB struct {\n    *sql.DB\n}\n\n// Repository pattern\ntype UserRepository struct {\n    db *db.DB\n}\n\nfunc NewUserRepository(db *db.DB) *UserRepository {\n    return &amp;UserRepository{db: db}\n}\n\n// Use standard database/sql methods\nfunc (r *UserRepository) GetByID(ctx context.Context, id uuid.UUID) (*models.User, error) {\n    user := &amp;models.User{}\n    err := r.db.QueryRowContext(ctx, queries.GetUserByID, id).Scan(\n        &amp;user.ID,\n        &amp;user.Username,\n        // ... other fields\n    )\n    if err == sql.ErrNoRows {\n        return nil, fmt.Errorf(\"user not found: %s\", id)\n    }\n    return user, err\n}\n</code></pre>"},{"location":"developer/backend/#service-layer-pattern","title":"Service Layer Pattern","text":"<p>Services contain business logic and orchestrate multiple repositories:</p> <pre><code>// internal/services/client/client_service.go\ntype ClientService struct {\n    clientRepo         *repository.ClientRepository\n    hashlistRepo       *repository.HashListRepository\n    clientSettingsRepo *repository.ClientSettingsRepository\n    retentionService   *retention.RetentionService\n}\n\nfunc (s *ClientService) DeleteClient(ctx context.Context, clientID uuid.UUID) error {\n    // Begin transaction\n    tx, err := s.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to start transaction: %w\", err)\n    }\n    defer tx.Rollback()\n\n    // Business logic here...\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#error-handling","title":"Error Handling","text":"<p>Use wrapped errors for better error tracking:</p> <pre><code>// Wrap errors with context\nif err != nil {\n    return fmt.Errorf(\"failed to create user: %w\", err)\n}\n\n// Custom error types\nvar (\n    ErrNotFound = errors.New(\"resource not found\")\n    ErrUnauthorized = errors.New(\"unauthorized\")\n)\n\n// Check error types\nif errors.Is(err, repository.ErrNotFound) {\n    http.Error(w, \"Not found\", http.StatusNotFound)\n    return\n}\n</code></pre>"},{"location":"developer/backend/#adding-new-endpoints","title":"Adding New Endpoints","text":""},{"location":"developer/backend/#step-1-define-the-model","title":"Step 1: Define the Model","text":"<pre><code>// internal/models/example.go\npackage models\n\nimport (\n    \"time\"\n    \"github.com/google/uuid\"\n)\n\ntype Example struct {\n    ID          uuid.UUID  `json:\"id\"`\n    Name        string     `json:\"name\"`\n    Description string     `json:\"description\"`\n    CreatedAt   time.Time  `json:\"created_at\"`\n    UpdatedAt   time.Time  `json:\"updated_at\"`\n}\n</code></pre> <p>Important Pattern - Nullable String Fields:</p> <p>For fields that may be null (e.g., passwords in hash records), use <code>*string</code> instead of <code>string</code>:</p> <pre><code>// internal/models/hashlist.go\ntype Hash struct {\n    ID           uuid.UUID  `json:\"id\"`\n    OriginalHash string     `json:\"original_hash\"`\n    Username     *string    `json:\"username,omitempty\"`    // Nullable\n    Domain       *string    `json:\"domain,omitempty\"`      // Nullable\n    HashTypeID   int        `json:\"hash_type_id\"`\n    IsCracked    bool       `json:\"is_cracked\"`\n    Password     *string    `json:\"password,omitempty\"`    // Nullable - only present when cracked\n    LastUpdated  time.Time  `json:\"last_updated\"`\n}\n</code></pre> <p>Why Use Pointers for Nullable Fields: - Distinguishes between empty string (<code>\"\"</code>) and no value (<code>nil</code>) - Matches SQL NULL semantics in the database - Prevents invalid empty string values in uncracked hashes - JSON marshaling with <code>omitempty</code> excludes <code>nil</code> fields from output</p> <p>When to Use <code>*string</code>: - Optional fields that may not be set (username, domain) - Fields only present under certain conditions (password only when cracked) - Fields that should be omitted from JSON when null</p> <p>When to Use <code>string</code>: - Required fields that always have a value - Fields where empty string is a valid value - Primary identifiers or keys</p>"},{"location":"developer/backend/#step-2-create-the-repository","title":"Step 2: Create the Repository","text":"<pre><code>// internal/repository/example_repository.go\npackage repository\n\ntype ExampleRepository struct {\n    db *db.DB\n}\n\nfunc NewExampleRepository(db *db.DB) *ExampleRepository {\n    return &amp;ExampleRepository{db: db}\n}\n\nfunc (r *ExampleRepository) Create(ctx context.Context, example *models.Example) error {\n    query := `\n        INSERT INTO examples (id, name, description, created_at, updated_at)\n        VALUES ($1, $2, $3, $4, $5)\n    `\n    _, err := r.db.ExecContext(ctx, query,\n        example.ID,\n        example.Name,\n        example.Description,\n        example.CreatedAt,\n        example.UpdatedAt,\n    )\n    return err\n}\n\nfunc (r *ExampleRepository) GetByID(ctx context.Context, id uuid.UUID) (*models.Example, error) {\n    example := &amp;models.Example{}\n    query := `SELECT id, name, description, created_at, updated_at FROM examples WHERE id = $1`\n\n    err := r.db.QueryRowContext(ctx, query, id).Scan(\n        &amp;example.ID,\n        &amp;example.Name,\n        &amp;example.Description,\n        &amp;example.CreatedAt,\n        &amp;example.UpdatedAt,\n    )\n\n    if err == sql.ErrNoRows {\n        return nil, ErrNotFound\n    }\n\n    return example, err\n}\n</code></pre>"},{"location":"developer/backend/#step-3-create-the-service-if-needed","title":"Step 3: Create the Service (if needed)","text":"<pre><code>// internal/services/example_service.go\npackage services\n\ntype ExampleService struct {\n    repo *repository.ExampleRepository\n}\n\nfunc NewExampleService(repo *repository.ExampleRepository) *ExampleService {\n    return &amp;ExampleService{repo: repo}\n}\n\nfunc (s *ExampleService) CreateExample(ctx context.Context, name, description string) (*models.Example, error) {\n    example := &amp;models.Example{\n        ID:          uuid.New(),\n        Name:        name,\n        Description: description,\n        CreatedAt:   time.Now(),\n        UpdatedAt:   time.Now(),\n    }\n\n    if err := s.repo.Create(ctx, example); err != nil {\n        return nil, fmt.Errorf(\"failed to create example: %w\", err)\n    }\n\n    return example, nil\n}\n</code></pre>"},{"location":"developer/backend/#step-4-create-the-handler","title":"Step 4: Create the Handler","text":"<pre><code>// internal/handlers/example/handler.go\npackage example\n\ntype Handler struct {\n    service *services.ExampleService\n}\n\nfunc NewHandler(service *services.ExampleService) *Handler {\n    return &amp;Handler{service: service}\n}\n\nfunc (h *Handler) Create(w http.ResponseWriter, r *http.Request) {\n    var req struct {\n        Name        string `json:\"name\"`\n        Description string `json:\"description\"`\n    }\n\n    if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {\n        http.Error(w, \"Invalid request\", http.StatusBadRequest)\n        return\n    }\n\n    // Get user ID from context (set by auth middleware)\n    userID := r.Context().Value(\"user_id\").(uuid.UUID)\n\n    example, err := h.service.CreateExample(r.Context(), req.Name, req.Description)\n    if err != nil {\n        debug.Error(\"Failed to create example: %v\", err)\n        http.Error(w, \"Internal server error\", http.StatusInternalServerError)\n        return\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(example)\n}\n\nfunc (h *Handler) GetByID(w http.ResponseWriter, r *http.Request) {\n    vars := mux.Vars(r)\n    id, err := uuid.Parse(vars[\"id\"])\n    if err != nil {\n        http.Error(w, \"Invalid ID\", http.StatusBadRequest)\n        return\n    }\n\n    example, err := h.service.repo.GetByID(r.Context(), id)\n    if err != nil {\n        if errors.Is(err, repository.ErrNotFound) {\n            http.Error(w, \"Not found\", http.StatusNotFound)\n            return\n        }\n        debug.Error(\"Failed to get example: %v\", err)\n        http.Error(w, \"Internal server error\", http.StatusInternalServerError)\n        return\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(example)\n}\n</code></pre>"},{"location":"developer/backend/#step-5-register-routes","title":"Step 5: Register Routes","text":"<pre><code>// internal/routes/routes.go\n// In SetupRoutes function:\n\n// Initialize repository and service\nexampleRepo := repository.NewExampleRepository(database)\nexampleService := services.NewExampleService(exampleRepo)\nexampleHandler := example.NewHandler(exampleService)\n\n// Register routes with authentication\njwtRouter.HandleFunc(\"/examples\", exampleHandler.Create).Methods(\"POST\")\njwtRouter.HandleFunc(\"/examples/{id}\", exampleHandler.GetByID).Methods(\"GET\")\n</code></pre>"},{"location":"developer/backend/#database-operations","title":"Database Operations","text":""},{"location":"developer/backend/#creating-migrations","title":"Creating Migrations","text":"<pre><code># Create a new migration\nmake migration name=add_example_table\n\n# This creates two files:\n# - db/migrations/XXXXXX_add_example_table.up.sql\n# - db/migrations/XXXXXX_add_example_table.down.sql\n</code></pre> <p>Example migration:</p> <pre><code>-- XXXXXX_add_example_table.up.sql\nCREATE TABLE IF NOT EXISTS examples (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    created_by UUID REFERENCES users(id) ON DELETE SET NULL\n);\n\nCREATE INDEX idx_examples_created_by ON examples(created_by);\n\n-- Add trigger for updated_at\nCREATE TRIGGER update_examples_updated_at BEFORE UPDATE ON examples\nFOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n\n-- XXXXXX_add_example_table.down.sql\nDROP TRIGGER IF EXISTS update_examples_updated_at ON examples;\nDROP TABLE IF EXISTS examples;\n</code></pre>"},{"location":"developer/backend/#transaction-management","title":"Transaction Management","text":"<pre><code>// Use transactions for complex operations\nfunc (s *Service) ComplexOperation(ctx context.Context) error {\n    tx, err := s.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to begin transaction: %w\", err)\n    }\n    defer func() {\n        if err != nil {\n            if rbErr := tx.Rollback(); rbErr != nil {\n                debug.Error(\"Failed to rollback: %v\", rbErr)\n            }\n        }\n    }()\n\n    // Perform operations using tx\n    if err = s.repo.CreateWithTx(tx, data); err != nil {\n        return err\n    }\n\n    if err = s.repo.UpdateWithTx(tx, id, updates); err != nil {\n        return err\n    }\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#query-patterns","title":"Query Patterns","text":"<pre><code>// Parameterized queries (always use placeholders)\nquery := `\n    SELECT h.id, h.hash_value, h.is_cracked, h.plain_text\n    FROM hashes h\n    WHERE h.hashlist_id = $1\n    AND h.created_at &gt; $2\n    ORDER BY h.created_at DESC\n    LIMIT $3\n`\n\nrows, err := db.QueryContext(ctx, query, hashlistID, since, limit)\nif err != nil {\n    return nil, fmt.Errorf(\"failed to query hashes: %w\", err)\n}\ndefer rows.Close()\n\nvar hashes []models.Hash\nfor rows.Next() {\n    var hash models.Hash\n    err := rows.Scan(&amp;hash.ID, &amp;hash.HashValue, &amp;hash.IsCracked, &amp;hash.PlainText)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to scan hash: %w\", err)\n    }\n    hashes = append(hashes, hash)\n}\n\nif err = rows.Err(); err != nil {\n    return nil, fmt.Errorf(\"error iterating hash rows: %w\", err)\n}\n</code></pre>"},{"location":"developer/backend/#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"developer/backend/#jwt-authentication-flow","title":"JWT Authentication Flow","text":"<ol> <li>Login: User provides credentials \u2192 Validate \u2192 Generate JWT \u2192 Set cookie</li> <li>Request: Extract token from cookie \u2192 Validate JWT \u2192 Check database \u2192 Add to context</li> <li>Logout: Remove token from database \u2192 Clear cookie</li> </ol>"},{"location":"developer/backend/#middleware-stack","title":"Middleware Stack","text":"<pre><code>// internal/middleware/auth.go\nfunc RequireAuth(database *db.DB) func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            // Skip for OPTIONS requests\n            if r.Method == \"OPTIONS\" {\n                next.ServeHTTP(w, r)\n                return\n            }\n\n            // Get token from cookie\n            cookie, err := r.Cookie(\"token\")\n            if err != nil {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Validate token\n            userID, err := jwt.ValidateJWT(cookie.Value)\n            if err != nil {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Verify token exists in database\n            exists, err := database.TokenExists(cookie.Value)\n            if !exists {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Add to context\n            ctx := context.WithValue(r.Context(), \"user_id\", userID)\n            ctx = context.WithValue(ctx, \"user_role\", role)\n            r = r.WithContext(ctx)\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n</code></pre>"},{"location":"developer/backend/#role-based-access-control","title":"Role-Based Access Control","text":"<pre><code>// internal/middleware/admin.go\nfunc RequireAdmin() func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            role := r.Context().Value(\"user_role\").(string)\n\n            if role != \"admin\" {\n                http.Error(w, \"Forbidden\", http.StatusForbidden)\n                return\n            }\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n\n// Usage in routes\nadminRouter := jwtRouter.PathPrefix(\"/admin\").Subrouter()\nadminRouter.Use(middleware.RequireAdmin())\n</code></pre>"},{"location":"developer/backend/#api-key-authentication-agents","title":"API Key Authentication (Agents)","text":"<pre><code>// internal/handlers/auth/api/middleware.go\nfunc RequireAPIKey(agentService *services.AgentService) func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            apiKey := r.Header.Get(\"X-API-Key\")\n            agentIDStr := r.Header.Get(\"X-Agent-ID\")\n\n            if apiKey == \"\" || agentIDStr == \"\" {\n                http.Error(w, \"API Key and Agent ID required\", http.StatusUnauthorized)\n                return\n            }\n\n            agent, err := agentService.GetByAPIKey(r.Context(), apiKey)\n            if err != nil {\n                http.Error(w, \"Invalid API Key\", http.StatusUnauthorized)\n                return\n            }\n\n            ctx := context.WithValue(r.Context(), \"agent_id\", agent.ID)\n            r = r.WithContext(ctx)\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n</code></pre>"},{"location":"developer/backend/#websocket-development","title":"WebSocket Development","text":""},{"location":"developer/backend/#websocket-handler-pattern","title":"WebSocket Handler Pattern","text":"<pre><code>// internal/websocket/agent_updates.go\ntype AgentUpdateHandler struct {\n    db           *db.DB\n    agentService *services.AgentService\n    upgrader     websocket.Upgrader\n}\n\nfunc (h *AgentUpdateHandler) HandleUpdates(w http.ResponseWriter, r *http.Request) {\n    // Authenticate before upgrading\n    apiKey := r.Header.Get(\"X-API-Key\")\n    agent, err := h.agentService.GetByAPIKey(r.Context(), apiKey)\n    if err != nil {\n        http.Error(w, \"Invalid API Key\", http.StatusUnauthorized)\n        return\n    }\n\n    // Upgrade connection\n    conn, err := h.upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        debug.Error(\"Failed to upgrade connection: %v\", err)\n        return\n    }\n    defer conn.Close()\n\n    // Configure connection\n    conn.SetReadLimit(maxMessageSize)\n    conn.SetReadDeadline(time.Now().Add(pongWait))\n    conn.SetPongHandler(func(string) error {\n        conn.SetReadDeadline(time.Now().Add(pongWait))\n        return nil\n    })\n\n    // Start ping ticker\n    ticker := time.NewTicker(pingPeriod)\n    defer ticker.Stop()\n\n    // Message handling loop\n    for {\n        messageType, message, err := conn.ReadMessage()\n        if err != nil {\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway) {\n                debug.Error(\"WebSocket error: %v\", err)\n            }\n            break\n        }\n\n        // Process message\n        if err := h.processMessage(agent.ID, message); err != nil {\n            debug.Error(\"Failed to process message: %v\", err)\n        }\n    }\n}\n</code></pre>"},{"location":"developer/backend/#message-processing-with-transactions","title":"Message Processing with Transactions","text":"<pre><code>func (h *AgentUpdateHandler) processCrackUpdate(ctx context.Context, agentID int, msg CrackUpdateMessage) error {\n    tx, err := h.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to start transaction: %w\", err)\n    }\n    defer func() {\n        if err != nil {\n            tx.Rollback()\n        }\n    }()\n\n    // Update hash status\n    err = h.hashRepo.UpdateCrackStatus(tx, msg.HashID, msg.Password)\n    if err != nil {\n        return err\n    }\n\n    // Update hashlist count\n    err = h.hashlistRepo.IncrementCrackedCountTx(tx, msg.HashlistID, 1)\n    if err != nil {\n        return err\n    }\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#websocket-message-types","title":"WebSocket Message Types","text":"<p>KrakenHashes uses JSON-based WebSocket messages with a <code>type</code> field for routing. Below are the key message types used in agent-backend communication:</p>"},{"location":"developer/backend/#agent-backend-messages","title":"Agent \u2192 Backend Messages","text":"<p>job_progress: Progress updates during task execution <pre><code>type JobProgress struct {\n    TaskID            uuid.UUID `json:\"task_id\"`\n    KeyspaceProcessed int64     `json:\"keyspace_processed\"`\n    ProgressPercent   float64   `json:\"progress_percent\"`\n    HashRate          int64     `json:\"hash_rate\"`\n    CrackedCount      int       `json:\"cracked_count\"`        // Expected cracks when Status=\"completed\"\n    Status            string    `json:\"status\"`               // \"running\" or \"completed\"\n    AllHashesCracked  bool      `json:\"all_hashes_cracked\"`   // Hashcat status code 6\n    DeviceMetrics     []Device  `json:\"device_metrics\"`\n}\n</code></pre></p> <p>crack_batch: Batched cracked passwords <pre><code>type CrackBatch struct {\n    TaskID        uuid.UUID      `json:\"task_id\"`\n    CrackedHashes []CrackedHash  `json:\"cracked_hashes\"`\n}\n\ntype CrackedHash struct {\n    Hash         string `json:\"hash\"`\n    Plaintext    string `json:\"plaintext\"`\n    OriginalLine string `json:\"original_line\"`\n}\n</code></pre></p> <p>crack_batches_complete: Signals all crack batches sent (NEW in v1.3.1) <pre><code>type CrackBatchesComplete struct {\n    TaskID uuid.UUID `json:\"task_id\"`\n}\n</code></pre> Agent sends this after sending all crack batches for a task. Backend uses this to transition task from <code>processing</code> to <code>completed</code> status.</p> <p>heartbeat: Agent status update <pre><code>type Heartbeat struct {\n    AgentID   int       `json:\"agent_id\"`\n    Timestamp time.Time `json:\"timestamp\"`\n    Status    string    `json:\"status\"`\n}\n</code></pre></p>"},{"location":"developer/backend/#backend-agent-messages","title":"Backend \u2192 Agent Messages","text":"<p>task_assignment: Assign new task to agent <pre><code>type TaskAssignment struct {\n    TaskID      uuid.UUID `json:\"task_id\"`\n    JobID       uuid.UUID `json:\"job_id\"`\n    AttackMode  int       `json:\"attack_mode\"`\n    HashFile    string    `json:\"hash_file\"`\n    Wordlists   []string  `json:\"wordlists\"`\n    Rules       []string  `json:\"rules\"`\n    // ... additional fields\n}\n</code></pre></p> <p>stop_task: Stop running task <pre><code>type StopTask struct {\n    TaskID uuid.UUID `json:\"task_id\"`\n    Reason string    `json:\"reason\"`\n}\n</code></pre></p>"},{"location":"developer/backend/#processing-status-workflow","title":"Processing Status Workflow","text":"<p>The processing status system prevents premature job completion:</p> <ol> <li>Agent sends final progress: <code>job_progress</code> with <code>Status=\"completed\"</code> and <code>CrackedCount</code> field</li> <li>Backend transitions to processing: Task status \u2192 <code>processing</code>, sets <code>expected_crack_count</code></li> <li>Agent sends crack batches: One or more <code>crack_batch</code> messages</li> <li>Agent signals completion: <code>crack_batches_complete</code> message</li> <li>Backend completes task: Checks <code>received_crack_count &gt;= expected_crack_count AND batches_complete_signaled</code></li> </ol> <p>New Repository Methods:</p> <pre><code>// JobTaskRepository\nSetTaskProcessing(ctx, taskID, expectedCracks)    // Transition to processing\nIncrementReceivedCrackCount(ctx, taskID, count)   // Track received batches\nMarkBatchesComplete(ctx, taskID)                  // Signal batches done\nCheckTaskReadyToComplete(ctx, taskID)             // Verify completion conditions\nGetTotalCracksForJob(ctx, jobExecutionID)         // Sum cracks across all tasks\n\n// JobExecutionRepository\nSetJobProcessing(ctx, jobExecutionID)             // Transition job to processing\nUpdateEmailStatus(ctx, jobID, sent, sentAt, err)  // Track completion email delivery\n</code></pre> <p>Handler Implementation:</p> <pre><code>// backend/internal/integration/job_websocket_integration.go\nfunc (s *JobWebSocketIntegration) HandleCrackBatchesComplete(\n    ctx context.Context,\n    agentID int,\n    message *models.CrackBatchesComplete,\n) error {\n    // Verify task ownership\n    task, err := s.jobTaskRepo.GetByID(ctx, message.TaskID)\n    if task.AgentID == nil || *task.AgentID != agentID {\n        return fmt.Errorf(\"task not assigned to this agent\")\n    }\n\n    // Mark batches complete\n    if err := s.jobTaskRepo.MarkBatchesComplete(ctx, message.TaskID); err != nil {\n        return err\n    }\n\n    // Check if ready to complete\n    ready, err := s.jobTaskRepo.CheckTaskReadyToComplete(ctx, message.TaskID)\n    if err != nil {\n        return err\n    }\n\n    if ready {\n        // Complete the task\n        if err := s.jobTaskRepo.CompleteTask(ctx, message.TaskID); err != nil {\n            return err\n        }\n\n        // Clear agent busy status\n        s.clearAgentBusyStatus(ctx, agentID, message.TaskID)\n\n        // Check if job can complete\n        s.checkJobCompletion(ctx, task.JobExecutionID)\n    }\n\n    return nil\n}\n</code></pre> <p>See Job Completion System and Crack Batching System for full details.</p>"},{"location":"developer/backend/#testing-strategies","title":"Testing Strategies","text":""},{"location":"developer/backend/#unit-testing","title":"Unit Testing","text":"<pre><code>// internal/handlers/auth/handler_test.go\nfunc TestLoginHandler(t *testing.T) {\n    // Setup\n    testutil.SetTestJWTSecret(t)\n    db := testutil.SetupTestDB(t)\n    emailService := testutil.NewMockEmailService()\n    handler := NewHandler(db, emailService)\n\n    // Create test user\n    testUser := testutil.CreateTestUser(t, db, \"testuser\", \"test@example.com\", \"password\", \"user\")\n\n    // Test successful login\n    t.Run(\"successful login\", func(t *testing.T) {\n        body := map[string]string{\n            \"username\": \"testuser\",\n            \"password\": \"password\",\n        }\n        jsonBody, _ := json.Marshal(body)\n\n        req := httptest.NewRequest(\"POST\", \"/api/login\", bytes.NewBuffer(jsonBody))\n        rr := httptest.NewRecorder()\n\n        handler.Login(rr, req)\n\n        assert.Equal(t, http.StatusOK, rr.Code)\n\n        var resp models.LoginResponse\n        json.Unmarshal(rr.Body.Bytes(), &amp;resp)\n        assert.True(t, resp.Success)\n        assert.NotEmpty(t, resp.Token)\n    })\n}\n</code></pre>"},{"location":"developer/backend/#integration-testing","title":"Integration Testing","text":"<pre><code>// internal/integration_test/auth_integration_test.go\nfunc TestAuthenticationFlow(t *testing.T) {\n    // Setup test environment\n    db := testutil.SetupTestDB(t)\n    router := setupTestRouter(db)\n\n    // Register user\n    registerResp := testutil.RegisterUser(t, router, \"testuser\", \"test@example.com\", \"password\")\n    assert.Equal(t, http.StatusOK, registerResp.Code)\n\n    // Login\n    loginResp := testutil.Login(t, router, \"testuser\", \"password\")\n    assert.Equal(t, http.StatusOK, loginResp.Code)\n\n    // Extract token\n    token := testutil.ExtractTokenFromResponse(t, loginResp)\n\n    // Access protected endpoint\n    req := httptest.NewRequest(\"GET\", \"/api/dashboard\", nil)\n    req.AddCookie(&amp;http.Cookie{Name: \"token\", Value: token})\n    rr := httptest.NewRecorder()\n\n    router.ServeHTTP(rr, req)\n    assert.Equal(t, http.StatusOK, rr.Code)\n}\n</code></pre>"},{"location":"developer/backend/#mock-services","title":"Mock Services","text":"<pre><code>// internal/testutil/mocks.go\ntype MockEmailService struct {\n    SentEmails []SentEmail\n}\n\nfunc (m *MockEmailService) SendMFACode(ctx context.Context, email, code string) error {\n    m.SentEmails = append(m.SentEmails, SentEmail{\n        To:      email,\n        Subject: \"MFA Code\",\n        Body:    code,\n    })\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#database-testing","title":"Database Testing","text":"<pre><code>// internal/testutil/db.go\nfunc SetupTestDB(t *testing.T) *db.DB {\n    // Connect to test database\n    testDB := os.Getenv(\"TEST_DATABASE_URL\")\n    if testDB == \"\" {\n        testDB = \"postgres://test:test@localhost/krakenhashes_test\"\n    }\n\n    sqlDB, err := sql.Open(\"postgres\", testDB)\n    require.NoError(t, err)\n\n    // Run migrations\n    err = database.RunMigrations()\n    require.NoError(t, err)\n\n    // Clean up after test\n    t.Cleanup(func() {\n        // Truncate all tables\n        tables := []string{\"users\", \"agents\", \"hashlists\", \"hashes\"}\n        for _, table := range tables {\n            sqlDB.Exec(fmt.Sprintf(\"TRUNCATE TABLE %s CASCADE\", table))\n        }\n        sqlDB.Close()\n    })\n\n    return &amp;db.DB{DB: sqlDB}\n}\n</code></pre>"},{"location":"developer/backend/#common-patterns-and-utilities","title":"Common Patterns and Utilities","text":""},{"location":"developer/backend/#debug-logging","title":"Debug Logging","text":"<pre><code>// Use the debug package for structured logging\nimport \"github.com/ZerkerEOD/krakenhashes/backend/pkg/debug\"\n\n// Log levels\ndebug.Debug(\"Processing request for user: %s\", userID)\ndebug.Info(\"Server starting on port %d\", port)\ndebug.Warning(\"Rate limit approaching for user: %s\", userID)\ndebug.Error(\"Failed to connect to database: %v\", err)\n\n// Conditional debug logging\nif debug.IsDebugEnabled() {\n    debug.Debug(\"Detailed request info: %+v\", req)\n}\n</code></pre>"},{"location":"developer/backend/#http-utilities","title":"HTTP Utilities","text":"<pre><code>// internal/pkg/httputil/httputil.go\nfunc WriteJSON(w http.ResponseWriter, status int, data interface{}) {\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.WriteHeader(status)\n    if err := json.NewEncoder(w).Encode(data); err != nil {\n        debug.Error(\"Failed to encode JSON response: %v\", err)\n    }\n}\n\nfunc ReadJSON(r *http.Request, dest interface{}) error {\n    if r.Header.Get(\"Content-Type\") != \"application/json\" {\n        return errors.New(\"content-type must be application/json\")\n    }\n\n    decoder := json.NewDecoder(r.Body)\n    decoder.DisallowUnknownFields()\n    return decoder.Decode(dest)\n}\n</code></pre>"},{"location":"developer/backend/#context-values","title":"Context Values","text":"<pre><code>// pkg/jwt/context.go\ntype contextKey string\n\nconst (\n    userIDKey   contextKey = \"user_id\"\n    userRoleKey contextKey = \"user_role\"\n    agentIDKey  contextKey = \"agent_id\"\n)\n\nfunc GetUserID(ctx context.Context) (uuid.UUID, bool) {\n    id, ok := ctx.Value(userIDKey).(uuid.UUID)\n    return id, ok\n}\n\nfunc GetUserRole(ctx context.Context) (string, bool) {\n    role, ok := ctx.Value(userRoleKey).(string)\n    return role, ok\n}\n</code></pre>"},{"location":"developer/backend/#file-operations","title":"File Operations","text":"<pre><code>// Use the centralized data directory\nfunc SaveUploadedFile(file multipart.File, filename string) error {\n    dataDir := config.GetDataDir()\n    destPath := filepath.Join(dataDir, \"uploads\", filename)\n\n    // Ensure directory exists\n    if err := os.MkdirAll(filepath.Dir(destPath), 0755); err != nil {\n        return fmt.Errorf(\"failed to create directory: %w\", err)\n    }\n\n    // Create destination file\n    dest, err := os.Create(destPath)\n    if err != nil {\n        return fmt.Errorf(\"failed to create file: %w\", err)\n    }\n    defer dest.Close()\n\n    // Copy content\n    if _, err := io.Copy(dest, file); err != nil {\n        return fmt.Errorf(\"failed to save file: %w\", err)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#validation-helpers","title":"Validation Helpers","text":"<pre><code>// Validate request data\nfunc ValidateCreateUserRequest(req *CreateUserRequest) error {\n    if req.Username == \"\" {\n        return errors.New(\"username is required\")\n    }\n\n    if len(req.Username) &lt; 3 || len(req.Username) &gt; 50 {\n        return errors.New(\"username must be between 3 and 50 characters\")\n    }\n\n    if !emailRegex.MatchString(req.Email) {\n        return errors.New(\"invalid email format\")\n    }\n\n    if err := password.Validate(req.Password); err != nil {\n        return fmt.Errorf(\"invalid password: %w\", err)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#working-with-nullable-fields","title":"Working with Nullable Fields","text":"<p>When working with <code>*string</code> and other pointer fields, always check for <code>nil</code> before dereferencing:</p> <pre><code>// Bad - will panic if password is nil\nfunc processHash(hash *models.Hash) string {\n    return *hash.Password // PANIC if hash not cracked!\n}\n\n// Good - safe nil checking\nfunc processHash(hash *models.Hash) string {\n    if hash.Password == nil {\n        return \"\" // or handle appropriately\n    }\n    return *hash.Password\n}\n\n// Best - guard clause pattern\nfunc processHash(hash *models.Hash) string {\n    if hash.Password == nil {\n        return \"\"\n    }\n\n    password := *hash.Password\n    // ... process password\n    return password\n}\n</code></pre> <p>Common Patterns:</p> <pre><code>// Analytics service - skip nil passwords\nfor _, pwd := range passwords {\n    if pwd.Password == nil {\n        continue // Skip entries without passwords\n    }\n    length := len([]rune(*pwd.Password))\n    // ... process\n}\n\n// Setting nullable fields\nhash := &amp;models.Hash{\n    OriginalHash: \"5F4DCC3B...\",\n    IsCracked:    true,\n}\nplaintext := \"password123\"\nhash.Password = &amp;plaintext  // Set pointer to value\n\n// Or inline\nhash.Password = stringPtr(\"password123\")\n\n// Helper function for creating string pointers\nfunc stringPtr(s string) *string {\n    return &amp;s\n}\n</code></pre> <p>Database Scanning:</p> <p>Use <code>sql.NullString</code> when scanning, then convert to <code>*string</code>:</p> <pre><code>var password sql.NullString\nerr := row.Scan(&amp;hash.ID, &amp;hash.OriginalHash, &amp;password)\n\nif password.Valid {\n    hash.Password = &amp;password.String\n} else {\n    hash.Password = nil\n}\n</code></pre> <p>JSON Handling:</p> <p>The <code>omitempty</code> tag automatically excludes <code>nil</code> pointer fields:</p> <pre><code>type Hash struct {\n    Password *string `json:\"password,omitempty\"`\n}\n\n// When marshaling:\nuncracked := Hash{Password: nil}\njson.Marshal(uncracked) // {\"id\": \"...\", \"original_hash\": \"...\"}\n// password field omitted\n\ncracked := Hash{Password: stringPtr(\"test123\")}\njson.Marshal(cracked) // {\"id\": \"...\", \"original_hash\": \"...\", \"password\": \"test123\"}\n// password field included\n</code></pre>"},{"location":"developer/backend/#debugging-and-logging","title":"Debugging and Logging","text":""},{"location":"developer/backend/#environment-variables-for-debugging","title":"Environment Variables for Debugging","text":"<pre><code># Enable debug logging\nKH_DEBUG=true\n\n# Set log level (DEBUG, INFO, WARNING, ERROR)\nKH_LOG_LEVEL=DEBUG\n\n# Enable SQL query logging\nKH_LOG_SQL=true\n</code></pre>"},{"location":"developer/backend/#debugging-database-queries","title":"Debugging Database Queries","text":"<pre><code>// Log SQL queries in development\nif debug.IsDebugEnabled() {\n    debug.Debug(\"Executing query: %s with args: %v\", query, args)\n}\n\n// Time query execution\nstart := time.Now()\nrows, err := db.QueryContext(ctx, query, args...)\ndebug.Debug(\"Query executed in %v\", time.Since(start))\n</code></pre>"},{"location":"developer/backend/#requestresponse-logging-middleware","title":"Request/Response Logging Middleware","text":"<pre><code>func loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n\n        // Wrap response writer to capture status\n        wrapped := &amp;responseWriter{ResponseWriter: w, statusCode: http.StatusOK}\n\n        // Log request\n        debug.Info(\"[%s] %s %s\", r.Method, r.URL.Path, r.RemoteAddr)\n\n        next.ServeHTTP(wrapped, r)\n\n        // Log response\n        duration := time.Since(start)\n        debug.Info(\"[%s] %s %s - %d (%v)\", \n            r.Method, r.URL.Path, r.RemoteAddr, \n            wrapped.statusCode, duration)\n    })\n}\n</code></pre>"},{"location":"developer/backend/#common-debugging-commands","title":"Common Debugging Commands","text":"<pre><code># View backend logs with context\ndocker-compose logs backend | grep -A 5 -B 5 \"ERROR\"\n\n# Monitor real-time logs\ndocker-compose logs -f backend | grep -E \"user_id|agent_id\"\n\n# Check database state\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes \\\n  -c \"SELECT * FROM users WHERE created_at &gt; NOW() - INTERVAL '1 hour';\"\n\n# Test endpoint with curl\ncurl -k -X POST https://localhost:8443/api/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\":\"test\",\"password\":\"test\"}' \\\n  -c cookies.txt -v\n\n# Use saved cookies for authenticated requests\ncurl -k https://localhost:8443/api/dashboard \\\n  -b cookies.txt -v\n</code></pre>"},{"location":"developer/backend/#best-practices","title":"Best Practices","text":"<ol> <li>Always use context: Pass context through all function calls for cancellation and timeouts</li> <li>Handle errors explicitly: Never ignore errors, always log or return them</li> <li>Use transactions: For operations that modify multiple tables</li> <li>Validate input: Validate all user input at the handler level</li> <li>Log appropriately: Use debug for development, info for important events, error for failures</li> <li>Test thoroughly: Write unit tests for business logic, integration tests for workflows</li> <li>Document APIs: Add comments to handlers explaining request/response formats</li> <li>Use prepared statements: Always use parameterized queries to prevent SQL injection</li> <li>Close resources: Always close database rows, files, and connections</li> <li>Follow Go conventions: Use gofmt, follow effective Go guidelines</li> </ol>"},{"location":"developer/backend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/backend/#common-issues","title":"Common Issues","text":"<ol> <li>Database connection errors</li> <li>Check DATABASE_URL environment variable</li> <li>Ensure PostgreSQL is running</li> <li> <p>Verify network connectivity in Docker</p> </li> <li> <p>Migration failures</p> </li> <li>Check migration syntax</li> <li>Ensure migrations are sequential</li> <li> <p>Verify database permissions</p> </li> <li> <p>Authentication issues</p> </li> <li>Check JWT_SECRET is set</li> <li>Verify token exists in database</li> <li> <p>Check cookie settings (secure, httpOnly)</p> </li> <li> <p>WebSocket connection failures</p> </li> <li>Verify TLS certificates</li> <li>Check CORS settings</li> <li> <p>Ensure proper authentication headers</p> </li> <li> <p>File upload issues</p> </li> <li>Check data directory permissions</li> <li>Verify multipart form parsing</li> <li>Check file size limits</li> </ol>"},{"location":"developer/backend/#debug-mode-features","title":"Debug Mode Features","text":"<p>When <code>KH_DEBUG=true</code>: - Detailed SQL query logging - Request/response body logging - Performance timing information - Stack traces on errors - WebSocket message logging</p> <p>Remember to disable debug mode in production for security and performance reasons.</p>"},{"location":"developer/frontend/","title":"KrakenHashes Frontend Development Guide","text":"<p>This guide provides comprehensive documentation for developing the KrakenHashes frontend application. The frontend is built with React, TypeScript, Material-UI, and React Query.</p>"},{"location":"developer/frontend/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Development Environment Setup</li> <li>Project Structure and Organization</li> <li>Component Development Patterns</li> <li>State Management with React Query</li> <li>API Integration and Services</li> <li>Material-UI Theming and Styling</li> <li>TypeScript Conventions</li> <li>Testing Approaches</li> </ol>"},{"location":"developer/frontend/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"developer/frontend/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18+ and npm 9+</li> <li>Docker and Docker Compose (for backend services)</li> <li>A modern web browser with developer tools</li> </ul>"},{"location":"developer/frontend/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes/frontend\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>npm install\n</code></pre></p> </li> <li> <p>Configure environment variables:    Create a <code>.env.local</code> file in the frontend directory:    <pre><code>REACT_APP_API_URL=https://localhost:31337\nREACT_APP_WS_URL=wss://localhost:31337\n</code></pre></p> </li> <li> <p>Start the development server: <pre><code>npm start\n</code></pre>    The application will be available at <code>http://localhost:3000</code></p> </li> </ol>"},{"location":"developer/frontend/#running-with-backend-services","title":"Running with Backend Services","text":"<p>For full functionality, run the backend services using Docker:</p> <pre><code># From the project root\ndocker-compose up -d --build\n</code></pre>"},{"location":"developer/frontend/#development-scripts","title":"Development Scripts","text":"<pre><code>npm start       # Start development server (port 3000)\nnpm run build   # Production build\nnpm test        # Run tests\nnpm run eject   # Eject from Create React App (use with caution)\n</code></pre>"},{"location":"developer/frontend/#project-structure-and-organization","title":"Project Structure and Organization","text":"<p>The frontend follows a feature-based organization pattern:</p> <pre><code>frontend/src/\n\u251c\u2500\u2500 api/                 # API version checking\n\u2502   \u2514\u2500\u2500 version.ts\n\u251c\u2500\u2500 components/          # Reusable components\n\u2502   \u251c\u2500\u2500 admin/          # Admin-specific components\n\u2502   \u251c\u2500\u2500 agent/          # Agent management components\n\u2502   \u251c\u2500\u2500 auth/           # Authentication components\n\u2502   \u251c\u2500\u2500 common/         # Shared common components\n\u2502   \u251c\u2500\u2500 hashlist/       # Hashlist management components\n\u2502   \u251c\u2500\u2500 pot/            # Cracked passwords components\n\u2502   \u2514\u2500\u2500 settings/       # Settings components\n\u251c\u2500\u2500 contexts/           # React contexts\n\u2502   \u2514\u2500\u2500 AuthContext.tsx # Authentication context provider\n\u251c\u2500\u2500 hooks/              # Custom React hooks\n\u2502   \u251c\u2500\u2500 useAuth.tsx     # Authentication hook\n\u2502   \u251c\u2500\u2500 useConfirm.tsx  # Confirmation dialog hook\n\u2502   \u251c\u2500\u2500 useDebounce.ts  # Debounce hook\n\u2502   \u2514\u2500\u2500 useVouchers.ts  # Voucher management hook\n\u251c\u2500\u2500 pages/              # Page components (routes)\n\u2502   \u251c\u2500\u2500 admin/          # Admin pages\n\u2502   \u251c\u2500\u2500 settings/       # Settings pages\n\u2502   \u2514\u2500\u2500 ...             # Other page components\n\u251c\u2500\u2500 services/           # API service layer\n\u2502   \u251c\u2500\u2500 api.ts          # Axios configuration\n\u2502   \u251c\u2500\u2500 auth.ts         # Authentication services\n\u2502   \u2514\u2500\u2500 ...             # Domain-specific services\n\u251c\u2500\u2500 styles/             # Global styles\n\u2502   \u2514\u2500\u2500 theme.ts        # Material-UI theme\n\u251c\u2500\u2500 types/              # TypeScript type definitions\n\u2502   \u251c\u2500\u2500 agent.ts        # Agent types\n\u2502   \u251c\u2500\u2500 auth.ts         # Authentication types\n\u2502   \u2514\u2500\u2500 ...             # Domain-specific types\n\u251c\u2500\u2500 utils/              # Utility functions\n\u2502   \u251c\u2500\u2500 formatters.ts   # Data formatting utilities\n\u2502   \u251c\u2500\u2500 validation.ts   # Validation utilities\n\u2502   \u2514\u2500\u2500 ...             # Other utilities\n\u251c\u2500\u2500 App.tsx             # Root application component\n\u251c\u2500\u2500 config.ts           # Application configuration\n\u2514\u2500\u2500 index.tsx           # Application entry point\n</code></pre>"},{"location":"developer/frontend/#component-development-patterns","title":"Component Development Patterns","text":""},{"location":"developer/frontend/#component-structure","title":"Component Structure","text":"<p>Components follow a consistent structure with TypeScript interfaces and comprehensive documentation:</p> <pre><code>/**\n * ComponentName - Brief description of the component\n * \n * Features:\n *   - Key feature 1\n *   - Key feature 2\n * \n * Dependencies:\n *   - External libraries used\n *   - Internal components/hooks\n * \n * Error Scenarios:\n *   - Possible error conditions\n *   - How they're handled\n */\n\nimport React, { useState, useCallback } from 'react';\nimport { Box, Typography } from '@mui/material';\n\ninterface ComponentNameProps {\n  title: string;\n  onAction?: () =&gt; void;\n}\n\nconst ComponentName: React.FC&lt;ComponentNameProps&gt; = ({ title, onAction }) =&gt; {\n  const [state, setState] = useState&lt;string&gt;('');\n\n  const handleAction = useCallback(() =&gt; {\n    // Handle action\n    onAction?.();\n  }, [onAction]);\n\n  return (\n    &lt;Box&gt;\n      &lt;Typography variant=\"h5\"&gt;{title}&lt;/Typography&gt;\n      {/* Component content */}\n    &lt;/Box&gt;\n  );\n};\n\nexport default ComponentName;\n</code></pre>"},{"location":"developer/frontend/#layout-components","title":"Layout Components","text":"<p>The application uses a main Layout component with navigation:</p> <pre><code>// components/Layout.tsx\nconst Layout: React.FC = () =&gt; {\n  const [open, setOpen] = useState&lt;boolean&gt;(true);\n  const navigate = useNavigate();\n  const { userRole } = useAuth();\n\n  return (\n    &lt;Box sx={{ display: 'flex', minHeight: '100vh' }}&gt;\n      &lt;AppBar position=\"fixed\"&gt;\n        &lt;Toolbar&gt;\n          &lt;Typography variant=\"h6\"&gt;KrakenHashes&lt;/Typography&gt;\n          &lt;UserMenu /&gt;\n        &lt;/Toolbar&gt;\n      &lt;/AppBar&gt;\n      &lt;Drawer variant=\"permanent\" open={open}&gt;\n        &lt;List&gt;\n          {menuItems.map((item) =&gt; (\n            &lt;ListItem button key={item.text} onClick={() =&gt; navigate(item.path)}&gt;\n              &lt;ListItemIcon&gt;{item.icon}&lt;/ListItemIcon&gt;\n              &lt;ListItemText primary={item.text} /&gt;\n            &lt;/ListItem&gt;\n          ))}\n        &lt;/List&gt;\n        {userRole === 'admin' &amp;&amp; &lt;AdminMenu /&gt;}\n      &lt;/Drawer&gt;\n      &lt;Box component=\"main\" sx={{ flexGrow: 1, p: 3 }}&gt;\n        &lt;Outlet /&gt;\n      &lt;/Box&gt;\n    &lt;/Box&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#form-components","title":"Form Components","text":"<p>Forms use React Hook Form with Material-UI integration:</p> <pre><code>import { useForm } from 'react-hook-form';\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport * as z from 'zod';\n\nconst schema = z.object({\n  name: z.string().min(1, 'Name is required'),\n  email: z.string().email('Invalid email'),\n});\n\ntype FormData = z.infer&lt;typeof schema&gt;;\n\nconst FormComponent: React.FC = () =&gt; {\n  const { register, handleSubmit, formState: { errors } } = useForm&lt;FormData&gt;({\n    resolver: zodResolver(schema),\n  });\n\n  const onSubmit = (data: FormData) =&gt; {\n    // Handle form submission\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit(onSubmit)}&gt;\n      &lt;TextField\n        {...register('name')}\n        error={!!errors.name}\n        helperText={errors.name?.message}\n        label=\"Name\"\n        fullWidth\n      /&gt;\n      &lt;Button type=\"submit\" variant=\"contained\"&gt;\n        Submit\n      &lt;/Button&gt;\n    &lt;/form&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#paginated-table-components","title":"Paginated Table Components","text":""},{"location":"developer/frontend/#hashlisthashestable-component","title":"HashlistHashesTable Component","text":"<p>File: <code>frontend/src/components/hashlist/HashlistHashesTable.tsx</code></p> <p>A full-featured paginated table component demonstrating best practices for large dataset display.</p> <p>Key Features: - React Query integration with automatic caching and refetching - Material-UI TablePagination with custom page size options [500, 1000, 1500, 2000, All] - Real-time search with debouncing (300ms delay) - Dynamic column widths for responsive layout - Copy-to-clipboard functionality with visual feedback - Automatic sorting (cracked hashes first)</p> <p>Usage Example: <pre><code>&lt;HashlistHashesTable\n  hashlistId=\"uuid-here\"\n  hashlistName=\"My Hashlist\"\n  totalHashes={1000}\n  crackedHashes={250}\n/&gt;\n</code></pre></p> <p>Table Layout Best Practice:</p> <p>Use dynamic layout with min/max width constraints instead of fixed percentages:</p> <pre><code>&lt;Table size=\"small\"&gt;\n  &lt;TableHead&gt;\n    &lt;TableRow&gt;\n      {/* Long content with flexible width */}\n      &lt;TableCell sx={{ minWidth: 300, maxWidth: 600 }}&gt;\n        Original Hash\n      &lt;/TableCell&gt;\n\n      {/* Fixed width columns */}\n      &lt;TableCell sx={{ minWidth: 120, width: 150 }}&gt;\n        Username\n      &lt;/TableCell&gt;\n\n      {/* Icon-only column */}\n      &lt;TableCell sx={{ width: 80 }} align=\"center\"&gt;\n        Actions\n      &lt;/TableCell&gt;\n    &lt;/TableRow&gt;\n  &lt;/TableHead&gt;\n&lt;/Table&gt;\n</code></pre> <p>Pagination with \"All\" Option:</p> <pre><code>const rowsPerPageOptions = [\n  500,\n  1000,\n  1500,\n  2000,\n  { label: 'All', value: -1 }\n];\n\n// Backend API supports -1 for unlimited results\nconst limit = rowsPerPage === -1 ? -1 : rowsPerPage;\n</code></pre> <p>Search Implementation:</p> <pre><code>const [searchTerm, setSearchTerm] = useState('');\nconst debouncedSearch = useDebounce(searchTerm, 300);\n\n// Use debounced value in API query\nconst { data } = useQuery(['hashes', hashlistId, debouncedSearch], ...);\n</code></pre> <p>Copy Button Pattern:</p> <pre><code>// Copy password if cracked, otherwise copy hash\n&lt;Tooltip title={hash.is_cracked ? 'Copy password' : 'Copy hash'}&gt;\n  &lt;IconButton\n    onClick={() =&gt; copyToClipboard(\n      hash.is_cracked &amp;&amp; hash.password\n        ? hash.password\n        : hash.original_hash\n    )}\n  &gt;\n    &lt;ContentCopyIcon fontSize=\"small\" /&gt;\n  &lt;/IconButton&gt;\n&lt;/Tooltip&gt;\n</code></pre>"},{"location":"developer/frontend/#integration-pattern","title":"Integration Pattern","text":"<p>File: <code>frontend/src/components/hashlist/HashlistDetailView.tsx</code></p> <p>Shows how to integrate paginated table components:</p> <pre><code>// Add domain field to interface\ninterface HashDetail {\n  id: string;\n  hash_value: string;\n  original_hash: string;\n  username?: string;\n  domain?: string;        // Added in v1.1+\n  hash_type_id: number;\n  is_cracked: boolean;\n  password?: string;\n  last_updated: string;\n}\n\n// Replace simple list with paginated table\n{hashlist &amp;&amp; (\n  &lt;HashlistHashesTable\n    hashlistId={id!}\n    hashlistName={hashlist.name}\n    totalHashes={hashlist.total_hashes || 0}\n    crackedHashes={hashlist.cracked_hashes || 0}\n  /&gt;\n)}\n</code></pre>"},{"location":"developer/frontend/#state-management-with-react-query","title":"State Management with React Query","text":""},{"location":"developer/frontend/#query-client-configuration","title":"Query Client Configuration","text":"<p>The application uses React Query for server state management:</p> <pre><code>// App.tsx\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      staleTime: 5 * 60 * 1000, // 5 minutes\n      refetchOnWindowFocus: true,\n    },\n  },\n});\n\nfunction App() {\n  return (\n    &lt;QueryClientProvider client={queryClient}&gt;\n      {/* Application components */}\n    &lt;/QueryClientProvider&gt;\n  );\n}\n</code></pre>"},{"location":"developer/frontend/#using-queries","title":"Using Queries","text":"<p>Example of fetching data with React Query:</p> <pre><code>import { useQuery } from '@tanstack/react-query';\nimport { getWordlists } from '../services/wordlists';\n\nconst WordlistsComponent: React.FC = () =&gt; {\n  const { data, isLoading, error, refetch } = useQuery({\n    queryKey: ['wordlists'],\n    queryFn: getWordlists,\n  });\n\n  if (isLoading) return &lt;CircularProgress /&gt;;\n  if (error) return &lt;Alert severity=\"error\"&gt;Failed to load wordlists&lt;/Alert&gt;;\n\n  return (\n    &lt;Box&gt;\n      {data?.map((wordlist) =&gt; (\n        &lt;WordlistItem key={wordlist.id} wordlist={wordlist} /&gt;\n      ))}\n    &lt;/Box&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#using-mutations","title":"Using Mutations","text":"<p>Example of mutating data with React Query:</p> <pre><code>import { useMutation, useQueryClient } from '@tanstack/react-query';\nimport { updateWordlist } from '../services/wordlists';\n\nconst EditWordlistDialog: React.FC&lt;{ wordlist: Wordlist }&gt; = ({ wordlist }) =&gt; {\n  const queryClient = useQueryClient();\n  const { enqueueSnackbar } = useSnackbar();\n\n  const mutation = useMutation({\n    mutationFn: (data: UpdateWordlistData) =&gt; updateWordlist(wordlist.id, data),\n    onSuccess: () =&gt; {\n      queryClient.invalidateQueries({ queryKey: ['wordlists'] });\n      enqueueSnackbar('Wordlist updated successfully', { variant: 'success' });\n    },\n    onError: (error) =&gt; {\n      enqueueSnackbar('Failed to update wordlist', { variant: 'error' });\n    },\n  });\n\n  const handleSubmit = (data: UpdateWordlistData) =&gt; {\n    mutation.mutate(data);\n  };\n\n  return (\n    &lt;Dialog open onClose={onClose}&gt;\n      &lt;DialogContent&gt;\n        {/* Form fields */}\n      &lt;/DialogContent&gt;\n      &lt;DialogActions&gt;\n        &lt;Button onClick={() =&gt; handleSubmit(formData)} disabled={mutation.isPending}&gt;\n          Save\n        &lt;/Button&gt;\n      &lt;/DialogActions&gt;\n    &lt;/Dialog&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#api-integration-and-services","title":"API Integration and Services","text":""},{"location":"developer/frontend/#axios-configuration","title":"Axios Configuration","text":"<p>The API service layer uses Axios with interceptors:</p> <pre><code>// services/api.ts\nimport axios from 'axios';\n\nconst API_URL = process.env.REACT_APP_API_URL || 'https://localhost:31337';\n\nexport const api = axios.create({\n  baseURL: API_URL,\n  withCredentials: true, // Required for cookies/session\n});\n\n// Request interceptor\napi.interceptors.request.use((config) =&gt; {\n  // Log requests in development\n  if (process.env.NODE_ENV === 'development') {\n    console.debug(`[API] ${config.method?.toUpperCase()} ${config.url}`, config.data);\n  }\n  return config;\n});\n\n// Response interceptor\napi.interceptors.response.use(\n  (response) =&gt; response,\n  async (error) =&gt; {\n    if (error.response?.status === 401) {\n      // Handle authentication errors\n      window.location.href = '/login';\n    }\n    return Promise.reject(error);\n  }\n);\n</code></pre>"},{"location":"developer/frontend/#service-layer-pattern","title":"Service Layer Pattern","text":"<p>Services encapsulate API calls:</p> <pre><code>// services/wordlists.ts\nimport { api } from './api';\nimport { Wordlist, WordlistUploadData } from '../types/wordlists';\n\nexport const wordlistService = {\n  getWordlists: async (): Promise&lt;Wordlist[]&gt; =&gt; {\n    const response = await api.get('/api/wordlists');\n    return response.data;\n  },\n\n  uploadWordlist: async (\n    formData: FormData,\n    onProgress?: (progress: number) =&gt; void\n  ): Promise&lt;Wordlist&gt; =&gt; {\n    const response = await api.post('/api/wordlists/upload', formData, {\n      headers: { 'Content-Type': 'multipart/form-data' },\n      onUploadProgress: (progressEvent) =&gt; {\n        if (progressEvent.total) {\n          const progress = Math.round((progressEvent.loaded * 100) / progressEvent.total);\n          onProgress?.(progress);\n        }\n      },\n    });\n    return response.data;\n  },\n\n  updateWordlist: async (id: string, data: Partial&lt;Wordlist&gt;): Promise&lt;Wordlist&gt; =&gt; {\n    const response = await api.put(`/api/wordlists/${id}`, data);\n    return response.data;\n  },\n\n  deleteWordlist: async (id: string): Promise&lt;void&gt; =&gt; {\n    await api.delete(`/api/wordlists/${id}`);\n  },\n};\n</code></pre>"},{"location":"developer/frontend/#authentication-service","title":"Authentication Service","text":"<p>Authentication is handled through a dedicated service:</p> <pre><code>// services/auth.ts\nimport { api } from './api';\nimport { LoginCredentials, AuthResponse } from '../types/auth';\n\nexport const authService = {\n  login: async (credentials: LoginCredentials): Promise&lt;AuthResponse&gt; =&gt; {\n    const response = await api.post('/api/login', credentials);\n    return response.data;\n  },\n\n  logout: async (): Promise&lt;void&gt; =&gt; {\n    await api.post('/api/logout');\n  },\n\n  isAuthenticated: async (): Promise&lt;{ authenticated: boolean; role?: string }&gt; =&gt; {\n    const response = await api.get('/api/check-auth');\n    return response.data;\n  },\n\n  refreshToken: async (): Promise&lt;void&gt; =&gt; {\n    await api.post('/api/refresh-token');\n  },\n};\n</code></pre>"},{"location":"developer/frontend/#material-ui-theming-and-styling","title":"Material-UI Theming and Styling","text":""},{"location":"developer/frontend/#theme-configuration","title":"Theme Configuration","text":"<p>The application uses a dark theme with custom overrides:</p> <pre><code>// styles/theme.ts\nimport { createTheme, Theme } from '@mui/material/styles';\n\nconst theme: Theme = createTheme({\n  palette: {\n    mode: 'dark',\n    primary: {\n      main: '#ff0000',\n    },\n    background: {\n      default: '#000000',\n      paper: '#121212',\n    },\n    text: {\n      primary: '#ffffff',\n    },\n  },\n  components: {\n    MuiCssBaseline: {\n      styleOverrides: {\n        body: {\n          backgroundColor: '#000000',\n          color: '#ffffff',\n        },\n      },\n    },\n    MuiDataGrid: {\n      styleOverrides: {\n        root: {\n          border: 'none',\n          backgroundColor: '#121212',\n          '&amp; .MuiDataGrid-columnHeaders': {\n            backgroundColor: 'rgba(255, 255, 255, 0.08)',\n          },\n          '&amp; .MuiDataGrid-row:hover': {\n            backgroundColor: 'rgba(255, 255, 255, 0.05)',\n          },\n        },\n      },\n    },\n  },\n});\n\nexport default theme;\n</code></pre>"},{"location":"developer/frontend/#component-styling","title":"Component Styling","text":"<p>Use the <code>sx</code> prop for component-specific styles:</p> <pre><code>&lt;Box\n  sx={{\n    display: 'flex',\n    flexDirection: 'column',\n    gap: 2,\n    p: 3,\n    backgroundColor: 'background.paper',\n    borderRadius: 1,\n    '&amp;:hover': {\n      backgroundColor: 'action.hover',\n    },\n  }}\n&gt;\n  {/* Content */}\n&lt;/Box&gt;\n</code></pre>"},{"location":"developer/frontend/#responsive-design","title":"Responsive Design","text":"<p>Use Material-UI's responsive utilities:</p> <pre><code>&lt;Grid container spacing={2}&gt;\n  &lt;Grid item xs={12} sm={6} md={4}&gt;\n    {/* Content adapts to screen size */}\n  &lt;/Grid&gt;\n&lt;/Grid&gt;\n\n&lt;Box\n  sx={{\n    width: { xs: '100%', sm: '60%', md: '40%' },\n    display: { xs: 'none', md: 'block' },\n  }}\n&gt;\n  {/* Responsive visibility and sizing */}\n&lt;/Box&gt;\n</code></pre>"},{"location":"developer/frontend/#typescript-conventions","title":"TypeScript Conventions","text":""},{"location":"developer/frontend/#type-definitions","title":"Type Definitions","text":"<p>All types are defined in the <code>types/</code> directory:</p> <pre><code>// types/wordlists.ts\nexport enum WordlistType {\n  GENERAL = 'general',\n  SPECIALIZED = 'specialized',\n  TARGETED = 'targeted',\n  CUSTOM = 'custom',\n}\n\nexport interface Wordlist {\n  id: string;\n  name: string;\n  description: string;\n  wordlist_type: WordlistType;\n  file_size: number;\n  word_count: number;\n  verification_status: 'pending' | 'verified' | 'failed';\n  created_at: string;\n  updated_at: string;\n}\n\nexport interface WordlistUploadData {\n  file: File;\n  name?: string;\n  description?: string;\n  wordlist_type: WordlistType;\n}\n</code></pre>"},{"location":"developer/frontend/#component-props","title":"Component Props","text":"<p>Always define interfaces for component props:</p> <pre><code>interface TableProps {\n  data: Wordlist[];\n  onEdit?: (wordlist: Wordlist) =&gt; void;\n  onDelete?: (id: string) =&gt; void;\n  loading?: boolean;\n}\n\nconst WordlistTable: React.FC&lt;TableProps&gt; = ({ \n  data, \n  onEdit, \n  onDelete, \n  loading = false \n}) =&gt; {\n  // Component implementation\n};\n</code></pre>"},{"location":"developer/frontend/#api-response-types","title":"API Response Types","text":"<p>Define types for API responses:</p> <pre><code>// types/api.ts\nexport interface ApiResponse&lt;T&gt; {\n  data: T;\n  message?: string;\n}\n\nexport interface PaginatedResponse&lt;T&gt; {\n  data: T[];\n  total: number;\n  page: number;\n  pageSize: number;\n}\n\nexport interface ApiError {\n  error: string;\n  message: string;\n  statusCode: number;\n}\n</code></pre>"},{"location":"developer/frontend/#utility-types","title":"Utility Types","text":"<p>Use TypeScript utility types effectively:</p> <pre><code>// Partial for update operations\ntype UpdateWordlistData = Partial&lt;Omit&lt;Wordlist, 'id' | 'created_at' | 'updated_at'&gt;&gt;;\n\n// Pick for specific field selections\ntype WordlistSummary = Pick&lt;Wordlist, 'id' | 'name' | 'word_count'&gt;;\n\n// Union types for status\ntype JobStatus = 'pending' | 'running' | 'completed' | 'failed';\n</code></pre>"},{"location":"developer/frontend/#testing-approaches","title":"Testing Approaches","text":""},{"location":"developer/frontend/#unit-testing-with-jest-and-react-testing-library","title":"Unit Testing with Jest and React Testing Library","text":"<p>Although the project doesn't have extensive tests yet, here's the recommended approach:</p> <pre><code>// WordlistTable.test.tsx\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\nimport WordlistTable from './WordlistTable';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: { retry: false },\n  },\n});\n\nconst wrapper = ({ children }: { children: React.ReactNode }) =&gt; (\n  &lt;QueryClientProvider client={queryClient}&gt;\n    {children}\n  &lt;/QueryClientProvider&gt;\n);\n\ndescribe('WordlistTable', () =&gt; {\n  const mockWordlists: Wordlist[] = [\n    {\n      id: '1',\n      name: 'Test Wordlist',\n      description: 'Test description',\n      wordlist_type: WordlistType.GENERAL,\n      file_size: 1024,\n      word_count: 100,\n      verification_status: 'verified',\n      created_at: '2024-01-01',\n      updated_at: '2024-01-01',\n    },\n  ];\n\n  it('renders wordlist data correctly', () =&gt; {\n    render(\n      &lt;WordlistTable data={mockWordlists} /&gt;,\n      { wrapper }\n    );\n\n    expect(screen.getByText('Test Wordlist')).toBeInTheDocument();\n    expect(screen.getByText('100')).toBeInTheDocument();\n  });\n\n  it('calls onEdit when edit button is clicked', () =&gt; {\n    const handleEdit = jest.fn();\n    render(\n      &lt;WordlistTable data={mockWordlists} onEdit={handleEdit} /&gt;,\n      { wrapper }\n    );\n\n    fireEvent.click(screen.getByLabelText('Edit'));\n    expect(handleEdit).toHaveBeenCalledWith(mockWordlists[0]);\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#integration-testing","title":"Integration Testing","text":"<p>Test components with API interactions:</p> <pre><code>// WordlistsManagement.integration.test.tsx\nimport { render, screen, waitFor } from '@testing-library/react';\nimport { rest } from 'msw';\nimport { setupServer } from 'msw/node';\nimport WordlistsManagement from './WordlistsManagement';\n\nconst server = setupServer(\n  rest.get('/api/wordlists', (req, res, ctx) =&gt; {\n    return res(\n      ctx.json({\n        data: [\n          {\n            id: '1',\n            name: 'Test Wordlist',\n            word_count: 100,\n          },\n        ],\n      })\n    );\n  })\n);\n\nbeforeAll(() =&gt; server.listen());\nafterEach(() =&gt; server.resetHandlers());\nafterAll(() =&gt; server.close());\n\ntest('loads and displays wordlists', async () =&gt; {\n  render(&lt;WordlistsManagement /&gt;);\n\n  await waitFor(() =&gt; {\n    expect(screen.getByText('Test Wordlist')).toBeInTheDocument();\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#testing-custom-hooks","title":"Testing Custom Hooks","text":"<pre><code>// useDebounce.test.ts\nimport { renderHook, act } from '@testing-library/react-hooks';\nimport useDebounce from './useDebounce';\n\ndescribe('useDebounce', () =&gt; {\n  jest.useFakeTimers();\n\n  it('returns initial value immediately', () =&gt; {\n    const { result } = renderHook(() =&gt; useDebounce('test', 500));\n    expect(result.current).toBe('test');\n  });\n\n  it('debounces value changes', () =&gt; {\n    const { result, rerender } = renderHook(\n      ({ value, delay }) =&gt; useDebounce(value, delay),\n      { initialProps: { value: 'test', delay: 500 } }\n    );\n\n    rerender({ value: 'updated', delay: 500 });\n    expect(result.current).toBe('test');\n\n    act(() =&gt; {\n      jest.advanceTimersByTime(500);\n    });\n\n    expect(result.current).toBe('updated');\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm test -- --watch\n\n# Run tests with coverage\nnpm test -- --coverage\n\n# Run specific test file\nnpm test WordlistTable.test.tsx\n</code></pre>"},{"location":"developer/frontend/#best-practices","title":"Best Practices","text":""},{"location":"developer/frontend/#component-guidelines","title":"Component Guidelines","text":"<ol> <li>Keep components focused: Each component should have a single responsibility</li> <li>Use TypeScript strictly: Enable strict mode and avoid <code>any</code> types</li> <li>Implement proper error handling: Use error boundaries and display user-friendly messages</li> <li>Optimize performance: Use React.memo, useCallback, and useMemo appropriately</li> <li>Follow accessibility standards: Use semantic HTML and ARIA attributes</li> </ol>"},{"location":"developer/frontend/#code-organization","title":"Code Organization","text":"<ol> <li>Consistent file naming: Use PascalCase for components, camelCase for utilities</li> <li>Co-locate related files: Keep tests, styles, and types near their components</li> <li>Extract reusable logic: Create custom hooks for shared functionality</li> <li>Document complex logic: Add JSDoc comments for non-trivial functions</li> </ol>"},{"location":"developer/frontend/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Lazy load routes: Use React.lazy for code splitting</li> <li>Optimize re-renders: Use React Query's staleTime and cacheTime</li> <li>Virtualize long lists: Use react-window for large datasets</li> <li>Debounce user input: Use the useDebounce hook for search fields</li> </ol>"},{"location":"developer/frontend/#security-considerations","title":"Security Considerations","text":"<ol> <li>Sanitize user input: Validate and sanitize all user-provided data</li> <li>Use HTTPS: Ensure all API calls use secure connections</li> <li>Handle authentication properly: Store tokens securely and implement refresh logic</li> <li>Implement CSP: Configure Content Security Policy headers</li> </ol>"},{"location":"developer/frontend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/frontend/#common-issues","title":"Common Issues","text":"<ol> <li>Certificate errors: The application includes a certificate check on startup</li> <li>CORS issues: Ensure the backend is configured to accept frontend origin</li> <li>Authentication loops: Check token refresh logic and API interceptors</li> <li>Build failures: Clear node_modules and package-lock.json, then reinstall</li> </ol>"},{"location":"developer/frontend/#debug-tips","title":"Debug Tips","text":"<ol> <li>Enable React Developer Tools</li> <li>Use Redux DevTools for React Query debugging</li> <li>Check network tab for API call failures</li> <li>Review console for error messages</li> </ol>"},{"location":"developer/frontend/#resources","title":"Resources","text":"<ul> <li>React Documentation</li> <li>Material-UI Documentation</li> <li>React Query Documentation</li> <li>TypeScript Documentation</li> <li>React Hook Form</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to KrakenHashes! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Overview</p> <p>Learn what KrakenHashes is and how it can help your security operations</p> </li> <li> <p> Quick Start</p> <p>Get KrakenHashes running in under 5 minutes with Docker</p> </li> <li> <p> Installation</p> <p>Detailed installation guide for production deployments</p> </li> <li> <p> First Password Crack</p> <p>Step-by-step tutorial for your first successful password crack</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li> Docker and Docker Compose installed</li> <li> At least 4GB of RAM</li> <li> 10GB of free disk space</li> <li> Linux-based system (recommended) or Windows with WSL2</li> <li>:material-gpu: GPU with CUDA support (optional but recommended)</li> </ul>"},{"location":"getting-started/#choose-your-path","title":"Choose Your Path","text":""},{"location":"getting-started/#i-want-to-try-it-quickly","title":"I want to try it quickly","text":"<p>\u2192 Go to Quick Start for a 5-minute Docker setup</p>"},{"location":"getting-started/#im-deploying-to-production","title":"I'm deploying to production","text":"<p>\u2192 Read the Installation Guide for detailed setup options</p>"},{"location":"getting-started/#i-want-to-understand-the-system","title":"I want to understand the system","text":"<p>\u2192 Start with the Overview to learn about KrakenHashes</p>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check our Troubleshooting Guide</li> <li>Join our Discord Community</li> <li>Report issues on GitHub</li> </ul>"},{"location":"getting-started/first-crack/","title":"Your First Password Crack","text":"<p>This guide walks you through performing your first password crack with KrakenHashes. By the end of this tutorial, you'll understand the complete workflow from setup to viewing results.</p> <p>Prerequisites</p> <ul> <li>KrakenHashes is installed and running</li> <li>You're logged in as an admin user</li> <li>You have access to a hashcat binary file</li> <li>At least one agent machine is available (can be the same as the server)</li> </ul>"},{"location":"getting-started/first-crack/#step-1-upload-hashcat-binary","title":"Step 1: Upload Hashcat Binary","text":"<p>First, we need to upload the hashcat binary that agents will use to perform the actual cracking.</p>"},{"location":"getting-started/first-crack/#navigate-to-binary-management","title":"Navigate to Binary Management","text":"<ol> <li>From the main menu, click Admin \u2192 Binary Management</li> <li>Click the Add Binary button in the top right</li> </ol>"},{"location":"getting-started/first-crack/#upload-the-binary","title":"Upload the Binary","text":"<ol> <li>In the upload dialog:</li> <li>Click Choose File or drag and drop your hashcat binary</li> <li>Supported formats: <code>.7z</code>, <code>.zip</code>, <code>.tar.gz</code></li> <li> <p>The binary should be compressed and include all necessary files</p> </li> <li> <p>Click Upload and wait for the process to complete</p> </li> </ol> <p>Getting Hashcat</p> <p>If you don't have hashcat, download it from hashcat.net/hashcat/</p> <p>For this tutorial, download the appropriate binary for your system: - Linux: <code>hashcat-6.2.6.7z</code> - Windows: <code>hashcat-6.2.6.exe</code> (compress to .zip first)</p> <ol> <li>After upload, the system will automatically verify the binary</li> <li>Status should show as verified \u2713</li> <li>Note the Binary ID for later reference</li> </ol> <p>Why Binary Upload is First</p> <p>Uploading a hashcat binary is required before most system features become available. Specifically: - The potfile preset job is created only after a binary exists - Agents need binaries to perform any cracking operations - Job creation requires selecting a binary version</p>"},{"location":"getting-started/first-crack/#step-2-upload-a-simple-wordlist","title":"Step 2: Upload a Simple Wordlist","text":"<p>Next, let's create a basic wordlist for our first crack attempt.</p>"},{"location":"getting-started/first-crack/#navigate-to-wordlist-management","title":"Navigate to Wordlist Management","text":"<ol> <li>From the main menu, click Resources \u2192 Wordlists</li> <li>Click the Add Wordlist button</li> </ol>"},{"location":"getting-started/first-crack/#create-a-test-wordlist","title":"Create a Test Wordlist","text":"<p>For this tutorial, let's create a simple wordlist file:</p> <ol> <li> <p>Create a text file named <code>common-passwords.txt</code> with these contents:    <pre><code>password\n123456\npassword123\nadmin\nletmein\nwelcome\nmonkey\ndragon\n</code></pre></p> </li> <li> <p>In the upload dialog:</p> </li> <li>Name: \"Common Passwords Tutorial\"</li> <li>Description: \"Basic wordlist for first crack tutorial\"</li> <li>Type: General</li> <li> <p>File: Select your <code>common-passwords.txt</code></p> </li> <li> <p>Click Upload</p> </li> </ol> <p>Wordlist Types</p> <ul> <li>General: Standard password lists</li> <li>Usernames: Lists of common usernames</li> <li>Custom: Specialized wordlists for specific targets</li> </ul>"},{"location":"getting-started/first-crack/#step-3-create-a-test-hashlist","title":"Step 3: Create a Test Hashlist","text":"<p>Now let's create some password hashes to crack. We'll use known passwords so you can verify the results.</p>"},{"location":"getting-started/first-crack/#prepare-test-hashes","title":"Prepare Test Hashes","text":"<ol> <li>Create a file named <code>test-hashes.txt</code> with these MD5 hashes:    <pre><code>5f4dcc3b5aa765d61d8327deb882cf99\ne10adc3949ba59abbe56e057f20f883e\n482c811da5d5b4bc6d497ffa98491e38\n</code></pre></li> </ol> <p>What are these hashes?</p> <p>These are MD5 hashes of: - <code>password</code> \u2192 5f4dcc3b5aa765d61d8327deb882cf99 - <code>123456</code> \u2192 e10adc3949ba59abbe56e057f20f883e - <code>password123</code> \u2192 482c811da5d5b4bc6d497ffa98491e38</p>"},{"location":"getting-started/first-crack/#upload-the-hashlist","title":"Upload the Hashlist","text":"<ol> <li>Navigate to Hashlists from the main menu</li> </ol> <p> Hashlist Management page with UPLOAD HASHLIST button highlighted, showing the interface where users can view existing hashlists and initiate new uploads</p> <ol> <li>Click Upload Hashlist</li> </ol> <p></p> <ol> <li>Fill in the details:</li> <li>Name: \"Tutorial Test Hashes\"</li> <li>Description: \"MD5 hashes for first crack tutorial\"</li> <li>Hash Type: MD5 (mode 0)</li> <li>Client: (Optional - leave blank for tutorial)</li> <li> <p>File: Select your <code>test-hashes.txt</code></p> </li> <li> <p>Click Upload</p> </li> </ol> <p>The system will process your hashlist and show: - Total hashes: 3 - Unique hashes: 3 - Status: Active</p>"},{"location":"getting-started/first-crack/#step-4-connect-an-agent","title":"Step 4: Connect an Agent","text":"<p>Agents are the worker machines that perform the actual password cracking. Let's connect one.</p>"},{"location":"getting-started/first-crack/#generate-a-claim-code","title":"Generate a Claim Code","text":"<ol> <li>Navigate to Admin \u2192 Agent Management</li> <li>Click Generate Claim Code</li> </ol> <ol> <li>A claim code will be generated (e.g., <code>ABCD-EFGH-IJKL</code>)</li> <li>Copy this code - you'll need it for the agent</li> </ol>"},{"location":"getting-started/first-crack/#install-and-configure-the-agent","title":"Install and Configure the Agent","text":"<p>On your agent machine (can be the same as the server):</p> <ol> <li>Download the KrakenHashes agent for your platform</li> <li>Extract it to a directory</li> <li> <p>Run the registration command:    <pre><code>./krakenhashes-agent --host your-server:31337 --claim ABCD-EFGH-IJKL\n</code></pre></p> </li> <li> <p>The agent will:</p> </li> <li>Register with the server</li> <li>Receive an API key</li> <li>Start heartbeat communication</li> <li>Sync required files (binaries, wordlists)</li> </ol> <p>Agent Connected!</p> <p>Once registered, you'll see the agent appear in the Agent Management page with: - Status: Online \ud83d\udfe2 - Hardware info (GPU details if available) - Last heartbeat timestamp</p>"},{"location":"getting-started/first-crack/#step-5-create-and-run-a-job","title":"Step 5: Create and Run a Job","text":"<p>Now for the exciting part - let's crack those passwords!</p>"},{"location":"getting-started/first-crack/#create-a-new-job","title":"Create a New Job","text":"<ol> <li>Go back to Hashlists</li> <li>Find your \"Tutorial Test Hashes\" and click on it</li> <li>Click Create Job button</li> </ol>"},{"location":"getting-started/first-crack/#configure-the-job","title":"Configure the Job","text":"<p>You have three options for creating jobs:</p> Quick Job (Recommended for Tutorial)Custom JobWorkflow <ol> <li>Select the Preset Jobs tab</li> <li>Choose \"Quick Dictionary Attack\"</li> <li>This preset includes:</li> <li>Dictionary attack with your wordlists</li> <li>Basic rules for variations</li> <li>Optimized for speed</li> </ol> <ol> <li>Select the Custom tab</li> <li>Configure:</li> <li>Name: \"Tutorial First Crack\"</li> <li>Attack Mode: Dictionary (0)</li> <li>Wordlists: Select \"Common Passwords Tutorial\"</li> <li>Rules: (Optional - leave empty for now)</li> </ol> <ol> <li>Select the Workflows tab</li> <li>Choose a predefined workflow that runs multiple attack strategies</li> </ol>"},{"location":"getting-started/first-crack/#start-the-job","title":"Start the Job","text":"<ol> <li>Review your settings</li> <li>Click Create Job</li> </ol> <p>The job will be: - Added to the queue - Assigned to an available agent - Started automatically</p>"},{"location":"getting-started/first-crack/#step-6-monitor-progress","title":"Step 6: Monitor Progress","text":""},{"location":"getting-started/first-crack/#view-job-status","title":"View Job Status","text":"<ol> <li>Navigate to Jobs from the main menu</li> <li>Find your job in the list</li> </ol> <p>You'll see: - Status: Running \ud83d\udd04 - Progress: Percentage complete - Speed: Hashes per second - Time: Elapsed and estimated remaining</p>"},{"location":"getting-started/first-crack/#real-time-updates","title":"Real-time Updates","text":"<p>The dashboard updates in real-time showing: - Candidates tested - Passwords cracked - Current speed - Agent assignment</p> <p>Understanding Speed</p> <p>Speed is measured in H/s (hashes per second). Higher is better! - CPU only: 1,000 - 100,000 H/s - Single GPU: 1,000,000+ H/s - Multiple GPUs: Much higher!</p>"},{"location":"getting-started/first-crack/#step-7-view-results","title":"Step 7: View Results","text":"<p>Once the job completes (should be quick for this tutorial):</p>"},{"location":"getting-started/first-crack/#check-the-results","title":"Check the Results","text":"<ol> <li>Go back to your hashlist</li> <li>You'll see the status has updated:</li> <li>Cracked: 3/3 (100%)</li> <li>Status indicators next to each hash</li> </ol>"},{"location":"getting-started/first-crack/#view-cracked-passwords","title":"View Cracked Passwords","text":"<ol> <li>Click on a cracked hash to see details</li> <li>The plaintext password will be displayed</li> <li>You should see:</li> <li><code>5f4dcc3b5aa765d61d8327deb882cf99</code> \u2192 <code>password</code></li> <li><code>e10adc3949ba59abbe56e057f20f883e</code> \u2192 <code>123456</code></li> <li><code>482c811da5d5b4bc6d497ffa98491e38</code> \u2192 <code>password123</code></li> </ol> <p> Hashlist view showing successfully cracked passwords with their plaintext values revealed</p>"},{"location":"getting-started/first-crack/#export-results","title":"Export Results","text":"<ol> <li>Click Export button</li> <li>Choose format:</li> <li>CSV: For spreadsheets</li> <li>JSON: For programming</li> <li> <p>Hashcat Potfile: hash:plain format</p> </li> <li> <p>Download includes:</p> </li> <li>Original hashes</li> <li>Cracked plaintexts</li> <li>Crack timestamps</li> <li>Metadata</li> </ol>"},{"location":"getting-started/first-crack/#understanding-what-happened","title":"Understanding What Happened","text":"<p>Let's review the complete workflow:</p> <pre><code>graph LR\n    A[Upload Binary] --&gt; B[Upload Wordlist]\n    B --&gt; C[Create Hashlist]\n    C --&gt; D[Connect Agent]\n    D --&gt; E[Create Job]\n    E --&gt; F[Agent Executes]\n    F --&gt; G[View Results]</code></pre>"},{"location":"getting-started/first-crack/#key-concepts","title":"Key Concepts","text":"<ol> <li>Hashcat Binary: The actual cracking engine</li> <li>Agents download and use this</li> <li> <p>Supports many hash types and attack modes</p> </li> <li> <p>Wordlists: Lists of potential passwords</p> </li> <li>Can be general or targeted</li> <li> <p>Quality matters more than quantity</p> </li> <li> <p>Hashlists: Your target hashes to crack</p> </li> <li>Organized by hash type</li> <li> <p>Can contain thousands or millions</p> </li> <li> <p>Agents: Distributed workers</p> </li> <li>Can be anywhere with internet</li> <li>Automatically sync files</li> <li> <p>Report progress in real-time</p> </li> <li> <p>Jobs: Work assignments</p> </li> <li>Define what to try (wordlists, rules, masks)</li> <li>Can be simple or complex workflows</li> <li>Automatically distributed to agents</li> </ol> <p> Main dashboard view showing hashlist management with crack statistics and job execution interface - what you'll see after completing your first successful password crack</p>"},{"location":"getting-started/first-crack/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first crack:</p>"},{"location":"getting-started/first-crack/#1-try-more-complex-attacks","title":"1. Try More Complex Attacks","text":"<ul> <li>Add Rules: Transform wordlists (password \u2192 Password123!)</li> <li>Use Masks: Pattern-based attacks (?u?l?l?l?d?d?d?d)</li> <li>Combination Attacks: Combine multiple wordlists</li> </ul>"},{"location":"getting-started/first-crack/#2-scale-up","title":"2. Scale Up","text":"<ul> <li>Add More Agents: Distribute work across multiple machines</li> <li>Use GPUs: Dramatically increase cracking speed</li> <li>Upload Larger Wordlists: Try rockyou.txt or custom lists</li> </ul>"},{"location":"getting-started/first-crack/#3-create-workflows","title":"3. Create Workflows","text":"<p>Build multi-stage attacks: 1. Quick dictionary attack 2. Dictionary with rules 3. Targeted masks 4. Brute force (last resort)</p>"},{"location":"getting-started/first-crack/#4-explore-features","title":"4. Explore Features","text":"<ul> <li>Client Management: Organize hashlists by client/project</li> <li>Scheduling: Control when agents run jobs</li> <li>Retention Policies: Automatic cleanup of old data</li> <li>Team Collaboration: Share access with team members</li> </ul>"},{"location":"getting-started/first-crack/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/first-crack/#common-issues","title":"Common Issues","text":"Agent won't connect <ul> <li>Check firewall rules (port 8443)</li> <li>Verify server URL is correct</li> <li>Ensure claim code hasn't expired</li> <li>Check agent logs for errors</li> </ul> Job stuck in queue <ul> <li>Verify agent is online</li> <li>Check agent has required hardware</li> <li>Ensure binary is verified</li> <li>Look for error messages</li> </ul> No passwords cracked <ul> <li>Try a different wordlist</li> <li>Add rules for variations</li> <li>Check hash type is correct</li> <li>Verify hash format</li> </ul> Slow performance <ul> <li>Use GPU-enabled agents</li> <li>Optimize wordlists (remove duplicates)</li> <li>Check network connectivity</li> <li>Monitor system resources</li> </ul>"},{"location":"getting-started/first-crack/#summary","title":"Summary","text":"<p>Congratulations! You've successfully:</p> <ul> <li>\u2705 Uploaded a hashcat binary</li> <li>\u2705 Created a wordlist</li> <li>\u2705 Uploaded target hashes</li> <li>\u2705 Connected an agent</li> <li>\u2705 Ran a cracking job</li> <li>\u2705 Retrieved cracked passwords</li> </ul> <p>You now understand the fundamental workflow of KrakenHashes. From here, you can scale up to crack real-world password hashes with more sophisticated attacks and distributed agents.</p> <p>Ready for Real Work!</p> <p>You're now equipped to:</p> <ul> <li>Handle client hashlist submissions</li> <li>Build effective attack strategies  </li> <li>Manage distributed cracking operations</li> <li>Deliver results efficiently</li> </ul>"},{"location":"getting-started/first-crack/#additional-resources","title":"Additional Resources","text":"<ul> <li>Understanding Hash Types</li> <li>Building Better Wordlists</li> <li>Advanced Attack Strategies</li> <li>Agent Deployment Guide</li> </ul>"},{"location":"getting-started/installation/","title":"KrakenHashes Installation Guide","text":"<p>This guide covers both production and development installation of KrakenHashes.</p>"},{"location":"getting-started/installation/#production-installation","title":"Production Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li> Docker Engine 20.10+ and Docker Compose 2.0+</li> <li> 4GB RAM minimum, 8GB RAM recommended (see System Requirements)</li> <li> 20GB+ disk space for hash files, wordlists, and rules</li> <li> GPU recommended for optimal performance (NVIDIA, AMD, or Intel)</li> <li> Linux host (Ubuntu 20.04+, Debian 11+, RHEL 8+, or similar)</li> </ul>"},{"location":"getting-started/installation/#docker-compose-v2-requirements","title":"Docker Compose v2 Requirements","text":"<p>KrakenHashes requires Docker Compose v2.0 or higher due to: - Advanced environment variable interpolation syntax - Improved health check support - Better service dependency handling</p> <p>The docker compose.yml uses syntax like <code>${LOG_DIR:-./logs}/postgres</code> which requires v2.</p>"},{"location":"getting-started/installation/#installing-docker-compose-v2","title":"Installing Docker Compose v2","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install docker compose-plugin\n\n# CentOS/RHEL/Fedora\nsudo yum install docker compose-plugin\n\n# Manual installation (all systems)\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker compose\nsudo chmod +x /usr/local/bin/docker compose\n\n# Verify installation\ndocker compose version\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>If you see this error: <pre><code>ERROR: Invalid interpolation format for \"postgres\" option in service \"services\"\n</code></pre></p> <p>You're using the old docker compose v1. Install v2 and use <code>docker compose</code> (with a space).</p>"},{"location":"getting-started/installation/#quick-start-with-docker-hub","title":"Quick Start with Docker Hub","text":"<p>The easiest way to run KrakenHashes is using the pre-built Docker image from Docker Hub.</p>"},{"location":"getting-started/installation/#1-create-a-docker-composeyml-file","title":"1. Create a docker compose.yml file","text":"<pre><code>services:\n    postgres:\n        image: postgres:15-alpine\n        container_name: krakenhashes-postgres\n        volumes:\n            - postgres_data:/var/lib/postgresql/data\n        environment:\n            - POSTGRES_USER=krakenhashes\n            - POSTGRES_PASSWORD=changeme # CHANGE THIS!\n            - POSTGRES_DB=krakenhashes\n        restart: unless-stopped\n        healthcheck:\n            test: [\"CMD-SHELL\", \"pg_isready -U krakenhashes\"]\n            interval: 5s\n            timeout: 5s\n            retries: 5\n\n    krakenhashes:\n        image: zerkereod/krakenhashes:latest\n        container_name: krakenhashes-app\n        depends_on:\n            postgres:\n                condition: service_healthy\n        ports:\n            - \"443:443\" # Frontend HTTPS\n            - \"31337:31337\" # Backend API HTTPS\n            - \"1337:1337\" # Backend API HTTP\n        volumes:\n            - krakenhashes_data:/var/lib/krakenhashes\n            - ./logs:/var/log/krakenhashes\n        environment:\n            - DB_HOST=postgres\n            - DB_PORT=5432\n            - DB_NAME=krakenhashes\n            - DB_USER=krakenhashes\n            - DB_PASSWORD=changeme # CHANGE THIS!\n            - JWT_SECRET=your-secret-key-here # CHANGE THIS!\n            - TLS_MODE=self-signed # Options: self-signed, user-provided, certbot\n        restart: unless-stopped\n\nvolumes:\n    postgres_data:\n    krakenhashes_data:\n</code></pre>"},{"location":"getting-started/installation/#2-create-a-env-file-optional-but-recommended","title":"2. Create a .env file (optional but recommended)","text":"<pre><code># Database Configuration\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\n\n# Security\nJWT_SECRET=your-very-long-random-string\n\n# TLS Configuration\nTLS_MODE=self-signed\n\n# Ports (optional, defaults shown)\nFRONTEND_PORT=443\nKH_HTTPS_PORT=31337\nKH_PORT=1337\n</code></pre>"},{"location":"getting-started/installation/#3-start-krakenhashes","title":"3. Start KrakenHashes","text":"<pre><code># Pull the latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Start the services\ndocker compose up -d\n\n# Check logs\ndocker compose logs -f\n</code></pre>"},{"location":"getting-started/installation/#4-access-the-application","title":"4. Access the Application","text":"<ul> <li>Frontend: https://localhost:443</li> <li>Backend API: https://localhost:31337</li> <li>Default admin credentials: admin:KrakenHashes1!</li> </ul>"},{"location":"getting-started/installation/#production-configuration","title":"Production Configuration","text":""},{"location":"getting-started/installation/#tlsssl-options","title":"TLS/SSL Options","text":"<p>KrakenHashes supports three TLS modes:</p> <ol> <li>self-signed (default) - Automatically generates self-signed certificates</li> <li>user-provided - Use your own certificates</li> <li>certbot - Automatically obtain Let's Encrypt certificates (tested and working)</li> </ol>"},{"location":"getting-started/installation/#using-your-own-certificates","title":"Using Your Own Certificates","text":"<pre><code>krakenhashes:\n    environment:\n        - TLS_MODE=user-provided\n    volumes:\n        - ./certs/server.crt:/etc/krakenhashes/certs/server.crt:ro\n        - ./certs/server.key:/etc/krakenhashes/certs/server.key:ro\n        - ./certs/ca.crt:/etc/krakenhashes/certs/ca.crt:ro # Optional\n</code></pre>"},{"location":"getting-started/installation/#using-lets-encrypt-certbot","title":"Using Let's Encrypt (Certbot)","text":"<p>Important Limitation</p> <p>Certbot cannot add IP addresses to certificates. You must access the system through the domain name for the certificate to be trusted. If you need IP access, use self-signed or user-provided certificates instead.</p> <pre><code>krakenhashes:\n    environment:\n        - TLS_MODE=certbot\n        - CERTBOT_EMAIL=admin@example.com\n        - CERTBOT_DOMAIN=krakenhashes.example.com\n</code></pre>"},{"location":"getting-started/installation/#data-persistence","title":"Data Persistence","text":"<p>Important directories that should be persisted:</p> <ul> <li><code>/var/lib/krakenhashes</code> - Application data (binaries, wordlists, rules, hashlists)</li> <li><code>/var/log/krakenhashes</code> - Application logs</li> <li>PostgreSQL data volume</li> </ul>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>DB_HOST</code> localhost PostgreSQL hostname <code>DB_PORT</code> 5432 PostgreSQL port <code>DB_NAME</code> krakenhashes Database name <code>DB_USER</code> krakenhashes Database user <code>DB_PASSWORD</code> krakenhashes Database password <code>JWT_SECRET</code> (random) JWT signing secret <code>TLS_MODE</code> self-signed TLS certificate mode <code>PUID</code> 1000 User ID for file permissions <code>PGID</code> 1000 Group ID for file permissions"},{"location":"getting-started/installation/#logging-configuration","title":"Logging Configuration","text":"<p>KrakenHashes provides comprehensive logging with configurable levels and component-specific debugging:</p>"},{"location":"getting-started/installation/#log-levels","title":"Log Levels","text":"<p>Set the <code>LOG_LEVEL</code> environment variable to control logging verbosity:</p> <ul> <li><code>DEBUG</code> - Detailed debugging information (verbose)</li> <li><code>INFO</code> - General information and status updates (default)</li> <li><code>WARNING</code> - Warning messages that need attention</li> <li><code>ERROR</code> - Error messages only</li> </ul>"},{"location":"getting-started/installation/#debug-flags","title":"Debug Flags","text":"<p>Enable component-specific debugging with these environment variables:</p> Flag Description <code>DEBUG_SQL</code> Log all SQL queries and parameters <code>DEBUG_HTTP</code> Log HTTP requests and responses <code>DEBUG_WEBSOCKET</code> Log WebSocket messages <code>DEBUG_AUTH</code> Log authentication attempts and JWT validation <code>DEBUG_JOBS</code> Log job processing and scheduling"},{"location":"getting-started/installation/#log-storage","title":"Log Storage","text":"<p>Logs are stored in the following directory structure:</p> <pre><code>$HOME/krakenhashes/logs/\n\u251c\u2500\u2500 backend/      # Backend application logs\n\u251c\u2500\u2500 frontend/     # Nginx access and error logs\n\u251c\u2500\u2500 nginx/        # Nginx configuration logs\n\u2514\u2500\u2500 postgres/     # PostgreSQL database logs\n</code></pre> <p>To view logs in real-time:</p> <pre><code># All logs\ndocker compose logs -f\n\n# Specific service\ndocker compose logs -f backend\n\n# Check for errors\ndocker compose logs | grep -i error\n</code></pre>"},{"location":"getting-started/installation/#production-best-practices","title":"Production Best Practices","text":"<ol> <li> <p>Security</p> <ul> <li>Always change default passwords</li> <li>Use a strong JWT_SECRET (minimum 32 characters)</li> <li>Use proper TLS certificates for production</li> <li>Restrict network access to necessary ports only</li> </ul> </li> <li> <p>Backup</p> <ul> <li>Regular PostgreSQL backups: <code>docker exec krakenhashes-postgres pg_dump -U krakenhashes krakenhashes &gt; backup.sql</code></li> <li>Backup the data volume: <code>/var/lib/krakenhashes</code></li> </ul> </li> <li> <p>Monitoring</p> <ul> <li>Monitor logs in <code>/var/log/krakenhashes</code></li> <li>Set up health checks for the application endpoints</li> <li>Monitor disk space for hash storage</li> </ul> </li> <li> <p>Updates</p> <pre><code># Pull latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Recreate container with new image\ndocker compose up -d --force-recreate krakenhashes\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#troubleshooting-production-issues","title":"Troubleshooting Production Issues","text":""},{"location":"getting-started/installation/#container-wont-start","title":"Container won't start","text":"<pre><code># Check logs\ndocker compose logs krakenhashes\n\n# Check if ports are already in use\nnetstat -tlnp | grep -E \"443|31337|1337|5432\"\n</code></pre>"},{"location":"getting-started/installation/#database-connection-issues","title":"Database connection issues","text":"<pre><code># Test database connectivity\ndocker exec krakenhashes-app nc -zv postgres 5432\n\n# Check database logs\ndocker compose logs postgres\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission issues","text":"<pre><code># Fix ownership (adjust PUID/PGID as needed)\ndocker exec krakenhashes-app chown -R 1000:1000 /var/lib/krakenhashes\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":""},{"location":"getting-started/installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+ and Docker Compose 2.0+</li> <li>Go 1.23.1+</li> <li>Node.js 20+</li> <li>Git</li> <li>8GB+ RAM recommended for development</li> </ul>"},{"location":"getting-started/installation/#development-setup-options","title":"Development Setup Options","text":""},{"location":"getting-started/installation/#option-1-docker-development-environment-recommended","title":"Option 1: Docker Development Environment (Recommended)","text":"<p>This setup provides hot-reloading for both backend and frontend.</p> <ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre> </li> <li> <p>Start development environment</p> <pre><code># Start all services with hot-reloading\ndocker compose -f docker compose.dev.yml up\n\n# Or run in background\ndocker compose -f docker compose.dev.yml up -d\n</code></pre> </li> <li> <p>Access the services</p> <ul> <li>Frontend: http://localhost:3000 (with hot-reload)</li> <li>Backend API: https://localhost:31337</li> <li>PostgreSQL: localhost:5432</li> </ul> </li> <li> <p>View logs</p> <pre><code># All services\ndocker compose -f docker compose.dev.yml logs -f\n\n# Specific service\ndocker compose -f docker compose.dev.yml logs -f backend\n</code></pre> </li> </ol> <p>The development environment features:</p> <ul> <li>Backend: Uses Air for Go hot-reloading</li> <li>Frontend: Uses React development server with hot-reload</li> <li>Database: PostgreSQL with persistent volume</li> <li>Volumes: Source code mounted for live updates</li> </ul>"},{"location":"getting-started/installation/#option-2-local-development-traditional","title":"Option 2: Local Development (Traditional)","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre> </li> <li> <p>Start PostgreSQL</p> <pre><code>cd backend\ndocker compose up -d\ncd ..\n</code></pre> </li> <li> <p>Run the backend</p> <pre><code>cd backend\ngo mod download\ngo run cmd/server/main.go\n</code></pre> </li> <li> <p>Run the frontend <pre><code>cd frontend\nnpm install\nnpm start\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#development-configuration","title":"Development Configuration","text":""},{"location":"getting-started/installation/#backend-development","title":"Backend Development","text":"<p>The backend uses Air for hot-reloading. Configuration is in <code>backend/.air.toml</code>:</p> <pre><code>[build]\n  cmd = \"go build -o ./tmp/main ./cmd/server\"\n  bin = \"./tmp/main\"\n  include_ext = [\"go\", \"tpl\", \"tmpl\", \"html\"]\n</code></pre> <p>Environment variables for development:</p> <pre><code>export DB_HOST=localhost\nexport DB_PORT=5432\nexport DB_NAME=krakenhashes\nexport DB_USER=krakenhashes\nexport DB_PASSWORD=krakenhashes\nexport JWT_SECRET=dev_jwt_secret\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"getting-started/installation/#frontend-development","title":"Frontend Development","text":"<p>The frontend uses Create React App with environment variables:</p> <pre><code>REACT_APP_API_URL=https://localhost:31337\nREACT_APP_WS_URL=wss://localhost:31337\nREACT_APP_DEBUG=true\n</code></pre>"},{"location":"getting-started/installation/#development-workflows","title":"Development Workflows","text":""},{"location":"getting-started/installation/#running-tests","title":"Running Tests","text":"<pre><code># Backend tests\ncd backend\ngo test ./...\ngo test -v ./internal/services\n\n# Frontend tests\ncd frontend\nnpm test\n</code></pre>"},{"location":"getting-started/installation/#building-for-production","title":"Building for Production","text":"<pre><code># Build production Docker image\ndocker build -f Dockerfile.prod -t krakenhashes:local .\n\n# Test production build locally\ndocker compose down\ndocker compose up -d\n</code></pre>"},{"location":"getting-started/installation/#database-migrations","title":"Database Migrations","text":"<pre><code># Apply migrations\ncd backend\nmake migrate-up\n\n# Rollback migrations\nmake migrate-down\n\n# Create new migration\nmake migrate-create name=add_new_table\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":""},{"location":"getting-started/installation/#logging","title":"Logging","text":"<p>View development logs with filtering:</p> <pre><code># Backend logs\ntail -f logs/backend.log | grep -i error\n\n# Frontend logs\ntail -f logs/frontend.log\n\n# All logs\ngrep -i error logs/*.log\n</code></pre>"},{"location":"getting-started/installation/#database-access","title":"Database Access","text":"<pre><code># Connect to development database\ndocker exec -it krakenhashes-postgres-dev psql -U krakenhashes -d krakenhashes\n\n# Quick query\ndocker exec krakenhashes-postgres-dev psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM users;\"\n</code></pre>"},{"location":"getting-started/installation/#switching-between-development-and-production","title":"Switching Between Development and Production","text":"<pre><code># Stop development environment\ndocker compose -f docker compose.dev.yml down\n\n# Start production environment\ndocker compose up -d\n\n# Switch back to development\ndocker compose down\ndocker compose -f docker compose.dev.yml up\n</code></pre>"},{"location":"getting-started/installation/#common-development-issues","title":"Common Development Issues","text":""},{"location":"getting-started/installation/#port-conflicts","title":"Port conflicts","text":"<pre><code># Check what's using the ports\nlsof -i :3000   # Frontend\nlsof -i :31337  # Backend HTTPS\nlsof -i :5432   # PostgreSQL\n</code></pre>"},{"location":"getting-started/installation/#go-module-issues","title":"Go module issues","text":"<pre><code># Clear module cache\ngo clean -modcache\n\n# Update dependencies\ngo mod tidy\ngo mod download\n</code></pre>"},{"location":"getting-started/installation/#frontend-dependency-issues","title":"Frontend dependency issues","text":"<pre><code># Clear npm cache\ncd frontend\nrm -rf node_modules package-lock.json\nnpm cache clean --force\nnpm install\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Review the configuration settings in your .env file</li> <li>Check the Admin Documentation for system administration</li> <li>See User Documentation for using KrakenHashes</li> <li>Join our community for support and updates</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#what-is-krakenhashes","title":"What is KrakenHashes?","text":"<p>KrakenHashes is a distributed password cracking management system that transforms the power of Hashcat into an enterprise-ready platform. While Hashcat excels as a command-line tool for password recovery, KrakenHashes adds the orchestration, management, and collaboration features needed for professional security teams.</p> <p>Think of KrakenHashes as the control center for your password auditing operations \u2013 coordinating multiple machines, managing vast collections of hashes, and providing actionable insights through an intuitive web interface.</p>"},{"location":"getting-started/overview/#why-krakenhashes","title":"Why KrakenHashes?","text":""},{"location":"getting-started/overview/#the-challenge-with-direct-hashcat-usage","title":"The Challenge with Direct Hashcat Usage","text":"<p>Running Hashcat directly presents several challenges for security teams:</p> <ul> <li>Single Machine Limitation: Hashcat runs on one machine at a time</li> <li>Manual Coordination: No built-in way to distribute work across multiple systems</li> <li>Limited Visibility: Command-line output makes it hard to track multiple jobs</li> <li>No Collaboration: Difficult for teams to share resources and results</li> <li>Manual Management: Wordlists, rules, and results require manual organization</li> </ul>"},{"location":"getting-started/overview/#the-krakenhashes-solution","title":"The KrakenHashes Solution","text":"<p>KrakenHashes addresses these limitations by providing:</p> <ul> <li>Distributed Processing: Coordinate multiple agents across your infrastructure</li> <li>Centralized Management: Single interface for all your cracking operations</li> <li>Team Collaboration: Share resources, results, and strategies</li> <li>Automated Workflows: Pre-configured attack patterns and job templates</li> <li>Enterprise Features: Authentication, authorization, and audit trails</li> </ul> <p> KrakenHashes main dashboard showing the hashlist overview table with crack progress and the jobs management interface with dark theme</p>"},{"location":"getting-started/overview/#key-features-and-capabilities","title":"Key Features and Capabilities","text":""},{"location":"getting-started/overview/#distributed-architecture","title":"\ud83d\ude80 Distributed Architecture","text":"<ul> <li> <p>Multi-Agent Support</p> <p>Deploy agents on multiple machines to leverage all available hardware</p> </li> <li> <p>Automatic Load Balancing</p> <p>Jobs are intelligently distributed based on agent capabilities</p> </li> <li> <p>Dynamic Scaling</p> <p>Add or remove agents without interrupting active jobs</p> </li> <li> <p>Resource Scheduling</p> <p>Configure when agents are available for processing</p> </li> </ul>"},{"location":"getting-started/overview/#job-management","title":"\ud83d\udcca Job Management","text":"<p>KrakenHashes transforms password cracking from ad-hoc commands into managed operations:</p> <ul> <li>Job Templates: Pre-configured attack strategies for common scenarios</li> <li>Workflows: Chain multiple attack types for comprehensive audits</li> <li>Priority System: Ensure critical jobs get resources first (0-1000 scale)</li> <li>Progress Tracking: Real-time visibility into job status and ETA</li> <li>Automatic Chunking: Large jobs are split for optimal distribution</li> </ul>"},{"location":"getting-started/overview/#resource-organization","title":"\ud83d\uddc2\ufe0f Resource Organization","text":"<p>Centralized management of all cracking resources:</p> <ul> <li>Wordlist Library: Upload once, use across all agents</li> <li>Rule Management: Organize and version your rule files</li> <li>Binary Versioning: Ensure all agents run compatible Hashcat versions</li> <li>Hash Organization: Group hashes by client, project, or engagement</li> <li>Result Storage: Automatic capture and organization of cracked passwords</li> </ul>"},{"location":"getting-started/overview/#security-compliance","title":"\ud83d\udd12 Security &amp; Compliance","text":"<p>Built with enterprise security requirements in mind:</p> <ul> <li>Multi-Factor Authentication: TOTP, email, and backup codes</li> <li>Role-Based Access: Admin and user roles with granular permissions</li> <li>TLS/SSL Support: Multiple certificate options including self-signed</li> <li>Audit Logging: Track all actions for compliance requirements</li> <li>Data Retention: Configurable policies for automatic cleanup</li> <li>Client Isolation: Keep customer data properly segregated</li> </ul>"},{"location":"getting-started/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Web Interface\"\n        UI[React Frontend]\n    end\n\n    subgraph \"Backend Services\"\n        API[REST API]\n        WS[WebSocket Hub]\n        SCH[Job Scheduler]\n        FS[File Storage]\n    end\n\n    subgraph \"Data Layer\"\n        PG[(PostgreSQL)]\n    end\n\n    subgraph \"Agent Pool\"\n        A1[Agent 1&lt;br/&gt;GPU: RTX 4090]\n        A2[Agent 2&lt;br/&gt;GPU: RTX 3080]\n        A3[Agent N&lt;br/&gt;CPU Only]\n    end\n\n    subgraph \"Hashcat\"\n        HC1[Hashcat Instance]\n        HC2[Hashcat Instance]\n        HC3[Hashcat Instance]\n    end\n\n    UI --&gt;|HTTPS| API\n    API --&gt; PG\n    API --&gt; FS\n    API --&gt; SCH\n    SCH --&gt; WS\n    WS &lt;--&gt;|WebSocket| A1\n    WS &lt;--&gt;|WebSocket| A2\n    WS &lt;--&gt;|WebSocket| A3\n    A1 --&gt; HC1\n    A2 --&gt; HC2\n    A3 --&gt; HC3</code></pre>"},{"location":"getting-started/overview/#component-breakdown","title":"Component Breakdown","text":"<p>Backend Service (Go)</p> <p>The brain of KrakenHashes, handling:</p> <ul> <li>RESTful API for all client operations</li> <li>WebSocket management for real-time agent communication</li> <li>Job scheduling with intelligent distribution algorithms</li> <li>File synchronization to ensure agents have required resources</li> <li>Database operations for persistent storage</li> </ul> <p>Agent System (Go)</p> <p>Distributed workers that:</p> <ul> <li>Execute Hashcat with optimized parameters</li> <li>Monitor hardware health (temperature, usage)</li> <li>Report progress in real-time</li> <li>Automatically sync required files from backend</li> <li>Support scheduling for resource availability</li> </ul> <p>Web Interface (React)</p> <p>Modern, responsive UI featuring:</p> <ul> <li>Real-time job monitoring dashboards</li> <li>Drag-and-drop file uploads</li> <li>Interactive result analysis</li> <li>Administrative controls</li> <li>Mobile-friendly responsive design</li> </ul> <p>PostgreSQL Database</p> <p>Reliable data storage for:</p> <ul> <li>User accounts and permissions</li> <li>Job definitions and results</li> <li>Hash lists and cracked passwords</li> <li>Agent registrations and metrics</li> <li>Audit logs and system configuration</li> </ul>"},{"location":"getting-started/overview/#typical-use-cases","title":"Typical Use Cases","text":""},{"location":"getting-started/overview/#penetration-testing","title":"\ud83d\udd0d Penetration Testing","text":"<p>During security assessments, teams often need to:</p> <ul> <li>Test password strength across multiple client environments</li> <li>Maintain separation between different client data</li> <li>Generate compliance reports showing password vulnerabilities</li> <li>Coordinate efforts across team members</li> </ul> <p>KrakenHashes Solution: Create separate clients for each engagement, assign team members appropriately, and use preset workflows to ensure consistent testing methodology.</p>"},{"location":"getting-started/overview/#incident-response","title":"\ud83d\udea8 Incident Response","text":"<p>When responding to security incidents:</p> <ul> <li>Time is critical \u2013 need results fast</li> <li>May have various hash formats from different systems</li> <li>Need to document the recovery process</li> <li>Multiple analysts working simultaneously</li> </ul> <p>KrakenHashes Solution: Use high-priority jobs to get critical results first, leverage distributed agents for speed, and maintain audit trails for evidence handling.</p>"},{"location":"getting-started/overview/#security-research","title":"\ud83d\udd2c Security Research","text":"<p>Researchers analyzing password security need:</p> <ul> <li>Consistent benchmarking across hash types</li> <li>Ability to test new attack methodologies</li> <li>Performance metrics and statistics</li> <li>Reproducible results</li> </ul> <p>KrakenHashes Solution: Built-in benchmarking system, custom workflow creation, detailed metrics collection, and comprehensive result export options.</p>"},{"location":"getting-started/overview/#corporate-security-audits","title":"\ud83c\udfe2 Corporate Security Audits","text":"<p>Internal security teams conducting password audits require:</p> <ul> <li>Regular testing of AD password dumps</li> <li>Tracking improvement over time</li> <li>Executive-friendly reporting</li> <li>Automated testing schedules</li> </ul> <p>KrakenHashes Solution: Scheduled jobs, historical tracking, client-based organization for different departments, and retention policies for compliance.</p>"},{"location":"getting-started/overview/#workflows-attack-strategy-made-simple","title":"Workflows: Attack Strategy Made Simple","text":"<p>One of KrakenHashes' most powerful features is the workflow system. Instead of manually running different Hashcat commands, you can define comprehensive attack strategies:</p>"},{"location":"getting-started/overview/#example-standard-password-audit-workflow","title":"Example: Standard Password Audit Workflow","text":"<pre><code>graph LR\n    A[Quick Wins] --&gt;|Found: 40%| B[Common Passwords]\n    B --&gt;|Found: 15%| C[Rule-Based Attacks]\n    C --&gt;|Found: 20%| D[Hybrid Attacks]\n    D --&gt;|Found: 10%| E[Brute Force]\n\n    style A fill:#90EE90\n    style B fill:#87CEEB\n    style C fill:#FFB6C1\n    style D fill:#FFE4B5\n    style E fill:#F0E68C</code></pre> <ol> <li>Quick Wins - Try the most common passwords first</li> <li>Common Passwords - Test against known breach compilations</li> <li>Rule-Based - Apply transformations to wordlists</li> <li>Hybrid Attacks - Combine wordlists with masks</li> <li>Brute Force - Last resort for remaining hashes</li> </ol>"},{"location":"getting-started/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"getting-started/overview/#data-protection","title":"\ud83d\udee1\ufe0f Data Protection","text":"<ul> <li>All agent communication is encrypted</li> <li>Passwords are never logged in plaintext</li> <li>Database connections use SSL/TLS</li> <li>File transfers are authenticated and encrypted</li> </ul>"},{"location":"getting-started/overview/#access-control","title":"\ud83d\udd10 Access Control","text":"<ul> <li>JWT-based authentication with refresh tokens</li> <li>Multi-factor authentication options</li> <li>Role-based permissions (Admin/User)</li> <li>Secure session management with token validation</li> </ul>"},{"location":"getting-started/overview/#compliance","title":"\ud83d\udcdd Compliance","text":"<ul> <li>Comprehensive audit logging</li> <li>Data retention policies</li> <li>Client data isolation</li> <li>Export capabilities for reporting</li> </ul>"},{"location":"getting-started/overview/#operational-security","title":"\u26a0\ufe0f Operational Security","text":"<p>Important Security Notes</p> <ul> <li>Deploy behind a firewall - never expose directly to internet</li> <li>Use strong passwords for all accounts</li> <li>Enable MFA for all users, especially administrators  </li> <li>Regularly review audit logs</li> <li>Keep agents and Hashcat binaries updated</li> <li>Use TLS certificates (self-signed minimum)</li> </ul>"},{"location":"getting-started/overview/#getting-started","title":"Getting Started","text":"<p>Ready to deploy KrakenHashes? Here's your path forward:</p> <ol> <li>Quick Start Guide - Get running in 5 minutes with Docker</li> <li>Installation Guide - Detailed setup for production</li> <li>First Crack Tutorial - Step-by-step first password recovery</li> <li>User Guide - Learn all features and capabilities</li> </ol>"},{"location":"getting-started/overview/#comparison-krakenhashes-vs-direct-hashcat","title":"Comparison: KrakenHashes vs Direct Hashcat","text":"Feature Hashcat KrakenHashes Setup Simple binary Docker deployment Interface Command line Web UI + API Multi-machine Manual coordination Automatic distribution Job Management Scripts/manual Database-backed queue Progress Tracking Terminal output Real-time dashboard Result Storage Text files Structured database Team Collaboration File sharing Built-in sharing Resource Management Manual copying Automatic sync User Management OS-level Application-level Audit Trail None Comprehensive Scheduling Cron jobs Built-in scheduler MFA Support N/A TOTP, Email, Backup codes"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand what KrakenHashes offers, choose your next step:</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in minutes with our Docker setup</p> </li> <li> <p> User Guide</p> <p>Deep dive into features and capabilities</p> </li> <li> <p> Security Setup</p> <p>Configure TLS and authentication for production</p> </li> <li> <p> Agent Deployment</p> <p>Learn how to deploy and manage agents</p> </li> </ul> <p>Version Note</p> <p>This documentation reflects KrakenHashes v0.1.0-alpha. As the project is under active development, features may change. Always refer to the GitHub repository for the latest updates.</p>"},{"location":"getting-started/quick-start/","title":"KrakenHashes Quick Start Guide","text":"<p>Get KrakenHashes up and running in under 5 minutes!</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 19.03.0+ and Docker Compose v2.0+ installed</li> <li>4GB RAM minimum</li> <li>Linux-based system recommended</li> </ul>"},{"location":"getting-started/quick-start/#verify-docker-compose-version","title":"Verify Docker Compose Version","text":"<pre><code># Check your version - should be 2.0 or higher\ndocker compose version\n\n# If you have the old docker-compose v1, you'll need to upgrade\n</code></pre> <p>\u26a0\ufe0f Important: This project requires Docker Compose v2 (plugin). The legacy <code>docker-compose</code> command will not work.</p>"},{"location":"getting-started/quick-start/#1-download-configuration-template","title":"1. Download configuration template","text":"<pre><code># Create a directory for KrakenHashes\nmkdir krakenhashes &amp;&amp; cd krakenhashes\n\n# Download the environment template\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/.env.example\ncp .env.example .env\n\n# Edit the .env file and change these at minimum:\n# - DB_PASSWORD (from default)\n# - JWT_SECRET (from default)\n# - PUID/PGID (to match your user: run 'id -u' and 'id -g')\n</code></pre>"},{"location":"getting-started/quick-start/#2-download-docker-composeyml","title":"2. Download docker-compose.yml","text":"<pre><code># Download the docker-compose configuration\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/docker-compose.yml\n</code></pre> <p>This downloads the latest production docker-compose configuration which includes:</p> <ul> <li>PostgreSQL with optimized memory settings</li> <li>HTTPS (443) and agent (31337) ports</li> <li>HTTP port (1337) for non-TLS connections</li> <li>Proper volume mounts for data persistence</li> <li>Network isolation</li> </ul>"},{"location":"getting-started/quick-start/#3-start-krakenhashes","title":"3. Start KrakenHashes","text":"<pre><code># Start the application\ndocker compose up -d\n\n# Wait for initialization (about 30 seconds)\ndocker compose logs -f krakenhashes\n</code></pre>"},{"location":"getting-started/quick-start/#4-access-the-application","title":"4. Access the Application","text":"<p>Open your browser and navigate to:</p> <ul> <li>https://localhost (redirects to port 443)</li> </ul> <p>Note: You'll see a certificate warning because we're using self-signed certificates. This is normal for local development.</p>"},{"location":"getting-started/quick-start/#5-first-login","title":"5. First Login","text":"<ol> <li>Log in with the default admin credentials:</li> <li>Username: <code>admin</code></li> <li> <p>Password: <code>KrakenHashes1!</code></p> </li> <li> <p>Important: Change the admin password immediately after first login for security</p> </li> </ol>"},{"location":"getting-started/quick-start/#6-quick-test","title":"6. Quick Test","text":"<ol> <li> <p>Upload hashcat binary (required first):</p> <ul> <li>Navigate to Admin \u2192 Binaries</li> <li>Click \"Upload Binary\"</li> <li>Upload your hashcat binary archive (download from https://hashcat.net/hashcat/)</li> <li>The archive contains binaries for all platforms (Linux, Windows, macOS)</li> </ul> <p>System Initialization</p> <p>After uploading your first binary, the system automatically: - Verifies the binary integrity - Creates the potfile preset job (if not already exists) - Makes the binary available to agents</p> </li> <li> <p>Upload a wordlist:</p> <ul> <li>Navigate to Admin \u2192 Wordlists</li> <li>Click \"Upload Wordlist\"</li> <li>Upload a small wordlist file (e.g., rockyou.txt or a test file)</li> </ul> </li> <li> <p>Create a test hashlist:</p> <ul> <li>Navigate to Hashlists</li> <li>Click \"Create Hashlist\"</li> <li>Add a few test hashes (e.g., MD5 hashes like <code>5f4dcc3b5aa765d61d8327deb882cf99</code> for \"password\")</li> </ul> </li> <li> <p>Create a job (when agents are connected):</p> <ul> <li>Navigate to your hashlist from the Hashlists page</li> <li>Click on your hashlist to view its details</li> <li>Click \"Create Job\" from the hashlist management page</li> <li>Choose a preset job template or configure custom settings</li> <li>Start the job</li> </ul> </li> </ol> <p>Note: Jobs require at least one connected agent to execute. Without agents, jobs will remain in pending status.</p>"},{"location":"getting-started/quick-start/#common-tasks","title":"Common Tasks","text":""},{"location":"getting-started/quick-start/#view-logs","title":"View Logs","text":"<p>Log files are stored in <code>$HOME/krakenhashes/logs/</code> by default. You can view them in several ways:</p> <pre><code># Live logs from Docker\ndocker compose logs -f\n\n# Backend logs only\ndocker compose logs -f krakenhashes\n\n# Check stored log files\ntail -f $HOME/krakenhashes/logs/backend/*.log\n\n# Check for errors across all logs\ngrep -i error $HOME/krakenhashes/logs/*/*.log\n</code></pre> <p>For debugging, you can enable verbose logging by setting <code>LOG_LEVEL=DEBUG</code> in your <code>.env</code> file.</p>"},{"location":"getting-started/quick-start/#stop-the-application","title":"Stop the Application","text":"<pre><code>docker compose down\n</code></pre>"},{"location":"getting-started/quick-start/#update-to-latest-version","title":"Update to Latest Version","text":"<pre><code># Pull latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Restart with new image\ndocker compose up -d\n</code></pre>"},{"location":"getting-started/quick-start/#backup-database","title":"Backup Database","text":"<pre><code>docker exec krakenhashes-postgres pg_dump -U krakenhashes krakenhashes &gt; backup.sql\n</code></pre>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#cannot-access-the-web-interface","title":"Cannot access the web interface","text":"<ol> <li>Check if containers are running: <code>docker compose ps</code></li> <li>Check logs for errors: <code>docker compose logs</code></li> <li>Ensure ports 443, 31337, and 1337 are not in use: <code>netstat -tlnp | grep -E \"443|31337|1337\"</code></li> </ol>"},{"location":"getting-started/quick-start/#database-connection-errors","title":"Database connection errors","text":"<ol> <li>Ensure PostgreSQL is healthy: <code>docker compose ps</code></li> <li>Check database logs: <code>docker compose logs postgres</code></li> <li>Verify environment variables match in both services</li> </ol>"},{"location":"getting-started/quick-start/#certificate-warnings","title":"Certificate warnings","text":"<p>This is normal with self-signed certificates. For production, see the Installation Guide for proper TLS setup.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>For Users: Read Understanding Jobs and Workflows</li> <li>For Admins: Review the full Installation Guide for production setup</li> <li>For Developers: See Development Setup</li> </ul>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Check the full documentation</li> <li>Report issues on GitHub</li> <li>Join our community chat (Discord): KrakenHashes</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Quick reference guides and technical specifications for KrakenHashes.</p>"},{"location":"reference/#in-this-section","title":"In This Section","text":"<ul> <li> <p> System Requirements</p> <p>Hardware requirements and memory configurations for all deployment scales</p> </li> <li> <p> PostgreSQL Tuning</p> <p>Database optimization for high-volume crack processing operations</p> </li> <li> <p> Environment Variables</p> <p>Complete list of all configuration options</p> </li> <li> <p> Database Schema</p> <p>Table structures and relationships</p> </li> <li> <p> Error Codes</p> <p>Error codes and their meanings</p> </li> <li> <p> Glossary</p> <p>Technical terms and concepts explained</p> </li> <li> <p> Architecture</p> <p>System architecture and implementation details</p> </li> </ul>"},{"location":"reference/#quick-links","title":"Quick Links","text":""},{"location":"reference/#system-configuration","title":"System Configuration","text":"<ul> <li>System Requirements by Tier</li> <li>PostgreSQL Memory Settings</li> <li>Applying Configuration Changes</li> <li>Performance Troubleshooting</li> </ul>"},{"location":"reference/#configuration-reference","title":"Configuration Reference","text":"<ul> <li>Backend Environment Variables</li> <li>Frontend Environment Variables</li> <li>Agent Environment Variables</li> <li>Docker Environment Variables</li> </ul>"},{"location":"reference/#database-reference","title":"Database Reference","text":"<ul> <li>Core Tables</li> <li>Relationships</li> <li>Indexes</li> <li>Migration History</li> </ul>"},{"location":"reference/#error-reference","title":"Error Reference","text":"<ul> <li>HTTP Status Codes</li> <li>Application Error Codes</li> <li>Agent Error Codes</li> <li>WebSocket Error Codes</li> </ul>"},{"location":"reference/#common-terms","title":"Common Terms","text":"<ul> <li>Password Cracking Terms</li> <li>System Architecture Terms</li> <li>Security Terms</li> <li>Performance Terms</li> </ul>"},{"location":"reference/#usage-tips","title":"Usage Tips","text":"<p>Finding Information</p> <ul> <li>Use <code>Ctrl+F</code> to search within pages</li> <li>Check the glossary for unfamiliar terms</li> <li>Environment variables are grouped by component</li> <li>Error codes include resolution steps</li> </ul>"},{"location":"reference/#contributing","title":"Contributing","text":"<p>Found an error or missing information? Please report it on GitHub.</p>"},{"location":"reference/database/","title":"KrakenHashes Database Schema Reference","text":"<p>This document provides a comprehensive reference for the KrakenHashes database schema, extracted from migration files (v0.1.0-alpha).</p>"},{"location":"reference/database/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Tables</li> <li>users</li> <li>teams</li> <li>user_teams</li> <li>Authentication &amp; Security</li> <li>auth_tokens</li> <li>user_passkeys</li> <li>pending_passkey_registration</li> <li>pending_passkey_authentication</li> <li>mfa_methods</li> <li>mfa_backup_codes</li> <li>login_attempts</li> <li>security_events</li> <li>Agent Management</li> <li>agents</li> <li>agent_metrics</li> <li>agent_teams</li> <li>claim_vouchers</li> <li>claim_voucher_usage</li> <li>Email System</li> <li>email_config</li> <li>email_templates</li> <li>email_usage</li> <li>Hash Management</li> <li>hashlists</li> <li>hashes</li> <li>hashcat_hash_types</li> <li>LM/NTLM Support</li> <li>lm_hash_metadata</li> <li>linked_hashlists</li> <li>linked_hashes</li> <li>Job Management</li> <li>job_workflows</li> <li>job_executions</li> <li>job_tasks</li> <li>job_increment_layers</li> <li>preset_increment_layers</li> <li>job_execution_settings</li> <li>Resource Management</li> <li>wordlists</li> <li>rules</li> <li>binary_versions</li> <li>Client &amp; Settings</li> <li>clients</li> <li>client_settings</li> <li>system_settings</li> <li>Performance &amp; Scheduling</li> <li>performance_metrics</li> <li>agent_scheduling</li> <li>Migration History</li> </ol>"},{"location":"reference/database/#core-tables","title":"Core Tables","text":""},{"location":"reference/database/#users","title":"users","text":"<p>User accounts for the system, including the special system user with UUID 00000000-0000-0000-0000-000000000000.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique user identifier username VARCHAR(255) UNIQUE NOT NULL Username for login first_name VARCHAR(255) User's first name last_name VARCHAR(255) User's last name email VARCHAR(255) UNIQUE NOT NULL User's email address password_hash VARCHAR(255) NOT NULL Bcrypt password hash role VARCHAR(50) NOT NULL, CHECK 'user' Role: user, admin, agent, system status VARCHAR(50) NOT NULL 'active' Account status created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Account creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_users_username (username) - idx_users_email (email) - idx_users_role (role)</p> <p>Triggers: - update_users_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#teams","title":"teams","text":"<p>Organizational teams for grouping users.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique team identifier name VARCHAR(100) NOT NULL, UNIQUE Team name description TEXT Team description created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Team creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_teams_name (name)</p> <p>Triggers: - update_teams_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#user_teams","title":"user_teams","text":"<p>Junction table for user-team relationships.</p> Column Type Constraints Default Description user_id UUID NOT NULL, FK \u2192 users(id) User reference team_id UUID NOT NULL, FK \u2192 teams(id) Team reference role VARCHAR(50) NOT NULL, CHECK 'member' Role in team: member, admin joined_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Join timestamp <p>Primary Key: (user_id, team_id)</p> <p>Indexes: - idx_user_teams_user_id (user_id) - idx_user_teams_team_id (team_id)</p>"},{"location":"reference/database/#authentication-security","title":"Authentication &amp; Security","text":""},{"location":"reference/database/#auth_tokens","title":"auth_tokens","text":"<p>Stores refresh tokens for JWT authentication.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Token identifier user_id UUID NOT NULL, FK \u2192 users(id) User reference token VARCHAR(255) NOT NULL, UNIQUE Refresh token value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Token creation time <p>Indexes: - idx_auth_tokens_token (token) - idx_auth_tokens_user_id (user_id)</p>"},{"location":"reference/database/#user_passkeys","title":"user_passkeys","text":"<p>Stores registered WebAuthn/FIDO2 passkey credentials for users.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Passkey identifier user_id UUID NOT NULL, FK \u2192 users(id) ON DELETE CASCADE User reference credential_id BYTEA NOT NULL, UNIQUE WebAuthn credential ID public_key BYTEA NOT NULL Public key for verification aaguid BYTEA Authenticator attestation GUID sign_count BIGINT NOT NULL 0 Sign counter for clone detection transports TEXT[] '{}' Supported transports (usb, nfc, ble, internal) name VARCHAR(255) NOT NULL 'Passkey' User-assigned passkey name backup_eligible BOOLEAN NOT NULL FALSE Passkey can be synced/backed up backup_state BOOLEAN NOT NULL FALSE Passkey is currently backed up created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Registration time last_used_at TIMESTAMP WITH TIME ZONE Last authentication time <p>Unique Constraint: (user_id, credential_id)</p> <p>Indexes: - idx_user_passkeys_user_id (user_id) - idx_user_passkeys_credential_id (credential_id)</p> <p>Security Features: - Clone Detection: Sign count must increase with each authentication; non-increasing counts indicate cloned authenticators - Backup Flags: Track whether passkey is synced across devices (Bitwarden, iCloud Keychain, etc.) - Phishing Resistant: Credentials are bound to the configured RP ID (domain)</p>"},{"location":"reference/database/#pending_passkey_registration","title":"pending_passkey_registration","text":"<p>Stores temporary challenges during passkey registration flow (5-minute expiry).</p> Column Type Constraints Default Description user_id UUID PRIMARY KEY, FK \u2192 users(id) ON DELETE CASCADE User registering passkey challenge BYTEA NOT NULL WebAuthn challenge bytes session_data BYTEA NOT NULL Serialized session state created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Challenge creation time <p>Notes: - Only one pending registration per user at a time - Challenges expire after 5 minutes - Cleanup trigger removes expired entries</p>"},{"location":"reference/database/#pending_passkey_authentication","title":"pending_passkey_authentication","text":"<p>Stores temporary challenges during passkey MFA authentication flow (5-minute expiry).</p> Column Type Constraints Default Description session_token TEXT PRIMARY KEY MFA session token user_id UUID NOT NULL, FK \u2192 users(id) ON DELETE CASCADE User authenticating challenge BYTEA NOT NULL WebAuthn challenge bytes session_data BYTEA NOT NULL Serialized session state created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Challenge creation time <p>Indexes: - idx_pending_passkey_auth_user_id (user_id)</p> <p>Notes: - Linked to MFA session token from login flow - Challenges expire after 5 minutes - Cleanup trigger removes expired entries</p> <p>Triggers: - trigger_cleanup_passkey_challenges: Cleans up expired registration and authentication challenges</p>"},{"location":"reference/database/#agent-management","title":"Agent Management","text":""},{"location":"reference/database/#agents","title":"agents","text":"<p>Registered compute agents for distributed processing.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Agent identifier name VARCHAR(255) NOT NULL Agent name status VARCHAR(50) NOT NULL 'inactive' Agent status last_heartbeat TIMESTAMP WITH TIME ZONE Last heartbeat received version VARCHAR(50) NOT NULL Agent version hardware JSONB NOT NULL Hardware configuration os_info JSONB NOT NULL '{}' Operating system info created_by_id UUID NOT NULL, FK \u2192 users(id) Creator user created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time api_key VARCHAR(64) UNIQUE Agent API key api_key_created_at TIMESTAMP WITH TIME ZONE API key creation time api_key_last_used TIMESTAMP WITH TIME ZONE API key last usage last_error TEXT Last error message metadata JSONB '{}' Additional metadata owner_id UUID FK \u2192 users(id) Agent owner (added in migration 30) extra_parameters TEXT Extra hashcat parameters (added in migration 30) is_enabled BOOLEAN NOT NULL true Agent enabled status (added in migration 31) <p>Indexes: - idx_agents_status (status) - idx_agents_created_by (created_by_id) - idx_agents_last_heartbeat (last_heartbeat) - idx_agents_api_key (api_key) - idx_agents_owner_id (owner_id)</p> <p>Triggers: - update_agents_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#agent_metrics","title":"agent_metrics","text":"<p>Time-series metrics data for agents.</p> Column Type Constraints Default Description agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference cpu_usage FLOAT NOT NULL CPU usage percentage gpu_utilization FLOAT NOT NULL GPU utilization percentage gpu_temp FLOAT NOT NULL GPU temperature memory_usage FLOAT NOT NULL Memory usage percentage gpu_metrics JSONB NOT NULL '{}' Additional GPU metrics timestamp TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Metric timestamp <p>Primary Key: (agent_id, timestamp)</p> <p>Indexes: - idx_agent_metrics_timestamp (timestamp)</p>"},{"location":"reference/database/#agent_teams","title":"agent_teams","text":"<p>Junction table for agent-team associations.</p> Column Type Constraints Default Description agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference team_id UUID NOT NULL, FK \u2192 teams(id) Team reference <p>Primary Key: (agent_id, team_id)</p>"},{"location":"reference/database/#claim_vouchers","title":"claim_vouchers","text":"<p>Stores active agent registration vouchers.</p> Column Type Constraints Default Description code VARCHAR(50) PRIMARY KEY Voucher code created_by_id UUID NOT NULL, FK \u2192 users(id) Creator user created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time is_continuous BOOLEAN NOT NULL false Can be used multiple times is_active BOOLEAN NOT NULL true Voucher active status used_at TIMESTAMP WITH TIME ZONE Usage timestamp used_by_agent_id INTEGER FK \u2192 agents(id) Agent that used voucher <p>Indexes: - idx_claim_vouchers_code (code) - idx_claim_vouchers_active (is_active) - idx_claim_vouchers_created_by (created_by_id)</p> <p>Triggers: - update_claim_vouchers_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#claim_voucher_usage","title":"claim_voucher_usage","text":"<p>Tracks usage attempts of claim vouchers.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Usage record ID voucher_code VARCHAR(50) NOT NULL, FK \u2192 claim_vouchers(code) Voucher reference attempted_by_id UUID NOT NULL, FK \u2192 users(id) User who attempted attempted_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Attempt timestamp success BOOLEAN NOT NULL false Success status ip_address VARCHAR(45) Client IP address user_agent TEXT Client user agent error_message TEXT Error message if failed <p>Indexes: - idx_claim_voucher_usage_voucher (voucher_code) - idx_claim_voucher_usage_attempted_by (attempted_by_id)</p>"},{"location":"reference/database/#email-system","title":"Email System","text":""},{"location":"reference/database/#email_config","title":"email_config","text":"<p>Email provider configuration.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Config ID provider_type email_provider_type NOT NULL Provider: mailgun, sendgrid, smtp (added in migration 084) api_key TEXT NOT NULL Provider API key additional_config JSONB Additional configuration monthly_limit INTEGER Monthly email limit reset_date TIMESTAMP WITH TIME ZONE Limit reset date is_active BOOLEAN NOT NULL false Active status created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last update time <p>Triggers: - update_email_config_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#email_templates","title":"email_templates","text":"<p>Email template definitions.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Template ID template_type email_template_type NOT NULL Type: security_event, job_completion, admin_error, mfa_code name VARCHAR(255) NOT NULL Template name subject VARCHAR(255) NOT NULL Email subject html_content TEXT NOT NULL HTML template text_content TEXT NOT NULL Plain text template created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last update time last_modified_by UUID FK \u2192 users(id) Last modifier <p>Indexes: - idx_email_templates_type (template_type)</p> <p>Triggers: - update_email_templates_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#email_usage","title":"email_usage","text":"<p>Tracks email usage for rate limiting.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Usage record ID month_year DATE NOT NULL, UNIQUE Month/year for tracking count INTEGER NOT NULL 0 Email count last_reset TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last reset time <p>Indexes: - idx_email_usage_month_year (month_year)</p>"},{"location":"reference/database/#hash-management","title":"Hash Management","text":""},{"location":"reference/database/#clients","title":"clients","text":"<p>Stores information about clients for whom hashlists are processed.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Client identifier name VARCHAR(255) NOT NULL, UNIQUE Client name description TEXT Client description contact_info TEXT Contact information created_at TIMESTAMPTZ NOT NULL NOW() Creation time updated_at TIMESTAMPTZ NOT NULL NOW() Last update time data_retention_months INT NULL Data retention policy (NULL = system default, 0 = keep forever) <p>Data Retention Notes: - <code>data_retention_months</code> overrides system default retention policy - NULL means use system default (<code>client_settings.default_data_retention_months</code>) - 0 means keep data forever (no automatic deletion) - Positive integers specify months to retain data after creation - When retention period expires, hashlists and associated data are securely deleted</p> <p>Indexes: - idx_clients_name (name)</p> <p>Triggers: - update_clients_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#hash_types","title":"hash_types","text":"<p>Stores information about supported hash types, keyed by hashcat mode ID.</p> Column Type Constraints Default Description id INT PRIMARY KEY Hashcat mode number name VARCHAR(255) NOT NULL Hash type name description TEXT Hash type description example TEXT Example hash needs_processing BOOLEAN NOT NULL FALSE Requires preprocessing processing_logic JSONB Processing rules as JSON is_enabled BOOLEAN NOT NULL TRUE Hash type enabled slow BOOLEAN NOT NULL FALSE Slow hash algorithm"},{"location":"reference/database/#hashlists","title":"hashlists","text":"<p>Stores metadata about uploaded hash lists.</p> Column Type Constraints Default Description id BIGSERIAL PRIMARY KEY Hashlist identifier name VARCHAR(255) NOT NULL Hashlist name user_id UUID NOT NULL, FK \u2192 users(id) Owner user client_id UUID FK \u2192 clients(id) Associated client hash_type_id INT NOT NULL, FK \u2192 hash_types(id) Hash type file_path VARCHAR(1024) File storage path total_hashes INT NOT NULL 0 Total hash count cracked_hashes INT NOT NULL 0 Cracked hash count created_at TIMESTAMPTZ NOT NULL NOW() Creation time updated_at TIMESTAMPTZ NOT NULL NOW() Last update time status TEXT NOT NULL, CHECK Status: uploading, processing, ready, error error_message TEXT Error details <p>Retention &amp; Deletion Behavior: - Deletion is CASCADE - removing a hashlist deletes:   - All associations in <code>hashlist_hashes</code>   - Related <code>agent_hashlists</code> entries   - Related <code>job_executions</code> and their <code>job_tasks</code> - File at <code>file_path</code> is securely overwritten with random data before deletion - Orphaned hashes (not linked to any other hashlist) are automatically deleted - VACUUM ANALYZE runs after deletion to prevent WAL recovery</p> <p>Indexes: - idx_hashlists_user_id (user_id) - idx_hashlists_client_id (client_id) - idx_hashlists_hash_type_id (hash_type_id) - idx_hashlists_status (status)</p> <p>Triggers: - update_hashlists_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#hashes","title":"hashes","text":"<p>Stores individual hash entries.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Hash identifier hash_value TEXT NOT NULL Hash value original_hash TEXT Original hash if processed username TEXT Associated username hash_type_id INT NOT NULL, FK \u2192 hash_types(id) Hash type is_cracked BOOLEAN NOT NULL FALSE Crack status password TEXT Cracked password last_updated TIMESTAMPTZ NOT NULL NOW() Last update time cracked_by_task_id UUID FK \u2192 job_tasks(id) ON DELETE SET NULL Task that cracked this hash (added in migration 098) <p>Indexes: - idx_hashes_hash_value (hash_value) - idx_hashes_original_hash_unique (original_hash) UNIQUE - Fast deduplication during bulk import (added in migration 096) - idx_hashes_cracked_by_task_id (cracked_by_task_id) WHERE cracked_by_task_id IS NOT NULL - Crack attribution lookup (added in migration 098)</p> <p>Triggers: - update_hashes_last_updated: Updates last_updated on row modification</p>"},{"location":"reference/database/#hashlist_hashes","title":"hashlist_hashes","text":"<p>Junction table for the many-to-many relationship between hashlists and hashes.</p> Column Type Constraints Default Description hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference hash_id UUID NOT NULL, FK \u2192 hashes(id) Hash reference <p>Primary Key: (hashlist_id, hash_id)</p> <p>Indexes: - idx_hashlist_hashes_hashlist_id (hashlist_id) - idx_hashlist_hashes_hash_id (hash_id)</p>"},{"location":"reference/database/#hashcat_hash_types","title":"hashcat_hash_types","text":"<p>Stores hashcat-specific hash type information (added in migration 16).</p> Column Type Constraints Default Description mode INT PRIMARY KEY Hashcat mode number name VARCHAR(255) NOT NULL Hash type name category VARCHAR(100) Hash category slow_hash BOOLEAN FALSE Is slow hash password_length_min INT Minimum password length password_length_max INT Maximum password length supports_brain BOOLEAN FALSE Supports brain feature example_hash_format TEXT Example hash format benchmark_mask VARCHAR(255) Benchmark mask benchmark_charset1 VARCHAR(255) Benchmark charset 1 autodetect_regex TEXT Regex for autodetection potfile_regex TEXT Regex for potfile parsing test_hash TEXT Test hash value test_password VARCHAR(255) Test password valid_hash_regex TEXT Valid hash format regex"},{"location":"reference/database/#lmntlm-support-v121","title":"LM/NTLM Support (v1.2.1+)","text":""},{"location":"reference/database/#lm_hash_metadata","title":"lm_hash_metadata","text":"<p>Tracks partial crack status for LM hashes (hash type 3000). This table is only populated for LM hashes and has zero impact on other hash types.</p> <p>Purpose: LM hashes consist of two 7-character halves that can be cracked independently. This table tracks the crack status of each half and stores the password fragments.</p> Column Type Constraints Default Description hash_id UUID PRIMARY KEY, FK \u2192 hashes(id) Reference to parent hash record first_half_cracked BOOLEAN NOT NULL FALSE True if first 16 chars of LM hash cracked second_half_cracked BOOLEAN NOT NULL FALSE True if last 16 chars of LM hash cracked first_half_password VARCHAR(7) NULL Password for first half (max 7 chars) second_half_password VARCHAR(7) NULL Password for second half (max 7 chars) created_at TIMESTAMP NOT NULL CURRENT_TIMESTAMP Record creation timestamp updated_at TIMESTAMP NOT NULL CURRENT_TIMESTAMP Last update timestamp <p>Indexes: - <code>PRIMARY KEY (hash_id)</code> - <code>idx_lm_metadata_crack_status (first_half_cracked, second_half_cracked)</code> - Fast partial crack queries - <code>idx_lm_metadata_hash_id (hash_id)</code> - Foreign key lookup</p> <p>ON DELETE: CASCADE - When parent hash is deleted, metadata is automatically removed</p> <p>Use Cases: - Track partial crack progress: \"First half cracked, second half pending\" - Analytics: Count of partially cracked LM hashes - Strategic intelligence: Keyspace reduction from 95^14 to 95^7 when one half known - LM-to-NTLM mask generation from partial crack patterns</p> <p>Example Query - Find Partial Cracks: <pre><code>SELECT h.username, h.domain,\n       lm.first_half_password, lm.second_half_password\nFROM lm_hash_metadata lm\nINNER JOIN hashes h ON lm.hash_id = h.id\nWHERE (lm.first_half_cracked OR lm.second_half_cracked)\n  AND NOT (lm.first_half_cracked AND lm.second_half_cracked);\n</code></pre></p>"},{"location":"reference/database/#linked_hashlists","title":"linked_hashlists","text":"<p>Manages relationships between entire hashlists (e.g., LM hashlist \u2194 NTLM hashlist from same pwdump file).</p> <p>Purpose: Track which hashlists are related to enable analytics calculations and proper hashlist counting.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique link identifier hashlist_id_1 BIGINT NOT NULL, FK \u2192 hashlists(id) First hashlist in relationship hashlist_id_2 BIGINT NOT NULL, FK \u2192 hashlists(id) Second hashlist in relationship link_type VARCHAR(50) NOT NULL Type of relationship (e.g., 'lm_ntlm') created_at TIMESTAMP NOT NULL CURRENT_TIMESTAMP Link creation timestamp <p>Constraints: - <code>UNIQUE (hashlist_id_1, hashlist_id_2)</code> - Prevents duplicate links - <code>CHECK (hashlist_id_1 != hashlist_id_2)</code> - Prevents self-linking</p> <p>Indexes: - <code>PRIMARY KEY (id)</code> - <code>idx_linked_hashlists_id2 (hashlist_id_2)</code> - Bidirectional lookup - <code>idx_linked_hashlists_type (link_type)</code> - Filter by link type</p> <p>ON DELETE: CASCADE - When either hashlist is deleted, link is automatically removed</p> <p>Link Types: - <code>lm_ntlm</code>: LM and NTLM hashlists from same pwdump file</p> <p>Use Cases: - Analytics: Count linked pairs as ONE hashlist (prevent double-counting) - Determine when to create individual hash-to-hash links - Track which hashlists were created together</p> <p>Example Query - Find Linked Hashlists: <pre><code>SELECT hl1.name AS lm_hashlist, hl2.name AS ntlm_hashlist\nFROM linked_hashlists lh\nINNER JOIN hashlists hl1 ON lh.hashlist_id_1 = hl1.id\nINNER JOIN hashlists hl2 ON lh.hashlist_id_2 = hl2.id\nWHERE lh.link_type = 'lm_ntlm';\n</code></pre></p>"},{"location":"reference/database/#linked_hashes","title":"linked_hashes","text":"<p>Manages relationships between individual hash records (e.g., specific LM hash \u2194 specific NTLM hash for same user).</p> <p>Purpose: Enable correlation analysis showing which users have both LM and NTLM hashes cracked, partially cracked, or uncracked.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique link identifier hash_id_1 UUID NOT NULL, FK \u2192 hashes(id) First hash in relationship (typically LM) hash_id_2 UUID NOT NULL, FK \u2192 hashes(id) Second hash in relationship (typically NTLM) link_type VARCHAR(50) NOT NULL Type of relationship (e.g., 'lm_ntlm') created_at TIMESTAMP NOT NULL CURRENT_TIMESTAMP Link creation timestamp <p>Constraints: - <code>UNIQUE (hash_id_1, hash_id_2)</code> - Prevents duplicate links - <code>CHECK (hash_id_1 != hash_id_2)</code> - Prevents self-linking</p> <p>Indexes: - <code>PRIMARY KEY (id)</code> - <code>idx_linked_hashes_id2 (hash_id_2)</code> - Bidirectional lookup - <code>idx_linked_hashes_type (link_type)</code> - Filter by link type</p> <p>ON DELETE: CASCADE - When either hash is deleted, link is automatically removed</p> <p>Link Types: - <code>lm_ntlm</code>: LM and NTLM hashes for same username/domain</p> <p>Linking Strategy: Links are created by matching <code>username</code> and <code>domain</code> columns in the <code>hashes</code> table. This approach handles: - Domain migrations (links persist across RID changes) - Account renames (links updated if username changes) - Multi-domain environments (links only within same domain)</p> <p>Use Cases: - Analytics: \"Linked Hash Correlation\" statistics - Show: \"Administrator's LM cracked but NTLM still unknown\" - Domain-filtered correlation analysis - Identify high-value targets (both hashes cracked = full compromise)</p> <p>Example Query - Correlation Statistics: <pre><code>SELECT\n    COUNT(*) AS total_pairs,\n    COUNT(CASE WHEN lm.is_cracked AND ntlm.is_cracked THEN 1 END) AS both_cracked,\n    COUNT(CASE WHEN NOT lm.is_cracked AND ntlm.is_cracked THEN 1 END) AS only_ntlm,\n    COUNT(CASE WHEN lm.is_cracked AND NOT ntlm.is_cracked THEN 1 END) AS only_lm,\n    COUNT(CASE WHEN NOT lm.is_cracked AND NOT ntlm.is_cracked THEN 1 END) AS neither\nFROM linked_hashes lh\nINNER JOIN hashes lm ON lh.hash_id_1 = lm.id\nINNER JOIN hashes ntlm ON lh.hash_id_2 = ntlm.id\nWHERE lh.link_type = 'lm_ntlm';\n</code></pre></p>"},{"location":"reference/database/#job-management","title":"Job Management","text":""},{"location":"reference/database/#preset_jobs","title":"preset_jobs","text":"<p>Stores predefined job configurations.</p> Column Type Constraints Default Description id UUID PRIMARY KEY uuid_generate_v4() Job identifier name TEXT UNIQUE NOT NULL Job name wordlist_ids JSONB NOT NULL '[]' Array of wordlist IDs rule_ids JSONB NOT NULL '[]' Array of rule IDs attack_mode INTEGER NOT NULL, CHECK 0 Attack mode: 0,1,3,6,7,9 priority INTEGER NOT NULL Job priority chunk_size_seconds INTEGER NOT NULL Chunk duration status_updates_enabled BOOLEAN NOT NULL true Enable status updates is_small_job BOOLEAN NOT NULL false Small job flag allow_high_priority_override BOOLEAN NOT NULL false Allows this job to interrupt lower priority running jobs when no agents available binary_version_id INTEGER NOT NULL, FK \u2192 binary_versions(id) Binary version mask TEXT NULL Mask pattern created_at TIMESTAMPTZ NOW() Creation time updated_at TIMESTAMPTZ NOW() Last update time keyspace_limit BIGINT Keyspace limit (added in migration 32) max_agents INTEGER Max agents allowed (added in migration 32) <p>Triggers: - update_preset_jobs_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_workflows","title":"job_workflows","text":"<p>Stores workflow definitions for multi-step attacks.</p> Column Type Constraints Default Description id UUID PRIMARY KEY uuid_generate_v4() Workflow identifier name TEXT UNIQUE NOT NULL Workflow name created_at TIMESTAMPTZ NOW() Creation time updated_at TIMESTAMPTZ NOW() Last update time <p>Triggers: - update_job_workflows_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_workflow_steps","title":"job_workflow_steps","text":"<p>Defines steps within a workflow.</p> Column Type Constraints Default Description id BIGSERIAL PRIMARY KEY Step identifier job_workflow_id UUID NOT NULL, FK \u2192 job_workflows(id) Workflow reference preset_job_id UUID NOT NULL, FK \u2192 preset_jobs(id) Preset job reference step_order INTEGER NOT NULL Execution order <p>Unique Constraint: (job_workflow_id, step_order)</p> <p>Indexes: - idx_job_workflow_steps_job_workflow_id (job_workflow_id) - idx_job_workflow_steps_preset_job_id (preset_job_id)</p>"},{"location":"reference/database/#job_executions","title":"job_executions","text":"<p>Tracks actual job runs.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Execution identifier preset_job_id UUID NOT NULL, FK \u2192 preset_jobs(id) Preset job reference hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference status VARCHAR(50) NOT NULL, CHECK 'pending' Status: pending, running, paused, processing, completed, failed, cancelled (added migration 085: processing status) priority INT NOT NULL 0 Execution priority total_keyspace BIGINT Total keyspace size processed_keyspace BIGINT 0 Processed keyspace attack_mode INT NOT NULL Attack mode created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time started_at TIMESTAMP WITH TIME ZONE Start time completed_at TIMESTAMP WITH TIME ZONE Completion time error_message TEXT Error details interrupted_by UUID FK \u2192 job_executions(id) ID of the higher priority job that interrupted this one created_by UUID FK \u2192 users(id) Creator user (added in migration 33) chunk_size INTEGER Chunk size override (added in migration 34) chunk_overlap INTEGER 0 Chunk overlap (added in migration 34) dispatched_keyspace BIGINT 0 Dispatched keyspace (added in migration 40) progress NUMERIC(6,3) 0 Progress percentage (added in migration 36, updated in migration 38) consecutive_failures INTEGER 0 Consecutive failure count (added in migration 37) last_failure_at TIMESTAMP WITH TIME ZONE Last failure time (added in migration 37) is_accurate_keyspace BOOLEAN false True when keyspace is from hashcat progress[1] values (added in migration 63) avg_rule_multiplier FLOAT Actual/estimated keyspace ratio for improving future estimates (added in migration 63) completion_email_sent BOOLEAN false Whether completion email was sent (added in migration 085) completion_email_sent_at TIMESTAMP WITH TIME ZONE When completion email was sent (added in migration 085) completion_email_error TEXT Error message if email sending failed (added in migration 085) cracking_completed_at TIMESTAMP WITH TIME ZONE When all tasks finished hashcat processing - job enters processing state (added in migration 100) <p>Indexes: - idx_job_executions_status (status) - idx_job_executions_priority (priority, created_at) - idx_job_executions_created_by (created_by) - idx_job_executions_consecutive_failures (consecutive_failures)</p>"},{"location":"reference/database/#job_tasks","title":"job_tasks","text":"<p>Individual chunks assigned to agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Task identifier job_execution_id UUID NOT NULL, FK \u2192 job_executions(id) Job execution reference agent_id INTEGER FK \u2192 agents(id) Assigned agent (nullable in migration 35) status VARCHAR(50) NOT NULL, CHECK 'pending' Status: pending, assigned, reconnect_pending, running, processing, completed, failed, cancelled (added migration 085: processing status) keyspace_start BIGINT NOT NULL Keyspace start keyspace_end BIGINT NOT NULL Keyspace end keyspace_processed BIGINT 0 Processed amount benchmark_speed BIGINT Hashes per second chunk_duration INT NOT NULL Duration in seconds assigned_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Assignment time started_at TIMESTAMP WITH TIME ZONE Start time completed_at TIMESTAMP WITH TIME ZONE Completion time last_checkpoint TIMESTAMP WITH TIME ZONE Last checkpoint error_message TEXT Error details created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time (added in migration 25) updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time (added in migration 26) progress NUMERIC(6,3) 0 Progress percentage (added in migration 36, updated in migration 38) consecutive_failures INTEGER 0 Consecutive failure count (added in migration 37) last_failure_at TIMESTAMP WITH TIME ZONE Last failure time (added in migration 37) chunk_number INTEGER Chunk number for rule splits (added in migration 44) effective_keyspace BIGINT Effective keyspace size (added in migration 47) is_actual_keyspace BOOLEAN false True when task has actual keyspace from hashcat progress[1] (added in migration 63) chunk_actual_keyspace BIGINT Immutable chunk size from hashcat progress[1] for accurate keyspace tracking (added in migration 64) crack_count INTEGER 0 Number of hashes cracked by this task (existing field) expected_crack_count INTEGER 0 Expected number of cracks from final progress message (added in migration 085) received_crack_count INTEGER 0 Number of cracks received via crack_batch messages (added in migration 085) batches_complete_signaled BOOLEAN false Whether agent has signaled all crack batches sent (added in migration 085) increment_layer_id UUID FK \u2192 job_increment_layers(id) References increment layer for increment mode jobs (added in migration 089) cracking_completed_at TIMESTAMP WITH TIME ZONE When hashcat finished for this task - task enters processing state (added in migration 100) retransmit_count INTEGER 0 Number of crack retransmission attempts (added in migration 099) last_retransmit_at TIMESTAMP WITH TIME ZONE Timestamp of last retransmission request (added in migration 099) <p>Indexes: - idx_job_tasks_agent_status (agent_id, status) - idx_job_tasks_execution (job_execution_id) - idx_job_tasks_consecutive_failures (consecutive_failures) - idx_job_tasks_chunk_number (job_execution_id, chunk_number) - idx_job_tasks_increment_layer (increment_layer_id) - added in migration 089 - idx_job_tasks_cracking_completed_at (cracking_completed_at) WHERE cracking_completed_at IS NOT NULL - Efficient completion state queries (added in migration 100)</p> <p>Triggers: - update_job_tasks_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_increment_layers","title":"job_increment_layers","text":"<p>Sub-layers for increment mode jobs. Each layer represents one mask length in the increment sequence. Added in migration 088.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Layer identifier job_execution_id UUID NOT NULL, FK \u2192 job_executions(id) ON DELETE CASCADE Parent job execution layer_index INT NOT NULL Order in sequence (1=first) mask VARCHAR(255) NOT NULL Layer-specific mask (e.g., <code>?l?l</code>) status VARCHAR(50) NOT NULL, CHECK 'pending' Status: pending, running, completed, failed, cancelled base_keyspace BIGINT Estimated keyspace from --keyspace effective_keyspace BIGINT Actual keyspace from benchmark processed_keyspace BIGINT 0 Completed keyspace dispatched_keyspace BIGINT 0 Assigned keyspace is_accurate_keyspace BOOLEAN FALSE TRUE after benchmark provides actual keyspace overall_progress_percent NUMERIC(5,2) 0.00 Layer completion percentage created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time started_at TIMESTAMP WITH TIME ZONE Layer start time completed_at TIMESTAMP WITH TIME ZONE Layer completion time error_message TEXT Error details if failed <p>Unique Constraint: (job_execution_id, layer_index)</p> <p>Indexes: - idx_job_increment_layers_execution (job_execution_id) - idx_job_increment_layers_status (status)</p> <p>Triggers: - update_job_increment_layers_updated_at: Updates updated_at on row modification</p> <p>Purpose: - Decomposes increment mode jobs into discrete layers for distributed processing - Each layer can be scheduled and tracked independently - Multiple agents can work on different layers simultaneously - Provides granular progress tracking per mask length</p> <p>Use Cases: - Track progress for each mask length in an increment mode attack - Enable parallel processing of different mask lengths - Provide detailed status per layer in the UI</p> <p>Example Query - Layer Progress: <pre><code>SELECT layer_index, mask, status,\n       overall_progress_percent,\n       processed_keyspace, effective_keyspace\nFROM job_increment_layers\nWHERE job_execution_id = 'uuid-here'\nORDER BY layer_index;\n</code></pre></p> <p>See Increment Mode Architecture for implementation details.</p>"},{"location":"reference/database/#preset_increment_layers","title":"preset_increment_layers","text":"<p>Pre-calculated increment layers for preset jobs. When a job is created from a preset with increment mode enabled, these layers are copied to <code>job_increment_layers</code>. Added in migration 090.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Layer identifier preset_job_id UUID NOT NULL, FK \u2192 preset_jobs(id) ON DELETE CASCADE Parent preset job layer_index INT NOT NULL Order in sequence (1=first) mask VARCHAR(512) NOT NULL Layer-specific mask (e.g., <code>?l?l</code>) base_keyspace BIGINT Estimated keyspace from --keyspace effective_keyspace BIGINT Calculated keyspace created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (preset_job_id, layer_index)</p> <p>Indexes: - idx_preset_increment_layers_preset_job_id (preset_job_id)</p> <p>Triggers: - update_preset_increment_layers_updated_at: Updates updated_at on row modification</p> <p>Purpose: - Pre-calculate layers at preset creation time rather than job creation time - Ensures consistent keyspace calculations across all jobs created from the same preset - Faster job creation (no need to re-run hashcat --keyspace for each layer) - Preset keyspace = sum of all layer effective_keyspaces</p> <p>Data Flow: 1. Admin creates preset job with increment mode \u2192 <code>preset_increment_layers</code> populated 2. User creates job from preset \u2192 layers copied from <code>preset_increment_layers</code> to <code>job_increment_layers</code> 3. Job inherits preset's total keyspace</p> <p>See Increment Mode Architecture for implementation details.</p>"},{"location":"reference/database/#job_execution_settings","title":"job_execution_settings","text":"<p>Settings for job executions (added in migration 21).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Settings ID name VARCHAR(255) NOT NULL, UNIQUE Setting name value TEXT NOT NULL Setting value description TEXT Setting description data_type VARCHAR(50) NOT NULL 'string' Data type created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_job_execution_settings_name (name)</p> <p>Triggers: - update_job_execution_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#resource-management","title":"Resource Management","text":""},{"location":"reference/database/#binary_versions","title":"binary_versions","text":"<p>Stores information about different versions of hash cracking binaries. Supports both URL downloads and direct uploads.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Version ID binary_type binary_type NOT NULL Type: hashcat, john compression_type compression_type NOT NULL Compression: 7z, zip, tar.gz, tar.xz source_type VARCHAR(50) NOT NULL 'url' Source type: 'url' or 'upload' source_url TEXT Download URL (NULL for uploads) file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes version VARCHAR(100) Version string (e.g., \"6.2.6\", \"7.1.2+338\") description TEXT Human-readable description created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user is_active BOOLEAN true Active status is_default BOOLEAN false Whether this is the default version last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed, deleted <p>Indexes: - idx_binary_versions_type_active (binary_type) WHERE is_active = true - idx_binary_versions_verification (verification_status)</p>"},{"location":"reference/database/#binary_version_audit_log","title":"binary_version_audit_log","text":"<p>Tracks all changes and actions performed on binary versions.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID binary_version_id INTEGER NOT NULL, FK \u2192 binary_versions(id) Binary version reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_binary_version_audit_binary_id (binary_version_id) - idx_binary_version_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#wordlists","title":"wordlists","text":"<p>Stores information about wordlists used for password cracking.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Wordlist ID name VARCHAR(255) NOT NULL Wordlist name description TEXT Description wordlist_type wordlist_type NOT NULL Type: general, specialized, targeted, custom format wordlist_format NOT NULL 'plaintext' Format: plaintext, compressed file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes word_count BIGINT Number of words created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed <p>Indexes: - idx_wordlists_name (name) - idx_wordlists_type (wordlist_type) - idx_wordlists_verification (verification_status) - idx_wordlists_md5 (md5_hash)</p>"},{"location":"reference/database/#wordlist_audit_log","title":"wordlist_audit_log","text":"<p>Tracks all changes and actions performed on wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_wordlist_audit_wordlist_id (wordlist_id) - idx_wordlist_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#wordlist_tags","title":"wordlist_tags","text":"<p>Stores tags associated with wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Tag ID wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference tag VARCHAR(50) NOT NULL Tag value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user <p>Unique Index: idx_wordlist_tags_unique (wordlist_id, tag)</p> <p>Indexes: - idx_wordlist_tags_tag (tag)</p>"},{"location":"reference/database/#rules","title":"rules","text":"<p>Stores information about rules used for password cracking.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Rule ID name VARCHAR(255) NOT NULL Rule name description TEXT Description rule_type rule_type NOT NULL Type: hashcat, john file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes rule_count INTEGER Number of rules created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed estimated_keyspace_multiplier FLOAT Keyspace multiplier estimate <p>Indexes: - idx_rules_name (name) - idx_rules_type (rule_type) - idx_rules_verification (verification_status) - idx_rules_md5 (md5_hash)</p>"},{"location":"reference/database/#rule_audit_log","title":"rule_audit_log","text":"<p>Tracks all changes and actions performed on rules.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_rule_audit_rule_id (rule_id) - idx_rule_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#rule_tags","title":"rule_tags","text":"<p>Stores tags associated with rules.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Tag ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference tag VARCHAR(50) NOT NULL Tag value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user <p>Unique Index: idx_rule_tags_unique (rule_id, tag)</p> <p>Indexes: - idx_rule_tags_tag (tag)</p>"},{"location":"reference/database/#rule_wordlist_compatibility","title":"rule_wordlist_compatibility","text":"<p>Stores compatibility information between rules and wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Compatibility ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference compatibility_score FLOAT NOT NULL 1.0 Score from 0.0 to 1.0 notes TEXT Compatibility notes created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater <p>Unique Index: idx_rule_wordlist_unique (rule_id, wordlist_id)</p> <p>Indexes: - idx_rule_wordlist_rule (rule_id) - idx_rule_wordlist_wordlist (wordlist_id)</p>"},{"location":"reference/database/#client-settings","title":"Client &amp; Settings","text":""},{"location":"reference/database/#client_settings","title":"client_settings","text":"<p>Stores client-specific settings (added in migration 17). Also used for system-wide settings without a client_id.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Settings ID client_id UUID NOT NULL, FK \u2192 clients(id) Client reference key VARCHAR(255) NOT NULL Setting key value TEXT Setting value data_type VARCHAR(50) NOT NULL 'string' Data type created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Important System-Wide Settings: - <code>default_data_retention_months</code> - Default retention period for all hashlists (when client_id is NULL) - <code>last_purge_run</code> - Timestamp of last retention purge execution</p> <p>Unique Constraint: (client_id, key)</p> <p>Indexes: - idx_client_settings_client (client_id)</p> <p>Triggers: - update_client_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#system_settings","title":"system_settings","text":"<p>Stores global system-wide settings.</p> Column Type Constraints Default Description key VARCHAR(255) PRIMARY KEY Setting key value TEXT Setting value description TEXT Setting description data_type VARCHAR(50) NOT NULL 'string' Data type: string, integer, boolean, float updated_at TIMESTAMPTZ NOT NULL NOW() Last update time <p>Default Settings: - max_job_priority: 1000 (integer) - agent_scheduling_enabled: false (boolean) - added in migration 42 - hashcat_speedtest_timeout: 300 (integer) - added in migration 39 - task_heartbeat_timeout: 300 (integer) - added in migration 46 - agent_overflow_allocation_mode: 'fifo' (string) - added in migration 82   - Values: 'fifo' (default) or 'round_robin'   - Controls how overflow agents are distributed among same-priority jobs   - FIFO: Oldest job gets all overflow agents   - Round-robin: Distribute evenly across all jobs - hashlist_bulk_batch_size: 100000 (integer) - added in migration 097   - Number of hashes to process per batch during hashlist uploads   - Higher values (500K-1M) may improve performance for large hashlists but use more memory   - Lower values reduce memory usage but increase processing time</p> <p>Triggers: - update_system_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#performance-scheduling","title":"Performance &amp; Scheduling","text":""},{"location":"reference/database/#benchmark_requests","title":"benchmark_requests","text":"<p>Tracks parallel benchmark execution requests (added in migration 83).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Request identifier agent_id INTEGER NOT NULL, FK \u2192 agents(id) ON DELETE CASCADE Agent reference job_execution_id UUID FK \u2192 job_executions(id) ON DELETE CASCADE Job execution reference (for forced benchmarks) hash_type INTEGER NOT NULL Hash type to benchmark attack_mode INTEGER NOT NULL Attack mode to benchmark benchmark_type VARCHAR(50) NOT NULL Type: 'forced' or 'agent_speed' status VARCHAR(50) NOT NULL 'pending' Status: pending, completed, failed requested_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Request timestamp completed_at TIMESTAMP WITH TIME ZONE Completion timestamp result JSONB Benchmark result data created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Purpose: - Enables polling-based coordination of async WebSocket benchmarks - Supports parallel benchmark execution for dramatic performance improvements - Tracks both forced benchmarks (for accurate keyspace) and agent speed benchmarks - Cleaned up after each scheduling cycle</p> <p>Benchmark Types: - forced: Run full hashcat benchmark with actual job configuration to obtain accurate keyspace - agent_speed: Standard hashcat speed test to update agent performance metrics</p> <p>Indexes: - idx_benchmark_requests_status (status) WHERE status = 'pending' - idx_benchmark_requests_agent (agent_id) - idx_benchmark_requests_job (job_execution_id)</p> <p>Performance Impact: - Before (Sequential): 15 agents \u00d7 30s = 450 seconds - After (Parallel): 15 agents in ~12 seconds - Improvement: 96% reduction (37.5x faster)</p>"},{"location":"reference/database/#agent_benchmarks","title":"agent_benchmarks","text":"<p>Stores benchmark results for agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Benchmark ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference attack_mode INT NOT NULL Attack mode hash_type INT NOT NULL Hash type speed BIGINT NOT NULL Hashes per second created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, attack_mode, hash_type)</p> <p>Indexes: - idx_agent_benchmarks_lookup (agent_id, attack_mode, hash_type)</p>"},{"location":"reference/database/#agent_performance_metrics","title":"agent_performance_metrics","text":"<p>Historical performance tracking for agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference metric_type VARCHAR(50) NOT NULL, CHECK Type: hash_rate, utilization, temperature, power_usage value NUMERIC NOT NULL Metric value timestamp TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Metric timestamp aggregation_level VARCHAR(20) NOT NULL, CHECK 'realtime' Level: realtime, daily, weekly period_start TIMESTAMP WITH TIME ZONE Aggregation period start period_end TIMESTAMP WITH TIME ZONE Aggregation period end <p>Indexes: - idx_agent_metrics_lookup (agent_id, metric_type, timestamp) - idx_agent_metrics_aggregation (aggregation_level, timestamp)</p>"},{"location":"reference/database/#performance_metrics","title":"performance_metrics","text":"<p>Detailed performance metrics (added in migration 41).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID job_task_id UUID FK \u2192 job_tasks(id) Job task reference agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference device_id INTEGER Device ID device_name VARCHAR(255) Device name timestamp TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Metric timestamp hash_rate BIGINT Current hash rate utilization FLOAT GPU utilization % temperature FLOAT Temperature in Celsius power_usage FLOAT Power usage in watts memory_used BIGINT Memory used in bytes memory_total BIGINT Total memory in bytes fan_speed FLOAT Fan speed % core_clock INTEGER Core clock in MHz memory_clock INTEGER Memory clock in MHz pcie_rx BIGINT PCIe RX throughput pcie_tx BIGINT PCIe TX throughput <p>Indexes: - idx_performance_metrics_timestamp (timestamp) - idx_performance_metrics_agent (agent_id, timestamp) - idx_performance_metrics_job_task (job_task_id) - idx_performance_metrics_device (agent_id, device_id, timestamp)</p>"},{"location":"reference/database/#job_performance_metrics","title":"job_performance_metrics","text":"<p>Job-level performance tracking.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID job_execution_id UUID NOT NULL, FK \u2192 job_executions(id) Job execution reference metric_type VARCHAR(50) NOT NULL, CHECK Type: hash_rate, progress_percentage, cracks_found value NUMERIC NOT NULL Metric value timestamp TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Metric timestamp aggregation_level VARCHAR(20) NOT NULL, CHECK 'realtime' Level: realtime, daily, weekly period_start TIMESTAMP WITH TIME ZONE Aggregation period start period_end TIMESTAMP WITH TIME ZONE Aggregation period end <p>Indexes: - idx_job_metrics_lookup (job_execution_id, metric_type, timestamp)</p>"},{"location":"reference/database/#agent_hashlists","title":"agent_hashlists","text":"<p>Tracks hashlist distribution to agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Record ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference file_path TEXT NOT NULL Local file path downloaded_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Download time last_used_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last usage time file_hash VARCHAR(32) MD5 hash for verification <p>Unique Constraint: (agent_id, hashlist_id)</p> <p>Indexes: - idx_agent_hashlists_cleanup (last_used_at)</p>"},{"location":"reference/database/#agent_devices","title":"agent_devices","text":"<p>Tracks individual physical compute devices with runtime selection support (updated in migration 81).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Device record ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference device_id INTEGER NOT NULL Physical device index (0-based) device_name VARCHAR(255) NOT NULL Device name device_type VARCHAR(50) NOT NULL Type: GPU or CPU enabled BOOLEAN NOT NULL TRUE Device enabled status runtime_options JSONB NOT NULL '[]'::jsonb Available runtimes with capabilities selected_runtime VARCHAR(50) Active runtime (CUDA/HIP/OpenCL) created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, device_id)</p> <p>Indexes: - idx_agent_devices_agent_id (agent_id) - idx_agent_devices_enabled (agent_id, enabled)</p> <p>Triggers: - update_agent_devices_updated_at: Updates updated_at on row modification</p> <p>Runtime Options Structure (JSONB): <pre><code>[\n  {\n    \"backend\": \"HIP\",\n    \"device_id\": 1,\n    \"processors\": 16,\n    \"clock\": 2208,\n    \"memory_total\": 8176,\n    \"memory_free\": 8064,\n    \"pci_address\": \"03:00.0\"\n  }\n]\n</code></pre></p> <p>Migration History: - Migration 29: Initial table creation - Migration 81: Added runtime_options and selected_runtime columns for GPU runtime selection</p>"},{"location":"reference/database/#agent_schedules","title":"agent_schedules","text":"<p>Stores daily scheduling information for agents (added in migration 42).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Schedule ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference day_of_week INTEGER NOT NULL, CHECK Day: 0=Sunday...6=Saturday start_time TIME NOT NULL Start time in UTC end_time TIME NOT NULL End time in UTC timezone VARCHAR(50) NOT NULL 'UTC' Original timezone is_active BOOLEAN NOT NULL true Schedule active status created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, day_of_week)</p> <p>Check Constraint: end_time != start_time (allows overnight schedules)</p> <p>Indexes: - idx_agent_schedules_agent_id (agent_id) - idx_agent_schedules_day_active (day_of_week, is_active)</p> <p>Triggers: - update_agent_schedules_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#authentication-security-extended","title":"Authentication &amp; Security (Extended)","text":"<p>The users table has been extended with additional security columns added through migrations:</p>"},{"location":"reference/database/#additional-users-columns","title":"Additional users columns","text":"Column Type Constraints Default Description mfa_enabled BOOLEAN FALSE MFA enabled status mfa_type text[] CHECK ARRAY['email'] MFA types enabled: email, authenticator, backup, passkey mfa_secret TEXT MFA secret backup_codes TEXT[] Hashed backup codes last_password_change TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last password change failed_login_attempts INT 0 Failed login count last_failed_attempt TIMESTAMP WITH TIME ZONE Last failed attempt account_locked BOOLEAN FALSE Account lock status account_locked_until TIMESTAMP WITH TIME ZONE Lock expiration account_enabled BOOLEAN TRUE Account enabled status last_login TIMESTAMP WITH TIME ZONE Last successful login disabled_reason TEXT Reason for disabling disabled_at TIMESTAMP WITH TIME ZONE Disable timestamp disabled_by UUID FK \u2192 users(id) Who disabled account preferred_mfa_method VARCHAR(20) Preferred MFA method deleted_at TIMESTAMP WITH TIME ZONE Soft delete timestamp (migration 106) <p>Indexes (users table): - idx_users_deleted_at (deleted_at) WHERE deleted_at IS NULL - Efficient filtering of active users</p>"},{"location":"reference/database/#tokens","title":"tokens","text":"<p>JWT token storage with sliding window session support (added in migration 7, updated in migration 101).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Token ID user_id UUID NOT NULL, FK \u2192 users(id) User reference token TEXT NOT NULL, UNIQUE Token value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time last_used_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last usage time expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time revoked BOOLEAN FALSE Revocation status revoked_at TIMESTAMP WITH TIME ZONE Revocation time revoked_reason TEXT Revocation reason superseded_at TIMESTAMP WITH TIME ZONE When token was replaced by a new token (migration 101) superseded_by UUID FK \u2192 tokens(id) Reference to the replacement token (migration 101) <p>Sliding Window Session Behavior: - Tokens are refreshed on user activity after \u2153 of the session time has passed - When refreshed, the old token is marked as superseded (not immediately invalidated) - Superseded tokens remain valid for a 5-minute grace period to handle concurrent requests - Token validity check: <code>superseded_at IS NULL OR superseded_at &gt; NOW() - INTERVAL '5 minutes'</code></p> <p>Relationships: - Referenced by <code>active_sessions(token_id)</code> with CASCADE delete (migration 65) - Deleting a token automatically removes all associated sessions - Self-referential via <code>superseded_by</code> to track token refresh chain</p> <p>Indexes: - idx_tokens_token (token) - idx_tokens_user_id (user_id) - idx_tokens_revoked (revoked) - idx_tokens_superseded_at (superseded_at) - For efficient grace period queries (migration 101)</p>"},{"location":"reference/database/#auth_settings","title":"auth_settings","text":"<p>Stores global authentication and security settings.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Settings ID min_password_length INT 15 Minimum password length require_uppercase BOOLEAN TRUE Require uppercase letters require_lowercase BOOLEAN TRUE Require lowercase letters require_numbers BOOLEAN TRUE Require numbers require_special_chars BOOLEAN TRUE Require special characters max_failed_attempts INT 5 Max failed login attempts lockout_duration_minutes INT 60 Account lockout duration require_mfa BOOLEAN FALSE Require MFA for all users jwt_expiry_minutes INT 60 JWT token expiry display_timezone VARCHAR(50) 'UTC' Display timezone notification_aggregation_minutes INT 60 Notification aggregation period allowed_mfa_methods JSONB '[\"email\", \"authenticator\"]' Allowed MFA methods email_code_validity_minutes INT 5 Email code validity backup_codes_count INT 8 Number of backup codes mfa_code_cooldown_minutes INT 1 MFA code cooldown mfa_code_expiry_minutes INT 5 MFA code expiry mfa_max_attempts INT 3 Max MFA attempts webauthn_rp_id VARCHAR(255) WebAuthn Relying Party ID (domain) webauthn_rp_origins TEXT[] '{}' Allowed WebAuthn origins webauthn_rp_display_name VARCHAR(255) 'KrakenHashes' Display name for passkey prompts <p>WebAuthn Configuration Notes: - <code>webauthn_rp_id</code> must be a domain name (not IP address per WebAuthn spec) - <code>webauthn_rp_origins</code> should include all URLs users access the system from - Changing <code>webauthn_rp_id</code> after passkeys are registered will invalidate all existing passkeys</p>"},{"location":"reference/database/#login_attempts","title":"login_attempts","text":"<p>Tracks login attempts for security monitoring.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Attempt ID user_id UUID FK \u2192 users(id) User reference (nullable) username VARCHAR(255) Attempted username ip_address INET NOT NULL Client IP address user_agent TEXT Client user agent success BOOLEAN NOT NULL Success status failure_reason TEXT Failure reason attempted_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Attempt time notified BOOLEAN FALSE Notification sent <p>Indexes: - idx_login_attempts_user_id (user_id) - idx_login_attempts_attempted_at (attempted_at) - idx_login_attempts_notified (notified)</p>"},{"location":"reference/database/#active_sessions","title":"active_sessions","text":"<p>Tracks active user sessions linked to JWT tokens (updated in migration 65).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Session ID user_id UUID FK \u2192 users(id) User reference ip_address INET NOT NULL Session IP address user_agent TEXT Client user agent created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Session start last_active_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last activity token_id UUID FK \u2192 tokens(id) ON DELETE CASCADE Linked JWT token (migration 65) <p>Security Features: - Session-Token Binding: Each session is linked to its authentication token via foreign key - CASCADE Delete: Deleting the token automatically removes the session - True Logout: Terminating a session deletes the token, immediately invalidating authentication - No Orphaned Sessions: Sessions cannot exist without valid tokens after migration 65</p> <p>Indexes: - idx_active_sessions_user_id (user_id) - idx_active_sessions_last_active (last_active_at) - idx_active_sessions_token_id (token_id)</p>"},{"location":"reference/database/#pending_mfa_setup","title":"pending_mfa_setup","text":"<p>Tracks pending MFA setup processes (added in migration 8).</p> Column Type Constraints Default Description user_id UUID PRIMARY KEY, FK \u2192 users(id) User reference method VARCHAR(20) NOT NULL, CHECK Method: email, authenticator secret TEXT MFA secret created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_pending_mfa_created_at (created_at)</p>"},{"location":"reference/database/#email_mfa_codes","title":"email_mfa_codes","text":"<p>Stores temporary MFA codes sent via email (added in migration 8).</p> Column Type Constraints Default Description user_id UUID PRIMARY KEY, FK \u2192 users(id) User reference code VARCHAR(6) NOT NULL MFA code attempts INT NOT NULL 0 Attempt count expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_email_mfa_expires_at (expires_at)</p>"},{"location":"reference/database/#mfa_methods","title":"mfa_methods","text":"<p>Stores user MFA method configurations (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Method ID user_id UUID NOT NULL, FK \u2192 users(id) User reference method VARCHAR(20) NOT NULL, CHECK Method: email, authenticator is_primary BOOLEAN FALSE Primary method flag created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time last_used_at TIMESTAMP WITH TIME ZONE Last usage time metadata JSONB Method-specific data <p>Unique Constraint: (user_id, method)</p> <p>Indexes: - idx_mfa_methods_user (user_id) - idx_mfa_methods_primary (user_id, is_primary)</p>"},{"location":"reference/database/#mfa_backup_codes","title":"mfa_backup_codes","text":"<p>Stores MFA backup codes (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Code ID user_id UUID NOT NULL, FK \u2192 users(id) User reference code_hash VARCHAR(255) NOT NULL Hashed backup code used_at TIMESTAMP WITH TIME ZONE Usage timestamp created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_mfa_backup_codes_user (user_id) - idx_mfa_backup_codes_unused (user_id, used_at) WHERE used_at IS NULL</p>"},{"location":"reference/database/#mfa_sessions","title":"mfa_sessions","text":"<p>Tracks MFA verification sessions during login (added in migration 11).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Session ID user_id UUID NOT NULL, FK \u2192 users(id) User reference session_token TEXT NOT NULL Session token expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time attempts INT NOT NULL 0 Failed attempts created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time <p>Indexes: - idx_mfa_sessions_user_id (user_id) - idx_mfa_sessions_session_token (session_token) - idx_mfa_sessions_expires_at (expires_at)</p> <p>Triggers: - enforce_mfa_max_attempts_trigger: Enforces max attempts limit - cleanup_expired_mfa_sessions_trigger: Cleans up expired sessions</p>"},{"location":"reference/database/#security_events","title":"security_events","text":"<p>Logs security-related events (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Event ID user_id UUID FK \u2192 users(id) User reference event_type VARCHAR(50) NOT NULL Event type ip_address INET Client IP address user_agent TEXT Client user agent details JSONB Event details created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Event time <p>Indexes: - idx_security_events_user (user_id) - idx_security_events_type (event_type) - idx_security_events_created (created_at)</p>"},{"location":"reference/database/#potfile-initialization-sequence","title":"Potfile Initialization Sequence","text":"<p>The potfile system initializes in stages during server startup:</p>"},{"location":"reference/database/#1-on-server-startup","title":"1. On Server Startup","text":"<ul> <li>Creates <code>/data/krakenhashes/wordlists/custom/potfile.txt</code> if missing</li> <li>Creates potfile wordlist entry in database with <code>is_potfile = true</code></li> <li>Attempts to create \"Potfile Run\" preset job</li> </ul>"},{"location":"reference/database/#2-binary-dependency","title":"2. Binary Dependency","text":"<ul> <li>Preset jobs require a <code>binary_version_id</code> (NOT NULL constraint in database)</li> <li>If no binaries exist, preset job creation is deferred</li> <li>A background monitor runs every 5 seconds checking for binary availability</li> <li>Monitor stops once preset job is successfully created</li> </ul>"},{"location":"reference/database/#3-completion","title":"3. Completion","text":"<ul> <li>Once a binary is uploaded and verified, the preset job is created</li> <li>System settings are updated with both <code>potfile_wordlist_id</code> and <code>potfile_preset_job_id</code></li> <li>The potfile system is fully operational</li> </ul>"},{"location":"reference/database/#related-tables","title":"Related Tables","text":"<ul> <li>wordlists: Contains potfile entry with <code>is_potfile = true</code></li> <li>preset_jobs: Contains \"Potfile Run\" job (once binary available)</li> <li>potfile_staging: Temporary storage for passwords before batch processing</li> <li>system_settings: Stores <code>potfile_wordlist_id</code> and <code>potfile_preset_job_id</code></li> </ul>"},{"location":"reference/database/#migration-history","title":"Migration History","text":"<p>The database schema has evolved through 106 migrations:</p> <ol> <li>000001: Initial schema - users, teams, user_teams</li> <li>000002: Add auth_tokens table</li> <li>000003: Create agents system</li> <li>000004: Create voucher system</li> <li>000005: Add email system</li> <li>000006: Add email templates (enhancement)</li> <li>000007: Auth security infrastructure</li> <li>000008: Add MFA tables</li> <li>000009: Update auth settings</li> <li>000010: Add preferred MFA method</li> <li>000011: Add MFA session</li> <li>000012: Add binary versions</li> <li>000013: Add wordlists</li> <li>000014: Add rules</li> <li>000015: Add hashlist tables</li> <li>000016: Add hashcat hash types</li> <li>000017: Add client settings</li> <li>000018: Add job workflows</li> <li>000019: Add system settings</li> <li>000020: Add job execution (fixed)</li> <li>000021: Add job execution settings</li> <li>000022: Enhance job tasks and system settings</li> <li>000023: Add max_agents column</li> <li>000024: Add interrupted status</li> <li>000025: Add job_tasks created_at</li> <li>000026: Add job_tasks updated_at</li> <li>000027: Fix hashes trigger</li> <li>000028: Fix cracked counts</li> <li>000029: Add agent devices</li> <li>000030: Add agent owner and extra parameters</li> <li>000031: Add agent is_enabled</li> <li>000032: Add preset job keyspace and max_agents</li> <li>000033: Add job created_by</li> <li>000034: Add enhanced chunking support</li> <li>000035: Make agent_id nullable in job_tasks</li> <li>000036: Add progress tracking</li> <li>000037: Add consecutive failures tracking</li> <li>000038: Update progress precision</li> <li>000039: Add speedtest timeout setting</li> <li>000040: Add dispatched_keyspace to job_executions</li> <li>000041: Add device tracking to performance_metrics</li> <li>000042: Add agent scheduling</li> <li>000043: Set owner_id for existing agents</li> <li>000044: Add chunk_number to job_tasks</li> <li>000045: Fix total_keyspace for rule split jobs</li> <li>000046: Add task heartbeat timeout setting</li> <li>000047: Add effective_keyspace to job_tasks</li> <li>000048: Add potfile support</li> <li>000049: Make job executions self-contained</li> <li>000050: Add reconnect_pending status</li> <li>000051: Add monitoring settings</li> <li>000052: Remove is_small_job column</li> <li>000053: Add binary default system</li> <li>000054: Add auth token last activity tracking</li> <li>000055: Add job notification tracking</li> <li>000056: Add reconnect grace period setting</li> <li>000057: Add agent download settings</li> <li>000058: Add agent sync status</li> <li>000059: Add average speed to tasks</li> <li>000060: Add missing hash types</li> <li>000061: Add hashlist potfile exclusion</li> <li>000062: Add client potfile exclusion</li> <li>000063: Add accurate keyspace tracking</li> <li>000064: Add chunk_actual_keyspace tracking</li> <li>000065: Link sessions to tokens with CASCADE delete for session security</li> <li>000066-000081: [Various enhancements and bug fixes]</li> <li>000082: Add agent_overflow_allocation_mode system setting</li> <li>Controls overflow agent distribution (FIFO vs round-robin)</li> <li>Applies to same-priority jobs exceeding max_agents limits</li> <li>Default value: 'fifo' (oldest job gets all overflow agents)</li> <li>Alternative: 'round_robin' (distribute evenly across jobs)</li> <li>000083: Add benchmark_requests table for parallel benchmark execution</li> <li>Enables polling-based coordination of async WebSocket benchmarks</li> <li>Tracks both forced benchmarks (accurate keyspace) and agent speed benchmarks</li> <li>Supports 96% performance improvement (15 agents: 450s \u2192 12s)</li> <li>Status tracking: pending, completed, failed</li> <li>Automatic cleanup after each scheduling cycle 84-87. 000084-000087: [Various enhancements]</li> <li>000088: Add job_increment_layers table for increment mode support</li> <li>Stores sub-layers for increment mode jobs</li> <li>Each layer represents one mask length in the increment sequence</li> <li>Enables parallel processing of different mask lengths</li> <li>Tracks per-layer progress and status</li> <li>000089: Add increment_layer_id to job_tasks</li> <li>Links tasks to their parent increment layer</li> <li>NULL for non-increment mode jobs</li> <li>Enables layer-specific task tracking</li> <li>000090: Add preset_increment_layers table</li> <li>Pre-calculated increment layers for preset jobs</li> <li>Layers are copied to job_increment_layers when job is created from preset</li> <li>Ensures consistent keyspace calculations across jobs from same preset</li> <li>000091: [Reserved]</li> <li>000092: Add WebAuthn/Passkey support</li> <li>Creates <code>user_passkeys</code> table for storing passkey credentials</li> <li>Creates <code>pending_passkey_registration</code> table for registration challenges</li> <li>Creates <code>pending_passkey_authentication</code> table for MFA authentication challenges</li> <li>Adds WebAuthn settings to <code>auth_settings</code> (rp_id, rp_origins, rp_display_name)</li> <li>Adds cleanup trigger for expired challenges</li> <li>000093: Add passkey to MFA type constraints</li> <li>Updates <code>users.mfa_type</code> CHECK constraint to allow 'passkey'</li> <li>Updates <code>users.preferred_mfa_method</code> CHECK constraint to allow 'passkey'</li> <li>000094: Add passkey backup flags</li> <li>Adds <code>backup_eligible</code> column to <code>user_passkeys</code></li> <li>Adds <code>backup_state</code> column to <code>user_passkeys</code></li> <li>Required for WebAuthn credential validation with synced passkeys</li> <li>000095: [Reserved/Internal]</li> <li>000096: Add original_hash unique index</li> <li>Creates <code>idx_hashes_original_hash_unique</code> unique index on <code>hashes.original_hash</code></li> <li>Enables fast deduplication during bulk import using <code>ON CONFLICT DO NOTHING</code></li> <li>Uses <code>CONCURRENTLY</code> to avoid locking during creation</li> <li>000097: Add hashlist bulk batch size setting</li> <li>Adds <code>hashlist_bulk_batch_size</code> system setting (default: 100000)</li> <li>Controls batch size during hashlist upload processing</li> <li>Higher values improve performance for large hashlists at cost of memory</li> <li>000098: Add cracked_by_task_id to hashes</li> <li>Adds <code>hashes.cracked_by_task_id</code> column referencing <code>job_tasks(id)</code></li> <li>Enables granular tracking of which task cracked each hash</li> <li>Used for retransmit deduplication in the outfile acknowledgment protocol</li> <li>ON DELETE SET NULL to preserve hashes when tasks are deleted</li> <li>000099: Add task retransmit tracking</li> <li>Adds <code>job_tasks.retransmit_count</code> to track retransmission attempts</li> <li>Adds <code>job_tasks.last_retransmit_at</code> for timing information</li> <li>Supports the outfile acknowledgment protocol for crack recovery</li> <li>000100: Add cracking_completed_at timestamps</li> <li>Adds <code>job_tasks.cracking_completed_at</code> - when hashcat finished (enters processing state)</li> <li>Adds <code>job_executions.cracking_completed_at</code> - when all tasks finished hashcat</li> <li>Distinguishes between hashcat completion and full processing completion</li> <li>Enables tracking of hashcat work time vs data transmission time</li> <li>000101: Add token sliding window session support</li> <li>Adds <code>tokens.superseded_at</code> column for tracking when a token was replaced</li> <li>Adds <code>tokens.superseded_by</code> column referencing the replacement token</li> <li>Creates <code>idx_tokens_superseded_at</code> index for efficient grace period queries</li> <li>Enables sliding window sessions that extend on user activity</li> <li>Old tokens remain valid for 5-minute grace period after refresh</li> <li>000102: Add preset job effective keyspace</li> <li>Adds <code>preset_jobs.effective_keyspace</code> for actual keyspace from --total-candidates</li> <li>Adds <code>preset_jobs.is_accurate_keyspace</code> boolean to track keyspace accuracy</li> <li>Adds <code>preset_jobs.use_rule_splitting</code> boolean for pre-computed splitting decisions</li> <li>Enables accurate keyspace calculation accounting for rule multipliers</li> <li>000103: Add preset job multiplication factor</li> <li>Adds <code>preset_jobs.multiplication_factor</code> column (default: 1)</li> <li>Stores rule multiplier (effective_keyspace / keyspace) for rule splitting</li> <li>Enables job creation to be a pure copy when is_accurate_keyspace = true</li> <li>000104: Add association attack support</li> <li>Adds <code>hashlists.original_file_path</code> for association attack file reference</li> <li>Adds <code>hashlists.has_mixed_work_factors</code> warning flag for different work factors</li> <li>Creates <code>association_wordlists</code> table linked to hashlists</li> <li>Adds <code>job_executions.association_wordlist_id</code> reference</li> <li>Enables hashcat -a 9 association attack mode</li> <li>000105: Add SSO support (LDAP, SAML, OAuth/OIDC)</li> <li>Adds SSO toggles to <code>auth_settings</code> (local_auth_enabled, ldap/saml/oauth_auth_enabled, auto_create/enable_users)</li> <li>Adds per-user auth overrides to <code>users</code> (local_auth_override, sso_auth_override, auth_override_notes)</li> <li>Creates <code>sso_providers</code> base table for all SSO providers</li> <li>Creates <code>ldap_configs</code> table with server URL, bind DN, search filters, TLS settings</li> <li>Creates <code>saml_configs</code> table with SP/IdP entity IDs, certificates, signing options</li> <li>Creates <code>oauth_configs</code> table with client ID/secret, discovery URL, scopes</li> <li>Creates <code>user_identities</code> table linking external identities to local accounts</li> <li>Creates <code>pending_oauth_authentication</code> and <code>pending_saml_authentication</code> for redirect flow state</li> <li>Extends <code>login_attempts</code> with provider_id and provider_type columns</li> <li>000106: Add user soft delete support</li> <li>Adds <code>users.deleted_at</code> column for soft delete timestamps</li> <li>Creates partial index <code>idx_users_deleted_at</code> WHERE deleted_at IS NULL</li> <li>Enables soft delete of user accounts while preserving historical data</li> <li>User listings filter out soft-deleted users automatically</li> </ol>"},{"location":"reference/database/#enums-and-custom-types","title":"Enums and Custom Types","text":""},{"location":"reference/database/#email_provider_type","title":"email_provider_type","text":"<ul> <li>mailgun</li> <li>sendgrid</li> <li>mailchimp</li> <li>gmail</li> </ul>"},{"location":"reference/database/#email_template_type","title":"email_template_type","text":"<ul> <li>security_event</li> <li>job_completion</li> <li>admin_error</li> <li>mfa_code</li> </ul>"},{"location":"reference/database/#binary_type","title":"binary_type","text":"<ul> <li>hashcat</li> <li>john</li> </ul>"},{"location":"reference/database/#compression_type","title":"compression_type","text":"<ul> <li>7z</li> <li>zip</li> <li>tar.gz</li> <li>tar.xz</li> </ul>"},{"location":"reference/database/#wordlist_type","title":"wordlist_type","text":"<ul> <li>general</li> <li>specialized</li> <li>targeted</li> <li>custom</li> </ul>"},{"location":"reference/database/#wordlist_format","title":"wordlist_format","text":"<ul> <li>plaintext</li> <li>compressed</li> </ul>"},{"location":"reference/database/#rule_type","title":"rule_type","text":"<ul> <li>hashcat</li> <li>john</li> </ul>"},{"location":"reference/database/#key-relationships","title":"Key Relationships","text":"<ol> <li>User System: users \u2194 teams (many-to-many via user_teams)</li> <li>Agent System: agents \u2192 users (created_by), agents \u2194 teams (many-to-many via agent_teams)</li> <li>Hash Management: hashlists \u2192 users, hashlists \u2192 clients, hashlists \u2194 hashes (many-to-many via hashlist_hashes)</li> <li>Job System: preset_jobs \u2192 binary_versions, job_executions \u2192 preset_jobs + hashlists, job_tasks \u2192 job_executions + agents</li> <li>Resource Management: wordlists/rules \u2192 users (created_by), rules \u2194 wordlists (compatibility)</li> <li>Authentication: Various MFA and security tables \u2192 users</li> <li>Session Security: tokens (parent) \u2192 active_sessions (child) with CASCADE delete - ensures session termination revokes authentication</li> </ol>"},{"location":"reference/database/#data-lifecycle-security","title":"Data Lifecycle &amp; Security","text":""},{"location":"reference/database/#data-retention-system","title":"Data Retention System","text":"<p>The database implements a comprehensive data retention system with automatic purging:</p> <ol> <li>Retention Policy Hierarchy</li> <li>System default: <code>client_settings.default_data_retention_months</code> (when client_id is NULL)</li> <li>Client-specific: <code>clients.data_retention_months</code> overrides system default</li> <li> <p>Special values: NULL = use system default, 0 = keep forever</p> </li> <li> <p>Automatic Purge Process</p> </li> <li>Runs daily at midnight and on backend startup</li> <li>Processes hashlists older than retention period based on <code>created_at</code></li> <li>Executes within database transactions for atomicity</li> <li> <p>Logs all deletions for audit compliance</p> </li> <li> <p>Secure Deletion Process</p> </li> <li>Database: Transactional deletion with CASCADE to dependent tables</li> <li>Filesystem: Files overwritten with random data before removal</li> <li>PostgreSQL: VACUUM ANALYZE on affected tables to prevent WAL recovery</li> <li> <p>Orphan Cleanup: Automatic removal of hashes not linked to any hashlist</p> </li> <li> <p>Affected Tables During Purge</p> </li> <li><code>hashlists</code> - Primary deletion target</li> <li><code>hashlist_hashes</code> - Junction table entries removed</li> <li><code>hashes</code> - Orphaned entries deleted</li> <li><code>agent_hashlists</code> - CASCADE deletion</li> <li><code>job_executions</code> - CASCADE deletion</li> <li><code>job_tasks</code> - CASCADE deletion via job_executions</li> </ol>"},{"location":"reference/database/#security-features","title":"Security Features","text":"<ol> <li>Deletion Security</li> <li>Files are securely overwritten with random data to prevent recovery</li> <li>VACUUM ANALYZE prevents recovery from PostgreSQL dead tuples</li> <li> <p>Audit trail maintained for compliance verification</p> </li> <li> <p>CASCADE Deletion Paths <pre><code>hashlists deletion triggers:\n\u251c\u2500\u2500 hashlist_hashes (explicit deletion)\n\u251c\u2500\u2500 hashes (orphan cleanup)\n\u251c\u2500\u2500 agent_hashlists (CASCADE)\n\u2514\u2500\u2500 job_executions (CASCADE)\n    \u2514\u2500\u2500 job_tasks (CASCADE)\n</code></pre></p> </li> <li> <p>Agent-Side Cleanup</p> </li> <li>Agents automatically clean files older than 3 days</li> <li>Prevents storage accumulation on compute nodes</li> <li>Preserves base resources (binaries, wordlists, rules)</li> </ol>"},{"location":"reference/database/#important-notes","title":"Important Notes","text":"<ol> <li>UUID Usage: Most primary keys use UUID except for legacy/performance-critical tables (agents, hashlists use SERIAL/BIGSERIAL)</li> <li>Soft Deletes: Implemented for users table via <code>deleted_at</code> column (migration 106). Other tables use CASCADE deletes for referential integrity.</li> <li>Audit Trails: Separate audit tables for binary_versions, wordlists, and rules</li> <li>Time Zones: All timestamps stored as TIMESTAMP WITH TIME ZONE</li> <li>JSON Storage: Heavy use of JSONB for flexible metadata storage</li> <li>System User: Special user with UUID 00000000-0000-0000-0000-000000000000 for system operations</li> <li>Data Retention: Automatic purging with secure deletion and WAL protection</li> </ol>"},{"location":"reference/environment/","title":"Environment Variables Reference","text":"<p>This document provides a comprehensive reference for all environment variables used in the KrakenHashes system.</p>"},{"location":"reference/environment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Backend Server</li> <li>Frontend Application</li> <li>Agent</li> <li>Docker &amp; Deployment</li> <li>Database</li> <li>Authentication &amp; Security</li> <li>TLS/SSL Configuration</li> <li>Logging &amp; Debugging</li> <li>WebSocket Configuration</li> </ul>"},{"location":"reference/environment/#backend-server","title":"Backend Server","text":""},{"location":"reference/environment/#core-configuration","title":"Core Configuration","text":"Variable Type Default Required Description <code>KH_HOST</code> string <code>localhost</code> (or <code>0.0.0.0</code> in Docker) No Host address for the server to bind to <code>KH_HTTPS_PORT</code> integer <code>31337</code> No Port for HTTPS API server <code>KH_HTTP_PORT</code> integer <code>1337</code> No Port for HTTP server (CA certificate distribution) <code>KH_IN_DOCKER</code> boolean <code>false</code> No Set to <code>TRUE</code> when running in Docker container"},{"location":"reference/environment/#data-storage","title":"Data &amp; Storage","text":"Variable Type Default Required Description <code>KH_CONFIG_DIR</code> string <code>~/.krakenhashes</code> No Base directory for configuration files <code>KH_DATA_DIR</code> string <code>~/.krakenhashes-data</code> No Base directory for mutable data (uploads, binaries, etc.) <code>KH_HASHLIST_BATCH_SIZE</code> integer <code>1000</code> No Maximum number of hashes to process in one database batch <code>KH_MAX_UPLOAD_SIZE_MB</code> integer <code>32</code> No Maximum file upload size in megabytes <code>KH_HASH_UPLOAD_DIR</code> string <code>{KH_DATA_DIR}/hashlist_uploads</code> No Directory for storing uploaded hashlists"},{"location":"reference/environment/#directory-structure","title":"Directory Structure","text":"<p>The backend automatically creates the following subdirectories under <code>KH_DATA_DIR</code>: - <code>binaries/</code> - Executable files (hashcat, john, etc.) - <code>wordlists/</code> - Wordlist files with subdirectories:   - <code>general/</code> - Common wordlists   - <code>specialized/</code> - Domain-specific wordlists   - <code>targeted/</code> - Client/project-specific wordlists   - <code>custom/</code> - User-created wordlists - <code>rules/</code> - Rule files with subdirectories:   - <code>hashcat/</code> - Hashcat-compatible rules   - <code>john/</code> - John the Ripper rules   - <code>custom/</code> - User-created rules - <code>hashlists/</code> - Hash files and crack results</p>"},{"location":"reference/environment/#frontend-application","title":"Frontend Application","text":""},{"location":"reference/environment/#api-configuration","title":"API Configuration","text":"Variable Type Default Required Description <code>REACT_APP_API_URL</code> string <code>https://localhost:31337</code> Yes HTTPS API endpoint URL <code>REACT_APP_HTTP_API_URL</code> string <code>http://localhost:1337</code> No HTTP API endpoint URL (for CA cert download) <code>REACT_APP_WS_URL</code> string <code>wss://localhost:31337</code> Yes WebSocket endpoint URL <code>REACT_APP_VERSION</code> string (from versions.json) No Frontend version (set during build)"},{"location":"reference/environment/#development-server","title":"Development Server","text":"Variable Type Default Required Description <code>HTTPS</code> boolean <code>true</code> No Enable HTTPS for development server <code>SSL_CRT_FILE</code> string - No Path to SSL certificate for dev server <code>SSL_KEY_FILE</code> string - No Path to SSL key for dev server <code>HOST</code> string <code>0.0.0.0</code> No Development server host <code>PORT</code> integer <code>3000</code> No Development server port <code>NODE_ENV</code> string <code>development</code> No Node environment <code>BROWSER</code> string <code>none</code> No Browser launch behavior"},{"location":"reference/environment/#debug-configuration","title":"Debug Configuration","text":"Variable Type Default Required Description <code>REACT_APP_DEBUG</code> boolean <code>false</code> No Enable debug mode in React app <code>REACT_APP_DEBUG_REDUX</code> boolean <code>false</code> No Enable Redux debugging"},{"location":"reference/environment/#agent","title":"Agent","text":""},{"location":"reference/environment/#core-configuration_1","title":"Core Configuration","text":"Variable Type Default Required Description <code>KH_DATA_DIR</code> string <code>{executable_dir}/data</code> No Base directory for agent data <code>KH_CONFIG_DIR</code> string <code>{executable_dir}/config</code> No Directory for agent configuration files <code>HASHCAT_EXTRA_PARAMS</code> string - No Extra parameters to pass to hashcat (e.g., <code>-O -w 3</code>) <p>The agent creates the same directory structure as the backend under its data directory.</p>"},{"location":"reference/environment/#docker-deployment","title":"Docker &amp; Deployment","text":""},{"location":"reference/environment/#container-configuration","title":"Container Configuration","text":"Variable Type Default Required Description <code>PUID</code> integer <code>1000</code> No User ID for file permissions <code>PGID</code> integer <code>1000</code> No Group ID for file permissions <code>TZ</code> string <code>UTC</code> No Container timezone"},{"location":"reference/environment/#volume-mounts","title":"Volume Mounts","text":"Variable Type Default Required Description <code>LOG_DIR</code> string <code>/var/log/krakenhashes</code> No Base directory for log files <code>KH_CONFIG_DIR_HOST</code> string <code>/etc/krakenhashes</code> No Host path for config directory <code>KH_DATA_DIR_HOST</code> string <code>/var/lib/krakenhashes</code> No Host path for data directory"},{"location":"reference/environment/#port-mappings","title":"Port Mappings","text":"Variable Type Default Required Description <code>FRONTEND_PORT</code> integer <code>443</code> No Host port for frontend (nginx)"},{"location":"reference/environment/#database","title":"Database","text":""},{"location":"reference/environment/#connection-settings","title":"Connection Settings","text":"Variable Type Default Required Description <code>DATABASE_URL</code> string - Yes* Full PostgreSQL connection string <code>DB_CONNECTION_STRING</code> string - Yes* Alternative to DATABASE_URL <code>DB_HOST</code> string <code>localhost</code> Yes** Database host <code>DB_PORT</code> integer <code>5432</code> Yes** Database port <code>DB_NAME</code> string <code>krakenhashes</code> Yes** Database name <code>DB_USER</code> string <code>krakenhashes</code> Yes** Database username <code>DB_PASSWORD</code> string <code>krakenhashes</code> Yes** Database password <p>* Either <code>DATABASE_URL</code> or individual DB_* variables must be set ** Required if <code>DATABASE_URL</code> is not provided</p>"},{"location":"reference/environment/#authentication-security","title":"Authentication &amp; Security","text":""},{"location":"reference/environment/#jwt-configuration","title":"JWT Configuration","text":"Variable Type Default Required Description <code>JWT_SECRET</code> string - Yes Secret key for JWT token signing <code>JWT_EXPIRATION</code> string <code>24h</code> No JWT token expiration time <code>DEFAULT_ADMIN_ID</code> string - No User ID of the default admin"},{"location":"reference/environment/#cors-configuration","title":"CORS Configuration","text":"Variable Type Default Required Description <code>CORS_ALLOWED_ORIGIN</code> string <code>https://localhost:443</code> No Allowed CORS origin <code>ALLOWED_ORIGINS</code> string <code>*</code> No Comma-separated list of allowed origins"},{"location":"reference/environment/#tlsssl-configuration","title":"TLS/SSL Configuration","text":""},{"location":"reference/environment/#certificate-mode","title":"Certificate Mode","text":"Variable Type Default Required Description <code>KH_TLS_MODE</code> string <code>self-signed</code> No TLS mode: <code>self-signed</code>, <code>provided</code>, or <code>certbot</code> <code>KH_CERTS_DIR</code> string <code>{KH_CONFIG_DIR}/certs</code> No Directory for storing certificates"},{"location":"reference/environment/#certificate-details","title":"Certificate Details","text":"Variable Type Default Required Description <code>KH_ADDITIONAL_DNS_NAMES</code> string - No Comma-separated additional DNS names for certificates <code>KH_ADDITIONAL_IP_ADDRESSES</code> string - No Comma-separated additional IP addresses for certificates <code>KH_KEY_SIZE</code> integer <code>4096</code> No RSA key size (2048 or 4096) <code>KH_SERVER_CERT_VALIDITY</code> integer <code>365</code> No Server certificate validity in days <code>KH_CA_CERT_VALIDITY</code> integer <code>3650</code> No CA certificate validity in days"},{"location":"reference/environment/#self-signed-ca-configuration","title":"Self-Signed CA Configuration","text":"Variable Type Default Required Description <code>KH_CA_COUNTRY</code> string <code>US</code> No CA certificate country code <code>KH_CA_ORGANIZATION</code> string <code>KrakenHashes</code> No CA organization name <code>KH_CA_ORGANIZATIONAL_UNIT</code> string <code>KrakenHashes CA</code> No CA organizational unit <code>KH_CA_COMMON_NAME</code> string <code>KrakenHashes Root CA</code> No CA common name"},{"location":"reference/environment/#user-provided-certificates","title":"User-Provided Certificates","text":"Variable Type Default Required Description <code>KH_CERT_FILE</code> string <code>{KH_CERTS_DIR}/server.crt</code> Yes* Path to certificate file <code>KH_KEY_FILE</code> string <code>{KH_CERTS_DIR}/server.key</code> Yes* Path to private key file <code>KH_CA_FILE</code> string <code>{KH_CERTS_DIR}/ca.crt</code> No Path to CA certificate file <p>* Required when <code>KH_TLS_MODE=provided</code></p>"},{"location":"reference/environment/#lets-encrypt-certbot-configuration","title":"Let's Encrypt (Certbot) Configuration","text":"Variable Type Default Required Description <code>KH_CERTBOT_DOMAIN</code> string - Yes* Domain name for Let's Encrypt <code>KH_CERTBOT_EMAIL</code> string - Yes* Email for Let's Encrypt notifications <code>KH_CERTBOT_STAGING</code> boolean <code>false</code> No Use Let's Encrypt staging server <code>KH_CERTBOT_AUTO_RENEW</code> boolean <code>true</code> No Enable automatic renewal <code>KH_CERTBOT_RENEW_HOOK</code> string - No Custom hook script after renewal <code>CLOUDFLARE_API_TOKEN</code> string - Yes** Cloudflare API token for DNS-01 challenge <p>* Required when <code>KH_TLS_MODE=certbot</code> ** Required for DNS-01 challenge with Cloudflare</p>"},{"location":"reference/environment/#logging-debugging","title":"Logging &amp; Debugging","text":""},{"location":"reference/environment/#debug-flags","title":"Debug Flags","text":"Variable Type Default Required Description <code>DEBUG</code> boolean <code>false</code> No Enable global debug output <code>LOG_LEVEL</code> string <code>INFO</code> No Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> <code>DEBUG_SQL</code> boolean <code>false</code> No Enable SQL query logging <code>DEBUG_HTTP</code> boolean <code>false</code> No Enable HTTP request/response logging <code>DEBUG_WEBSOCKET</code> boolean <code>false</code> No Enable WebSocket message logging <code>DEBUG_AUTH</code> boolean <code>false</code> No Enable authentication debugging <code>DEBUG_JOBS</code> boolean <code>false</code> No Enable job processing debugging"},{"location":"reference/environment/#log-directories","title":"Log Directories","text":"Variable Type Default Required Description <code>BACKEND_LOG_DIR</code> string <code>${LOG_DIR}/backend</code> No Backend log directory <code>FRONTEND_LOG_DIR</code> string <code>${LOG_DIR}/frontend</code> No Frontend log directory <code>NGINX_LOG_DIR</code> string <code>${LOG_DIR}/nginx</code> No Nginx log directory <code>POSTGRES_LOG_DIR</code> string <code>${LOG_DIR}/postgres</code> No PostgreSQL log directory"},{"location":"reference/environment/#nginx-logging","title":"Nginx Logging","text":"Variable Type Default Required Description <code>NGINX_ACCESS_LOG_LEVEL</code> string <code>info</code> No Nginx access log level <code>NGINX_ERROR_LOG_LEVEL</code> string <code>warn</code> No Nginx error log level <code>NGINX_CLIENT_MAX_BODY_SIZE</code> string <code>50M</code> No Maximum client body size"},{"location":"reference/environment/#websocket-configuration","title":"WebSocket Configuration","text":"Variable Type Default Required Description <code>KH_WRITE_WAIT</code> duration <code>10s</code> No Time allowed to write messages <code>KH_PONG_WAIT</code> duration <code>60s</code> No Time to wait for pong response <code>KH_PING_PERIOD</code> duration <code>54s</code> No How often to send pings <p>Duration format: <code>10s</code>, <code>5m</code>, <code>1h</code>, etc.</p>"},{"location":"reference/environment/#environment-variable-priority","title":"Environment Variable Priority","text":"<ol> <li>Explicit environment variables take precedence</li> <li>Docker environment files (<code>.env</code>) are loaded next</li> <li>Default values are used as fallback</li> </ol>"},{"location":"reference/environment/#best-practices","title":"Best Practices","text":"<ol> <li>Security: Never commit sensitive values (passwords, JWT secrets) to version control</li> <li>Production: Always set strong values for <code>JWT_SECRET</code>, <code>DB_PASSWORD</code>, and certificate configurations</li> <li>Development: Use <code>.env</code> files for local development configuration</li> <li>Docker: Mount configuration directories to persist data between container restarts</li> <li>Paths: Use absolute paths for file and directory configurations</li> <li>Validation: The backend validates critical environment variables on startup</li> </ol>"},{"location":"reference/environment/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/environment/#minimal-development-setup","title":"Minimal Development Setup","text":"<pre><code># .env\nDB_CONNECTION_STRING=postgres://krakenhashes:krakenhashes@localhost:5432/krakenhashes?sslmode=disable\nJWT_SECRET=dev-secret-change-in-production\nDEBUG=true\n</code></pre>"},{"location":"reference/environment/#production-docker-setup","title":"Production Docker Setup","text":"<pre><code># .env.production\nPUID=1000\nPGID=1000\nDB_HOST=postgres\nDB_PASSWORD=strong-random-password\nJWT_SECRET=very-long-random-secret\nKH_TLS_MODE=certbot\nKH_CERTBOT_DOMAIN=kraken.example.com\nKH_CERTBOT_EMAIL=admin@example.com\nCLOUDFLARE_API_TOKEN=your-cloudflare-api-token\nDEBUG=false\nLOG_LEVEL=WARNING\n</code></pre>"},{"location":"reference/environment/#agent-configuration","title":"Agent Configuration","text":"<pre><code># Agent environment\nKH_DATA_DIR=/opt/krakenhashes-agent/data\nKH_CONFIG_DIR=/opt/krakenhashes-agent/config\nHASHCAT_EXTRA_PARAMS=-O -w 3\n</code></pre>"},{"location":"reference/errors/","title":"KrakenHashes Error Codes Reference","text":"<p>This document provides a comprehensive reference for all error codes, HTTP status codes, and error conditions used throughout the KrakenHashes system.</p>"},{"location":"reference/errors/#table-of-contents","title":"Table of Contents","text":"<ul> <li>HTTP Status Codes</li> <li>Application Error Types</li> <li>Agent Error Conditions</li> <li>WebSocket Error Messages</li> <li>Common Error Scenarios</li> </ul>"},{"location":"reference/errors/#http-status-codes","title":"HTTP Status Codes","text":"<p>The KrakenHashes API uses standard HTTP status codes to indicate the success or failure of requests.</p>"},{"location":"reference/errors/#success-codes-2xx","title":"Success Codes (2xx)","text":"Code Name Usage 200 OK Standard successful response for GET, PUT, DELETE requests 201 Created Resource successfully created (POST requests) 204 No Content Successful request with no response body (DELETE requests)"},{"location":"reference/errors/#client-error-codes-4xx","title":"Client Error Codes (4xx)","text":"Code Name Common Usage 400 Bad Request Invalid request format, missing required fields, validation errors 401 Unauthorized Missing or invalid authentication credentials 403 Forbidden Valid credentials but insufficient permissions 404 Not Found Requested resource does not exist 409 Conflict Conflict with current state (e.g., duplicate records)"},{"location":"reference/errors/#server-error-codes-5xx","title":"Server Error Codes (5xx)","text":"Code Name Usage 500 Internal Server Error Unexpected server error, database errors, system failures 502 Bad Gateway WebSocket upgrade failures, proxy errors 503 Service Unavailable Server temporarily unavailable (maintenance, overload)"},{"location":"reference/errors/#application-error-types","title":"Application Error Types","text":""},{"location":"reference/errors/#model-layer-errors-backendinternalmodelserrorsgo","title":"Model Layer Errors (<code>backend/internal/models/errors.go</code>)","text":"<pre><code>var (\n    ErrNotFound     = errors.New(\"record not found\")\n    ErrInvalidInput = errors.New(\"invalid input\")\n)\n</code></pre>"},{"location":"reference/errors/#repository-layer-errors-backendinternalrepositoryerrorsgo","title":"Repository Layer Errors (<code>backend/internal/repository/errors.go</code>)","text":"<pre><code>var (\n    // Resource Errors\n    ErrNotFound         = errors.New(\"resource not found\")\n    ErrDuplicateRecord  = errors.New(\"duplicate record\")\n\n    // Validation Errors\n    ErrInvalidStatus    = errors.New(\"invalid status\")\n    ErrInvalidToken     = errors.New(\"invalid token\")\n    ErrInvalidHardware  = errors.New(\"invalid hardware information\")\n    ErrInvalidMetrics   = errors.New(\"invalid metrics\")\n\n    // Voucher Errors\n    ErrInvalidVoucher      = errors.New(\"invalid or expired voucher\")\n    ErrVoucherAlreadyUsed  = errors.New(\"voucher has already been used\")\n    ErrVoucherDeactivated  = errors.New(\"voucher has been deactivated\")\n    ErrVoucherExpired      = errors.New(\"voucher has expired\")\n\n    // Agent Errors\n    ErrDuplicateToken  = errors.New(\"agent token already exists\")\n    ErrAgentNotFound   = errors.New(\"agent not found\")\n)\n</code></pre>"},{"location":"reference/errors/#agent-error-conditions","title":"Agent Error Conditions","text":""},{"location":"reference/errors/#registration-errors","title":"Registration Errors","text":"Error HTTP Status Description Invalid claim code 400 The provided claim code is invalid or has been used Expired claim code 400 The claim code has expired Registration failed 500 Server error during agent registration"},{"location":"reference/errors/#authentication-errors","title":"Authentication Errors","text":"Error HTTP Status Description Missing API key 401 API key header not provided Invalid API key 401 API key does not match any registered agent Agent ID mismatch 401 API key does not match the provided agent ID TLS required 400 WebSocket connection requires TLS"},{"location":"reference/errors/#connection-errors","title":"Connection Errors","text":"Error Description WebSocket upgrade failed Failed to upgrade HTTP connection to WebSocket Connection timeout Agent failed to send heartbeat within timeout period Invalid message format WebSocket message does not match expected JSON format"},{"location":"reference/errors/#websocket-error-messages","title":"WebSocket Error Messages","text":""},{"location":"reference/errors/#message-types","title":"Message Types","text":"<p>The WebSocket protocol uses typed messages for communication between agents and the server.</p>"},{"location":"reference/errors/#server-to-agent-error-messages","title":"Server to Agent Error Messages","text":"<pre><code>{\n    \"type\": \"error_report\",\n    \"payload\": {\n        \"error\": \"error_message\",\n        \"code\": \"ERROR_CODE\",\n        \"details\": {}\n    }\n}\n</code></pre>"},{"location":"reference/errors/#agent-to-server-error-messages","title":"Agent to Server Error Messages","text":"<pre><code>{\n    \"type\": \"error_report\",\n    \"payload\": {\n        \"agent_id\": 123,\n        \"error\": \"error description\",\n        \"stack\": \"stack trace if available\",\n        \"context\": {},\n        \"reported_at\": \"2025-01-20T10:30:00Z\"\n    }\n}\n</code></pre>"},{"location":"reference/errors/#websocket-message-types","title":"WebSocket Message Types","text":""},{"location":"reference/errors/#agent-server-messages","title":"Agent \u2192 Server Messages","text":"Type Purpose <code>heartbeat</code> Regular heartbeat to maintain connection <code>task_status</code> Task execution status updates <code>job_progress</code> Job progress updates <code>benchmark_result</code> GPU benchmark results <code>agent_status</code> Agent status changes <code>error_report</code> Error reporting <code>hardware_info</code> Hardware capability updates <code>file_sync_response</code> File synchronization responses <code>file_sync_status</code> File sync progress updates <code>hashcat_output</code> Hashcat execution output <code>device_detection</code> GPU device detection results <code>device_update</code> GPU device status updates"},{"location":"reference/errors/#server-agent-messages","title":"Server \u2192 Agent Messages","text":"Type Purpose <code>task_assignment</code> New task assignment <code>job_stop</code> Stop job execution <code>benchmark_request</code> Request GPU benchmark <code>agent_command</code> Generic agent command <code>config_update</code> Configuration updates <code>file_sync_request</code> Request file inventory <code>file_sync_command</code> File download commands <code>force_cleanup</code> Force cleanup of resources"},{"location":"reference/errors/#common-error-scenarios","title":"Common Error Scenarios","text":""},{"location":"reference/errors/#authentication-flow-errors","title":"Authentication Flow Errors","text":"<ol> <li>Login Failures</li> <li>Invalid credentials \u2192 401 Unauthorized</li> <li>System user login attempt \u2192 401 Unauthorized</li> <li> <p>MFA required but not provided \u2192 Response with MFA session</p> </li> <li> <p>Token Errors</p> </li> <li>Expired access token \u2192 401 Unauthorized</li> <li>Invalid refresh token \u2192 401 Unauthorized</li> <li> <p>Expired refresh token \u2192 401 Unauthorized</p> </li> <li> <p>MFA Errors</p> </li> <li>Invalid TOTP code \u2192 400 Bad Request</li> <li>Invalid email code \u2192 400 Bad Request</li> <li>Expired MFA session \u2192 401 Unauthorized</li> <li>Invalid backup code \u2192 400 Bad Request</li> </ol>"},{"location":"reference/errors/#file-operation-errors","title":"File Operation Errors","text":"<ol> <li>Upload Errors</li> <li>File too large \u2192 400 Bad Request</li> <li>Invalid file type \u2192 400 Bad Request</li> <li> <p>Disk space exceeded \u2192 507 Insufficient Storage</p> </li> <li> <p>Download Errors</p> </li> <li>File not found \u2192 404 Not Found</li> <li>Access denied \u2192 403 Forbidden</li> </ol>"},{"location":"reference/errors/#job-execution-errors","title":"Job Execution Errors","text":"<ol> <li>Job Creation</li> <li>Invalid job parameters \u2192 400 Bad Request</li> <li>Missing required fields \u2192 400 Bad Request</li> <li> <p>Invalid hashlist ID \u2192 404 Not Found</p> </li> <li> <p>Job Execution</p> </li> <li>No available agents \u2192 503 Service Unavailable</li> <li>Agent disconnected \u2192 Job marked as failed</li> <li>Hashcat execution error \u2192 Job error status</li> </ol>"},{"location":"reference/errors/#database-errors","title":"Database Errors","text":"<ol> <li>Connection Errors</li> <li>Database unreachable \u2192 500 Internal Server Error</li> <li> <p>Connection pool exhausted \u2192 500 Internal Server Error</p> </li> <li> <p>Query Errors</p> </li> <li>Record not found \u2192 404 Not Found</li> <li>Duplicate key violation \u2192 409 Conflict</li> <li>Foreign key constraint \u2192 400 Bad Request</li> </ol>"},{"location":"reference/errors/#error-resolution-guide","title":"Error Resolution Guide","text":""},{"location":"reference/errors/#for-api-consumers","title":"For API Consumers","text":"<ol> <li>401 Unauthorized</li> <li>Check if access token is expired</li> <li>Refresh token if needed</li> <li> <p>Ensure proper authentication headers</p> </li> <li> <p>403 Forbidden</p> </li> <li>Verify user has required permissions</li> <li> <p>Check role-based access requirements</p> </li> <li> <p>404 Not Found</p> </li> <li>Verify resource ID is correct</li> <li> <p>Check if resource was deleted</p> </li> <li> <p>500 Internal Server Error</p> </li> <li>Retry with exponential backoff</li> <li>Check server logs for details</li> <li>Contact support if persistent</li> </ol>"},{"location":"reference/errors/#for-agent-developers","title":"For Agent Developers","text":"<ol> <li>WebSocket Connection Issues</li> <li>Ensure TLS is enabled</li> <li>Verify API key is valid</li> <li> <p>Check network connectivity</p> </li> <li> <p>File Sync Errors</p> </li> <li>Verify file permissions</li> <li>Check disk space</li> <li> <p>Ensure file hashes match</p> </li> <li> <p>Job Execution Errors</p> </li> <li>Check hashcat installation</li> <li>Verify GPU drivers</li> <li>Monitor system resources</li> </ol>"},{"location":"reference/errors/#error-logging","title":"Error Logging","text":"<p>All errors are logged with appropriate severity levels:</p> <ul> <li>DEBUG: Detailed debugging information</li> <li>INFO: General informational messages</li> <li>WARNING: Warning messages for potential issues</li> <li>ERROR: Error messages for failures</li> <li>CRITICAL: Critical system failures</li> </ul> <p>Error logs include: - Timestamp - Error message - Stack trace (when available) - Request context - User/Agent information</p>"},{"location":"reference/errors/#best-practices","title":"Best Practices","text":"<ol> <li>Client-Side Error Handling</li> <li>Always check HTTP status codes</li> <li>Parse error response bodies</li> <li>Implement retry logic for transient errors</li> <li> <p>Display user-friendly error messages</p> </li> <li> <p>Server-Side Error Handling</p> </li> <li>Use consistent error formats</li> <li>Include helpful error details</li> <li>Log errors with appropriate context</li> <li> <p>Monitor error rates and patterns</p> </li> <li> <p>Agent Error Handling</p> </li> <li>Report errors via WebSocket</li> <li>Implement local error recovery</li> <li>Maintain error history</li> <li>Include system state in error reports</li> </ol>"},{"location":"reference/glossary/","title":"KrakenHashes Glossary","text":"<p>This glossary provides definitions for terms used throughout the KrakenHashes system, organized by category.</p>"},{"location":"reference/glossary/#password-cracking-terminology","title":"Password Cracking Terminology","text":""},{"location":"reference/glossary/#a-z","title":"A-Z","text":"<p>Association Attack: Hashcat attack mode (-a 9) that tests password candidates against hashes in a 1:1 line-by-line mapping. Hash on line N is tested only against candidate on line N. Useful for targeted attacks with user-specific password intelligence. (v1.4.0+)</p> <p>Association Wordlist: A wordlist specifically designed for association attacks where each line corresponds to a specific hash in the target hashlist, maintaining line-order correspondence. Line count must exactly match the hashlist hash count. (v1.4.0+)</p> <p>Attack Mode: The method used by hashcat to attempt password recovery. Common modes include dictionary attack (-a 0), combinator attack (-a 1), brute-force/mask attack (-a 3), hybrid attacks (-a 6, -a 7), and association attack (-a 9).</p> <p>Benchmark: A test run to measure the cracking speed (hashes per second) of specific hardware against various hash algorithms.</p> <p>Brute Force Attack: An attack method that systematically tries all possible character combinations within a defined character set and length range.</p> <p>Candidate: A potential password generated during the cracking process that will be tested against the target hash.</p> <p>Charset: A defined set of characters used in mask or brute-force attacks (e.g., ?l = lowercase, ?u = uppercase, ?d = digits, ?s = special characters).</p> <p>Combinator Attack: An attack that combines words from two wordlists to create password candidates (e.g., \"password\" + \"123\" = \"password123\").</p> <p>Cracked Hash: A hash that has been successfully reversed to reveal its plaintext password.</p> <p>Dictionary Attack: An attack using a wordlist of common passwords and variations to attempt hash cracking.</p> <p>Hash: A one-way cryptographic function output that represents a password. Common types include MD5, SHA1, SHA256, bcrypt, and NTLM.</p> <p>Hash Algorithm: The specific cryptographic function used to create a hash (e.g., MD5, SHA-1, SHA-256, bcrypt, scrypt, Argon2).</p> <p>Hash Rate: The speed at which password candidates are tested, measured in hashes per second (H/s), kilohashes/s (KH/s), megahashes/s (MH/s), or gigahashes/s (GH/s).</p> <p>Hashcat: The underlying password recovery tool used by KrakenHashes for distributed cracking operations.</p> <p>Hashlist: A collection of password hashes to be cracked, typically organized by source, client, or campaign.</p> <p>Hybrid Attack: An attack combining wordlist entries with masks or rules to generate password candidates.</p> <p>Increment Mode: A hashcat feature that automatically runs mask attacks for each length from a minimum to maximum value. KrakenHashes decomposes increment mode into discrete layers for distributed processing. See Increment Mode Architecture.</p> <p>Increment Inverse: Variant of increment mode that processes masks from longest to shortest instead of shortest to longest. Useful when longer passwords are more likely.</p> <p>Increment Layer: A sub-component of an increment mode job representing one specific mask length. Each layer is scheduled and tracked independently, enabling parallel processing across multiple agents.</p> <p>Keyspace: The total number of possible password combinations for a given attack configuration.</p> <p>Mask: A pattern defining the structure of passwords to generate in a mask attack (e.g., ?u?l?l?l?d?d?d?d for Abcd1234 format).</p> <p>Mask Attack: A targeted brute-force approach using patterns to generate password candidates based on known password structures.</p> <p>Password Candidate: A potential password being tested against a hash during the cracking process.</p> <p>Plaintext: The original, unencrypted password that produces a given hash.</p> <p>Potfile: A file storing previously cracked hashes and their plaintext passwords to avoid redundant work.</p> <p>Rainbow Table: Pre-computed tables of hash-to-plaintext mappings (not used by hashcat/KrakenHashes).</p> <p>Rule: A transformation applied to wordlist entries to generate password variants (e.g., appending numbers, capitalizing letters, character substitution).</p> <p>Rule Splitting: KrakenHashes feature that divides large rule files into chunks for distributed processing across multiple agents.</p> <p>Salt: Random data added to passwords before hashing to prevent identical passwords from producing identical hashes.</p> <p>Wordlist: A file containing potential passwords, one per line, used as input for dictionary attacks.</p>"},{"location":"reference/glossary/#system-architecture-terms","title":"System Architecture Terms","text":""},{"location":"reference/glossary/#a-z_1","title":"A-Z","text":"<p>Agent: A compute node running the KrakenHashes agent software that executes hashcat jobs and reports results to the backend.</p> <p>Agent Pool: A group of agents that can be assigned to work together on jobs.</p> <p>API Key: Authentication credential used by agents to communicate with the backend server.</p> <p>Backend: The central KrakenHashes server that manages jobs, stores data, and coordinates agent activities.</p> <p>Claim Code: A one-time voucher code used to register new agents with the system.</p> <p>Client: In KrakenHashes context, a customer or engagement for which password cracking services are performed.</p> <p>Chunk: A portion of work (keyspace segment or rule subset) assigned to an individual agent for processing.</p> <p>Chunking: The process of dividing large cracking jobs into smaller segments for distributed processing.</p> <p>Data Retention: Policies and mechanisms for automatically removing old data based on configured time periods.</p> <p>FIFO Mode: First-In-First-Out agent overflow allocation mode where the oldest job at the same priority receives all overflow agents beyond max_agents limits. Default allocation mode in KrakenHashes.</p> <p>Heartbeat: Regular status updates sent by agents to the backend to indicate they are alive and processing.</p> <p>Job: A single password cracking task with specific parameters, wordlists, rules, and target hashes.</p> <p>Job Execution: An instance of a job being run, which may involve multiple agents and chunks.</p> <p>Job Template: A reusable job configuration that can be applied to different hashlists.</p> <p>Job Workflow: A sequence of jobs designed to implement a comprehensive attack strategy.</p> <p>Max Agents: Maximum number of agents a job can use simultaneously. Respected for jobs at the same priority; overridden when a job has higher priority than all others.</p> <p>Max Agents Override: Behavior where higher priority jobs receive ALL available agents regardless of their max_agents setting, ensuring critical work completes as fast as possible.</p> <p>Mixed Work Factors: A condition where hashes in a hashlist have different computational cost parameters (e.g., bcrypt hashes with varying cost values like \\(2a\\)10$ vs \\(2a\\)12$). This prevents association attacks since hash order may not be preserved during processing. (v1.4.0+)</p> <p>Overflow Agents: Agents available beyond the max_agents limits of jobs at the same priority. Distributed according to the agent overflow allocation mode (FIFO or round-robin).</p> <p>Overflow Allocation Mode: System setting that controls how overflow agents are distributed among jobs at the same priority. Options: FIFO (oldest job gets all) or round-robin (distributed evenly).</p> <p>Preset: Pre-configured job templates or workflows for common attack scenarios.</p> <p>Repository Pattern: Software design pattern used in KrakenHashes for database access abstraction.</p> <p>Round-Robin Mode: Agent overflow allocation mode where overflow agents are distributed evenly among all jobs at the same priority, ensuring balanced progress across multiple jobs.</p> <p>Service Layer: Business logic layer in the backend that processes requests between handlers and repositories.</p> <p>WebSocket: Protocol used for real-time bidirectional communication between agents and the backend.</p> <p>Work Directory: Temporary directory where agents store files and data during job execution.</p> <p>Work Factor: The computational cost parameter in certain hash algorithms (e.g., bcrypt cost, scrypt N/r/p parameters, Argon2 memory/iterations). Higher work factors increase the time required to compute each hash, making password cracking slower. (v1.4.0+)</p>"},{"location":"reference/glossary/#security-and-authentication-terms","title":"Security and Authentication Terms","text":""},{"location":"reference/glossary/#a-z_2","title":"A-Z","text":"<p>2FA/MFA: Two-Factor/Multi-Factor Authentication requiring multiple verification methods for user login.</p> <p>Access Token: Short-lived JWT token used for API authentication.</p> <p>API Authentication: Token-based authentication system for programmatic access to KrakenHashes.</p> <p>Backup Codes: One-time use codes for account recovery when primary MFA method is unavailable.</p> <p>Certificate Authority (CA): Entity that issues digital certificates for TLS/SSL encryption.</p> <p>CORS: Cross-Origin Resource Sharing - security feature controlling which domains can access the API.</p> <p>JWT: JSON Web Token - standard for securely transmitting information between parties as a JSON object.</p> <p>LDAP: Lightweight Directory Access Protocol - external authentication system support.</p> <p>Rate Limiting: Security measure limiting the number of API requests per time period.</p> <p>RBAC: Role-Based Access Control - authorization system based on user roles (admin, user, agent, system).</p> <p>Refresh Token: Long-lived token used to obtain new access tokens without re-authentication.</p> <p>Self-Signed Certificate: TLS certificate signed by its creator rather than a trusted CA.</p> <p>Session Management: System for tracking and controlling user login sessions.</p> <p>TLS/SSL: Transport Layer Security/Secure Sockets Layer - encryption protocols for secure communication.</p> <p>TOTP: Time-based One-Time Password - MFA method using authenticator apps.</p> <p>Voucher: Authorization code for specific actions like agent registration or user invitation.</p>"},{"location":"reference/glossary/#performance-and-optimization-terms","title":"Performance and Optimization Terms","text":""},{"location":"reference/glossary/#a-z_3","title":"A-Z","text":"<p>Benchmark Score: Measured performance of hardware against specific hash algorithms.</p> <p>Cache: Temporary storage of frequently accessed data to improve performance.</p> <p>Concurrency: Number of simultaneous operations or connections the system can handle.</p> <p>GPU: Graphics Processing Unit - primary hardware for high-speed password cracking.</p> <p>GPU Utilization: Percentage of GPU resources being used during cracking operations.</p> <p>Hash Rate: Speed of password testing, measured in hashes per second (H/s).</p> <p>Keyspace Distribution: Method of dividing the total keyspace among multiple agents for parallel processing.</p> <p>Load Balancing: Distribution of work across multiple agents based on their capabilities.</p> <p>Memory Usage: RAM consumption by hashcat and the agent during operations.</p> <p>Optimization: Techniques to improve cracking speed or resource efficiency.</p> <p>Parallel Processing: Simultaneous execution of job chunks across multiple agents.</p> <p>Performance Metrics: Measurements of system efficiency including hash rate, completion time, and resource usage.</p> <p>Resource Allocation: Assignment of CPU, GPU, and memory resources to cracking operations.</p> <p>Thermal Throttling: Automatic reduction in GPU performance to prevent overheating.</p> <p>Workload Distribution: Strategy for assigning job chunks to agents based on their capabilities.</p>"},{"location":"reference/glossary/#job-and-task-statuses","title":"Job and Task Statuses","text":""},{"location":"reference/glossary/#job-execution-statuses","title":"Job Execution Statuses","text":"<p>Pending: Job has been created but has not yet started execution. Waiting for available agents or higher priority jobs to complete.</p> <p>Running: Job is actively executing with one or more tasks assigned to agents. Agents are processing the job's keyspace.</p> <p>Paused: Job execution has been temporarily halted by user action. Tasks are stopped but can be resumed later.</p> <p>Processing: Job has finished execution but is waiting for crack batches to be transmitted from agents and processed by the backend. Progress shows 100% but final completion is pending crack data receipt.</p> <p>Completed: Job has finished all work, all crack batches have been received and processed, and email notifications (if enabled) have been sent.</p> <p>Failed: Job encountered an unrecoverable error during execution and cannot continue.</p> <p>Cancelled: Job was manually cancelled by a user before completion.</p>"},{"location":"reference/glossary/#task-statuses","title":"Task Statuses","text":"<p>Pending: Task has been created but not yet assigned to an agent.</p> <p>Assigned: Task has been assigned to a specific agent but the agent hasn't started executing it yet.</p> <p>Reconnect Pending: Task was assigned to an agent that disconnected. Waiting for agent to reconnect or task to be reassigned.</p> <p>Running: Task is actively executing on an agent. Hashcat is processing the assigned keyspace.</p> <p>Processing: Task has completed hashcat execution but is waiting for all crack batches to be transmitted to the backend. Agent has signaled completion but crack data is still being sent.</p> <p>Completed: Task has finished execution, all crack batches have been received, and the task is fully complete.</p> <p>Failed: Task encountered an error during execution and could not complete successfully.</p> <p>Cancelled: Task was manually cancelled before completion, either by user action or due to job cancellation.</p>"},{"location":"reference/glossary/#common-abbreviations","title":"Common Abbreviations","text":""},{"location":"reference/glossary/#a-z_4","title":"A-Z","text":"<p>API: Application Programming Interface</p> <p>CA: Certificate Authority</p> <p>CLI: Command Line Interface</p> <p>CPU: Central Processing Unit</p> <p>CRUD: Create, Read, Update, Delete (database operations)</p> <p>CSV: Comma-Separated Values</p> <p>DB: Database</p> <p>DNS: Domain Name System</p> <p>DTO: Data Transfer Object</p> <p>GPU: Graphics Processing Unit</p> <p>H/s: Hashes per second</p> <p>HTTP/HTTPS: Hypertext Transfer Protocol (Secure)</p> <p>ID: Identifier</p> <p>IP: Internet Protocol</p> <p>JSON: JavaScript Object Notation</p> <p>JWT: JSON Web Token</p> <p>KH/s: Kilohashes per second (1,000 H/s)</p> <p>LDAP: Lightweight Directory Access Protocol</p> <p>MFA: Multi-Factor Authentication</p> <p>MH/s: Megahashes per second (1,000,000 H/s)</p> <p>NTLM: NT LAN Manager (Windows password hash format)</p> <p>ORM: Object-Relational Mapping</p> <p>OS: Operating System</p> <p>RAM: Random Access Memory</p> <p>RBAC: Role-Based Access Control</p> <p>REST: Representational State Transfer</p> <p>SHA: Secure Hash Algorithm</p> <p>SMTP: Simple Mail Transfer Protocol (email transmission protocol)</p> <p>SQL: Structured Query Language</p> <p>SSL: Secure Sockets Layer</p> <p>TLS: Transport Layer Security</p> <p>TOTP: Time-based One-Time Password</p> <p>UI/UX: User Interface/User Experience</p> <p>URI/URL: Uniform Resource Identifier/Locator</p> <p>UUID: Universally Unique Identifier</p> <p>VRAM: Video Random Access Memory (GPU memory)</p> <p>WS: WebSocket</p> <p>XML: Extensible Markup Language</p> <p>This glossary is continuously updated as new features and terminology are introduced to the KrakenHashes system.</p>"},{"location":"reference/hash-types/","title":"Hash Types Reference","text":"<p>This document provides comprehensive information about the hash types supported by KrakenHashes for password cracking operations.</p>"},{"location":"reference/hash-types/#overview","title":"Overview","text":"<p>Hash types in KrakenHashes correspond to hashcat modes and define the algorithm and format used to hash passwords. Each hash type has a unique numerical identifier (mode number) that matches hashcat's mode system. KrakenHashes currently supports 504 different hash types covering a wide range of algorithms and applications.</p>"},{"location":"reference/hash-types/#hash-type-categories","title":"Hash Type Categories","text":""},{"location":"reference/hash-types/#fast-hash-types","title":"Fast Hash Types","text":"<p>Fast hash algorithms are computationally inexpensive and can be cracked at high speeds. These include basic cryptographic hashes and simple salted variants.</p> <p>Examples: - MD5 (mode 0) - SHA1 (mode 100) - SHA2-256 (mode 1400) - SHA2-512 (mode 1700) - NTLM (mode 1000)</p>"},{"location":"reference/hash-types/#slow-hash-types","title":"Slow Hash Types","text":"<p>Slow hash algorithms are intentionally computationally expensive to resist brute-force attacks. These require significantly more time and resources to crack but provide better security. KrakenHashes supports 132 slow hash types.</p> <p>Examples: - bcrypt (mode 3200) - PBKDF2 variants (modes 10900, 12000, 12100) - scrypt (mode 8900) - Argon2 variants - TrueCrypt/VeraCrypt containers</p>"},{"location":"reference/hash-types/#application-specific-hashes","title":"Application-Specific Hashes","text":"<p>Many hash types are specific to particular applications, operating systems, or protocols.</p> <p>Categories include: - Database Systems: MySQL, PostgreSQL, Oracle, MSSQL - Operating Systems: Windows (NTLM, NetNTLM), Unix/Linux (crypt variants), macOS - Web Applications: WordPress, Joomla, Django, Drupal - Network Protocols: WPA/WPA2, Kerberos, SNMP - Archive Formats: ZIP, RAR, 7-Zip - Cryptocurrency: Bitcoin wallets, Ethereum wallets</p>"},{"location":"reference/hash-types/#hash-type-structure","title":"Hash Type Structure","text":"<p>Each hash type in KrakenHashes has the following properties:</p> <ul> <li>ID: Unique numerical identifier (hashcat mode number)</li> <li>Name: Descriptive name of the hash algorithm</li> <li>Example: Sample hash format showing the expected input structure</li> <li>Needs Processing: Flag indicating if special preprocessing is required</li> <li>Is Enabled: Whether the hash type is currently supported</li> <li>Slow: Flag indicating if this is a computationally expensive algorithm</li> </ul>"},{"location":"reference/hash-types/#common-hash-types-by-use-case","title":"Common Hash Types by Use Case","text":""},{"location":"reference/hash-types/#web-application-security-testing","title":"Web Application Security Testing","text":"Mode Algorithm Common Applications 400 phpass WordPress, phpBB 500 md5crypt Traditional Unix systems 1600 Apache \\(apr1\\) Apache htpasswd 7900 Drupal7 Drupal CMS 124 Django (SHA-1) Django framework 10000 Django (PBKDF2-SHA256) Modern Django"},{"location":"reference/hash-types/#enterpriseactive-directory","title":"Enterprise/Active Directory","text":"Mode Algorithm Use Case 1000 NTLM Windows password hashes 3000 LM Legacy LAN Manager hashes (pre-Vista) 5500 NetNTLMv1 Network authentication 5600 NetNTLMv2 Network authentication 7500 Kerberos 5 AS-REQ Domain authentication 13100 Kerberos 5 TGS-REP Ticket Granting Service"},{"location":"reference/hash-types/#database-security","title":"Database Security","text":"Mode Algorithm Database 12 PostgreSQL PostgreSQL MD5 300 MySQL4.1/MySQL5 MySQL SHA1 200 MySQL323 Legacy MySQL 131 MSSQL (2000) SQL Server 132 MSSQL (2005) SQL Server 1731 MSSQL (2012, 2014) SQL Server"},{"location":"reference/hash-types/#filearchive-security","title":"File/Archive Security","text":"Mode Algorithm Application 13000 RAR5 WinRAR archives 12500 RAR3-hp WinRAR archives 11600 7-Zip 7-Zip archives 17200 PKZIP (Compressed) ZIP archives 17210 PKZIP (Uncompressed) ZIP archives"},{"location":"reference/hash-types/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/hash-types/#speed-classifications","title":"Speed Classifications","text":"<p>Ultra-Fast (&gt;1 billion attempts/sec on modern GPUs): - MD4 (900), MD5 (0), SHA1 (100) - Simple salted variants</p> <p>Fast (100M-1B attempts/sec): - SHA2 variants, NTLM - Basic application-specific hashes</p> <p>Medium (1M-100M attempts/sec): - Multiple iteration hashes - Complex salted variants</p> <p>Slow (&lt;1M attempts/sec): - PBKDF2, bcrypt, scrypt - Full disk encryption - Cryptocurrency wallets</p>"},{"location":"reference/hash-types/#resource-requirements","title":"Resource Requirements","text":"<p>GPU Memory Considerations: - Large wordlists may require significant GPU memory - Rule-based attacks can multiply memory requirements - Some hash types have higher per-hash memory overhead</p> <p>CPU vs GPU Performance: - Most hash types benefit significantly from GPU acceleration - Some algorithms may perform better on CPU for small datasets - Hybrid attacks may utilize both CPU and GPU resources</p>"},{"location":"reference/hash-types/#hash-format-examples","title":"Hash Format Examples","text":""},{"location":"reference/hash-types/#basic-formats","title":"Basic Formats","text":"<pre><code>MD5:           8743b52063cd84097a65d1633f5c74f5\nSHA1:          b89eaac7e61417341b710b727768294d0e6a277b\nSHA256:        127e6fbfe24a750e72930c220a8e138275656b8e5d8f48a98c3c92df2caba935\nNTLM:          b4b9b02e6f09a9bd760f388b67351e2b\n</code></pre>"},{"location":"reference/hash-types/#salted-formats","title":"Salted Formats","text":"<pre><code>md5($pass.$salt):     01dfae6e5d4d90d9892622325959afbe:7050461\nsha1($salt.$pass):    cac35ec206d868b7d7cb0b55f31d9425b075082b:5363620024\nsha256($pass.$salt):  c73d08de890479518ed60cf670d17faa26a4a71f995c1dcc978165399401a6c4:53743528\n</code></pre>"},{"location":"reference/hash-types/#application-specific-formats","title":"Application-Specific Formats","text":"<pre><code>WordPress:      $P$984478476IagS59wHZvyQMArzfx58u.\nbcrypt:         $2a$05$LhayLxezLhK1LhWvKxCyLOj0j1u.Kj0jZ0pEmm134uzrQlFvQJLF6\nDjango:         pbkdf2_sha256$20000$H0dPx8NeajVu$GiC4k5kqbbR9qWBlsRgDywNqC2vd9kqfk7zdorEnNas=\nNetNTLMv2:      admin::N46iSNekpT:08ca45b7d7ea58ee:88dcbe4446168966a153a0064958dac6\n</code></pre>"},{"location":"reference/hash-types/#username-and-domain-extraction","title":"Username and Domain Extraction","text":"<p>KrakenHashes v1.1+ automatically extracts username and domain information from hash formats that contain identity data. This enables better client reporting, user tracking, and prioritization of high-value accounts.</p>"},{"location":"reference/hash-types/#supported-hash-types","title":"Supported Hash Types","text":"Hash Type Mode Extraction Pattern Example NTLM 1000 <code>DOMAIN\\username:sid:LM:NT:::</code> <code>CONTOSO\\Administrator:500:...</code> NetNTLMv1 5500 <code>username::domain:challenge:response</code> <code>alice::CORP:1122334455667788:...</code> NetNTLMv2 5600 <code>username::domain:challenge:response</code> <code>bob::ENTERPRISE:abcdef01:...</code> Kerberos AS-REP 18200 <code>$krb5asrep$23$user@domain:hash</code> <code>$krb5asrep$23$john@CORP.COM:...</code> LastPass 6800 <code>hash:iterations:email</code> <code>a1b2c3...:500:user@example.com</code> DCC/MS Cache 1100 <code>hash:username</code> <code>a1b2c3...:jsmith</code>"},{"location":"reference/hash-types/#domain-formats","title":"Domain Formats","text":"<p>The system recognizes two standard domain notation formats:</p> <p>Windows/NetBIOS Style: <pre><code>DOMAIN\\username    \u2192    username: \"username\", domain: \"DOMAIN\"\nCORP\\alice         \u2192    username: \"alice\", domain: \"CORP\"\n</code></pre></p> <p>Kerberos/Email Style: <pre><code>user@domain.com         \u2192    username: \"user\", domain: \"domain.com\"\njohn@CORP.LOCAL         \u2192    username: \"john\", domain: \"CORP.LOCAL\"\nadmin@sales.corp.com    \u2192    username: \"admin\", domain: \"sales.corp.com\"\n</code></pre></p>"},{"location":"reference/hash-types/#machine-account-support","title":"Machine Account Support","text":"<p>Machine accounts (computer accounts) are identified by a trailing <code>$</code> character. The extraction system preserves this indicator:</p> <pre><code>CONTOSO\\COMPUTER01$     \u2192    username: \"COMPUTER01$\", domain: \"CONTOSO\"\nWKS01$::DOMAIN:...      \u2192    username: \"WKS01$\", domain: \"DOMAIN\"\nSRV-WEB$@CORP.LOCAL     \u2192    username: \"SRV-WEB$\", domain: \"CORP.LOCAL\"\n</code></pre> <p>Note: Machine accounts are important security indicators as they can provide lateral movement opportunities in domain environments.</p>"},{"location":"reference/hash-types/#fallback-extraction","title":"Fallback Extraction","text":"<p>For hash types without custom extractors, a heuristic approach attempts extraction:</p> <ol> <li>Checks for common separators (<code>:</code>, <code>\\</code>, <code>@</code>)</li> <li>Identifies potential username patterns</li> <li>Preserves special characters like <code>$</code> for machine accounts</li> <li>Stores <code>NULL</code> for domain when not detected</li> </ol>"},{"location":"reference/hash-types/#lm-hash-special-processing-v121","title":"LM Hash Special Processing (v1.2.1+)","text":"<p>LM (LAN Manager) hashes have unique characteristics that require special handling in KrakenHashes. This section explains the structure, weaknesses, and processing approach.</p>"},{"location":"reference/hash-types/#structure-and-weaknesses","title":"Structure and Weaknesses","text":"<p>Hash Structure: - Full hash: 32 hexadecimal characters - Two halves: Each 16 characters (representing 7-character DES-encrypted password segments) - Maximum password length: 14 characters total (7 + 7)</p> <p>Example: <pre><code>Full LM Hash: 01FC5A6BE7BC6929AAD3B435B51404EE\nFirst Half:   01FC5A6BE7BC6929 (represents chars 1-7: \"PASSWOR\")\nSecond Half:  AAD3B435B51404EE (represents chars 8-14 or blank)\n</code></pre></p> <p>Critical Weaknesses: 1. Uppercase Conversion: Passwords are converted to uppercase before hashing, eliminating case complexity 2. Split Processing: Each 7-character half is encrypted independently using DES 3. No Salting: All identical password halves produce identical hashes 4. Weak Encryption: DES encryption is cryptographically weak and fast to brute-force</p> <p>Security Impact: - Keyspace Reduction: Uppercase-only dramatically reduces combinations (26 vs 52 for letters) - Parallel Cracking: Each half can be cracked independently in parallel - Pattern Recognition: Common password halves can be precomputed (rainbow tables) - Maximum 7-char Complexity: Each half limited to 7 characters reduces search space</p>"},{"location":"reference/hash-types/#krakenhashes-processing","title":"KrakenHashes Processing","text":"<p>Agent Download Behavior:</p> <p>KrakenHashes processes LM hashes differently from other hash types to optimize cracking efficiency:</p> <ol> <li>Hash Splitting: Backend splits each 32-char LM hash into two 16-char halves</li> <li>Unique Halves: System sends only unique 16-char halves to agents (not full 32-char hashes)</li> <li>Automatic Deduplication: Common halves (like blank constant) appear only once</li> <li>Hashcat Processing: Agents crack each 16-char half as an independent hash</li> </ol> <p>Why This Approach: - Hashcat Requirement: Hashcat mode 3000 expects 16-char halves, not 32-char full hashes - Efficiency: Deduplication eliminates redundant work on common password halves - Parallel Capability: Two halves can be cracked simultaneously on different agents - Partial Crack Support: System can track when only one half is cracked</p> <p>Example Workflow: <pre><code>Upload: DOMAIN\\user:500:01FC5A6BE7BC6929AAD3B435B51404EE:hash::: (pwdump format)\n\nProcessing:\n1. Extract LM hash: 01FC5A6BE7BC6929AAD3B435B51404EE\n2. Split into halves:\n   - First:  01FC5A6BE7BC6929\n   - Second: AAD3B435B51404EE\n3. Check second half = blank constant (aad3b435b51404ee)\n4. Send only first half to agents for cracking\n5. Agents crack: 01FC5A6BE7BC6929 \u2192 \"PASSWOR\"\n\nResult: Partial crack (first 7 chars known)\n</code></pre></p>"},{"location":"reference/hash-types/#partial-crack-tracking","title":"Partial Crack Tracking","text":"<p>KrakenHashes v1.2.1+ tracks partial crack status for LM hashes in the <code>lm_hash_metadata</code> table:</p> <p>Partial Crack States: - First Half Only: First 7 characters cracked (e.g., <code>PASSWOR</code>) - Second Half Only: Last 7 characters cracked (e.g., <code>D123</code>) - Both Halves: Full password assembled (e.g., <code>PASSWORD123</code>)</p> <p>Database Storage: <pre><code>lm_hash_metadata table:\n- hash_id: Reference to main hash\n- first_half_cracked: Boolean\n- first_half_password: VARCHAR(7)  -- Up to 7 chars\n- second_half_cracked: Boolean\n- second_half_password: VARCHAR(7) -- Up to 7 chars\n</code></pre></p> <p>Strategic Value: Partial cracks provide significant intelligence: - Knowing one half reduces remaining keyspace from ~95^14 to ~95^7 combinations - Pattern recognition helps inform attacks on other hashes - Can generate targeted masks for NTLM attacks (see Analytics section)</p> <p>See Hashlists - LM Hash Special Processing for upload and processing details.</p>"},{"location":"reference/hash-types/#pwdump-format-support","title":"Pwdump Format Support","text":"<p>KrakenHashes automatically detects and processes pwdump-format files containing both LM and NTLM hashes:</p> <p>Pwdump Format: <pre><code>DOMAIN\\username:RID:LM_HASH:NTLM_HASH:::\n</code></pre></p> <p>Example: <pre><code>CORP\\Administrator:500:01FC5A6BE7BC6929AAD3B435B51404EE:0CB6948805F797BF2A82807973B89537:::\nCORP\\Guest:501:AAD3B435B51404EEAAD3B435B51404EE:31D6CFE0D16AE931B73C59D7E0C089C0:::\n</code></pre></p> <p>Automatic Linking: - System can create separate but linked LM and NTLM hashlists - Links maintained by matching username and domain - Analytics show correlation between LM and NTLM crack status - See Hashlists - LM/NTLM Linked Hashlists</p>"},{"location":"reference/hash-types/#blank-lm-hash-constant","title":"Blank LM Hash Constant","text":"<p>The constant <code>aad3b435b51404eeaad3b435b51404ee</code> appears in pwdump files when:</p> <p>Causes: - Password was blank/empty when LM hash was created - LM hash storage was disabled (Windows Vista+ default setting) - Account created after LM storage was disabled via Group Policy</p> <p>KrakenHashes Handling: - Automatically detected and filtered during processing - Not counted in hash totals - Not sent to agents for cracking - Displayed in upload dialog for user awareness</p> <p>Group Policy Setting: <pre><code>\"Network security: Do not store LAN Manager hash value on next password change\"\n</code></pre> When enabled, Windows stores only the blank constant instead of the actual LM hash.</p>"},{"location":"reference/hash-types/#security-implications-and-deprecation","title":"Security Implications and Deprecation","text":"<p>Industry Status: - Deprecated: Microsoft deprecated LM hashing with Windows Vista (2007) - Disabled by Default: Windows Vista+ do not store LM hashes by default - Legacy Support Only: May still be found in older Active Directory environments</p> <p>Security Risk Assessment: - CRITICAL: Presence of LM hashes indicates severe security misconfiguration - Fast Cracking: LM hashes can be cracked in seconds to minutes with modern hardware - No Defense: Uppercase conversion and 7-char splitting make LM indefensible - Compliance Violations: Many security frameworks prohibit LM hash storage</p> <p>Remediation: 1. Disable LM Hash Storage: Configure Group Policy on all domain controllers 2. Force Password Resets: Require all users to change passwords after policy change 3. Verify Removal: Audit domain controllers to confirm LM hashes are no longer stored 4. Network Segmentation: Isolate any systems that still require LM support</p> <p>Analytics Integration: KrakenHashes automatically generates CRITICAL severity recommendations when LM hashes are detected in analytics reports. See Analytics Reports - Windows Hash Analytics.</p>"},{"location":"reference/hash-types/#hash-type-identification","title":"Hash Type Identification","text":""},{"location":"reference/hash-types/#automatic-detection","title":"Automatic Detection","text":"<p>KrakenHashes can often identify hash types based on: - Length: Different algorithms produce different output lengths - Character set: Hex vs base64 vs custom encoding - Delimiters: Colons, dollar signs, or other separators - Prefixes: Algorithm identifiers like <code>$2a$</code>, <code>{SHA}</code>, etc.</p>"},{"location":"reference/hash-types/#manual-identification-guidelines","title":"Manual Identification Guidelines","text":"<p>By Length (hex-encoded): - 32 characters: MD5, NTLM, MD4 - 40 characters: SHA1, MySQL4.1 - 56 characters: SHA2-224 - 64 characters: SHA2-256, BLAKE2s-256 - 96 characters: SHA2-384 - 128 characters: SHA2-512, BLAKE2b-512</p> <p>By Format Patterns: - <code>$algorithm$</code>: crypt-style formats (bcrypt, sha512crypt, etc.) - <code>{ALGORITHM}</code>: LDAP-style formats - <code>hash:salt</code>: Simple salted hashes - Complex delimited formats for specific applications</p>"},{"location":"reference/hash-types/#common-identification-mistakes","title":"Common Identification Mistakes","text":"<ul> <li>Confusing MD5 with NTLM (both 32 hex characters)</li> <li>Misidentifying base64-encoded vs hex-encoded hashes</li> <li>Not recognizing application-specific wrapper formats</li> </ul>"},{"location":"reference/hash-types/#best-practices","title":"Best Practices","text":""},{"location":"reference/hash-types/#hash-type-selection","title":"Hash Type Selection","text":"<ol> <li>Verify the source: Confirm the application/system that generated the hashes</li> <li>Check format carefully: Pay attention to delimiters, prefixes, and encoding</li> <li>Test with known samples: Use test hashes to verify correct identification</li> <li>Consider variations: Many applications have multiple hash format variants</li> </ol>"},{"location":"reference/hash-types/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Start with fast hashes: Test identification with quick attacks first</li> <li>Use appropriate wordlists: Match wordlist complexity to hash strength</li> <li>Consider slow hash implications: Budget time appropriately for PBKDF2, bcrypt, etc.</li> <li>Monitor resource usage: Slow hashes can consume significant GPU memory</li> </ol>"},{"location":"reference/hash-types/#security-considerations","title":"Security Considerations","text":"<ol> <li>Handle sensitive data properly: Ensure secure storage and transmission</li> <li>Use appropriate attack methods: Don't waste resources on over-engineered attacks</li> <li>Respect rate limits: Some hash types may benefit from attack rate limiting</li> <li>Document findings: Keep records of successful techniques for similar engagements</li> </ol>"},{"location":"reference/hash-types/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/hash-types/#processing-requirements","title":"Processing Requirements","text":"<p>Some hash types require special preprocessing before cracking: - NTLM (mode 1000): Requires UTF-16LE encoding conversion - Character set normalization for international passwords - Case conversion requirements for specific applications</p>"},{"location":"reference/hash-types/#multi-hash-support","title":"Multi-Hash Support","text":"<p>KrakenHashes supports attacking multiple hashes of the same type simultaneously: - Efficient memory usage for large hashlist processing - Optimized GPU kernels for batch operations - Progress tracking per individual hash</p>"},{"location":"reference/hash-types/#custom-hash-types","title":"Custom Hash Types","text":"<p>For specialized requirements: - Contact development team for custom hash type implementation - Provide detailed specification and test vectors - Consider performance implications for custom algorithms</p>"},{"location":"reference/hash-types/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/hash-types/#common-issues","title":"Common Issues","text":"<ol> <li>Hash not cracking: Verify hash type identification</li> <li>Slow performance: Check if hash type is marked as \"slow\"</li> <li>GPU errors: Some hash types require specific GPU capabilities</li> <li>Memory errors: Large hashlists may exceed available GPU memory</li> </ol>"},{"location":"reference/hash-types/#getting-help","title":"Getting Help","text":"<ul> <li>Check the hash format against provided examples</li> <li>Verify the source application and version</li> <li>Test with known password/hash pairs</li> <li>Consult the troubleshooting guide for hardware-specific issues</li> </ul>"},{"location":"reference/hash-types/#complete-hash-type-list","title":"Complete Hash Type List","text":"<p>For a complete list of all supported hash types with examples, consult the database directly or use the admin interface. The list includes detailed information about: - Algorithm specifications - Example hash formats - Performance characteristics - Special requirements or limitations</p> <p>Last updated: Based on KrakenHashes database with 504 supported hash types For the most current information, always check the application's hash type management interface</p>"},{"location":"reference/postgresql-tuning/","title":"PostgreSQL Tuning Guide","text":"<p>This guide explains PostgreSQL memory settings and how they impact KrakenHashes performance, particularly for high-volume crack processing operations.</p>"},{"location":"reference/postgresql-tuning/#overview","title":"Overview","text":"<p>PostgreSQL memory configuration is critical for KrakenHashes performance. Poor memory settings can lead to:</p> <ul> <li>\"No space left on device\" errors during crack processing</li> <li>Slow hash lookups (disk I/O instead of memory)</li> <li>Unnecessary retry attempts</li> <li>Poor multi-agent scalability</li> </ul> <p>With proper tuning, KrakenHashes can process millions of hashes efficiently with zero errors.</p>"},{"location":"reference/postgresql-tuning/#why-memory-matters-for-password-cracking","title":"Why Memory Matters for Password Cracking","text":"<p>Unlike traditional database applications, password cracking systems have unique characteristics:</p> <ol> <li>Bulk Hash Lookups: Processing 10,000 hash values simultaneously in a single query</li> <li>High Write Volume: Millions of crack updates in rapid succession</li> <li>Frequent Table Scans: Checking hash status across large tables</li> <li>Memory-Intensive Operations: Array operations for batch processing</li> </ol> <p>Poor PostgreSQL memory configuration directly causes the errors you've seen: <pre><code>pq: could not resize shared memory segment to 1669056 bytes: No space left on device\n</code></pre></p> <p>This isn't about disk space\u2014it's about PostgreSQL running out of working memory for query operations.</p>"},{"location":"reference/postgresql-tuning/#understanding-each-setting","title":"Understanding Each Setting","text":""},{"location":"reference/postgresql-tuning/#shared_buffers-the-main-database-cache","title":"shared_buffers - The Main Database Cache","text":"<p>What It Does: - PostgreSQL's primary cache for frequently accessed data pages - Shared across ALL database connections - Caches table rows, indexes, and query results</p> <p>Default Value: 128MB (way too low!)</p> <p>Recommended Values: - 4GB systems: 512MB - 8GB systems: 1GB \u2b50 (default) - 16GB systems: 4GB - 32GB+ systems: 8-16GB</p> <p>Formula: 12-25% of total system RAM</p> <p>Impact on KrakenHashes:</p> <pre><code>WITHOUT proper shared_buffers (128MB):\n- First hash lookup: Reads from disk (slow)\n- Second hash lookup: Still reads from disk (cache too small)\n- 1.75M hash lookups: Constant disk thrashing\n- Job completion time: 30+ minutes\n\nWITH proper shared_buffers (4GB):\n- First hash lookup: Reads from disk, caches in memory\n- Second hash lookup: Reads from memory cache (100x faster!)\n- 1.75M hash lookups: Nearly all from memory\n- Job completion time: 2-3 minutes\n</code></pre> <p>Performance Multiplier</p> <p>Increasing <code>shared_buffers</code> from 128MB to 4GB can improve hash lookup performance by 50-100x for large jobs!</p>"},{"location":"reference/postgresql-tuning/#work_mem-per-operation-working-memory","title":"work_mem - Per-Operation Working Memory","text":"<p>What It Does: - Memory allocated per query operation (sorts, hashes, joins, array operations) - Each connection can use <code>work_mem</code> multiple times per query - Formula with parallel workers: <code>work_mem \u00d7 hash_mem_multiplier \u00d7 (parallel_workers + 1)</code></p> <p>KrakenHashes Default: 256MB (with parallel hash disabled)</p> <p>Why This Configuration Works:</p> <p>Setting work_mem provides significant performance benefits, but requires careful tuning to avoid memory explosion with parallel hash joins:</p> <p>Memory Allocation Math: - Default: 256MB \u00d7 1 (hash_mem_multiplier) \u00d7 1 (no parallel hash) = 256MB per operation - With parallel hash (disabled): Each worker builds own hash table independently - Total memory: Predictable and manageable even with 2-3 parallel workers</p> <p>Parallel Hash vs. Non-Parallel Hash:</p> Configuration Memory per Query Performance Stability 4MB default, parallel hash on 4MB \u00d7 2 \u00d7 3 = 24MB Fast (but won't parallelize) \u2705 Stable 256MB, parallel hash ON 256MB \u00d7 2 \u00d7 3 = 1.5GB Fastest (risky) \u274c Memory failures 256MB, parallel hash OFF 256MB \u00d7 1 \u00d7 1 = 256MB Fast \u2705 Stable <p>Benefits of 256MB work_mem: - \u2705 64x more memory than default for large sorts/aggregations - \u2705 Faster query performance for bulk operations - \u2705 Better handling of array operations (10k crack batches) - \u2705 Parallel workers still active for scans/sorts (not hash joins)</p> <p>Recommended Configuration</p> <p>Use 256MB work_mem with parallel hash disabled for optimal balance of performance and stability. This provides significant speed improvements while avoiding memory allocation failures.</p>"},{"location":"reference/postgresql-tuning/#effective_cache_size-query-planner-hint","title":"effective_cache_size - Query Planner Hint","text":"<p>What It Does: - Tells PostgreSQL's query planner how much memory is available for caching - Does NOT allocate memory itself (just a planning hint) - Influences whether PostgreSQL uses indexes or sequential scans</p> <p>Default Value: 4GB (reasonable default)</p> <p>Recommended Values: - 4GB systems: 2GB - 8GB systems: 4GB \u2b50 (default) - 16GB systems: 8GB - 32GB+ systems: 16-32GB</p> <p>Formula: 50% of total system RAM</p> <p>Impact: - Helps PostgreSQL make better decisions about query execution plans - Higher values encourage index usage - Lower values encourage sequential scans</p> <p>Planner Hint Only</p> <p>Unlike <code>shared_buffers</code> and <code>work_mem</code>, this setting doesn't allocate memory. It's safe to set relatively high.</p>"},{"location":"reference/postgresql-tuning/#maintenance_work_mem-index-and-vacuum-operations","title":"maintenance_work_mem - Index and VACUUM Operations","text":"<p>What It Does: - Memory for maintenance operations: <code>VACUUM</code>, <code>CREATE INDEX</code>, <code>ALTER TABLE</code> - Used during index builds and database cleanup - Does NOT affect normal query processing</p> <p>Default Value: 64MB</p> <p>Recommended Values: - 4GB systems: 128MB - 8GB systems: 256MB \u2b50 (default) - 16GB systems: 1GB - 32GB+ systems: 2-8GB</p> <p>Formula: 2-5% of total system RAM</p> <p>Impact on KrakenHashes: - Faster index creation when setting up hashlists - Faster <code>VACUUM</code> operations (database cleanup) - Better autovacuum performance</p>"},{"location":"reference/postgresql-tuning/#max_connections-connection-pool-size","title":"max_connections - Connection Pool Size","text":"<p>What It Does: - Maximum number of simultaneous database connections - Each connection can use multiple <code>work_mem</code> allocations</p> <p>Default Value: 100 \u2b50 (good for most deployments)</p> <p>Recommended Values: - Most deployments: 100 - High-concurrency: 150-250 - Low-resource systems: 50</p> <p>Memory Implications:</p> <p>Each connection can potentially use: - <code>work_mem</code> \u00d7 (number of operations in query) - Typical query: 2-3 work_mem allocations</p> <p>Maximum theoretical memory: <pre><code>max_connections \u00d7 work_mem \u00d7 operations per query\n100 connections \u00d7 256MB \u00d7 3 operations = 76.8 GB theoretical max\n</code></pre></p> <p>Real-World Usage</p> <p>In practice, not all connections are active simultaneously. KrakenHashes typically uses 10-20 active connections under normal load.</p>"},{"location":"reference/postgresql-tuning/#enable_parallel_hash-parallel-hash-join-control","title":"enable_parallel_hash - Parallel Hash Join Control","text":"<p>What It Does: - Controls whether hash joins use shared parallel hash tables - Introduced in PostgreSQL 11 for performance optimization - KrakenHashes disables this to prevent memory explosion</p> <p>KrakenHashes Setting: <code>off</code> (disabled)</p> <p>Why Disabled:</p> <p>Parallel hash joins create shared hash tables across workers, multiplying memory usage:</p> <p>With <code>enable_parallel_hash=on</code> (PostgreSQL default): - Workers share one large hash table - Memory formula: <code>work_mem \u00d7 hash_mem_multiplier \u00d7 (workers + 1)</code> - Example: 256MB \u00d7 2 \u00d7 3 = 1.5GB per hash join - Risk: Exceeds shared memory segment limits \u2192 \"No space left on device\"</p> <p>With <code>enable_parallel_hash=off</code> (KrakenHashes): - Each worker builds own smaller hash table - Memory formula: <code>work_mem \u00d7 hash_mem_multiplier \u00d7 1</code> - Example: 256MB \u00d7 1 \u00d7 1 = 256MB per worker - Benefit: Predictable memory usage, no shared memory failures</p> <p>What You Still Get: - \u2705 Parallel sequential scans (faster table scans) - \u2705 Parallel aggregations (faster GROUP BY) - \u2705 Parallel sorts (faster ORDER BY) - \u274c Parallel hash joins (disabled for stability)</p> <p>Recommended Setting</p> <p>Keep <code>enable_parallel_hash=off</code> for stable operation with 256MB work_mem. The performance loss is minimal compared to the stability gain.</p>"},{"location":"reference/postgresql-tuning/#hash_mem_multiplier-hash-operation-memory-multiplier","title":"hash_mem_multiplier - Hash Operation Memory Multiplier","text":"<p>What It Does: - Multiplies available memory for hash-based operations (hash joins, hash aggregations) - Allows hash operations to use more memory than sorts - Default in PostgreSQL 15+: 2.0 (doubled from 1.0)</p> <p>KrakenHashes Setting: <code>1</code> (reduced from default)</p> <p>Why Reduced:</p> <p>The default 2.0 multiplier doubles memory allocation for hash operations:</p> <p>Math with 256MB work_mem: - <code>hash_mem_multiplier=2</code>: 256MB \u00d7 2 = 512MB per hash operation - <code>hash_mem_multiplier=1</code>: 256MB \u00d7 1 = 256MB per hash operation</p> <p>Combined with Parallel Workers: - With multiplier=2: 512MB \u00d7 3 workers = 1.5GB (risky) - With multiplier=1: 256MB \u00d7 1 worker = 256MB (safe)</p> <p>Benefits of Setting to 1: - \u2705 Reduces memory pressure by 50% - \u2705 More predictable resource usage - \u2705 Safer for concurrent queries - \u2705 Still 64x more than default 4MB</p> <p>Performance vs. Stability</p> <p>Setting <code>hash_mem_multiplier=1</code> provides excellent performance (256MB for hashes) while maintaining stability. The default 2.0 is optimized for systems with abundant memory, but KrakenHashes prioritizes reliable operation at scale.</p>"},{"location":"reference/postgresql-tuning/#batch-size-and-memory-relationship","title":"Batch Size and Memory Relationship","text":""},{"location":"reference/postgresql-tuning/#why-10k-batch-size-is-optimal","title":"Why 10k Batch Size is Optimal","text":"<p>KrakenHashes uses 10,000 crack batches for several reasons:</p> <ol> <li>Memory Efficiency: ~1.7 MB per batch works well with PostgreSQL's default memory settings</li> <li>Network Efficiency: ~500 KB WebSocket message size (good balance)</li> <li>Transaction Time: 10-15 seconds per batch (avoids long-running transactions)</li> <li>Lock Contention: Shorter transactions = fewer deadlocks</li> </ol>"},{"location":"reference/postgresql-tuning/#could-we-increase-batch-size","title":"Could We Increase Batch Size?","text":"<p>Technically, yes. Practically, no need.</p> Batch Size Memory Required Transaction Time Benefit 10k 1.7 MB 10-15s \u2705 Optimal 20k 3.4 MB 20-30s Minimal gain 50k 8.5 MB 60-90s Risk of timeouts 100k 17 MB 2-3 minutes Lock contention issues <p>Recommendation: Keep 10k batch size. It's well-tested and performs excellently.</p>"},{"location":"reference/postgresql-tuning/#calculating-settings-for-your-system","title":"Calculating Settings for Your System","text":""},{"location":"reference/postgresql-tuning/#conservative-formula-recommended","title":"Conservative Formula (Recommended)","text":"<pre><code># shared_buffers: 12% of total RAM\nPOSTGRES_SHARED_BUFFERS = Total_RAM \u00d7 0.12\n\n# effective_cache_size: 50% of total RAM\nPOSTGRES_EFFECTIVE_CACHE_SIZE = Total_RAM \u00d7 0.50\n\n# maintenance_work_mem: 3% of total RAM\nPOSTGRES_MAINTENANCE_WORK_MEM = Total_RAM \u00d7 0.03\n</code></pre> <p>Examples:</p> <p>For 8GB system: - shared_buffers: 8192 MB \u00d7 0.12 = 983 MB (round to 1GB) - effective_cache_size: 8192 MB \u00d7 0.50 = 4096 MB (4GB) - maintenance_work_mem: 8192 MB \u00d7 0.03 = 245 MB (round to 256MB)</p> <p>For 32GB system: - shared_buffers: 32768 MB \u00d7 0.12 = 3932 MB (round to 4GB) - effective_cache_size: 32768 MB \u00d7 0.50 = 16384 MB (16GB) - maintenance_work_mem: 32768 MB \u00d7 0.03 = 983 MB (round to 1GB)</p>"},{"location":"reference/postgresql-tuning/#aggressive-formula-dedicated-database-server","title":"Aggressive Formula (Dedicated Database Server)","text":"<p>If KrakenHashes is the only major application on the system:</p> <pre><code># shared_buffers: 25% of total RAM\nPOSTGRES_SHARED_BUFFERS = Total_RAM \u00d7 0.25\n\n# effective_cache_size: 75% of total RAM\nPOSTGRES_EFFECTIVE_CACHE_SIZE = Total_RAM \u00d7 0.75\n\n# maintenance_work_mem: 5% of total RAM\nPOSTGRES_MAINTENANCE_WORK_MEM = Total_RAM \u00d7 0.05\n</code></pre> <p>Dedicated Server Only</p> <p>Use aggressive settings only if KrakenHashes is the primary workload. Leave room for OS and other services!</p> <p>work_mem Not Configured</p> <p>KrakenHashes intentionally does not configure work_mem, using PostgreSQL's default (4MB). Do not add work_mem to your configuration.</p>"},{"location":"reference/postgresql-tuning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/postgresql-tuning/#no-space-left-on-device-errors","title":"\"No space left on device\" Errors","text":"<p>Symptom: <pre><code>ERROR: pq: could not resize shared memory segment \"/PostgreSQL.XXXXXX\" to XXXXXX bytes: No space left on device\n</code></pre></p> <p>Root Cause: Large <code>work_mem</code> settings cause PostgreSQL to attempt oversized memory allocations that exceed kernel limits</p> <p>Solution: 1. Remove work_mem configuration if you've manually set it:    <pre><code># In .env file - REMOVE these lines if present:\n# POSTGRES_WORK_MEM=...\n</code></pre></p> <ol> <li> <p>Remove from docker-compose files if manually added</p> </li> <li> <p>Restart PostgreSQL:    <pre><code>docker-compose restart postgres\n</code></pre></p> </li> <li> <p>Verify using default:    <pre><code>docker exec krakenhashes-postgres psql -U krakenhashes -d krakenhashes -c \"SHOW work_mem\"\n# Should show: 4MB\n</code></pre></p> </li> </ol> <p>Why This Works: PostgreSQL's default 4MB work_mem prevents oversized shared memory allocations. KrakenHashes queries are optimized to work efficiently with this default.</p>"},{"location":"reference/postgresql-tuning/#slow-hash-lookups","title":"Slow Hash Lookups","text":"<p>Symptom: - Jobs take much longer than expected - High disk I/O usage - <code>docker stats</code> shows low PostgreSQL memory usage</p> <p>Root Cause: <code>shared_buffers</code> too small, forcing disk reads</p> <p>Solution: 1. Increase <code>shared_buffers</code>:    <pre><code># In .env file\nPOSTGRES_SHARED_BUFFERS=4GB  # or appropriate for your RAM\n</code></pre></p> <ol> <li> <p>Restart PostgreSQL</p> </li> <li> <p>Run a test job and monitor performance improvement</p> </li> </ol> <p>Expected Results: - 10-100x faster hash lookups after cache warms up - Higher PostgreSQL memory usage (this is good!) - Reduced disk I/O</p>"},{"location":"reference/postgresql-tuning/#retry-logic-triggering-frequently","title":"Retry Logic Triggering Frequently","text":"<p>Symptom: Logs show many retry attempts: <pre><code>[WARNING] Transient database error processing crack batch (attempt 1/3)\n[INFO] Retrying crack batch processing (attempt 2/3, delay=1s)\n</code></pre></p> <p>Root Cause: Insufficient <code>shared_buffers</code> causing disk I/O contention under concurrent load</p> <p>Solution: 1. Increase <code>shared_buffers</code>:    <pre><code>POSTGRES_SHARED_BUFFERS=2GB  # or higher based on your RAM\n</code></pre></p> <ol> <li> <p>Restart PostgreSQL</p> </li> <li> <p>Test with high-concurrency workload</p> </li> </ol> <p>Goal: Zero retry attempts under normal load</p> <p>Note: Do NOT increase work_mem - this can make the problem worse by triggering shared memory allocation errors.</p>"},{"location":"reference/postgresql-tuning/#high-postgresql-memory-usage","title":"High PostgreSQL Memory Usage","text":"<p>Symptom: <code>docker stats</code> shows PostgreSQL using lots of RAM</p> <p>Is This a Problem? NO! This is correct behavior.</p> <p>Explanation: - PostgreSQL actively uses <code>shared_buffers</code> for caching - High memory usage = good caching = fast queries - \"Free\" memory is wasted memory in database systems</p> <p>When It IS a Problem: - Host system runs out of memory - Other services are starved - System becomes unresponsive</p> <p>Solution (if needed): - Reduce <code>shared_buffers</code> to leave more room for OS - Consider upgrading system RAM - Review System Requirements for proper sizing</p>"},{"location":"reference/postgresql-tuning/#monitoring-postgresql-performance","title":"Monitoring PostgreSQL Performance","text":""},{"location":"reference/postgresql-tuning/#check-current-settings","title":"Check Current Settings","text":"<pre><code>docker exec krakenhashes-postgres psql -U krakenhashes -d krakenhashes -c \"\nSELECT name, setting, unit, source\nFROM pg_settings\nWHERE name IN ('shared_buffers', 'effective_cache_size', 'maintenance_work_mem', 'max_connections')\nORDER BY name;\"\n</code></pre>"},{"location":"reference/postgresql-tuning/#monitor-memory-usage","title":"Monitor Memory Usage","text":"<pre><code># PostgreSQL memory usage\ndocker stats krakenhashes-postgres --no-stream\n\n# Detailed breakdown\ndocker exec krakenhashes-postgres psql -U krakenhashes -d krakenhashes -c \"\nSELECT\n    pg_size_pretty(pg_database_size('krakenhashes')) as db_size,\n    pg_size_pretty(pg_total_relation_size('hashes')) as hashes_table_size,\n    pg_size_pretty(pg_total_relation_size('hashlists')) as hashlists_table_size;\"\n</code></pre>"},{"location":"reference/postgresql-tuning/#query-performance-analysis","title":"Query Performance Analysis","text":"<pre><code># Show slow queries (&gt;1 second)\ndocker exec krakenhashes-postgres psql -U krakenhashes -d krakenhashes -c \"\nSELECT query, calls, total_time, mean_time\nFROM pg_stat_statements\nWHERE mean_time &gt; 1000\nORDER BY total_time DESC\nLIMIT 10;\"\n</code></pre>"},{"location":"reference/postgresql-tuning/#best-practices","title":"Best Practices","text":""},{"location":"reference/postgresql-tuning/#1-start-with-defaults-tune-as-needed","title":"1. Start with Defaults, Tune as Needed","text":"<p>KrakenHashes ships with sensible 8GB defaults. Don't over-optimize prematurely.</p>"},{"location":"reference/postgresql-tuning/#2-monitor-before-tuning","title":"2. Monitor Before Tuning","text":"<p>Collect performance data first: - Are there memory errors? - How long do jobs take? - What's the retry rate?</p>"},{"location":"reference/postgresql-tuning/#3-change-one-setting-at-a-time","title":"3. Change One Setting at a Time","text":"<p>When tuning: 1. Change one setting 2. Restart PostgreSQL 3. Test thoroughly 4. Measure impact 5. Repeat</p>"},{"location":"reference/postgresql-tuning/#4-document-your-changes","title":"4. Document Your Changes","text":"<p>Keep notes on what you changed and why. Include: - System specifications - Workload characteristics - Performance before/after - Any issues encountered</p>"},{"location":"reference/postgresql-tuning/#5-leave-room-for-growth","title":"5. Leave Room for Growth","text":"<p>Don't allocate 100% of system RAM to PostgreSQL. Leave buffer for: - OS overhead (1-2 GB) - Backend processes (1 GB) - Temporary spikes - Future growth</p>"},{"location":"reference/postgresql-tuning/#advanced-tuning","title":"Advanced Tuning","text":""},{"location":"reference/postgresql-tuning/#for-very-large-jobs-50m-hashes","title":"For Very Large Jobs (50M+ hashes)","text":"<pre><code>POSTGRES_SHARED_BUFFERS=16GB\nPOSTGRES_WORK_MEM=512MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=32GB\nPOSTGRES_MAINTENANCE_WORK_MEM=4GB\nPOSTGRES_MAX_CONNECTIONS=150\n</code></pre>"},{"location":"reference/postgresql-tuning/#for-high-concurrency-20-agents","title":"For High Concurrency (20+ agents)","text":"<pre><code>POSTGRES_SHARED_BUFFERS=8GB\nPOSTGRES_WORK_MEM=128MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=16GB\nPOSTGRES_MAINTENANCE_WORK_MEM=2GB\nPOSTGRES_MAX_CONNECTIONS=200\n</code></pre>"},{"location":"reference/postgresql-tuning/#for-memory-constrained-systems-4gb-ram","title":"For Memory-Constrained Systems (4GB RAM)","text":"<pre><code>POSTGRES_SHARED_BUFFERS=512MB\nPOSTGRES_WORK_MEM=32MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=2GB\nPOSTGRES_MAINTENANCE_WORK_MEM=128MB\nPOSTGRES_MAX_CONNECTIONS=50\n</code></pre>"},{"location":"reference/postgresql-tuning/#related-documentation","title":"Related Documentation","text":"<ul> <li>System Requirements - Hardware sizing guide</li> <li>Performance Optimization - System-wide tuning</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"reference/system-requirements/","title":"System Requirements","text":"<p>This guide provides detailed system requirements for deploying KrakenHashes across different scales, from development environments to enterprise production deployments.</p>"},{"location":"reference/system-requirements/#overview","title":"Overview","text":"<p>KrakenHashes is optimized by default for 8GB RAM systems, providing a balance between accessibility and performance. Multi-agent operations are supported at all tiers, making distributed password cracking accessible regardless of your infrastructure scale.</p> <p>Real-World Context</p> <p>Our stress testing includes 1.75 million hash jobs with 100% crack rates to validate system stability under extreme conditions. Typical production environments see 10-30% crack rates per job, which require significantly fewer resources.</p>"},{"location":"reference/system-requirements/#quick-reference-table","title":"Quick Reference Table","text":"Total RAM Configuration PostgreSQL Settings (SB/WM/EC/MW) Typical Use Case Multi-Agent Support 4GB Minimum 512MB / 32MB / 2GB / 128MB Development, small jobs (&lt;500k hashes) \u2705 Yes 8GB Default/Recommended 1GB / 256MB / 4GB / 256MB Production, standard operations (1-10M hashes) \u2705 Yes 16GB High-Volume 4GB / 256MB / 8GB / 1GB Large-scale operations, multiple concurrent jobs \u2705 Yes 32GB Enterprise 8GB / 512MB / 16GB / 2GB Enterprise multi-agent deployments \u2705 Yes 64GB Large-Scale 16GB / 512MB / 32GB / 4GB Extreme parallel processing \u2705 Yes 96-128GB Extreme 24-32GB / 1GB / 48GB / 8GB Maximum throughput operations \u2705 Yes <p>SB=shared_buffers, WM=work_mem, EC=effective_cache_size, MW=maintenance_work_mem</p> <p>8GB Default</p> <p>KrakenHashes ships with PostgreSQL settings optimized for 8GB systems. No configuration changes are needed for standard deployments.</p>"},{"location":"reference/system-requirements/#detailed-configuration-guides","title":"Detailed Configuration Guides","text":""},{"location":"reference/system-requirements/#minimum-configuration-4gb-ram","title":"Minimum Configuration (4GB RAM)","text":"<p>System Allocation: - PostgreSQL: ~1.5 GB - Backend + Frontend + Nginx: ~1 GB - OS + Buffer: ~1.5 GB</p> <p>PostgreSQL Settings: <pre><code>POSTGRES_SHARED_BUFFERS=512MB\nPOSTGRES_WORK_MEM=32MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=2GB\nPOSTGRES_MAINTENANCE_WORK_MEM=128MB\nPOSTGRES_MAX_CONNECTIONS=100\nPOSTGRES_ENABLE_PARALLEL_HASH=off\nPOSTGRES_HASH_MEM_MULTIPLIER=1\n</code></pre></p> <p>Suitable For: - Development environments - Small hash jobs (&lt;500k hashes) - Single or few agents - Testing and evaluation</p> <p>Limitations: - May experience occasional memory pressure under load - Retry logic will handle transient failures automatically - Slower performance on large batch operations</p>"},{"location":"reference/system-requirements/#default-configuration-8gb-ram-recommended","title":"Default Configuration (8GB RAM) \u2b50 Recommended","text":"<p>System Allocation: - PostgreSQL: ~3.5 GB - Backend + Frontend + Nginx: ~1 GB - OS + Buffer: ~3.5 GB</p> <p>PostgreSQL Settings (Default): <pre><code>POSTGRES_SHARED_BUFFERS=1GB\nPOSTGRES_WORK_MEM=256MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=4GB\nPOSTGRES_MAINTENANCE_WORK_MEM=256MB\nPOSTGRES_MAX_CONNECTIONS=100\nPOSTGRES_ENABLE_PARALLEL_HASH=off\nPOSTGRES_HASH_MEM_MULTIPLIER=1\n</code></pre></p> <p>Suitable For: - Production deployments - Multi-agent operations - Hash jobs up to 10M - Standard crack rates (10-30%) - Multiple concurrent jobs</p> <p>Performance: - Zero memory exhaustion errors under normal load - Fast hash lookups with memory caching - Smooth multi-agent coordination - Retry logic acts as safety net only</p>"},{"location":"reference/system-requirements/#high-volume-configuration-16gb-ram","title":"High-Volume Configuration (16GB RAM)","text":"<p>System Allocation: - PostgreSQL: ~8 GB - Backend + Frontend + Nginx: ~2 GB - OS + Buffer: ~6 GB</p> <p>PostgreSQL Settings: <pre><code>POSTGRES_SHARED_BUFFERS=4GB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=8GB\nPOSTGRES_MAINTENANCE_WORK_MEM=1GB\nPOSTGRES_MAX_CONNECTIONS=100\n</code></pre></p> <p>Suitable For: - High-volume operations (10M+ hashes) - Multiple concurrent large jobs - Many agents running simultaneously - Stress testing scenarios</p> <p>Performance: - Entire hash tables fit in memory - 50-100x faster hash lookups - Minimal disk I/O - Ideal for sustained high-throughput</p>"},{"location":"reference/system-requirements/#enterprise-configuration-32gb-ram","title":"Enterprise Configuration (32GB RAM)","text":"<p>System Allocation: - PostgreSQL: ~16 GB - Backend + Frontend + Nginx: ~4 GB - OS + Buffer: ~12 GB</p> <p>PostgreSQL Settings: <pre><code>POSTGRES_SHARED_BUFFERS=8GB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=16GB\nPOSTGRES_MAINTENANCE_WORK_MEM=2GB\nPOSTGRES_MAX_CONNECTIONS=150\n</code></pre></p> <p>Suitable For: - Enterprise multi-team environments - Dozens of agents - Multiple 10M+ hash jobs concurrently - High-frequency job submission</p> <p>Performance: - Maximum query performance - Large concurrent workloads - Extensive memory caching - Optimal for 24/7 operations</p>"},{"location":"reference/system-requirements/#large-scale-configuration-64gb-ram","title":"Large-Scale Configuration (64GB RAM)","text":"<p>System Allocation: - PostgreSQL: ~32 GB - Backend + Frontend + Nginx: ~4 GB - OS + Buffer: ~28 GB</p> <p>PostgreSQL Settings: <pre><code>POSTGRES_SHARED_BUFFERS=16GB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=32GB\nPOSTGRES_MAINTENANCE_WORK_MEM=4GB\nPOSTGRES_MAX_CONNECTIONS=200\n</code></pre></p> <p>Suitable For: - Massive parallel processing - Hundreds of agents - Continuous high-volume operations - Very large individual jobs (50M+ hashes)</p>"},{"location":"reference/system-requirements/#extreme-configuration-96-128gb-ram","title":"Extreme Configuration (96-128GB RAM)","text":"<p>System Allocation: - PostgreSQL: ~48-64 GB - Backend + Frontend + Nginx: ~4 GB - OS + Buffer: ~44-60 GB</p> <p>PostgreSQL Settings: <pre><code>POSTGRES_SHARED_BUFFERS=24GB  # or 32GB for 128GB systems\nPOSTGRES_EFFECTIVE_CACHE_SIZE=48GB\nPOSTGRES_MAINTENANCE_WORK_MEM=8GB\nPOSTGRES_MAX_CONNECTIONS=250\n</code></pre></p> <p>Suitable For: - Maximum throughput scenarios - Enterprise-scale deployments - Extremely large jobs (100M+ hashes) - Research and development at scale</p>"},{"location":"reference/system-requirements/#applying-configuration-changes","title":"Applying Configuration Changes","text":""},{"location":"reference/system-requirements/#step-1-edit-env-file","title":"Step 1: Edit .env File","text":"<p>Add or modify the PostgreSQL memory settings in your <code>.env</code> file:</p> <pre><code># PostgreSQL Memory Configuration\nPOSTGRES_SHARED_BUFFERS=1GB\nPOSTGRES_WORK_MEM=256MB\nPOSTGRES_EFFECTIVE_CACHE_SIZE=4GB\nPOSTGRES_MAINTENANCE_WORK_MEM=256MB\nPOSTGRES_MAX_CONNECTIONS=100\n\n# PostgreSQL Query Optimization (prevents memory issues with large datasets)\nPOSTGRES_ENABLE_PARALLEL_HASH=off\nPOSTGRES_HASH_MEM_MULTIPLIER=1\n</code></pre> <p>Restart Required</p> <p>PostgreSQL memory changes require a container restart to take effect.</p>"},{"location":"reference/system-requirements/#step-2-restart-postgresql","title":"Step 2: Restart PostgreSQL","text":"<pre><code>docker-compose restart postgres\n</code></pre> <p>Or for a full restart:</p> <pre><code>docker-compose down\ndocker-compose up -d\n</code></pre>"},{"location":"reference/system-requirements/#step-3-verify-settings","title":"Step 3: Verify Settings","text":"<pre><code>docker exec krakenhashes-postgres psql -U krakenhashes -d krakenhashes -c \"SHOW shared_buffers; SHOW effective_cache_size; SHOW maintenance_work_mem;\"\n</code></pre> <p>Expected output: <pre><code> shared_buffers\n----------------\n 1GB\n(1 row)\n\n effective_cache_size\n----------------------\n 4GB\n(1 row)\n\n maintenance_work_mem\n----------------------\n 256MB\n(1 row)\n</code></pre></p>"},{"location":"reference/system-requirements/#performance-expectations-by-tier","title":"Performance Expectations by Tier","text":""},{"location":"reference/system-requirements/#hash-lookup-performance","title":"Hash Lookup Performance","text":"Configuration First Lookup Subsequent Lookups Benefit 4GB Disk I/O Partially cached Baseline 8GB Partially cached Mostly cached 10-20x faster 16GB Cached Fully cached 50-100x faster 32GB+ Fully cached Fully cached Maximum speed"},{"location":"reference/system-requirements/#batch-processing-capability","title":"Batch Processing Capability","text":"<p>All configurations handle 10k crack batches efficiently. The difference is in sustained throughput and concurrent job capacity:</p> <ul> <li>4GB: 1-2 concurrent jobs</li> <li>8GB: 3-5 concurrent jobs</li> <li>16GB: 10+ concurrent jobs</li> <li>32GB+: 20+ concurrent jobs</li> </ul>"},{"location":"reference/system-requirements/#retry-logic-behavior","title":"Retry Logic Behavior","text":"Configuration Expected Retries Behavior 4GB Occasional 1-5% of batches under load 8GB Rare &lt;0.1% under normal load 16GB+ Never Safety net only"},{"location":"reference/system-requirements/#additional-considerations","title":"Additional Considerations","text":""},{"location":"reference/system-requirements/#disk-space-requirements","title":"Disk Space Requirements","text":"<ul> <li>Minimum: 20GB (OS, application, small wordlists)</li> <li>Recommended: 100GB+ (moderate wordlist collection)</li> <li>Production: 500GB - 2TB (extensive wordlist/rule libraries)</li> </ul> <p>Wordlist Size Matters</p> <p>Disk requirements scale primarily with your wordlist and rule collection size. Hash storage is comparatively minimal (~100 bytes per hash).</p>"},{"location":"reference/system-requirements/#cpu-requirements","title":"CPU Requirements","text":"<ul> <li>Minimum: 2 cores</li> <li>Recommended: 4-8 cores</li> <li>Production: 8-16+ cores for concurrent job processing</li> </ul> <p>CPU vs GPU</p> <p>CPUs handle job coordination and database operations. GPUs (on agents) handle the actual password cracking. Both are important!</p>"},{"location":"reference/system-requirements/#network-bandwidth","title":"Network Bandwidth","text":"<ul> <li>Agent-Backend Communication: ~100 KB/s per agent (WebSocket)</li> <li>Crack Transmission: ~500 KB-1 MB per 10k crack batch</li> <li>File Synchronization: Depends on wordlist/rule sizes</li> </ul> <p>For 10 agents running continuously: - ~1-2 Mbps sustained - ~10-20 Mbps peak during file sync</p>"},{"location":"reference/system-requirements/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/system-requirements/#no-space-left-on-device-errors","title":"\"No space left on device\" Errors","text":"<p>Symptom: PostgreSQL shared memory errors in logs</p> <p>Cause: Large work_mem settings can cause PostgreSQL to attempt oversized memory allocations that exceed kernel limits.</p> <p>Solution: KrakenHashes uses PostgreSQL's default work_mem (4MB) which prevents these errors. If you've manually configured work_mem, remove it from your configuration.</p> <p>See PostgreSQL Tuning Guide for detailed troubleshooting.</p>"},{"location":"reference/system-requirements/#slow-hash-lookups","title":"Slow Hash Lookups","text":"<p>Symptom: Jobs take much longer than expected</p> <p>Solution: Increase <code>POSTGRES_SHARED_BUFFERS</code></p> <pre><code># In .env file\nPOSTGRES_SHARED_BUFFERS=2GB  # or higher\n</code></pre>"},{"location":"reference/system-requirements/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptom: Docker container using excessive RAM</p> <p>Solution: This is usually correct behavior. PostgreSQL actively uses allocated memory for caching. If the host system is struggling:</p> <ol> <li>Reduce <code>POSTGRES_SHARED_BUFFERS</code></li> <li>Consider upgrading system RAM</li> </ol>"},{"location":"reference/system-requirements/#related-documentation","title":"Related Documentation","text":"<ul> <li>PostgreSQL Tuning Guide - Detailed tuning instructions</li> <li>Performance Optimization - System-wide performance tips</li> <li>Installation Guide - Initial setup instructions</li> </ul>"},{"location":"reference/architecture/association-attack/","title":"Association Attack Architecture","text":"<p>This document describes the technical implementation of association attack mode (-a 9) in KrakenHashes.</p>"},{"location":"reference/architecture/association-attack/#overview","title":"Overview","text":"<p>Association attacks require a 1:1 mapping between hashes and password candidates. Unlike other attack modes where hashlists are deduplicated and candidates are tested against all hashes, mode 9 tests each candidate against only its corresponding hash by line number.</p>"},{"location":"reference/architecture/association-attack/#key-requirements","title":"Key Requirements","text":""},{"location":"reference/architecture/association-attack/#hash-order-preservation","title":"Hash Order Preservation","text":"<p>Challenge: KrakenHashes normally processes uploaded hashlists to: - Remove duplicates - Extract usernames - Normalize format</p> <p>This changes the order of hashes, breaking the 1:1 correspondence needed for association attacks.</p> <p>Solution: When a hashlist is uploaded, the system now: 1. Saves a copy of the original uploaded file (<code>original_file_path</code>) 2. For mode 9 jobs, agents download the original file instead of the processed version</p>"},{"location":"reference/architecture/association-attack/#line-count-validation","title":"Line Count Validation","text":"<p>The association wordlist line count must exactly match the hashlist hash count. Validation occurs: - At upload time (immediate feedback) - At job creation time (prevents stale data issues)</p>"},{"location":"reference/architecture/association-attack/#mixed-work-factor-detection","title":"Mixed Work Factor Detection","text":"<p>Hash types with variable computational costs (bcrypt, scrypt, etc.) are detected during processing: - If hashes have different cost parameters, <code>has_mixed_work_factors</code> flag is set - Association attacks are blocked for these hashlists - UI displays a warning explaining why</p>"},{"location":"reference/architecture/association-attack/#data-flow","title":"Data Flow","text":"<pre><code>User uploads hashlist\n    \u2502\n    \u25bc\nProcess hashlist \u2500\u2500\u25ba Save original file path\n    \u2502                      \u2502\n    \u25bc                      \u25bc\nUser uploads association wordlist\n    \u2502\n    \u25bc\nValidate line count matches hash count\n    \u2502\n    \u25bc\nUser creates mode 9 job\n    \u2502\n    \u25bc\nAgent receives task assignment\n    \u2502\n    \u25bc\nAgent downloads ORIGINAL hashlist (not processed)\n    \u2502\n    \u25bc\nAgent downloads association wordlist\n    \u2502\n    \u25bc\nAgent executes: hashcat -a 9 -m {mode} original.hash assoc.txt [rules]\n</code></pre>"},{"location":"reference/architecture/association-attack/#file-storage","title":"File Storage","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 hashlists/\n\u2502   \u251c\u2500\u2500 {id}.hash          # Processed hashlist (normal attacks)\n\u2502   \u2514\u2500\u2500 original/\n\u2502       \u2514\u2500\u2500 {id}_{filename} # Original uploaded file (mode 9)\n\u2514\u2500\u2500 wordlists/\n    \u2514\u2500\u2500 association/\n        \u2514\u2500\u2500 {hashlist_id}_{filename}  # Association wordlists\n</code></pre>"},{"location":"reference/architecture/association-attack/#keyspace-calculation","title":"Keyspace Calculation","text":"<p>Mode 9 does not support hashcat's <code>--keyspace</code> flag. Keyspace is estimated as:</p> <pre><code>keyspace = association_wordlist_lines \u00d7 rule_count\n</code></pre> <p>If <code>rule_count = 0</code>, keyspace equals the wordlist line count.</p> <p>Forced Benchmark</p> <p>Because keyspace cannot be calculated accurately, association attack jobs always trigger a forced benchmark to obtain actual speed from hashcat's <code>progress[1]</code> value.</p>"},{"location":"reference/architecture/association-attack/#rule-splitting-support","title":"Rule Splitting Support","text":"<p>Association attacks support rule splitting for large rule files: - Same threshold-based logic as straight attacks (mode 0) - Decision deferred until benchmark provides actual speed - Each task processes full wordlist with a subset of rules</p> <p>See Rule Splitting for details on how rule splitting works.</p>"},{"location":"reference/architecture/association-attack/#database-schema","title":"Database Schema","text":""},{"location":"reference/architecture/association-attack/#new-tables","title":"New Tables","text":"<p>association_wordlists <pre><code>CREATE TABLE association_wordlists (\n    id UUID PRIMARY KEY,\n    hashlist_id INTEGER NOT NULL REFERENCES hashlists(id),\n    file_name VARCHAR(255) NOT NULL,\n    file_path VARCHAR(500) NOT NULL,\n    line_count BIGINT NOT NULL,\n    file_size BIGINT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre></p> <p>agent_association_files <pre><code>CREATE TABLE agent_association_files (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER NOT NULL REFERENCES agents(id),\n    association_wordlist_id UUID NOT NULL REFERENCES association_wordlists(id),\n    downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    file_path VARCHAR(500) NOT NULL\n);\n</code></pre></p>"},{"location":"reference/architecture/association-attack/#modified-tables","title":"Modified Tables","text":"<p>hashlists - <code>original_file_path VARCHAR(500)</code> - Path to original uploaded file - <code>has_mixed_work_factors BOOLEAN DEFAULT FALSE</code> - Flag for mixed cost parameters</p> <p>job_executions - <code>association_wordlist_id UUID REFERENCES association_wordlists(id)</code> - Selected association wordlist</p>"},{"location":"reference/architecture/association-attack/#agent-file-download","title":"Agent File Download","text":"<p>When an agent receives a mode 9 task:</p> <ol> <li>Hashlist Download: Agent calls the hashlist download endpoint with <code>?mode=9</code> query parameter</li> <li>Backend returns original file content instead of processed hashlist</li> <li> <p>Content-Type header indicates original filename</p> </li> <li> <p>Association Wordlist Download: Agent downloads from dedicated endpoint</p> </li> <li>Path: <code>/api/agent/wordlists/association/{hashlist_id}/{filename}</code></li> <li> <p>Stored in <code>data/wordlists/association/</code> directory</p> </li> <li> <p>File Cleanup: After job completion, downloaded association files are tracked for cleanup</p> </li> <li><code>agent_association_files</code> table tracks what each agent has</li> <li>Cleanup triggered when hashlist or association wordlist is deleted</li> </ol>"},{"location":"reference/architecture/association-attack/#hashcat-command-construction","title":"Hashcat Command Construction","text":"<p>The agent builds the hashcat command for mode 9:</p> <pre><code>hashcat -a 9 -m {hash_type} {original_hashlist} {association_wordlist} [rules...]\n</code></pre> <p>Key differences from other modes: - Uses <code>-a 9</code> instead of <code>-a 0</code> (straight) - Uses original hashlist file (preserves line order) - Association wordlist is the primary input (not a standard wordlist) - No <code>-skip</code> or <code>-limit</code> parameters (entire keyspace processed)</p>"},{"location":"reference/architecture/association-attack/#error-handling","title":"Error Handling","text":""},{"location":"reference/architecture/association-attack/#line-count-mismatch","title":"Line Count Mismatch","text":"<p>If line count doesn't match at upload time: <pre><code>{\n  \"error\": \"Association wordlist line count (1500) does not match hashlist hash count (1000)\"\n}\n</code></pre></p>"},{"location":"reference/architecture/association-attack/#mixed-work-factors","title":"Mixed Work Factors","text":"<p>If hashlist has mixed work factors: <pre><code>{\n  \"error\": \"Association attacks not available for hashlists with mixed work factors\"\n}\n</code></pre></p>"},{"location":"reference/architecture/association-attack/#job-creation-validation","title":"Job Creation Validation","text":"<p>The system validates at job creation time: 1. Association wordlist exists and belongs to the selected hashlist 2. Line count still matches (hashlist may have changed) 3. Hashlist doesn't have mixed work factors</p>"},{"location":"reference/architecture/association-attack/#related-documentation","title":"Related Documentation","text":"<ul> <li>Jobs &amp; Workflows - Association Attacks</li> <li>Hashlists - Association Wordlists</li> <li>Rule Splitting</li> <li>Glossary - Association Attack</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/","title":"Benchmark-Based Job Assignment Workflow","text":""},{"location":"reference/architecture/benchmark-workflow/#overview","title":"Overview","text":"<p>The job scheduling service now implements a benchmark-first approach for job assignment. Before assigning work to an agent, the system verifies that the agent has a valid benchmark for the specific attack mode and hash type combination.</p>"},{"location":"reference/architecture/benchmark-workflow/#workflow","title":"Workflow","text":"<ol> <li>Job Assignment Request</li> <li>Scheduler identifies an available agent and a pending job</li> <li> <p>Job execution details are retrieved, including the hashlist</p> </li> <li> <p>Benchmark Check</p> </li> <li>System checks if agent has a benchmark for the attack mode and hash type</li> <li>If benchmark exists, checks if it's still valid (default: 7 days cache)</li> <li> <p>Cache duration can be configured via <code>benchmark_cache_duration_hours</code> setting</p> </li> <li> <p>Benchmark Request (if needed)</p> </li> <li>If no valid benchmark exists, system sends enhanced benchmark request</li> <li>Request includes actual job configuration:<ul> <li>Binary version (determined by Agent Override \u2192 Job Binary \u2192 Default hierarchy)</li> <li>Wordlists and rules (if applicable)</li> <li>Mask (for brute force attacks)</li> <li>Hash type and attack mode</li> <li>Test duration (30 seconds)</li> </ul> </li> <li>Binary version is determined using the hierarchy:<ol> <li>Agent-specific binary override (if configured)</li> <li>Job execution's binary version</li> <li>System default binary</li> </ol> </li> <li> <p>Job assignment is deferred until benchmark completes</p> </li> <li> <p>Benchmark Execution (Agent side)</p> </li> <li>Agent receives benchmark request with full job configuration</li> <li>Runs actual hashcat benchmark with the specific parameters</li> <li> <p>Reports back real-world performance metrics</p> </li> <li> <p>Job Assignment (after benchmark)</p> </li> <li>Once benchmark is received and stored, agent becomes available again</li> <li>Next scheduling cycle will find the valid benchmark</li> <li>Chunk calculation uses accurate performance data</li> <li>Job task is assigned with properly sized chunks</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#benefits","title":"Benefits","text":"<ul> <li>Accurate Performance Estimation: Benchmarks use actual job configuration</li> <li>Optimal Chunk Sizing: Prevents under/over-utilization of agents</li> <li>Reduced Job Failures: Avoids assigning work that agents can't handle</li> <li>Better Resource Utilization: Chunks are sized based on real performance</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#configuration","title":"Configuration","text":"<ul> <li><code>benchmark_cache_duration_hours</code>: How long benchmarks remain valid (default: 168 hours / 7 days)</li> <li><code>chunk_fluctuation_percentage</code>: Tolerance for final chunk size variations (default: 20%)</li> <li><code>default_chunk_duration</code>: Target duration for each chunk in seconds (default: 1200 / 20 minutes)</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/benchmark-workflow/#modified-components","title":"Modified Components","text":"<ol> <li>JobSchedulingService (<code>assignWorkToAgent</code>)</li> <li>Added benchmark validation before chunk calculation</li> <li>Defers assignment if benchmark is needed</li> <li> <p>Retrieves hashlist to get hash type</p> </li> <li> <p>JobWebSocketIntegration (<code>RequestAgentBenchmark</code>)</p> </li> <li>New method implementing the interface</li> <li>Sends enhanced benchmark request with full job configuration</li> <li>Includes wordlists, rules, mask, and binary information</li> <li> <p>Uses <code>DetermineBinaryForTask()</code> to select appropriate binary (Agent \u2192 Job \u2192 Default)</p> </li> <li> <p>WebSocket Types</p> </li> <li><code>BenchmarkRequestPayload</code> enhanced with job-specific fields</li> <li>Supports real-world speed testing with actual attack parameters</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#error-handling","title":"Error Handling","text":"<ul> <li>Missing benchmarks trigger requests instead of failures</li> <li>Invalid benchmarks are detected and refreshed</li> <li>WebSocket unavailability is properly handled</li> <li>Graceful degradation if benchmark request fails</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#accurate-keyspace-tracking","title":"Accurate Keyspace Tracking","text":"<p>In addition to benchmarking for performance estimation, the system captures accurate keyspace values from hashcat to ensure precise progress tracking.</p>"},{"location":"reference/architecture/benchmark-workflow/#why-accurate-keyspace-tracking","title":"Why Accurate Keyspace Tracking?","text":"<p>When using rules or combination attacks, estimating the total keyspace can be inaccurate. For example: - Rule-based attacks: Estimated keyspace = wordlist_size \u00d7 rule_count, but actual keyspace varies based on rule effectiveness - Combination attacks: Certain combinations may be invalid or duplicates</p> <p>Hashcat provides the actual keyspace through <code>progress[1]</code> values, which the system captures to ensure accurate progress reporting.</p>"},{"location":"reference/architecture/benchmark-workflow/#keyspace-capture-workflow","title":"Keyspace Capture Workflow","text":"<ol> <li>Initial Job Creation</li> <li>Job created with estimated <code>effective_keyspace</code> based on wordlists/rules</li> <li>Flag <code>is_accurate_keyspace</code> set to <code>false</code></li> <li> <p>Estimation needed for rule splitting decisions</p> </li> <li> <p>Forced Benchmark for First Agent</p> </li> <li>When first agent connects (taskCount = 0), system requests benchmark</li> <li>Benchmark includes actual job configuration (wordlists, rules, mask, hash type)</li> <li> <p>Agent runs hashcat benchmark and captures <code>progress[1]</code> value</p> </li> <li> <p>Accurate Keyspace Capture</p> </li> <li>Backend receives benchmark result with <code>TotalEffectiveKeyspace</code> from <code>progress[1]</code></li> <li>Updates job execution:<ul> <li>Sets <code>effective_keyspace</code> to actual value from hashcat</li> <li>Sets <code>is_accurate_keyspace</code> to <code>true</code></li> <li>Calculates <code>avg_rule_multiplier</code> = actual / estimated</li> </ul> </li> <li> <p>Subsequent agents skip benchmark and use cached job-level keyspace</p> </li> <li> <p>Fallback: First Progress Update</p> </li> <li>If benchmark doesn't provide keyspace, first task progress update does</li> <li>Agent sends <code>progress[1]</code> value in first progress message with <code>IsFirstUpdate</code> flag</li> <li>Backend updates both job-level and task-level keyspace</li> <li> <p>Sets <code>is_actual_keyspace</code> to <code>true</code> for the task</p> </li> <li> <p>Future Task Improvements</p> </li> <li>New tasks use <code>avg_rule_multiplier</code> to improve estimated keyspace</li> <li>Provides better estimates for chunks not yet processed</li> <li>Helps with more accurate progress reporting across the job</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#benefits-of-accurate-keyspace-tracking","title":"Benefits of Accurate Keyspace Tracking","text":"<ul> <li>Precise Progress: Progress percentages reflect actual hashcat progress, not estimates</li> <li>Better Task Distribution: Chunk sizes calculated based on real keyspace</li> <li>Improved Estimates: Future tasks benefit from multiplier derived from actual values</li> <li>Consistency: All agents working on same job use same accurate keyspace</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#database-columns","title":"Database Columns","text":"<p>job_executions table: - <code>is_accurate_keyspace</code> (boolean): True when keyspace is from hashcat <code>progress[1]</code> - <code>avg_rule_multiplier</code> (float): Ratio of actual/estimated keyspace for improving future estimates</p> <p>job_tasks table: - <code>is_actual_keyspace</code> (boolean): True when task has actual keyspace from progress update</p>"},{"location":"reference/architecture/benchmark-workflow/#hashlist-download-strategy-for-benchmarks","title":"Hashlist Download Strategy for Benchmarks","text":""},{"location":"reference/architecture/benchmark-workflow/#always-fresh-hashlist-downloads","title":"Always-Fresh Hashlist Downloads","text":"<p>To ensure accurate keyspace calculations, benchmarks ALWAYS download a fresh copy of the hashlist from the backend, even if a local copy exists. This prevents stale hash counts from affecting keyspace estimates.</p> <p>Why This Matters: - Hashlists change as hashes are cracked and files are regenerated - Benchmark keyspace must reflect the CURRENT number of uncracked hashes - Stale local copies can lead to incorrect <code>effective_keyspace</code> values - Cross-hashlist crack propagation means files update frequently</p> <p>Implementation: <pre><code>// Agent removes existing hashlist before benchmark\nif _, err := os.Stat(localPath); err == nil {\n    debug.Info(\"Removing existing hashlist to download fresh copy for benchmark\")\n    os.Remove(localPath)\n}\n\n// Download fresh copy from backend\nfileInfo := &amp;filesync.FileInfo{\n    Name:     fmt.Sprintf(\"%d.hash\", hashlistID),\n    FileType: \"hashlist\",\n    ID:       int(hashlistID),\n    MD5Hash:  \"\", // Skip verification for speed\n}\nc.fileSync.DownloadFileFromInfo(ctx, fileInfo)\n</code></pre></p> <p>Benchmark Workflow with Fresh Download: 1. Backend requests benchmark for job execution 2. Agent receives benchmark request with hashlist ID 3. Agent deletes any existing local hashlist file 4. Agent downloads current version from backend (may be empty if all cracked) 5. Agent runs hashcat benchmark with fresh hashlist 6. Agent reports actual keyspace from <code>progress[1]</code></p> <p>Benefits: - Keyspace values always accurate - Benchmarks work correctly even after massive crack batches - Prevents \"empty hashlist\" errors from hashcat - Consistent behavior across all agents</p> <p>Performance Impact: - Minimal: Hashlists are typically &lt; 10 MB - Download completes in seconds over LAN - Only occurs once per job (first agent) - Subsequent agents use job-level cached keyspace</p>"},{"location":"reference/architecture/benchmark-workflow/#task-execution-strategy","title":"Task Execution Strategy","text":"<p>Similar to benchmarks, job tasks also ALWAYS re-download hashlists:</p> <p>Rationale: - Ensures consistent behavior between benchmarks and tasks - Prevents agents from working with stale data - Handles cross-hashlist crack propagation automatically - Eliminates edge cases with modified local files</p> <p>Implementation: <pre><code>// Agent ensures fresh hashlist for each task\nif _, err := os.Stat(localPath); err == nil {\n    debug.Info(\"Removing existing hashlist to download fresh copy\")\n    os.Remove(localPath)\n}\n\n// Download current version\ns.fileSync.DownloadFileFromInfo(ctx, fileInfo)\n</code></pre></p> <p>Trade-offs: - Slightly higher network usage - Guaranteed data freshness - Simplified agent logic (no staleness checks) - Better fault tolerance</p>"},{"location":"reference/architecture/benchmark-workflow/#parallel-benchmark-execution-system","title":"Parallel Benchmark Execution System","text":""},{"location":"reference/architecture/benchmark-workflow/#overview_1","title":"Overview","text":"<p>The job scheduling service implements an intelligent parallel benchmarking system that dramatically improves benchmark completion time by executing all benchmark requests simultaneously.</p> <p>Performance Improvement: - Before (Sequential): 15 agents \u00d7 30s = 450 seconds total - After (Parallel): 15 agents in ~12 seconds - Result: 96% reduction in benchmark time (37.5x faster)</p>"},{"location":"reference/architecture/benchmark-workflow/#architecture","title":"Architecture","text":"<p>The parallel benchmarking system consists of three main components:</p>"},{"location":"reference/architecture/benchmark-workflow/#1-benchmark-planning-job_scheduling_benchmark_planninggo","title":"1. Benchmark Planning (<code>job_scheduling_benchmark_planning.go</code>)","text":"<p>Core Functions: - <code>CreateBenchmarkPlan()</code>: Analyzes system state and creates intelligent execution plan - <code>ExecuteBenchmarkPlan()</code>: Sends all benchmarks in parallel using goroutines - <code>WaitForBenchmarks()</code>: Polls database for completion with configurable timeout - <code>PrioritizeForcedBenchmarkAgents()</code>: Gives priority to agents for job's first task</p> <p>Planning Algorithm: 1. Identifies jobs needing benchmarks (taskCount = 0, no accurate keyspace) 2. Identifies agents needing speed benchmarks (missing hash_type/attack_mode combinations) 3. Distributes benchmark requests using round-robin allocation by priority 4. Respects benchmark cache duration (system setting: <code>benchmark_cache_duration_hours</code>)</p>"},{"location":"reference/architecture/benchmark-workflow/#2-benchmark-requests-table-migration-083","title":"2. Benchmark Requests Table (Migration 083)","text":"<p>The <code>benchmark_requests</code> table enables polling-based coordination of async WebSocket benchmarks:</p> <pre><code>CREATE TABLE benchmark_requests (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    agent_id INTEGER NOT NULL REFERENCES agents(id) ON DELETE CASCADE,\n    job_execution_id UUID REFERENCES job_executions(id) ON DELETE CASCADE,\n    hash_type INTEGER NOT NULL,\n    attack_mode INTEGER NOT NULL,\n    benchmark_type VARCHAR(50) NOT NULL,  -- 'forced' or 'agent_speed'\n    status VARCHAR(50) NOT NULL DEFAULT 'pending',\n    requested_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    completed_at TIMESTAMP WITH TIME ZONE,\n    result JSONB,\n    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre> <p>Purpose: - Tracks pending benchmark requests - Enables blocking wait for completion - Supports cleanup after each scheduling cycle - Allows forced benchmark agent prioritization</p>"},{"location":"reference/architecture/benchmark-workflow/#3-websocket-integration","title":"3. WebSocket Integration","text":"<p>Enhanced HandleBenchmarkResult(): - Updates <code>benchmark_requests</code> table on completion - Sets forced benchmark completion metadata for prioritization - Maintains compatibility with existing keyspace tracking - Updates both job-level and agent-level benchmark data</p>"},{"location":"reference/architecture/benchmark-workflow/#benchmark-types","title":"Benchmark Types","text":"<p>The system supports two types of benchmarks:</p>"},{"location":"reference/architecture/benchmark-workflow/#forced-benchmarks","title":"Forced Benchmarks","text":"<ul> <li>Purpose: Obtain accurate keyspace from hashcat for new jobs</li> <li>Trigger: Job with taskCount = 0 and <code>is_accurate_keyspace = false</code></li> <li>Behavior: Runs full hashcat benchmark with actual job configuration</li> <li>Result: Updates <code>job_executions.effective_keyspace</code> with <code>progress[1]</code> value</li> <li>Priority: Agents completing forced benchmarks get first task for their job</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#agent-speed-benchmarks","title":"Agent Speed Benchmarks","text":"<ul> <li>Purpose: Update agent performance metrics for chunk calculations</li> <li>Trigger: Missing agent benchmark for hash_type + attack_mode combination</li> <li>Behavior: Standard hashcat speed test</li> <li>Result: Updates <code>agent_benchmarks</code> table</li> <li>Duration: Uses <code>speedtest_timeout_seconds</code> system setting</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#execution-flow","title":"Execution Flow","text":""},{"location":"reference/architecture/benchmark-workflow/#integration-with-job-scheduling","title":"Integration with Job Scheduling","text":"<p>The parallel benchmark system executes within the scheduling cycle as a blocking operation:</p> <pre><code>func (s *JobSchedulingService) ScheduleJobs(ctx context.Context) {\n    // ... existing code ...\n\n    // Execute benchmarks in parallel and wait\n    benchmarkPlan := s.CreateBenchmarkPlan(ctx, availableAgents, pendingJobs)\n    if len(benchmarkPlan.Requests) &gt; 0 {\n        s.ExecuteBenchmarkPlan(ctx, benchmarkPlan)\n        s.WaitForBenchmarks(ctx, benchmarkPlan)\n\n        // Refresh available agents after benchmarks complete\n        availableAgents = s.GetAvailableAgents(ctx)\n\n        // Prioritize agents that completed forced benchmarks\n        s.PrioritizeForcedBenchmarkAgents(ctx, &amp;availableAgents, benchmarkPlan)\n    }\n\n    // Proceed with task assignment\n    // ... existing code ...\n}\n</code></pre>"},{"location":"reference/architecture/benchmark-workflow/#parallel-execution-with-goroutines","title":"Parallel Execution with Goroutines","text":"<p>All benchmark requests are sent simultaneously:</p> <pre><code>func (s *JobSchedulingService) ExecuteBenchmarkPlan(ctx context.Context, plan *BenchmarkPlan) {\n    var wg sync.WaitGroup\n\n    for _, req := range plan.Requests {\n        wg.Add(1)\n        go func(request BenchmarkRequest) {\n            defer wg.Done()\n            s.sendBenchmarkRequest(ctx, request)\n        }(req)\n    }\n\n    wg.Wait() // Wait for all goroutines to send requests\n}\n</code></pre>"},{"location":"reference/architecture/benchmark-workflow/#polling-based-completion-detection","title":"Polling-Based Completion Detection","text":"<p>The system polls the database to detect completion:</p> <pre><code>func (s *JobSchedulingService) WaitForBenchmarks(ctx context.Context, plan *BenchmarkPlan) {\n    timeout := time.Duration(speedtestTimeout + 5) * time.Second\n    pollInterval := 500 * time.Millisecond\n\n    deadline := time.Now().Add(timeout)\n\n    for time.Now().Before(deadline) {\n        completed, err := s.checkBenchmarkCompletion(ctx, plan.RequestIDs)\n        if completed {\n            return\n        }\n        time.Sleep(pollInterval)\n    }\n}\n</code></pre>"},{"location":"reference/architecture/benchmark-workflow/#round-robin-distribution","title":"Round-Robin Distribution","text":"<p>Benchmarks are distributed evenly across agents to prevent overloading:</p> <p>Algorithm: 1. Group pending jobs by hash type 2. For each hash type group (ordered by priority):    - Assign one benchmark request to each available agent    - Use round-robin to distribute across jobs 3. Ensures even distribution and respects priority</p> <p>Example with 5 agents and 3 jobs: <pre><code>Agent 1 \u2192 Job A (Priority 100, Hash Type 1000)\nAgent 2 \u2192 Job B (Priority 100, Hash Type 1000)\nAgent 3 \u2192 Job C (Priority 50, Hash Type 1000)\nAgent 4 \u2192 Job A (Priority 100, Hash Type 1000)  # Round-robin back to A\nAgent 5 \u2192 Job B (Priority 100, Hash Type 1000)  # Round-robin to B\n</code></pre></p>"},{"location":"reference/architecture/benchmark-workflow/#configuration_1","title":"Configuration","text":"<p>System Settings: - <code>benchmark_cache_duration_hours</code> (default: 168 = 7 days): How long to cache benchmarks - <code>speedtest_timeout_seconds</code> (default: 180): Timeout for individual benchmarks - Parallel system adds 5s buffer: total wait = speedtest_timeout + 5s</p>"},{"location":"reference/architecture/benchmark-workflow/#benefits_1","title":"Benefits","text":"<ol> <li>Dramatic Performance Improvement: 96% reduction in benchmark time</li> <li>Scalability: Handles hundreds of agents efficiently</li> <li>Intelligent Distribution: Round-robin ensures fair allocation</li> <li>Priority Awareness: Higher priority jobs get benchmarks first</li> <li>Resource Efficiency: Blocking behavior prevents wasted task assignments</li> <li>Agent Prioritization: Forced benchmark agents get first crack at their job</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#testing","title":"Testing","text":"<p>Verified with 15 mock agents + 3 jobs: - 10 benchmarks completed in 12 seconds (2 forced, 8 agent speed) - Round-robin distribution working correctly - Database tracking and cleanup functioning properly - Mock agents handle benchmark requests correctly</p>"},{"location":"reference/architecture/benchmark-workflow/#related-systems","title":"Related Systems","text":"<p>This benchmark workflow integrates with several other systems:</p> <ul> <li>Cross-Hashlist Sync: Understanding why hashlists change frequently</li> <li>Job Update System: How keyspace values flow into job calculations, including progressive refinement</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Benchmark History: Track benchmark trends over time</li> <li>Performance Prediction: Use ML to predict performance for new combinations</li> <li>Dynamic Re-benchmarking: Trigger new benchmarks on performance anomalies</li> <li>Multi-GPU Optimization: Per-device benchmark tracking</li> <li>Keyspace Prediction: Use historical multipliers to improve initial estimates</li> <li>Intelligent Caching: Detect when hashlist hasn't changed to skip download</li> <li>Adaptive Timeout: Adjust timeout based on historical benchmark completion times</li> <li>Benchmark Prioritization: Queue management for benchmark requests during high load</li> </ol>"},{"location":"reference/architecture/chunking/","title":"KrakenHashes Chunking System","text":""},{"location":"reference/architecture/chunking/#overview","title":"Overview","text":"<p>KrakenHashes uses an intelligent chunking system to distribute password cracking workloads across multiple agents. This document explains how chunks are created, distributed, and tracked for different attack types.</p>"},{"location":"reference/architecture/chunking/#what-is-chunking","title":"What is Chunking?","text":"<p>Chunking divides large password cracking jobs into smaller, manageable pieces that can be: - Distributed across multiple agents for parallel processing - Completed within a reasonable time frame (default: 20 minutes) - Resumed if interrupted or failed - Tracked for accurate progress reporting</p>"},{"location":"reference/architecture/chunking/#how-chunking-works","title":"How Chunking Works","text":""},{"location":"reference/architecture/chunking/#basic-chunking-no-rules","title":"Basic Chunking (No Rules)","text":"<p>For simple dictionary attacks without rules: 1. The system calculates the total keyspace (number of password candidates) 2. Based on agent benchmark speeds, it determines optimal chunk sizes 3. Each chunk processes a portion of the wordlist using hashcat's <code>--skip</code> and <code>--limit</code> parameters</p> <p>Example:  - Wordlist: 1,000,000 passwords - Agent speed: 1,000,000 H/s - Target chunk time: 1,200 seconds (20 minutes) - Chunk size: 1,200,000,000 candidates - Result: Single chunk processes entire wordlist</p>"},{"location":"reference/architecture/chunking/#enhanced-chunking-with-rules","title":"Enhanced Chunking with Rules","text":"<p>When rules are applied, the effective keyspace multiplies:</p> <p>Effective Keyspace = Wordlist Size \u00d7 Number of Rules</p> <p>For example: - Wordlist: 1,000,000 passwords - Rules: 1,000 rules - Effective keyspace: 1,000,000,000 candidates</p>"},{"location":"reference/architecture/chunking/#rule-splitting","title":"Rule Splitting","text":"<p>When a job with rules would take significantly longer than the target chunk time, KrakenHashes can split the rules:</p> <ol> <li>Detection: If estimated time &gt; 2\u00d7 target chunk time</li> <li>Splitting: Divides rules into smaller files</li> <li>Distribution: Each agent receives full wordlist + partial rules</li> <li>Progress: Tracks completion across all rule chunks</li> </ol> <p>Example: - Wordlist: 1,000,000 passwords - Rules: 10,000 rules - Agent speed: 1,000,000 H/s - Without splitting: 10,000 seconds (2.8 hours) per chunk - With splitting into 10 chunks: 1,000 rules each, ~1,000 seconds per chunk</p>"},{"location":"reference/architecture/chunking/#combination-attacks","title":"Combination Attacks","text":"<p>For combination attacks (-a 1), the effective keyspace is:</p> <p>Effective Keyspace = Wordlist1 Size \u00d7 Wordlist2 Size</p> <p>The system tracks progress through the virtual keyspace while hashcat processes the first wordlist sequentially.</p>"},{"location":"reference/architecture/chunking/#attack-mode-support","title":"Attack Mode Support","text":"Attack Mode Description Chunking Method 0 (Straight) Dictionary Wordlist position + optional rule splitting 1 (Combination) Two wordlists Virtual keyspace tracking 3 (Brute-force) Mask attack Mask position chunking 6 (Hybrid W+M) Wordlist + Mask Wordlist position chunking 7 (Hybrid M+W) Mask + Wordlist Mask position chunking 9 (Association) Per-hash rules Rule splitting when applicable"},{"location":"reference/architecture/chunking/#progress-tracking","title":"Progress Tracking","text":""},{"location":"reference/architecture/chunking/#standard-progress","title":"Standard Progress","text":"<ul> <li>Shows candidates tested vs total keyspace</li> <li>Updates in real-time via WebSocket</li> <li>Accurate percentage completion</li> </ul>"},{"location":"reference/architecture/chunking/#with-rule-multiplication","title":"With Rule Multiplication","text":"<ul> <li>Display format: \"X / Y (\u00d7Z)\" where Z is the multiplication factor</li> <li>Accounts for all rules across all chunks</li> <li>Aggregates progress from distributed rule chunks</li> </ul>"},{"location":"reference/architecture/chunking/#progress-bar-visualization","title":"Progress Bar Visualization","text":"<p>The progress bar always shows: - Green: Completed keyspace - Gray: Remaining keyspace - Percentage: Based on effective keyspace</p>"},{"location":"reference/architecture/chunking/#accurate-keyspace-tracking-from-hashcat","title":"Accurate Keyspace Tracking from Hashcat","text":""},{"location":"reference/architecture/chunking/#overview_1","title":"Overview","text":"<p>KrakenHashes captures actual keyspace values directly from hashcat's <code>progress[1]</code> field to ensure precise progress reporting, especially for jobs with rules or combination attacks where estimation can be inaccurate.</p>"},{"location":"reference/architecture/chunking/#how-it-works","title":"How It Works","text":"<ol> <li>Initial Job Creation: Job created with estimated keyspace based on wordlists/rules, <code>is_accurate_keyspace = false</code></li> <li>First Progress Update: Agent sends <code>progress[1]</code> value from hashcat, backend captures it as <code>chunk_actual_keyspace</code></li> <li>Cascade Recalculation: All subsequent chunks' start/end positions are recalculated using cumulative actual keyspace from completed chunks</li> <li>Self-Correcting: The system handles out-of-order chunk completion correctly by recalculating all subsequent chunks whenever a chunk receives its actual keyspace</li> </ol>"},{"location":"reference/architecture/chunking/#technical-details","title":"Technical Details","text":"<p>Chunk Actual Keyspace: - Each chunk's <code>chunk_actual_keyspace</code> field stores the immutable size from hashcat's <code>progress[1]</code> value - This represents the actual keyspace processed by hashcat for that specific chunk - Used to calculate exact start/end positions for subsequent chunks</p> <p>Cascade Updates: When a chunk completes and provides its actual keyspace: 1. Backend updates the chunk's <code>chunk_actual_keyspace</code> and <code>effective_keyspace_end</code> 2. All subsequent chunks (higher chunk numbers) have their positions recalculated 3. New chunk start position = sum of all previous chunks' actual keyspace 4. Works correctly even if chunks complete out of order</p> <p>Dispatched Keyspace Adjustments: - When actual keyspace differs from estimate, <code>dispatched_keyspace</code> is adjusted - Ensures job progress accurately reflects work distribution</p>"},{"location":"reference/architecture/chunking/#database-schema","title":"Database Schema","text":"<p>job_tasks table: - <code>chunk_actual_keyspace</code> (BIGINT): Immutable chunk size from hashcat <code>progress[1]</code> - <code>is_actual_keyspace</code> (BOOLEAN): True when task has actual keyspace from hashcat</p> <p>job_executions table: - <code>is_accurate_keyspace</code> (BOOLEAN): True when keyspace is from hashcat <code>progress[1]</code> - <code>avg_rule_multiplier</code> (FLOAT): Actual/estimated ratio for improving future estimates</p>"},{"location":"reference/architecture/chunking/#benefits","title":"Benefits","text":"<ol> <li>Precise Progress: Progress percentages match hashcat's actual progress, not estimates</li> <li>Better Task Distribution: Chunk sizes calculated based on real keyspace</li> <li>Improved Future Estimates: Multiplier derived from actual values helps estimate remaining work</li> <li>Self-Correcting Ranges: Chunks automatically adjust when actual differs from estimate</li> <li>No Backwards Ranges: Ensures ascending keyspace ranges (10.45T \u2192 20.14T, not 10.45T \u2192 9.60T)</li> </ol>"},{"location":"reference/architecture/chunking/#example","title":"Example","text":"<p>Before accurate tracking: - Estimated: 10.00T per chunk - Reality: 10.51T, 9.63T, 9.71T (varies by rule effectiveness) - Problem: Progress bars and ETAs inaccurate</p> <p>After accurate tracking: - Chunk 1: 0 \u2192 10.51T (actual from hashcat) - Chunk 2: 10.51T \u2192 20.14T (starts where chunk 1 ended) - Chunk 3: 20.14T \u2192 29.85T (continues from chunk 2) - Result: Perfect cumulative chain with accurate progress</p> <p>See Benchmark Workflow for more details on keyspace capture during benchmarking.</p>"},{"location":"reference/architecture/chunking/#dynamic-updates-during-execution","title":"Dynamic Updates During Execution","text":"<p>When wordlists, rules, or potfiles are modified while jobs are running, the chunking system adapts:</p>"},{"location":"reference/architecture/chunking/#automatic-recalculation","title":"Automatic Recalculation","text":"<ul> <li>Keyspace Updates: Effective keyspace recalculates based on current file state</li> <li>Forward-Only: Only undispatched chunks are affected by changes</li> <li>No Interruption: Active chunks continue with their original parameters</li> </ul>"},{"location":"reference/architecture/chunking/#update-scenarios","title":"Update Scenarios","text":"<ol> <li>Wordlist Growth: New words available for future chunks (if not using rule splitting)</li> <li>Rule Changes: Multiplication factor adjusts for remaining work</li> <li>Potfile Updates: Manual refresh triggers keyspace recalculation</li> </ol> <p>For detailed information about how file updates affect running jobs, see the Job Update System documentation.</p>"},{"location":"reference/architecture/chunking/#configuration","title":"Configuration","text":"<p>Administrators can tune chunking behavior via system settings:</p> Setting Default Description <code>default_chunk_duration</code> 1200s Target time per chunk (20 minutes) <code>chunk_fluctuation_percentage</code> 20% Threshold for merging final chunks <code>rule_split_enabled</code> true Enable automatic rule splitting <code>rule_split_threshold</code> 2.0 Time multiplier to trigger splitting <code>rule_split_min_rules</code> 100 Minimum rules before considering split"},{"location":"reference/architecture/chunking/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/chunking/#for-users","title":"For Users","text":"<ol> <li>Large Rule Files: Will automatically split for better distribution</li> <li>Multiple Rule Files: Multiplication is handled automatically</li> <li>Progress Monitoring: Check effective keyspace in job details</li> <li>Benchmarks: Ensure agents have current benchmarks for accurate chunking</li> </ol>"},{"location":"reference/architecture/chunking/#for-administrators","title":"For Administrators","text":"<ol> <li>Chunk Duration: Balance between progress granularity and overhead</li> <li>Rule Splitting: Monitor temp directory space for large rule files</li> <li>Benchmarks: Configure benchmark validity period appropriately</li> <li>Resource Usage: Rule splitting creates temporary files</li> </ol>"},{"location":"reference/architecture/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/chunking/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check if effective keyspace is much larger than expected</li> <li>Verify agent benchmarks are current</li> <li>Consider enabling rule splitting if disabled</li> </ul>"},{"location":"reference/architecture/chunking/#uneven-distribution","title":"Uneven Distribution","text":"<ul> <li>Some chunks may be larger due to:</li> <li>Fluctuation threshold preventing small final chunks</li> <li>Rule count not evenly divisible</li> <li>Different agent speeds</li> </ul>"},{"location":"reference/architecture/chunking/#rule-splitting-not-occurring","title":"Rule Splitting Not Occurring","text":"<p>Verify: - <code>rule_split_enabled</code> is true - Rule file has &gt; <code>rule_split_min_rules</code> rules - Estimated time exceeds threshold</p>"},{"location":"reference/architecture/chunking/#technical-details_1","title":"Technical Details","text":""},{"location":"reference/architecture/chunking/#keyspace-calculation","title":"Keyspace Calculation","text":"<pre><code>Attack Mode 0 (Dictionary):\n- Without rules: wordlist_size\n- With rules: wordlist_size \u00d7 total_rule_count\n\nAttack Mode 1 (Combination):\n- Always: wordlist1_size \u00d7 wordlist2_size\n\nAttack Mode 3 (Brute-force):\n- Calculated from mask: charset_size^length\n\nAttack Mode 6/7 (Hybrid):\n- Wordlist_size \u00d7 mask_keyspace\n</code></pre>"},{"location":"reference/architecture/chunking/#chunk-assignment","title":"Chunk Assignment","text":"<ol> <li>Agent requests work</li> <li>System calculates optimal chunk size based on:</li> <li>Agent's benchmark speed</li> <li>Target chunk duration</li> <li>Remaining keyspace</li> <li>Chunk boundaries determined:</li> <li>Start position (skip)</li> <li>Chunk size (limit)</li> <li>Agent receives chunk assignment</li> <li>Progress tracked and aggregated</li> </ol>"},{"location":"reference/architecture/chunking/#rule-chunk-files","title":"Rule Chunk Files","text":"<p>When rule splitting is active: - Temporary files created in configured directory - Named: <code>job_[ID]_chunk_[N].rule</code> - Automatically cleaned up after job completion - Synced to agents like normal rule files</p>"},{"location":"reference/architecture/chunking/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Pre-calculation of optimal chunk distribution</li> <li>Dynamic chunk resizing based on actual speed</li> <li>Rule deduplication before splitting</li> <li>Compression for rule chunk transfers</li> </ul>"},{"location":"reference/architecture/crack-batching-system/","title":"Crack Batching System","text":""},{"location":"reference/architecture/crack-batching-system/#overview","title":"Overview","text":"<p>The crack batching system is a critical performance optimization that prevents agent crashes and message flooding during high-volume password cracking operations. By batching cracked passwords before transmission and implementing a dual-channel architecture, the system achieves a 100x reduction in message volume while maintaining zero data loss.</p>"},{"location":"reference/architecture/crack-batching-system/#the-problem","title":"The Problem","text":"<p>Prior to the crack batching implementation, agents faced severe issues during high-volume cracking:</p>"},{"location":"reference/architecture/crack-batching-system/#message-flood-scenario","title":"Message Flood Scenario","text":"<ul> <li>Volume: When thousands of hashes crack simultaneously (e.g., 4,000+ in seconds)</li> <li>Individual Messages: Each crack generated a separate WebSocket message</li> <li>Channel Overflow: 256-message outbound buffer would fill instantly</li> <li>Agent Crashes: Buffer overflow caused panic and agent disconnection</li> <li>Data Loss Risk: Cracked passwords could be lost during crashes</li> </ul>"},{"location":"reference/architecture/crack-batching-system/#real-world-impact","title":"Real-World Impact","text":"<pre><code>Before: 4,000 cracks = 8,000+ WebSocket messages\nAfter:  4,000 cracks = 80 batched messages (100x reduction)\n</code></pre>"},{"location":"reference/architecture/crack-batching-system/#architecture","title":"Architecture","text":""},{"location":"reference/architecture/crack-batching-system/#dual-channel-message-system","title":"Dual-Channel Message System","text":"<p>The system separates status updates from crack data transmission:</p>"},{"location":"reference/architecture/crack-batching-system/#1-status-channel-synchronous","title":"1. Status Channel (Synchronous)","text":"<p>Purpose: Real-time job progress without crack payload</p> <p>Message Type: <code>JobStatus</code> <pre><code>{\n  \"task_id\": \"uuid\",\n  \"keyspace_processed\": 1234567,\n  \"progress_percent\": 45.2,\n  \"hash_rate\": 1500000000,\n  \"cracked_count\": 150,\n  \"device_metrics\": [...],\n  \"status\": \"running\"\n}\n</code></pre></p> <p>Characteristics: - Sent every progress update interval - Contains only crack count, not actual data - Lightweight for real-time UI updates - Includes all performance metrics</p>"},{"location":"reference/architecture/crack-batching-system/#2-crack-channel-asynchronous","title":"2. Crack Channel (Asynchronous)","text":"<p>Purpose: Batched transmission of actual cracked passwords</p> <p>Message Type: <code>CrackBatch</code> <pre><code>{\n  \"task_id\": \"uuid\",\n  \"is_retransmit\": false,\n  \"cracked_hashes\": [\n    {\n      \"hash\": \"5f4dcc3b5aa765d61d8327deb882cf99\",\n      \"plaintext\": \"password\",\n      \"original_line\": \"user:5f4dcc3b5aa765d61d8327deb882cf99\"\n    },\n    ...\n  ]\n}\n</code></pre></p> <p>Characteristics: - Sent in batches (10,000 cracks or 500ms window) - Independent from progress updates - Optimized for bulk database operations - Preserves all crack metadata</p>"},{"location":"reference/architecture/crack-batching-system/#batching-parameters","title":"Batching Parameters","text":"Parameter Value Purpose Batch Window 500ms Time to accumulate cracks before flush Buffer Size 10,000 cracks Maximum cracks per batch Channel Buffer 4,096 messages Agent outbound message queue WebSocket Max 50MB Maximum message size (backend)"},{"location":"reference/architecture/crack-batching-system/#batching-logic","title":"Batching Logic","text":"<pre><code>graph TD\n    A[Hashcat Outputs Crack] --&gt; B{Buffer Full?}\n    B --&gt;|Yes 10k cracks| C[Flush Immediately]\n    B --&gt;|No| D[Add to Buffer]\n    D --&gt; E{Timer Expired?}\n    E --&gt;|Yes 500ms| C\n    E --&gt;|No| F[Continue Accumulating]\n    C --&gt; G[Send CrackBatch Message]\n    G --&gt; H[Reset Buffer &amp; Timer]\n    F --&gt; I[Wait for More Cracks]\n    I --&gt; B</code></pre>"},{"location":"reference/architecture/crack-batching-system/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/crack-batching-system/#agent-side-processing","title":"Agent-Side Processing","text":""},{"location":"reference/architecture/crack-batching-system/#crack-detection-and-buffering","title":"Crack Detection and Buffering","text":"<p>Location: <code>agent/internal/jobs/hashcat_executor.go</code></p> <ol> <li>Outfile Monitoring</li> <li>Hashcat writes cracks to <code>--outfile</code></li> <li>Agent monitors file with offset tracking</li> <li> <p>New lines detected via polling (every 100ms)</p> </li> <li> <p>Deduplication <pre><code>// Track sent cracks to prevent duplicates\nOutfileSentHashes map[string]bool\n\n// Key format: \"hash:plaintext\"\nlineKey := fmt.Sprintf(\"%s:%s\", hash, plaintext)\nif !process.OutfileSentHashes[lineKey] {\n    process.OutfileSentHashes[lineKey] = true\n    // Add to batch buffer\n}\n</code></pre></p> </li> <li> <p>Batch Buffer Management <pre><code>// Initialize buffer with 10k capacity\ncrackBatchBuffers[taskID] = make([]CrackedHash, 0, 10000)\n\n// Add crack to buffer\ncrackBatchBuffers[taskID] = append(crackBatchBuffers[taskID], crack)\n\n// Check flush conditions\nif len(crackBatchBuffers[taskID]) &gt;= 10000 {\n    flushCrackBatchLocked(process)\n}\n</code></pre></p> </li> <li> <p>Timer-Based Flushing <pre><code>// 500ms batching window\ntimer := time.NewTimer(500 * time.Millisecond)\ncrackBatchTimers[taskID] = timer\n\n// Flush on timer expiration\n&lt;-timer.C\nflushCrackBatchLocked(process)\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/crack-batching-system/#message-transmission","title":"Message Transmission","text":"<p>Connection Management: <code>agent/internal/agent/connection.go</code></p> <ol> <li> <p>Increased Channel Buffer <pre><code>// Old: 256 messages\n// New: 4096 messages (16x capacity)\noutbound: make(chan []byte, 4096)\n</code></pre></p> </li> <li> <p>Channel Monitoring <pre><code>// Monitor buffer fullness\nfullness := float64(len(ac.outbound)) / float64(cap(ac.outbound)) * 100\n\nif fullness &gt;= 90.0 {\n    logger.Error(\"Outbound channel critically full (%.1f%%)\", fullness)\n} else if fullness &gt;= 75.0 {\n    logger.Warning(\"Outbound channel filling up (%.1f%%)\", fullness)\n}\n</code></pre></p> </li> <li> <p>Graceful Drop Handling <pre><code>select {\ncase ac.outbound &lt;- message:\n    // Message queued successfully\ndefault:\n    // Channel full - log and drop\n    logger.Error(\"Dropped message - channel full (%.1f%%)\", fullness)\n}\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/crack-batching-system/#backend-side-processing","title":"Backend-Side Processing","text":""},{"location":"reference/architecture/crack-batching-system/#crack-batch-handler","title":"Crack Batch Handler","text":"<p>Location: <code>backend/internal/integration/job_websocket_integration.go</code></p> <p>New Handler Function: <pre><code>func (s *JobWebSocketIntegration) HandleCrackBatch(\n    ctx context.Context,\n    agentID int,\n    crackBatch *models.CrackBatch,\n) error {\n    // Validate task exists and belongs to agent\n    task := s.jobTaskRepo.GetByID(ctx, crackBatch.TaskID)\n\n    // Process cracks in optimized batches\n    err := s.processCrackedHashes(ctx, crackBatch.TaskID, crackBatch.CrackedHashes)\n\n    return err\n}\n</code></pre></p>"},{"location":"reference/architecture/crack-batching-system/#optimized-bulk-processing","title":"Optimized Bulk Processing","text":"<ol> <li> <p>Single Bulk Lookup <pre><code>// OLD: N individual queries (1.75M cracks = 1.75M queries)\nfor _, crack := range crackedHashes {\n    hash := hashRepo.GetByHashValue(crack.Hash)\n}\n\n// NEW: One bulk query\nallHashes := hashRepo.GetByHashValues(allHashValues)\n</code></pre></p> </li> <li> <p>Mini-Batch Transactions <pre><code>// Process in 20k chunks to prevent connection leaks\nconst batchSize = 20000\nfor i := 0; i &lt; len(updates); i += batchSize {\n    batch := updates[i:min(i+batchSize, len(updates))]\n    tx := db.Begin()\n    // Process batch\n    tx.Commit()\n}\n</code></pre></p> </li> <li> <p>Pre-Loaded Settings <pre><code>// OLD: Query settings for every crack (N+1 problem)\nfor _, crack := range crackedHashes {\n    potfileEnabled := getSystemSetting(\"potfile_enabled\")\n}\n\n// NEW: Load once before loop\npotfileEnabled := getSystemSetting(\"potfile_enabled\")\nfor _, crack := range crackedHashes {\n    // Use pre-loaded value\n}\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/crack-batching-system/#websocket-configuration","title":"WebSocket Configuration","text":"<p>Location: <code>backend/internal/handlers/websocket/handler.go</code></p> <pre><code>// Increased from 512KB to support large batches\nmaxMessageSize = 50 * 1024 * 1024  // 50MB\n</code></pre> <p>Capacity Calculation: - 10,000 cracks/batch - ~500 bytes/crack (with metadata) - ~5MB total per batch - 50MB max provides 10x safety margin</p>"},{"location":"reference/architecture/crack-batching-system/#performance-impact","title":"Performance Impact","text":""},{"location":"reference/architecture/crack-batching-system/#message-reduction","title":"Message Reduction","text":"Scenario Before After Reduction 4,000 cracks 8,000+ messages 80 messages 100x 100,000 cracks 200,000+ messages 100 messages 2000x 1M cracks 2M+ messages 100 messages 20000x"},{"location":"reference/architecture/crack-batching-system/#network-efficiency","title":"Network Efficiency","text":"<ol> <li>WebSocket Frame Batching</li> <li>Multiple small messages \u2192 Frequent frame overhead</li> <li>Large batched messages \u2192 Amortized frame cost</li> <li> <p>Compression works better on larger payloads</p> </li> <li> <p>Database Efficiency</p> </li> <li>Individual inserts \u2192 N round trips</li> <li>Bulk operations \u2192 Single round trip</li> <li> <p>Transaction overhead reduced by 99%+</p> </li> <li> <p>CPU Utilization</p> </li> <li>Serialization overhead reduced</li> <li>Fewer context switches</li> <li>Better cache locality</li> </ol>"},{"location":"reference/architecture/crack-batching-system/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"reference/architecture/crack-batching-system/#agent-side-metrics","title":"Agent-Side Metrics","text":"<p>Channel Fullness Warnings: <pre><code>[WARNING] Outbound channel filling up (78.2%)\n[ERROR] Outbound channel critically full (92.5%)\n[ERROR] Dropped message - channel full (95.0%)\n</code></pre></p> <p>Batch Flush Events: <pre><code>[INFO] Crack batch buffer reached size limit for task abc-123 (10000 cracks), flushing immediately\n[DEBUG] Flushing crack batch for task abc-123: 523 cracks\n</code></pre></p>"},{"location":"reference/architecture/crack-batching-system/#backend-side-metrics","title":"Backend-Side Metrics","text":"<p>Batch Processing Logs: <pre><code>[INFO] Processing crack batch from agent 5: task=abc-123, crack_count=8472\n[DEBUG] Bulk lookup found 8472 hashes in database\n[INFO] Processed 8472 cracked hashes in 2.3 seconds\n</code></pre></p> <p>Performance Tracking: <pre><code>-- Monitor batch processing times\nSELECT\n    DATE_TRUNC('minute', timestamp) as minute,\n    COUNT(*) as batch_count,\n    AVG(crack_count) as avg_cracks_per_batch,\n    AVG(processing_time_ms) as avg_processing_ms\nFROM crack_batch_metrics\nWHERE timestamp &gt; NOW() - INTERVAL '1 hour'\nGROUP BY minute\nORDER BY minute DESC;\n</code></pre></p>"},{"location":"reference/architecture/crack-batching-system/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"reference/architecture/crack-batching-system/#agent-crash-protection","title":"Agent Crash Protection","text":"<ol> <li>Outfile Offset Tracking</li> <li>Persistent offset prevents duplicate sends</li> <li>Recovery from agent restart</li> <li> <p>No crack data loss</p> </li> <li> <p>Timer Cleanup <pre><code>// Stop timers on task cancellation\nif timer := crackBatchTimers[taskID]; timer != nil {\n    timer.Stop()\n    delete(crackBatchTimers, taskID)\n}\n</code></pre></p> </li> <li> <p>Buffer Cleanup <pre><code>// Flush remaining cracks before exit\ndefer func() {\n    if len(crackBatchBuffers[taskID]) &gt; 0 {\n        flushCrackBatchLocked(process)\n    }\n}()\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/crack-batching-system/#backend-resilience","title":"Backend Resilience","text":"<ol> <li>Task Validation</li> <li>Verify task exists before processing</li> <li>Check agent ownership</li> <li> <p>Ignore orphaned batches</p> </li> <li> <p>Transaction Safety <pre><code>tx, err := db.Begin()\nif err != nil {\n    return fmt.Errorf(\"failed to start transaction: %w\", err)\n}\ndefer tx.Rollback() // Rollback on error\n\n// Process batch\nif err := processBatch(tx, batch); err != nil {\n    return err\n}\n\nreturn tx.Commit()\n</code></pre></p> </li> <li> <p>Graceful Degradation</p> </li> <li>Individual crack failures don't stop batch</li> <li>Partial success logged and tracked</li> <li>Retry logic for transient errors</li> </ol>"},{"location":"reference/architecture/crack-batching-system/#interaction-with-other-systems","title":"Interaction with Other Systems","text":""},{"location":"reference/architecture/crack-batching-system/#potfile-integration","title":"Potfile Integration","text":"<p>Crack batches feed into the potfile staging system:</p> <pre><code>// Stage entire batch at once\npotfileService.StageBatch(ctx, entries)\n</code></pre> <p>See Potfile Management for details.</p>"},{"location":"reference/architecture/crack-batching-system/#job-completion-detection","title":"Job Completion Detection","text":"<p>The <code>AllHashesCracked</code> flag is sent via status messages, not crack batches:</p> <pre><code>// Status message includes completion flag\nstatus := JobStatus{\n    TaskID:           taskID,\n    AllHashesCracked: true,  // Detected from hashcat exit code 6\n    // ...\n}\n</code></pre> <p>See Job Completion System for details.</p>"},{"location":"reference/architecture/crack-batching-system/#progress-tracking","title":"Progress Tracking","text":"<p>Crack counts in status messages provide real-time feedback:</p> <pre><code>// UI shows count without waiting for batch\nstatus.CrackedCount = 1523  // Updated immediately\n// Actual crack data arrives asynchronously\n</code></pre>"},{"location":"reference/architecture/crack-batching-system/#processing-status-integration","title":"Processing Status Integration","text":"<p>The crack batching system integrates with the processing status workflow to ensure jobs don't complete before all crack batches are received.</p> <p>Workflow:</p> <ol> <li>Task Completes Execution:</li> <li>Agent sends final progress message with <code>Status=\"completed\"</code> and <code>CrackedCount</code> field</li> <li>Backend transitions task to <code>processing</code> status</li> <li> <p><code>expected_crack_count</code> set from progress message</p> </li> <li> <p>Crack Batches Transmitted:</p> </li> <li>Agent sends crack batches via <code>crack_batch</code> WebSocket messages</li> <li>Backend increments <code>received_crack_count</code> for each batch</li> <li> <p>Batches processed and stored in database</p> </li> <li> <p>Batch Completion Signal:</p> </li> <li>Agent sends <code>crack_batches_complete</code> WebSocket message</li> <li>Backend sets <code>batches_complete_signaled</code> to true</li> <li> <p>Agent is free to accept new work</p> </li> <li> <p>Task Completion Check:</p> </li> <li>Backend checks: <code>received_crack_count &gt;= expected_crack_count AND batches_complete_signaled == true</code></li> <li>When conditions met: Task transitions from <code>processing</code> to <code>completed</code></li> <li>Job completion check triggered</li> </ol> <p>New WebSocket Message:</p> <p><code>crack_batches_complete</code> (Agent \u2192 Backend): <pre><code>{\n  \"type\": \"crack_batches_complete\",\n  \"task_id\": \"uuid-here\",\n  \"is_retransmit\": false\n}\n</code></pre></p> <p>New Database Fields (job_tasks): - <code>expected_crack_count</code> (INTEGER): Expected cracks from final progress message - <code>received_crack_count</code> (INTEGER): Cracks received via batches - <code>batches_complete_signaled</code> (BOOLEAN): Agent signaled all batches sent</p> <p>Backend Handler:</p> <pre><code>func (s *JobWebSocketIntegration) HandleCrackBatchesComplete(\n    ctx context.Context,\n    agentID int,\n    message *models.CrackBatchesComplete,\n) error {\n    // Mark batches complete\n    err := s.jobTaskRepo.MarkBatchesComplete(ctx, message.TaskID)\n\n    // Check if task ready to complete\n    ready, err := s.jobTaskRepo.CheckTaskReadyToComplete(ctx, message.TaskID)\n    if ready {\n        // Complete the task\n        s.checkTaskCompletion(ctx, message.TaskID)\n    }\n\n    return nil\n}\n</code></pre> <p>Repository Methods:</p> <pre><code>// JobTaskRepository\nSetTaskProcessing(taskID, expectedCracks)    // Transition to processing\nIncrementReceivedCrackCount(taskID, count)   // Track received batches\nMarkBatchesComplete(taskID)                  // Signal batches done\nCheckTaskReadyToComplete(taskID)             // Verify completion conditions\n</code></pre> <p>Benefits: - \u2705 Jobs don't complete prematurely - \u2705 Completion emails have accurate crack counts - \u2705 No race conditions between crack batches and job completion - \u2705 Agent can accept new work immediately after signaling completion</p> <p>See Job Completion System for full processing status workflow.</p>"},{"location":"reference/architecture/crack-batching-system/#outfile-acknowledgment-protocol","title":"Outfile Acknowledgment Protocol","text":"<p>The Outfile Acknowledgment Protocol ensures reliable crack recovery when an agent reconnects after a disconnection. This prevents data loss from outfiles that weren't fully transmitted before the agent went offline.</p>"},{"location":"reference/architecture/crack-batching-system/#problem-solved","title":"Problem Solved","text":"<p>When an agent disconnects mid-job: - Outfiles may contain cracks that weren't transmitted - Agent restart could lose these cracks without proper recovery - Manual recovery is error-prone and time-consuming</p>"},{"location":"reference/architecture/crack-batching-system/#protocol-flow","title":"Protocol Flow","text":"<pre><code>sequenceDiagram\n    participant Agent\n    participant Backend\n\n    Note over Agent: Agent connects/reconnects\n    Agent-&gt;&gt;Backend: pending_outfiles (list of outfile paths)\n\n    loop For each outfile\n        Backend-&gt;&gt;Agent: request_crack_retransmit (task_id, outfile_path)\n        Agent-&gt;&gt;Backend: crack_batch (is_retransmit=true)\n        Agent-&gt;&gt;Backend: crack_batches_complete (is_retransmit=true)\n\n        alt Retransmit successful\n            Backend-&gt;&gt;Agent: outfile_delete_approved (task_id, outfile_path)\n            Note over Agent: Agent deletes outfile\n        else Retransmit failed\n            Backend-&gt;&gt;Agent: outfile_delete_rejected (task_id, reason)\n            Note over Agent: Agent retains outfile for retry\n        end\n    end</code></pre>"},{"location":"reference/architecture/crack-batching-system/#websocket-messages","title":"WebSocket Messages","text":"<p>1. <code>pending_outfiles</code> (Agent \u2192 Backend)</p> <p>Sent when agent connects with existing outfiles from previous sessions:</p> <pre><code>{\n  \"type\": \"pending_outfiles\",\n  \"outfiles\": [\n    {\n      \"task_id\": \"uuid-1\",\n      \"outfile_path\": \"/data/agent/outfiles/task-uuid-1.out\",\n      \"line_count\": 1523\n    },\n    {\n      \"task_id\": \"uuid-2\",\n      \"outfile_path\": \"/data/agent/outfiles/task-uuid-2.out\",\n      \"line_count\": 847\n    }\n  ]\n}\n</code></pre> <p>2. <code>request_crack_retransmit</code> (Backend \u2192 Agent)</p> <p>Backend requests agent to retransmit cracks from a specific outfile:</p> <pre><code>{\n  \"type\": \"request_crack_retransmit\",\n  \"task_id\": \"uuid-1\",\n  \"outfile_path\": \"/data/agent/outfiles/task-uuid-1.out\"\n}\n</code></pre> <p>3. <code>outfile_delete_approved</code> (Backend \u2192 Agent)</p> <p>Backend confirms all cracks received, agent can delete the outfile:</p> <pre><code>{\n  \"type\": \"outfile_delete_approved\",\n  \"task_id\": \"uuid-1\",\n  \"outfile_path\": \"/data/agent/outfiles/task-uuid-1.out\"\n}\n</code></pre> <p>4. <code>outfile_delete_rejected</code> (Backend \u2192 Agent)</p> <p>Backend indicates retransmission incomplete, agent should retain outfile:</p> <pre><code>{\n  \"type\": \"outfile_delete_rejected\",\n  \"task_id\": \"uuid-1\",\n  \"outfile_path\": \"/data/agent/outfiles/task-uuid-1.out\",\n  \"reason\": \"Line count mismatch: expected 1523, received 1400\"\n}\n</code></pre>"},{"location":"reference/architecture/crack-batching-system/#retransmit-flag-behavior","title":"Retransmit Flag Behavior","text":"<p>When <code>is_retransmit: true</code> is set on <code>crack_batch</code> and <code>crack_batches_complete</code> messages:</p> <ol> <li>Duplicate Prevention: Backend uses the flag to identify retransmitted cracks</li> <li>Idempotent Processing: Cracks are matched against existing records</li> <li>Count Tracking: <code>retransmit_count</code> incremented in <code>job_tasks</code> table</li> <li>Timestamp Recording: <code>last_retransmit_at</code> updated for monitoring</li> </ol> <pre><code>// Backend handling of retransmit batches\nif crackBatch.IsRetransmit {\n    // Increment retransmit tracking\n    repo.IncrementRetransmitCount(ctx, taskID)\n\n    // Use idempotent upsert for crack processing\n    // Duplicate cracks are silently ignored\n}\n</code></pre>"},{"location":"reference/architecture/crack-batching-system/#safety-checks","title":"Safety Checks","text":"<p>Line Count Verification: - Agent reports <code>line_count</code> in <code>pending_outfiles</code> - Backend tracks received crack count - Mismatch triggers <code>outfile_delete_rejected</code></p> <p>Race Condition Prevention: - Backend validates task ownership before processing - Retransmits only processed for tasks in <code>processing</code> or <code>completed</code> state - Concurrent retransmits from same outfile are serialized</p> <p>O(1) Hashlist Lookup: - Backend pre-loads hashlist into memory map for fast crack matching - Prevents O(n) scans during large retransmissions - Map keyed by hash value for instant lookup</p> <pre><code>// Optimized hashlist lookup during crack processing\nhashlistMap := make(map[string]*models.Hash)\nfor _, hash := range hashlist.Hashes {\n    hashlistMap[hash.HashValue] = hash\n}\n\n// O(1) lookup per crack\nif existingHash, ok := hashlistMap[crack.Hash]; ok {\n    // Process crack\n}\n</code></pre>"},{"location":"reference/architecture/crack-batching-system/#database-fields-for-retransmit-tracking","title":"Database Fields for Retransmit Tracking","text":"<p>Added to <code>job_tasks</code> table: - <code>retransmit_count</code> (INTEGER, default 0): Number of retransmission attempts - <code>last_retransmit_at</code> (TIMESTAMP): When last retransmission was requested</p> <p>These fields help identify problematic agents or network issues that cause frequent retransmissions.</p>"},{"location":"reference/architecture/crack-batching-system/#benefits","title":"Benefits","text":"<ul> <li>\u2705 Zero data loss during agent disconnections</li> <li>\u2705 Automatic recovery without manual intervention</li> <li>\u2705 Idempotent processing prevents duplicate cracks</li> <li>\u2705 Verification ensures complete retransmission</li> <li>\u2705 Agent cleanup only after backend confirmation</li> </ul>"},{"location":"reference/architecture/crack-batching-system/#configuration-and-tuning","title":"Configuration and Tuning","text":""},{"location":"reference/architecture/crack-batching-system/#default-settings","title":"Default Settings","text":"<p>The default configuration is optimized for most deployments:</p> Setting Default Rationale Batch window 500ms Balance latency vs efficiency Buffer size 10,000 Safe under 50MB message limit Channel buffer 4,096 Handles burst traffic"},{"location":"reference/architecture/crack-batching-system/#custom-tuning","title":"Custom Tuning","text":"<p>For specialized environments, adjust parameters:</p> <pre><code>// Low-latency environment (faster batches)\ncrackBatchInterval = 100 * time.Millisecond\n\n// High-volume environment (larger batches)\ncrackBatchBufferSize = 50000  // Requires testing max message size\n\n// Constrained memory (smaller buffers)\nchannelBufferSize = 1024\n</code></pre> <p>\u26a0\ufe0f Warning: Custom tuning requires thorough testing to prevent message drops or memory exhaustion.</p>"},{"location":"reference/architecture/crack-batching-system/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/crack-batching-system/#for-administrators","title":"For Administrators","text":"<ol> <li>Monitor Channel Fullness</li> <li>Set up alerting for 75%+ warnings</li> <li>Investigate persistent high fullness</li> <li> <p>Check network bandwidth limitations</p> </li> <li> <p>Watch Batch Sizes</p> </li> <li>Typical batches: 500-2000 cracks</li> <li>Large batches (5k+): High-volume cracking (normal)</li> <li> <p>Max batches (10k): Possible tuning needed</p> </li> <li> <p>Database Performance</p> </li> <li>Monitor crack processing time</li> <li>Should be &lt;5 seconds for 10k batch</li> <li>Longer times indicate DB bottleneck</li> </ol>"},{"location":"reference/architecture/crack-batching-system/#for-developers","title":"For Developers","text":"<ol> <li>Preserve Deduplication</li> <li>Always check <code>OutfileSentHashes</code> before adding</li> <li>Use consistent key format: <code>\"hash:plaintext\"</code></li> <li> <p>Clear map on task completion</p> </li> <li> <p>Respect Buffer Limits</p> </li> <li>Never skip flush when buffer is full</li> <li>Always flush remaining cracks on cleanup</li> <li> <p>Handle timer cleanup properly</p> </li> <li> <p>Error Handling</p> </li> <li>Log but don't crash on send failures</li> <li>Track dropped messages for debugging</li> <li>Implement retry logic carefully</li> </ol>"},{"location":"reference/architecture/crack-batching-system/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"reference/architecture/crack-batching-system/#test-scenarios","title":"Test Scenarios","text":"<ol> <li> <p>High-Volume Cracking <pre><code># Test with rockyou.txt on fast GPU\n# Should see batches of 1000-10000 cracks\n# No channel fullness warnings\n</code></pre></p> </li> <li> <p>Agent Crash Recovery <pre><code># Kill agent mid-job\n# Restart agent\n# Verify no duplicate cracks sent\n# Check offset tracking works\n</code></pre></p> </li> <li> <p>Network Disruption <pre><code># Introduce packet loss\n# Verify batches retry correctly\n# Check for data loss\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/crack-batching-system/#success-criteria","title":"Success Criteria","text":"<ul> <li>\u2705 No agent crashes during high-volume cracking</li> <li>\u2705 Message volume reduced by &gt;90%</li> <li>\u2705 Zero crack data loss</li> <li>\u2705 Channel fullness stays &lt;50% under normal load</li> <li>\u2705 Batch processing time &lt;5 seconds for 10k cracks</li> </ul>"},{"location":"reference/architecture/crack-batching-system/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/crack-batching-system/#channel-fullness-warnings","title":"Channel Fullness Warnings","text":"<p>Symptom: Regular warnings about channel filling up</p> <p>Causes: - Backend processing too slow - Network bandwidth insufficient - Database bottleneck</p> <p>Solutions: 1. Check backend logs for slow crack processing 2. Monitor database query times 3. Verify network bandwidth 4. Consider reducing batch size</p>"},{"location":"reference/architecture/crack-batching-system/#missing-cracks","title":"Missing Cracks","text":"<p>Symptom: Crack count doesn't match expected</p> <p>Diagnostic: <pre><code># Check agent logs for drops\ngrep -i \"dropped message\" agent.log\n\n# Check outfile for all cracks\nwc -l /path/to/hashcat.outfile\n\n# Compare with database count\nSELECT COUNT(*) FROM hashes WHERE is_cracked = true AND hashlist_id = ?\n</code></pre></p> <p>Common Causes: - Deduplication working correctly (not an issue) - Channel overflow (check fullness warnings) - Database transaction failures (check backend logs)</p>"},{"location":"reference/architecture/crack-batching-system/#large-batch-performance","title":"Large Batch Performance","text":"<p>Symptom: Slow processing of large batches</p> <p>Solutions: 1. Increase database connection pool 2. Add database indexes on hash_value 3. Optimize bulk update queries 4. Consider reducing batch size</p>"},{"location":"reference/architecture/crack-batching-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements under consideration:</p> <ol> <li>Adaptive Batching</li> <li>Adjust window based on crack rate</li> <li>Smaller batches for low volume</li> <li> <p>Larger batches for sustained high volume</p> </li> <li> <p>Priority Queuing</p> </li> <li>Critical messages bypass batching</li> <li>Completion notifications sent immediately</li> <li> <p>Error reports prioritized</p> </li> <li> <p>Compression</p> </li> <li>Compress batch payloads</li> <li>Reduce network bandwidth</li> <li> <p>Trade CPU for I/O savings</p> </li> <li> <p>Persistent Queuing</p> </li> <li>Disk-backed buffer for extreme volumes</li> <li>Survive agent crashes</li> <li>Replay on reconnection</li> </ol>"},{"location":"reference/architecture/crack-batching-system/#related-documentation","title":"Related Documentation","text":"<ul> <li>Job Completion System - Hashlist completion detection</li> <li>Potfile Management - Crack storage and reuse</li> <li>Agent Troubleshooting - Connection and stability issues</li> <li>System Monitoring - Performance tracking</li> </ul>"},{"location":"reference/architecture/cross-hashlist-sync/","title":"Cross-Hashlist Crack Synchronization","text":""},{"location":"reference/architecture/cross-hashlist-sync/#overview","title":"Overview","text":"<p>KrakenHashes implements a sophisticated cross-hashlist synchronization system that ensures when a hash is cracked, ALL hashlists containing that hash are automatically updated. This system maintains consistency across multiple hashlists while minimizing redundant work and ensuring agents always have current data.</p>"},{"location":"reference/architecture/cross-hashlist-sync/#core-concepts","title":"Core Concepts","text":""},{"location":"reference/architecture/cross-hashlist-sync/#hash-deduplication-model","title":"Hash Deduplication Model","text":"<p>KrakenHashes stores hashes in a central <code>hashes</code> table with two key fields:</p> <ul> <li><code>hash_value</code>: The canonical hash value (e.g., <code>5F4DCC3B5AA765D61D8327DEB882CF99</code>)</li> <li><code>original_hash</code>: The complete original line from upload (e.g., <code>Administrator:500:...:5F4DCC3B5AA765D61D8327DEB882CF99:::</code>)</li> </ul> <p>Key Insight: Multiple users can share the same password hash but have different <code>original_hash</code> values. The system deduplicates by <code>hash_value</code> for cracking efficiency while preserving all original entries.</p>"},{"location":"reference/architecture/cross-hashlist-sync/#many-to-many-relationship","title":"Many-to-Many Relationship","text":"<p>The <code>hashlist_hashes</code> join table links hashlists to hashes: <pre><code>hashlist_1 \u2500\u2500\u2500\u2510\nhashlist_2 \u2500\u2500\u2500\u253c\u2500\u2500\u2192 hash_123 (hash_value: ABC...)\nhashlist_3 \u2500\u2500\u2500\u2518\n</code></pre></p> <p>When <code>hash_123</code> is cracked, all three hashlists need their files regenerated.</p>"},{"location":"reference/architecture/cross-hashlist-sync/#how-it-works","title":"How It Works","text":""},{"location":"reference/architecture/cross-hashlist-sync/#1-crack-detection","title":"1. Crack Detection","text":"<p>When an agent reports cracked hashes via the crack batch mechanism:</p> <pre><code>crackedHashes := []string{\n    \"5F4DCC3B5AA765D61D8327DEB882CF99:password123\",\n    \"098F6BCD4621D373CADE4E832627B4F6:test\",\n}\n</code></pre>"},{"location":"reference/architecture/cross-hashlist-sync/#2-hash-update","title":"2. Hash Update","text":"<p>The system updates the central <code>hashes</code> table: - Sets <code>is_cracked = true</code> - Stores the plaintext password - Updates <code>last_updated</code> timestamp</p> <p>Important: ALL hashes with the same <code>hash_value</code> are marked as cracked, regardless of <code>original_hash</code> or hashlist association.</p>"},{"location":"reference/architecture/cross-hashlist-sync/#3-affected-hashlist-identification","title":"3. Affected Hashlist Identification","text":"<p>The system queries which hashlists contain the cracked hashes:</p> <pre><code>SELECT DISTINCT hl.*\nFROM hashlists hl\nJOIN hashlist_hashes hh ON hl.id = hh.hashlist_id\nJOIN hashes h ON hh.hash_id = h.id\nWHERE h.hash_value = ANY($1)\n</code></pre> <p>This identifies ALL hashlists that need file regeneration.</p>"},{"location":"reference/architecture/cross-hashlist-sync/#4-counter-updates","title":"4. Counter Updates","text":"<p>For each affected hashlist, the system increments its <code>cracked_hashes</code> counter:</p> <pre><code>// Example: If 2 cracked hashes belong to hashlists [98, 98, 99, 100]:\n// - Hashlist 98 increments by 2\n// - Hashlist 99 increments by 1\n// - Hashlist 100 increments by 1\n</code></pre>"},{"location":"reference/architecture/cross-hashlist-sync/#5-file-regeneration","title":"5. File Regeneration","text":"<p>Each affected hashlist file is regenerated from scratch:</p> <p>Process: 1. Query all uncracked hashes for the hashlist 2. Write to temporary file: <code>{hashlist_id}.hash.tmp</code> 3. Atomically rename to <code>{hashlist_id}.hash</code> 4. Calculate new MD5 hash of the file</p> <p>Example: <pre><code>Before crack:\nAdministrator:500:...:5F4DCC3B5AA765D61D8327DEB882CF99:::\nUser1:501:...:098F6BCD4621D373CADE4E832627B4F6:::\nGuest:502:...:E10ADC3949BA59ABBE56E057F20F883E:::\n\nAfter cracking 5F4DCC3B... and 098F6BCD...:\nGuest:502:...:E10ADC3949BA59ABBE56E057F20F883E:::\n</code></pre></p>"},{"location":"reference/architecture/cross-hashlist-sync/#6-agent-synchronization","title":"6. Agent Synchronization","text":"<p>For each affected hashlist, the system updates all agent records: 1. Updates <code>agent_hashlists.file_hash</code> to new MD5 2. This marks agent copies as outdated 3. On next connection, agents detect the mismatch 4. Agents automatically download the updated file</p>"},{"location":"reference/architecture/cross-hashlist-sync/#benefits","title":"Benefits","text":""},{"location":"reference/architecture/cross-hashlist-sync/#1-consistency-across-hashlists","title":"1. Consistency Across Hashlists","text":"<p>If the same hash appears in multiple hashlists (e.g., corporate environments with shared passwords), cracking it once updates all:</p> <pre><code>Scenario: Password \"Summer2024!\" used by:\n- Hashlist A: john@domain.com\n- Hashlist B: john.doe@otherdomain.com\n- Hashlist C: jdoe@thirddomain.com\n\nResult: Cracking ANY of these updates ALL three hashlists automatically\n</code></pre>"},{"location":"reference/architecture/cross-hashlist-sync/#2-efficient-cracking","title":"2. Efficient Cracking","text":"<p>Hashcat never receives duplicate hashes: - Uses <code>DISTINCT hash_value</code> when generating hashlist files - Even if 1000 users share password \"Password1\", hashcat only cracks it once - System propagates the crack to all 1000 entries automatically</p>"},{"location":"reference/architecture/cross-hashlist-sync/#3-real-time-updates","title":"3. Real-Time Updates","text":"<p>Agents always work with current data: - Stale hashlists automatically detected via MD5 mismatch - Fresh files downloaded before task execution - Prevents wasted work on already-cracked hashes</p>"},{"location":"reference/architecture/cross-hashlist-sync/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/cross-hashlist-sync/#code-flow","title":"Code Flow","text":"<p>Backend: <code>HandleCrackBatch</code> in <code>job_websocket_integration.go</code></p> <pre><code>// Track affected hashlists (map[hashlist_id]crack_count)\naffectedHashlists := make(map[int64]int)\n\n// Process each crack\nfor _, crackedEntry := range crackedHashes {\n    // Update hash in database\n    hash.IsCracked = true\n    hash.Password = &amp;plaintext\n\n    // Find which hashlists contain this hash\n    hashlistIDs, _ := s.hashRepo.GetHashlistIDsForHash(ctx, hash.ID)\n\n    // Increment counter for each affected hashlist\n    for _, hashlistID := range hashlistIDs {\n        affectedHashlists[hashlistID]++\n    }\n}\n\n// Update counters and regenerate files\nfor hashlistID, count := range affectedHashlists {\n    s.hashlistRepo.IncrementCrackedCount(ctx, hashlistID, count)\n}\n\n// Trigger file regeneration for all affected hashlists\ns.hashlistSyncService.UpdateHashlistAfterCracks(ctx, hashlistID, crackedHashValues)\n</code></pre> <p>Backend: <code>UpdateHashlistAfterCracks</code> in <code>hashlist_sync_service.go</code></p> <pre><code>// Find ALL hashlists containing these cracked hashes\naffectedHashlists := s.hashlistRepo.GetHashlistsContainingHashes(ctx, hashValues)\n\n// Regenerate each hashlist file\nfor _, hashlist := range affectedHashlists {\n    // Get uncracked hashes\n    uncrackedHashes := s.hashRepo.GetUncrackedHashValuesByHashlistID(ctx, hashlist.ID)\n\n    // Write to temp file\n    file.WriteString(hash + \"\\n\") // for each uncrackedHash\n\n    // Atomic replace\n    os.Rename(tempFile, actualFile)\n\n    // Update agent records\n    for _, agentHashlist := range distribution {\n        agentHashlist.FileHash = &amp;newMD5\n        s.agentHashlistRepo.CreateOrUpdate(ctx, agentHashlist)\n    }\n}\n</code></pre> <p>Agent: <code>ensureHashlist</code> in <code>jobs.go</code></p> <pre><code>// ALWAYS re-download for each task to ensure fresh copy\nif _, err := os.Stat(localPath); err == nil {\n    debug.Info(\"Removing existing hashlist to download fresh copy\")\n    os.Remove(localPath)\n}\n\n// Download fresh copy from backend\nfileInfo := &amp;filesync.FileInfo{\n    Name:     fmt.Sprintf(\"%d.hash\", hashlistID),\n    FileType: \"hashlist\",\n    ID:       int(hashlistID),\n    MD5Hash:  \"\", // Skip verification for speed\n}\ns.fileSync.DownloadFileFromInfo(ctx, fileInfo)\n</code></pre>"},{"location":"reference/architecture/cross-hashlist-sync/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/architecture/cross-hashlist-sync/#scalability","title":"Scalability","text":"<p>File Regeneration Cost: O(U) where U = uncracked hashes per hashlist - Small hashlists (&lt; 10k): Instant regeneration - Medium hashlists (10k-100k): &lt; 1 second - Large hashlists (100k-1M): 1-5 seconds - Very large (&gt; 1M): 5-30 seconds</p> <p>Multi-Hashlist Impact: If 10 hashlists share hashes, ALL 10 regenerate - Sequential processing prevents race conditions - Failures on one hashlist don't block others - Agents notified asynchronously</p>"},{"location":"reference/architecture/cross-hashlist-sync/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Batched Processing: Cracks processed in batches (default: 50 per batch)</li> <li>Atomic Updates: Temp files prevent partial writes</li> <li>Lazy Agent Sync: Agents discover updates on-demand, not pushed</li> <li>Distinct Queries: Hashcat receives deduplicated hashes</li> </ol>"},{"location":"reference/architecture/cross-hashlist-sync/#database-efficiency","title":"Database Efficiency","text":"<p>Counter Updates: Batch increments reduce transaction overhead <pre><code>// Single update per hashlist, not per hash\nIncrementCrackedCount(hashlistID, totalCracksForThisHashlist)\n</code></pre></p> <p>Index Utilization: - <code>hash_value</code> indexed for fast duplicate detection - <code>hashlist_hashes</code> join table indexed on both FKs - <code>is_cracked</code> index for uncracked hash queries</p>"},{"location":"reference/architecture/cross-hashlist-sync/#edge-cases","title":"Edge Cases","text":""},{"location":"reference/architecture/cross-hashlist-sync/#1-hash-in-multiple-hashlists","title":"1. Hash in Multiple Hashlists","text":"<p>Scenario: Same hash in 5 different hashlists</p> <p>Behavior: - Hash marked cracked once in <code>hashes</code> table - All 5 hashlists get counter increments - All 5 hashlist files regenerated - All agents with any of the 5 hashlists notified</p>"},{"location":"reference/architecture/cross-hashlist-sync/#2-partially-failed-regeneration","title":"2. Partially Failed Regeneration","text":"<p>Scenario: Hashlist file regeneration fails for 1 of 5 affected hashlists</p> <p>Behavior: - Error logged but processing continues - Other 4 hashlists still regenerated successfully - Failed hashlist can retry on next crack - Database counters still updated correctly</p>"},{"location":"reference/architecture/cross-hashlist-sync/#3-agent-offline-during-update","title":"3. Agent Offline During Update","text":"<p>Scenario: Agent offline when hashlist updated</p> <p>Behavior: - Agent's <code>file_hash</code> still updated in database - On reconnection, file sync detects mismatch - Agent automatically downloads fresh file - No manual intervention required</p>"},{"location":"reference/architecture/cross-hashlist-sync/#4-empty-hashlist-after-cracks","title":"4. Empty Hashlist After Cracks","text":"<p>Scenario: All hashes in a hashlist get cracked</p> <p>Behavior: - Hashlist file becomes empty (0 bytes) - File still exists (prevents 404 errors) - Hashlist status remains \"ready\" - Progress shows 100% cracked</p>"},{"location":"reference/architecture/cross-hashlist-sync/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"reference/architecture/cross-hashlist-sync/#key-metrics","title":"Key Metrics","text":"<p>Monitor these for cross-hashlist sync health:</p> <ol> <li>File Regeneration Time: Track duration per hashlist</li> <li>Affected Hashlist Count: How many hashlists per crack batch</li> <li>Agent Sync Lag: Time between file update and agent download</li> </ol>"},{"location":"reference/architecture/cross-hashlist-sync/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to trace sync flow:</p> <pre><code>DEBUG: Found affected hashlists for cross-hashlist update\nDEBUG: affected_count=3, hashlist_ids=[98,99,100]\nDEBUG: Regenerating hashlist file 98\nDEBUG: Found 4523 uncracked hashes for hashlist 98\nDEBUG: Updated hashlist 98 file and marked 5 agents for sync\n</code></pre>"},{"location":"reference/architecture/cross-hashlist-sync/#common-issues","title":"Common Issues","text":"<p>Issue: Agents keep downloading same hashlist repeatedly</p> <p>Cause: File regeneration producing different MD5 each time</p> <p>Solution: Ensure consistent hash ordering in queries (ORDER BY hash_value)</p> <p>Issue: Hashlist counters don't match file contents</p> <p>Cause: Counter increments succeeded but file regeneration failed</p> <p>Solution: Check backend logs for file write errors, verify disk space</p> <p>Issue: Cross-hashlist updates slow</p> <p>Cause: Many hashlists sharing same hashes</p> <p>Solution: Normal behavior, consider separating unrelated hashlists</p>"},{"location":"reference/architecture/cross-hashlist-sync/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/cross-hashlist-sync/#for-users","title":"For Users","text":"<ol> <li>Separate Unrelated Hashlists: Don't combine disparate hash sources if not needed</li> <li>Monitor Crack Rates: Expect brief spikes in file I/O during large crack batches</li> <li>Agent Connectivity: Keep agents connected for timely file updates</li> </ol>"},{"location":"reference/architecture/cross-hashlist-sync/#for-administrators","title":"For Administrators","text":"<ol> <li>Disk I/O Monitoring: Watch for I/O spikes during high-volume cracking</li> <li>Database Indexing: Ensure indexes on join tables are maintained</li> <li>Log Review: Periodically check for file regeneration failures</li> </ol>"},{"location":"reference/architecture/cross-hashlist-sync/#for-developers","title":"For Developers","text":"<ol> <li>Transaction Boundaries: Always wrap counter updates and file operations together</li> <li>Error Handling: Log failures but continue processing other hashlists</li> <li>Atomic File Operations: Use temp files + rename for atomic updates</li> <li>Consistent Ordering: Always ORDER BY hash_value for deterministic output</li> </ol>"},{"location":"reference/architecture/cross-hashlist-sync/#related-systems","title":"Related Systems","text":"<ul> <li>Crack Batching System: How cracks are collected and sent</li> <li>Job Update System: How jobs adapt to file changes</li> <li>File Sync: Agent file synchronization mechanism</li> </ul>"},{"location":"reference/architecture/cross-hashlist-sync/#summary","title":"Summary","text":"<p>Cross-hashlist crack synchronization ensures consistency across the entire KrakenHashes system. By automatically propagating cracks to all affected hashlists and regenerating files, the system eliminates redundant work while keeping all components synchronized. This architecture scales efficiently from small deployments to enterprise environments with thousands of hashlists.</p>"},{"location":"reference/architecture/file-hash-cache/","title":"File Hash Cache System","text":""},{"location":"reference/architecture/file-hash-cache/#overview","title":"Overview","text":"<p>The File Hash Cache is an in-memory caching system that optimizes the directory monitor service by eliminating redundant MD5 hash calculations for unchanged files. This dramatically reduces disk I/O and prevents SSD wear in deployments with large wordlists and rule files.</p>"},{"location":"reference/architecture/file-hash-cache/#problem-statement","title":"Problem Statement","text":"<p>The directory monitor service runs every 30 seconds to detect changes in wordlist and rule files. Before the cache implementation:</p> <ul> <li>Every scan calculated MD5 hashes for ALL files regardless of whether they changed</li> <li>For large wordlists (10-15GB+), this caused ~500MB/s constant disk I/O</li> <li>SSD wear: Continuous reading would rapidly wear out solid-state drives</li> <li>Resource waste: CPU cycles spent hashing unchanged files</li> </ul>"},{"location":"reference/architecture/file-hash-cache/#solution-architecture","title":"Solution Architecture","text":""},{"location":"reference/architecture/file-hash-cache/#file-hash-cache-backendinternalcachefilehashcachego","title":"File Hash Cache (<code>backend/internal/cache/filehash/cache.go</code>)","text":"<p>The cache stores file metadata alongside cached hash values:</p> <pre><code>type CachedFileInfo struct {\n    Path    string\n    ModTime time.Time\n    Size    int64\n    MD5Hash string\n}\n\ntype Cache struct {\n    entries map[string]CachedFileInfo\n    mu      sync.RWMutex\n}\n</code></pre> <p>Key Features:</p> <ol> <li>ModTime+Size Validation: Before recalculating MD5, the cache checks if the file's modification time and size have changed</li> <li>Thread-Safe: Uses RWMutex for concurrent read access with exclusive writes</li> <li>Self-Populating: Cache entries are created on first access via <code>GetOrCalculate()</code></li> <li>Background Population: Asynchronous startup population to avoid blocking server start</li> </ol>"},{"location":"reference/architecture/file-hash-cache/#cache-lookup-flow","title":"Cache Lookup Flow","text":"<pre><code>GetOrCalculate(filePath)\n    \u2502\n    \u251c\u2500\u2500 os.Stat(filePath) \u2192 Get current modTime, size\n    \u2502\n    \u251c\u2500\u2500 RLock \u2192 Check cache\n    \u2502   \u2502\n    \u2502   \u2514\u2500\u2500 Cache hit? (modTime AND size match)\n    \u2502       \u2502\n    \u2502       \u251c\u2500\u2500 YES \u2192 Return cached hash (no disk read)\n    \u2502       \u2502\n    \u2502       \u2514\u2500\u2500 NO \u2192 Calculate MD5, update cache\n    \u2502\n    \u2514\u2500\u2500 Return hash\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#integration-points","title":"Integration Points","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     main.go         \u2502\n\u2502  (creates cache)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   MonitorService    \u2502\n\u2502   (receives cache)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  DirectoryMonitor   \u2502\n\u2502  (uses cache for    \u2502\n\u2502   hash lookups)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#potfile-hash-history","title":"Potfile Hash History","text":""},{"location":"reference/architecture/file-hash-cache/#the-problem","title":"The Problem","text":"<p>During heavy crack ingestion (e.g., processing 24 million cracked passwords over several hours):</p> <ol> <li>Potfile MD5 changes every few seconds as batches are written</li> <li>Agent downloads potfile (takes ~30 seconds for large files)</li> <li>By the time download completes, the potfile has changed</li> <li>Agent's hash doesn't match current hash \u2192 triggers re-download</li> <li>Infinite loop: Agent continuously re-downloads the potfile</li> </ol>"},{"location":"reference/architecture/file-hash-cache/#the-solution-rolling-hash-history","title":"The Solution: Rolling Hash History","text":"<p>The <code>PotfileHistory</code> maintains a 5-minute window of recent potfile hashes:</p> <pre><code>type PotfileHashEntry struct {\n    MD5Hash   string\n    Timestamp time.Time\n    Size      int64\n}\n\ntype PotfileHistory struct {\n    entries []PotfileHashEntry\n    maxAge  time.Duration  // 5 minutes\n    mu      sync.RWMutex\n}\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#how-it-works","title":"How It Works","text":"<ol> <li>Recording: After each potfile update, the new MD5 hash is added to the history</li> <li>Validation: When an agent reports its potfile hash, the system checks if it matches ANY hash in the 5-minute window</li> <li>Acceptance: If the agent's hash is in the history, the potfile is considered \"in sync enough\"</li> <li>Expiration: After ingestion stops, old hashes expire, ensuring eventual consistency</li> </ol>"},{"location":"reference/architecture/file-hash-cache/#flow-during-heavy-ingestion","title":"Flow During Heavy Ingestion","text":"<pre><code>Heavy Ingestion Scenario:\n\nt=0:   Batch N written \u2192 MD5_N added to history\nt=5:   Agent starts downloading potfile\nt=10:  Batch N+1 written \u2192 MD5_N+1 added to history\nt=35:  Agent finishes download with MD5_N\nt=35:  File sync check: \"Is MD5_N valid?\"\n       \u2192 potfileHistory.IsValid(MD5_N) = TRUE (within 5-min window)\n       \u2192 Agent skips re-download\n\nAfter ingestion stops (5+ minutes idle):\nt=340: Old hashes expire from history\nt=345: Next agent sync: only current MD5 in history\nt=350: Agent with old MD5 \u2192 IsValid() = FALSE \u2192 downloads latest\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#websocket-handler-integration","title":"WebSocket Handler Integration","text":"<p>In <code>determineFilesToSync()</code>, the potfile check occurs before standard MD5 comparison:</p> <pre><code>for _, file := range backendFiles {\n    agentFile, exists := agentFileMap[key]\n\n    // Special handling for potfile during heavy ingestion\n    if file.FileType == \"wordlist\" &amp;&amp; strings.HasSuffix(file.Name, \"potfile.txt\") {\n        if exists &amp;&amp; h.potfileHistory.IsValid(agentFile.MD5Hash) {\n            // Agent has a recent valid potfile - skip re-download\n            continue\n        }\n    }\n\n    // Normal comparison for all other files\n    if !exists || agentFile.MD5Hash != file.MD5Hash {\n        filesToSync = append(filesToSync, file)\n    }\n}\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/file-hash-cache/#files-created","title":"Files Created","text":"File Purpose <code>backend/internal/cache/filehash/cache.go</code> File hash cache with modTime+size validation <code>backend/internal/cache/filehash/potfile_history.go</code> Rolling 5-minute potfile hash history"},{"location":"reference/architecture/file-hash-cache/#files-modified","title":"Files Modified","text":"File Changes <code>backend/internal/monitor/directory_monitor.go</code> Inject cache, use <code>GetOrCalculate()</code> <code>backend/internal/services/monitor_service.go</code> Accept and pass cache to DirectoryMonitor <code>backend/cmd/server/main.go</code> Create cache and history, wire dependencies <code>backend/internal/services/potfile_service.go</code> Add hash to history after updates <code>backend/internal/handlers/websocket/handler.go</code> Check potfile history during sync <code>backend/internal/routes/websocket_with_jobs.go</code> Pass potfileHistory to handler"},{"location":"reference/architecture/file-hash-cache/#performance-metrics","title":"Performance Metrics","text":""},{"location":"reference/architecture/file-hash-cache/#before-vs-after","title":"Before vs After","text":"Metric Before After Improvement Disk I/O (steady state) ~500 MB/s Near zero 99%+ reduction MD5 calculations per cycle All files Only changed files Variable Memory usage N/A ~100 bytes/file Minimal Agent potfile re-downloads during ingestion Continuous None 100% reduction"},{"location":"reference/architecture/file-hash-cache/#memory-footprint","title":"Memory Footprint","text":"<ul> <li>File hash cache: ~100 bytes per file entry</li> <li>Potfile history: ~50 bytes per entry, pruned every 5 minutes</li> <li>Typical deployment: &lt;10MB total memory overhead</li> </ul>"},{"location":"reference/architecture/file-hash-cache/#configuration","title":"Configuration","text":"<p>No configuration required. The file hash cache and potfile history are:</p> <ul> <li>Automatically initialized at server startup</li> <li>Self-managing (automatic population and expiration)</li> <li>Transparent to users and administrators</li> </ul>"},{"location":"reference/architecture/file-hash-cache/#startup-behavior","title":"Startup Behavior","text":"<ol> <li>Cache is created empty</li> <li>Background goroutine populates cache by walking directories</li> <li>Server continues starting without waiting for population</li> <li>Cache entries are also created on-demand during directory scans</li> </ol>"},{"location":"reference/architecture/file-hash-cache/#skip-patterns","title":"Skip Patterns","text":"<p>The following patterns are excluded from cache population:</p> <ul> <li><code>potfile.txt</code> - Handled separately via potfile history</li> <li><code>association/</code> - Association attack wordlists are job-specific</li> </ul>"},{"location":"reference/architecture/file-hash-cache/#debugging","title":"Debugging","text":""},{"location":"reference/architecture/file-hash-cache/#log-messages","title":"Log Messages","text":"<p>Cache activity is logged at DEBUG level:</p> <pre><code>DEBUG: Skipping wordlist with unchanged hash: general/crackstation.txt\nDEBUG: Skipping rule with unchanged hash: hashcat/best64.rule\nDEBUG: Agent has valid recent potfile hash 8ef087e..., skipping sync\n</code></pre>"},{"location":"reference/architecture/file-hash-cache/#verifying-cache-is-working","title":"Verifying Cache is Working","text":"<p>Check backend logs for \"Skipping ... with unchanged hash\" messages during directory monitor cycles.</p>"},{"location":"reference/architecture/file-hash-cache/#verifying-potfile-history","title":"Verifying Potfile History","text":"<p>Look for \"Agent has valid recent potfile hash\" messages during agent file sync operations.</p>"},{"location":"reference/architecture/file-hash-cache/#risk-assessment","title":"Risk Assessment","text":"Risk Likelihood Mitigation Cache returns stale hash Low modTime+size checked on every access Memory exhaustion Very Low ~100 bytes per file, bounded by filesystem Concurrency issues Low RWMutex pattern, proven in production Potfile sync issues Low 5-minute window ensures eventual consistency"},{"location":"reference/architecture/file-hash-cache/#related-documentation","title":"Related Documentation","text":"<ul> <li>Job Update System - How file changes trigger job updates</li> <li>Potfile Management - Potfile operational guide</li> <li>Performance Tuning - General performance optimization</li> </ul>"},{"location":"reference/architecture/increment-mode/","title":"Increment Mode Implementation","text":""},{"location":"reference/architecture/increment-mode/#overview","title":"Overview","text":"<p>KrakenHashes now supports hashcat's <code>--increment</code> and <code>--increment-inverse</code> modes for mask-based attacks (bruteforce and hybrid). Instead of letting hashcat handle increment mode internally (which breaks distributed task assignment), the backend decomposes increment mode into discrete \"layers\" - one per mask length - and schedules them independently.</p>"},{"location":"reference/architecture/increment-mode/#architecture","title":"Architecture","text":""},{"location":"reference/architecture/increment-mode/#layer-based-approach","title":"Layer-Based Approach","text":"<p>When a job is created with increment mode enabled:</p> <ol> <li>Job Creation: The mask (e.g., <code>?l?l?l</code>) with <code>increment_min=2</code> and <code>increment_max=3</code> is decomposed into layers</li> <li>Layer 0: <code>?l?l</code> (length 2)</li> <li> <p>Layer 1: <code>?l?l?l</code> (length 3)</p> </li> <li> <p>Independent Scheduling: Each layer is treated like a separate job by the scheduler</p> </li> <li>Layers are scheduled in order (increment: shortest\u2192longest, increment_inverse: longest\u2192shortest)</li> <li>When a layer runs out of work, the next scheduling cycle picks up the next layer</li> <li> <p>All layers share the job's <code>max_agents</code> limit</p> </li> <li> <p>Progress Aggregation: Progress flows through three levels</p> </li> <li>Tasks \u2192 Layers (via polling service)</li> <li>Layers \u2192 Job (via polling service)</li> <li>Job progress represents completion across all layers</li> </ol>"},{"location":"reference/architecture/increment-mode/#database-schema","title":"Database Schema","text":""},{"location":"reference/architecture/increment-mode/#job_increment_layers","title":"job_increment_layers","text":"<p>Created by migration <code>000088_create_job_increment_layers.up.sql</code></p> <pre><code>CREATE TABLE job_increment_layers (\n    id UUID PRIMARY KEY,\n    job_execution_id UUID NOT NULL REFERENCES job_executions(id) ON DELETE CASCADE,\n    layer_index INT NOT NULL,\n    mask VARCHAR(255) NOT NULL,\n    status VARCHAR(50) NOT NULL DEFAULT 'pending',\n    base_keyspace BIGINT,\n    effective_keyspace BIGINT,\n    processed_keyspace BIGINT DEFAULT 0,\n    dispatched_keyspace BIGINT DEFAULT 0,\n    is_accurate_keyspace BOOLEAN DEFAULT FALSE,\n    overall_progress_percent NUMERIC(5,2) DEFAULT 0.00,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    error_message TEXT,\n    UNIQUE(job_execution_id, layer_index)\n);\n</code></pre> <p>Status Flow (same as job_executions since layers are jobs in their own right): - <code>pending</code> \u2192 <code>running</code> \u2192 <code>completed</code>/<code>failed</code>/<code>cancelled</code> - Use <code>is_accurate_keyspace</code> to determine if benchmark is needed (not a separate status)</p>"},{"location":"reference/architecture/increment-mode/#job_tasksincrement_layer_id","title":"job_tasks.increment_layer_id","text":"<p>Created by migration <code>000089_add_increment_layer_id_to_job_tasks.up.sql</code></p> <pre><code>ALTER TABLE job_tasks\n    ADD COLUMN increment_layer_id UUID REFERENCES job_increment_layers(id) ON DELETE CASCADE;\n</code></pre> <p>Links tasks to their specific layer.</p>"},{"location":"reference/architecture/increment-mode/#preset_increment_layers","title":"preset_increment_layers","text":"<p>Created by migration <code>000090_create_preset_increment_layers.up.sql</code></p> <p>Pre-calculated increment layers for preset jobs. When a job is created from a preset with increment mode enabled, these layers are copied to <code>job_increment_layers</code> rather than being recalculated.</p> <pre><code>CREATE TABLE preset_increment_layers (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    preset_job_id UUID NOT NULL REFERENCES preset_jobs(id) ON DELETE CASCADE,\n    layer_index INT NOT NULL,\n    mask VARCHAR(512) NOT NULL,\n    base_keyspace BIGINT,\n    effective_keyspace BIGINT,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    UNIQUE(preset_job_id, layer_index)\n);\n</code></pre> <p>Purpose: Pre-calculate layers at preset creation time rather than job creation time. This ensures: - Consistent keyspace calculations across all jobs created from the same preset - Faster job creation (no need to re-run hashcat --keyspace for each layer) - Preset keyspace = sum of all layer effective_keyspaces</p> <p>Data Flow: 1. Admin creates preset job with increment mode \u2192 <code>preset_increment_layers</code> populated 2. User creates job from preset \u2192 layers copied from <code>preset_increment_layers</code> to <code>job_increment_layers</code> 3. Job inherits preset's total keyspace</p>"},{"location":"reference/architecture/increment-mode/#key-components","title":"Key Components","text":""},{"location":"reference/architecture/increment-mode/#1-mask-parser-backendinternalutilsmask_parsergo","title":"1. Mask Parser (<code>backend/internal/utils/mask_parser.go</code>)","text":"<p>Handles all hashcat mask placeholders: - <code>?l</code> = lowercase (26) - <code>?u</code> = uppercase (26) - <code>?d</code> = digits (10) - <code>?s</code> = special chars (33) - <code>?a</code> = all printable (95) - <code>?b</code> = binary (0x00-0xFF) - <code>?1-?9</code> = custom charsets</p> <p>Key Functions: - <code>ParseMask(mask string)</code> - Parses mask into positions - <code>GenerateIncrementLayers(mask, min, max, isInverse)</code> - Generates layer masks - <code>GetMaskLength(mask string)</code> - Returns position count</p>"},{"location":"reference/architecture/increment-mode/#2-layer-initialization-backendinternalservicesjob_increment_layer_servicego","title":"2. Layer Initialization (<code>backend/internal/services/job_increment_layer_service.go</code>)","text":"<p><code>initializeIncrementLayers()</code> - Called during job creation</p> <ol> <li>Validates increment settings:</li> <li><code>increment_min &gt;= 1</code></li> <li><code>increment_max &gt;= increment_min</code></li> <li><code>increment_min &lt;= mask_length</code></li> <li> <p><code>increment_max &lt;= mask_length</code></p> </li> <li> <p>Generates layer masks using <code>utils.GenerateIncrementLayers()</code></p> </li> <li> <p>Calculates base_keyspace per layer:    <pre><code>hashcat --keyspace -a 3 -m &lt;hash_type&gt; &lt;layer_mask&gt;\n</code></pre></p> </li> <li> <p>Creates layer records in <code>job_increment_layers</code> table</p> </li> <li> <p>Updates job.total_keyspace to sum of all layer keyspaces</p> </li> </ol>"},{"location":"reference/architecture/increment-mode/#3-benchmark-integration-backendinternalservicesjob_scheduling_benchmark_planninggo","title":"3. Benchmark Integration (<code>backend/internal/services/job_scheduling_benchmark_planning.go</code>)","text":"<p>Modified <code>collectJobHashTypeInfo()</code>: - Detects jobs with increment layers - Adds each layer needing benchmark (<code>is_accurate_keyspace=false</code>) to planning queue - Sorting: Priority \u2192 Created Time \u2192 Layer Index</p> <p><code>ForcedBenchmarkTask</code> now includes: <pre><code>type ForcedBenchmarkTask struct {\n    AgentID    int\n    JobID      uuid.UUID\n    LayerID    *uuid.UUID  // NEW\n    Mask       string      // NEW\n    HashType   int\n    AttackMode models.AttackMode\n    Priority   int\n}\n</code></pre></p>"},{"location":"reference/architecture/increment-mode/#4-task-assignment-backendinternalservicesjob_scheduling_task_assignmentgo","title":"4. Task Assignment (<code>backend/internal/services/job_scheduling_task_assignment.go</code>)","text":"<p>JobPlanningState tracks current layer: <pre><code>type JobPlanningState struct {\n    JobExecution    *models.JobExecution\n    CurrentLayer    *models.JobIncrementLayer     // Active layer\n    AvailableLayers []models.JobIncrementLayer   // All layers with pending work\n    // ... other fields\n}\n</code></pre></p> <p>Layer Loading in <code>CreateTaskAssignmentPlans()</code>: <pre><code>if job.IncrementMode != \"\" &amp;&amp; job.IncrementMode != \"off\" {\n    layers, err := jobIncrementLayerRepo.GetLayersWithPendingWork(ctx, job.ID)\n    if len(layers) &gt; 0 {\n        state.CurrentLayer = &amp;layers[0]  // First layer with work\n    }\n}\n</code></pre></p> <p>Layer-Aware Chunking in <code>calculateKeyspaceChunk()</code>: - Uses <code>layer.EffectiveKeyspace</code> instead of <code>job.TotalKeyspace</code> - Uses <code>layer.DispatchedKeyspace</code> for tracking - Updates layer's <code>dispatched_keyspace</code> via <code>IncrementDispatchedKeyspace()</code></p> <p>TaskAssignmentPlan includes layer context: <pre><code>type TaskAssignmentPlan struct {\n    IncrementLayerID *uuid.UUID  // Links task to layer\n    LayerMask        string      // Layer-specific mask\n    // ... other fields\n}\n</code></pre></p>"},{"location":"reference/architecture/increment-mode/#5-command-building-backendinternalservicesjob_execution_servicego","title":"5. Command Building (<code>backend/internal/services/job_execution_service.go</code>)","text":"<p><code>buildAttackCommand()</code> signature updated: <pre><code>func buildAttackCommand(ctx, presetJob, job, layerMask string) (string, error)\n</code></pre></p> <p>Layer-Aware Behavior: - When <code>layerMask != \"\"</code>, uses it instead of <code>job.Mask</code> - Skips increment flags (<code>--increment</code>, <code>--increment-min</code>, <code>--increment-max</code>) - Backend handles layer distribution, not hashcat</p>"},{"location":"reference/architecture/increment-mode/#6-progress-calculation-backendinternalservicesjob_progress_calculation_servicego","title":"6. Progress Calculation (<code>backend/internal/services/job_progress_calculation_service.go</code>)","text":"<p>Three-Level Aggregation:</p> <pre><code>// Regular jobs: Tasks \u2192 Job\nfunc calculateRegularJobProgress(ctx, job) (*JobProgressUpdate, error)\n\n// Increment jobs: Tasks \u2192 Layers \u2192 Job\nfunc calculateIncrementJobProgress(ctx, job) (*JobProgressUpdate, error) {\n    // 1. Update each layer from its tasks\n    for _, layer := range layers {\n        calculateAndUpdateLayerProgress(ctx, layer)\n    }\n\n    // 2. Aggregate layers to job\n    totalProcessed = sum(layer.ProcessedKeyspace)\n    totalEffective = sum(layer.EffectiveKeyspace)\n    progressPercent = (totalProcessed / totalEffective) * 100\n}\n</code></pre> <p>Polling Frequency: Every 2 seconds</p>"},{"location":"reference/architecture/increment-mode/#7-job-detection-backendinternalrepositoryjob_execution_repositorygo","title":"7. Job Detection (<code>backend/internal/repository/job_execution_repository.go</code>)","text":"<p><code>GetJobsWithPendingWork()</code> includes layer check: <pre><code>WHERE je.status IN ('pending', 'running')\nAND (\n    -- Regular conditions...\n    OR\n    -- Increment mode job with layers that have pending work\n    (je.increment_mode IS NOT NULL AND je.increment_mode != 'off'\n     AND EXISTS (\n        SELECT 1 FROM job_increment_layers jil\n        WHERE jil.job_execution_id = je.id\n          AND jil.status IN ('ready', 'running')\n          AND (jil.effective_keyspace IS NULL\n               OR jil.dispatched_keyspace &lt; jil.effective_keyspace)\n     ))\n)\n</code></pre></p>"},{"location":"reference/architecture/increment-mode/#8-api-endpoints-backendinternalhandlersjobsuser_jobsgo","title":"8. API Endpoints (<code>backend/internal/handlers/jobs/user_jobs.go</code>)","text":""},{"location":"reference/architecture/increment-mode/#get-apijobsidlayers","title":"GET <code>/api/jobs/{id}/layers</code>","text":"<p>Returns all layers for a job with statistics.</p> <p>Response: Array of <code>JobIncrementLayerWithStats</code> <pre><code>[\n  {\n    \"id\": \"uuid\",\n    \"job_execution_id\": \"uuid\",\n    \"layer_index\": 0,\n    \"mask\": \"?l?l\",\n    \"status\": \"running\",\n    \"base_keyspace\": 676,\n    \"effective_keyspace\": 676,\n    \"processed_keyspace\": 338,\n    \"dispatched_keyspace\": 676,\n    \"is_accurate_keyspace\": true,\n    \"overall_progress_percent\": 50.00,\n    \"active_task_count\": 2,\n    \"completed_task_count\": 1,\n    \"cracked_count\": 5\n  }\n]\n</code></pre></p>"},{"location":"reference/architecture/increment-mode/#get-apijobsidlayerslayer_idtasks","title":"GET <code>/api/jobs/{id}/layers/{layer_id}/tasks</code>","text":"<p>Returns all tasks for a specific layer.</p> <p>Response: Array of <code>JobTask</code> (filtered by <code>increment_layer_id</code>)</p>"},{"location":"reference/architecture/increment-mode/#workflow-example","title":"Workflow Example","text":""},{"location":"reference/architecture/increment-mode/#job-creation-lll-with-min2-max3","title":"Job Creation: <code>?l?l?l</code> with min=2, max=3","text":"<ol> <li>User creates job with:</li> <li>Attack mode: Bruteforce (3)</li> <li>Mask: <code>?l?l?l</code></li> <li>Increment mode: <code>increment</code></li> <li>Increment min: 2</li> <li> <p>Increment max: 3</p> </li> <li> <p>Validation (<code>initializeIncrementLayers</code>):    <pre><code>min=2 &gt;= 1 \u2713\nmax=3 &gt;= min=2 \u2713\nmin=2 &lt;= mask_length=3 \u2713\nmax=3 &lt;= mask_length=3 \u2713\n</code></pre></p> </li> <li> <p>Layer Generation:    <pre><code>Layer 0: ?l?l (index=0, base_keyspace=676)\nLayer 1: ?l?l?l (index=1, base_keyspace=17576)\n</code></pre></p> </li> <li> <p>Job Record Created:</p> </li> <li><code>total_keyspace = 676 + 17576 = 18252</code></li> <li><code>increment_mode = \"increment\"</code></li> <li>Status: <code>pending</code></li> </ol>"},{"location":"reference/architecture/increment-mode/#scheduling-cycle-1-benchmark-layer-0","title":"Scheduling Cycle 1: Benchmark Layer 0","text":"<ol> <li> <p>Scheduler detects Layer 0 needs benchmark (<code>is_accurate_keyspace=false</code>)</p> </li> <li> <p>Agent allocated for forced benchmark:</p> </li> <li>Command: <code>hashcat -a 3 -m &lt;type&gt; ?l?l --keyspace-only</code></li> <li> <p>Captures <code>progress[1]</code> value \u2192 <code>effective_keyspace</code></p> </li> <li> <p>Layer updated:</p> </li> <li><code>effective_keyspace = 676</code> (from hashcat)</li> <li><code>is_accurate_keyspace = true</code></li> <li><code>status = 'ready'</code></li> </ol>"},{"location":"reference/architecture/increment-mode/#scheduling-cycle-2-work-on-layer-0","title":"Scheduling Cycle 2: Work on Layer 0","text":"<ol> <li> <p>Scheduler detects Layer 0 has pending work</p> </li> <li> <p>Agent 1 allocated:</p> </li> <li>Plan created with <code>IncrementLayerID = layer0.id</code>, <code>LayerMask = \"?l?l\"</code></li> <li>Command: <code>hashcat -a 3 -m &lt;type&gt; ?l?l</code> (no --increment flags!)</li> <li>Task created with <code>increment_layer_id = layer0.id</code></li> <li> <p>Layer's <code>dispatched_keyspace += 676</code></p> </li> <li> <p>Layer 0 exhausted: <code>dispatched_keyspace (676) &gt;= effective_keyspace (676)</code></p> </li> </ol>"},{"location":"reference/architecture/increment-mode/#scheduling-cycle-3-benchmark-layer-1","title":"Scheduling Cycle 3: Benchmark Layer 1","text":"<ol> <li> <p>Scheduler detects Layer 1 needs benchmark</p> </li> <li> <p>Agent allocated for benchmark of <code>?l?l?l</code></p> </li> <li> <p>Layer 1 ready for work</p> </li> </ol>"},{"location":"reference/architecture/increment-mode/#scheduling-cycle-4-work-on-layer-1","title":"Scheduling Cycle 4: Work on Layer 1","text":"<ol> <li> <p>Scheduler detects Layer 1 has pending work</p> </li> <li> <p>Multiple agents allocated:</p> </li> <li>Each gets chunk of Layer 1's keyspace</li> <li>Commands use mask <code>?l?l?l</code> with <code>--skip</code> and <code>--limit</code> if splitting</li> </ol>"},{"location":"reference/architecture/increment-mode/#progress-updates-every-2-seconds","title":"Progress Updates (Every 2 seconds)","text":"<ol> <li> <p>Task progress reported by agents via WebSocket</p> </li> <li> <p>Polling service aggregates:    <pre><code>Layer 0: processed_keyspace = 676, progress = 100%\nLayer 1: processed_keyspace = 8788, progress = 50%\n\nJob: processed_keyspace = 9464, progress = 51.8%\n</code></pre></p> </li> </ol>"},{"location":"reference/architecture/increment-mode/#job-completion","title":"Job Completion","text":"<p>All layers reach <code>status='completed'</code> \u2192 Job marked as <code>completed</code></p>"},{"location":"reference/architecture/increment-mode/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"reference/architecture/increment-mode/#1-why-not-let-hashcat-handle-increment","title":"1. Why Not Let Hashcat Handle Increment?","text":"<p>Problem: Hashcat's increment mode runs sequentially internally: <pre><code>hashcat -a 3 -m 1000 hash.txt ?l?l?l --increment --increment-min=2\n# Internally runs:\n# 1. ?l?l\n# 2. ?l?l?l\n</code></pre></p> <p>With <code>--skip</code> and <code>--limit</code>, hashcat applies them to the entire increment range, making it impossible to: - Distribute work across agents properly - Track progress accurately - Resume from specific points</p> <p>Solution: Backend decomposes into layers and schedules each independently.</p>"},{"location":"reference/architecture/increment-mode/#2-why-store-base_keyspace-and-effective_keyspace","title":"2. Why Store base_keyspace AND effective_keyspace?","text":"<ul> <li>base_keyspace: From <code>hashcat --keyspace</code> (fast, estimated)</li> <li>effective_keyspace: From <code>progress[1]</code> during benchmark (accurate, actual)</li> </ul> <p>Reason: Rule multipliers and hashcat internals can cause differences. We calculate estimated totals immediately but refine with accurate values after benchmarks.</p>"},{"location":"reference/architecture/increment-mode/#3-why-max_agents-applies-across-all-layers","title":"3. Why max_agents Applies Across All Layers?","text":"<p>User Expectation: \"Max 5 agents\" means for the entire job, not per layer.</p> <p>Implementation: Scheduler treats layers as parts of one job: <pre><code>Job A (Layer 0): 3 agents running\nJob A (Layer 1): Can't start yet (exhausted Layer 0, max_agents=5)\n\nNext cycle:\nJob A (Layer 1): Can use up to 5 agents now\n</code></pre></p>"},{"location":"reference/architecture/increment-mode/#4-why-no-dynamic-layer-switching","title":"4. Why No Dynamic Layer Switching?","text":"<p>Design: Layer exhaustion = stop planning, next cycle picks up next layer</p> <p>Why: Simpler, cleaner: - Avoids complex state management during planning - Natural scheduler behavior (polls for work each cycle) - Prevents race conditions with concurrent task creation</p>"},{"location":"reference/architecture/increment-mode/#testing","title":"Testing","text":""},{"location":"reference/architecture/increment-mode/#test-case-1-single-agent","title":"Test Case 1: Single Agent","text":"<p>Setup: - Job: Mask <code>?l?l?l</code>, increment min=2, max=3 - 1 agent available</p> <p>Expected Behavior: 1. Agent benchmarks Layer 0 (<code>?l?l</code>) 2. Agent works on Layer 0 until complete 3. Agent benchmarks Layer 1 (<code>?l?l?l</code>) 4. Agent works on Layer 1 until complete 5. Job completes</p> <p>Verification: - Check layer records created correctly - Verify tasks have correct <code>increment_layer_id</code> - Confirm hashcat commands don't include <code>--increment</code> flags - Validate progress aggregation</p>"},{"location":"reference/architecture/increment-mode/#test-case-2-multiple-agents","title":"Test Case 2: Multiple Agents","text":"<p>Setup: - Job: Mask <code>?d?d?d?d</code>, increment min=2, max=4, max_agents=3 - 5 agents available</p> <p>Expected Behavior: 1. One agent benchmarks Layer 0 (<code>?d?d</code>) 2. Up to 3 agents work on Layer 0 in parallel 3. Layer 0 completes 4. One agent benchmarks Layer 1 (<code>?d?d?d</code>) 5. Up to 3 agents work on Layer 1 6. Layer 1 completes 7. Process continues for Layer 2</p> <p>Verification: - Max 3 agents active per layer (respects max_agents) - Work distributed evenly (--skip/--limit used correctly) - No gaps or overlaps in keyspace coverage - Progress accurate across all layers</p>"},{"location":"reference/architecture/increment-mode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/increment-mode/#layer-not-getting-benchmarked","title":"Layer Not Getting Benchmarked","text":"<p>Symptom: Layer status is <code>pending</code> and <code>is_accurate_keyspace</code> is FALSE, no progress</p> <p>Check: 1. Are agents available? <code>SELECT * FROM agents WHERE status='online'</code> 2. Does agent have benchmark? <code>SELECT * FROM agent_benchmarks WHERE agent_id=X AND hash_type_id=Y</code> 3. Check scheduler logs for benchmark allocation</p> <p>Fix: Manually trigger benchmark or check agent connectivity</p>"},{"location":"reference/architecture/increment-mode/#progress-not-updating","title":"Progress Not Updating","text":"<p>Symptom: Layer or job progress stuck at 0%</p> <p>Check: 1. Is polling service running? Check logs for \"Job progress calculation service\" 2. Are tasks reporting progress? Check <code>job_tasks.effective_keyspace_processed</code> 3. Layer has <code>effective_keyspace</code> set? Check <code>job_increment_layers.effective_keyspace</code></p> <p>Fix: Restart polling service or check agent WebSocket connection</p>"},{"location":"reference/architecture/increment-mode/#tasks-using-wrong-mask","title":"Tasks Using Wrong Mask","text":"<p>Symptom: Tasks show job mask instead of layer mask</p> <p>Check: 1. Task has <code>increment_layer_id</code> set? <code>SELECT * FROM job_tasks WHERE job_execution_id=X</code> 2. Layer mask correct? <code>SELECT mask FROM job_increment_layers WHERE job_execution_id=X</code> 3. Check <code>attack_cmd</code> field in task record</p> <p>Fix: Likely scheduler bug - check task creation logic</p>"},{"location":"reference/architecture/increment-mode/#validation-errors","title":"Validation Errors","text":"<p>Symptom: Job creation fails with increment validation error</p> <p>Common Causes: - <code>increment_min &lt; 1</code> - <code>increment_max &lt; increment_min</code> - <code>increment_min &gt; mask_length</code> - <code>increment_max &gt; mask_length</code></p> <p>Fix: Adjust increment settings to valid ranges</p>"},{"location":"reference/architecture/increment-mode/#files-modifiedcreated","title":"Files Modified/Created","text":""},{"location":"reference/architecture/increment-mode/#database","title":"Database","text":"<ul> <li><code>backend/db/migrations/000088_create_job_increment_layers.up.sql</code></li> <li><code>backend/db/migrations/000089_add_increment_layer_id_to_job_tasks.up.sql</code></li> </ul>"},{"location":"reference/architecture/increment-mode/#models","title":"Models","text":"<ul> <li><code>backend/internal/models/jobs.go</code> - Added <code>JobIncrementLayer</code>, <code>JobIncrementLayerStatus</code></li> </ul>"},{"location":"reference/architecture/increment-mode/#repositories","title":"Repositories","text":"<ul> <li><code>backend/internal/repository/job_increment_layer_repository.go</code> - NEW</li> <li><code>backend/internal/repository/job_execution_repository.go</code> - Updated <code>GetJobsWithPendingWork()</code></li> </ul>"},{"location":"reference/architecture/increment-mode/#services","title":"Services","text":"<ul> <li><code>backend/internal/services/job_increment_layer_service.go</code> - NEW</li> <li><code>backend/internal/services/job_execution_service.go</code> - Updated <code>buildAttackCommand()</code></li> <li><code>backend/internal/services/job_progress_calculation_service.go</code> - Added layer aggregation</li> <li><code>backend/internal/services/job_scheduling_benchmark_planning.go</code> - Added layer detection</li> <li><code>backend/internal/services/job_scheduling_task_assignment.go</code> - Added layer planning</li> </ul>"},{"location":"reference/architecture/increment-mode/#utilities","title":"Utilities","text":"<ul> <li><code>backend/internal/utils/mask_parser.go</code> - NEW</li> <li><code>backend/internal/utils/mask_parser_test.go</code> - NEW</li> </ul>"},{"location":"reference/architecture/increment-mode/#handlers","title":"Handlers","text":"<ul> <li><code>backend/internal/handlers/jobs/user_jobs.go</code> - Added <code>GetJobLayers()</code>, <code>GetJobLayerTasks()</code></li> </ul>"},{"location":"reference/architecture/increment-mode/#routes","title":"Routes","text":"<ul> <li><code>backend/internal/routes/user.go</code> - Registered layer endpoints</li> </ul>"},{"location":"reference/architecture/increment-mode/#main","title":"Main","text":"<ul> <li><code>backend/cmd/server/main.go</code> - Added <code>jobIncrementLayerRepo</code> initialization</li> </ul>"},{"location":"reference/architecture/increment-mode/#future-enhancements","title":"Future Enhancements","text":""},{"location":"reference/architecture/increment-mode/#1-dynamic-chunking-optimization","title":"1. Dynamic Chunking Optimization","text":"<p>Currently uses existing chunking logic. Could optimize: - Skip --skip/--limit for small layers that fit on one agent - Adjust chunk size based on layer size vs agent count</p>"},{"location":"reference/architecture/increment-mode/#2-layer-parallelization","title":"2. Layer Parallelization","text":"<p>Currently runs layers sequentially. Could allow: - Multiple layers running simultaneously - Requires careful max_agents allocation logic</p>"},{"location":"reference/architecture/increment-mode/#3-layer-priorities","title":"3. Layer Priorities","text":"<p>Allow users to prioritize specific layers: - Start with longer/shorter masks first - Custom layer ordering</p>"},{"location":"reference/architecture/increment-mode/#4-estimated-completion-time-per-layer","title":"4. Estimated Completion Time Per Layer","text":"<p>Show ETA for each layer based on: - Agent speeds - Remaining keyspace - Historical completion times</p>"},{"location":"reference/architecture/increment-mode/#5-layer-pauseresume","title":"5. Layer Pause/Resume","text":"<p>Allow pausing specific layers: - Useful for long-running increment jobs - Focus resources on specific mask lengths</p>"},{"location":"reference/architecture/increment-mode/#summary","title":"Summary","text":"<p>The increment mode implementation provides: - \u2705 Full <code>--increment</code> and <code>--increment-inverse</code> support - \u2705 Proper distributed task assignment - \u2705 Accurate progress tracking per layer and overall - \u2705 Validation at job creation - \u2705 Clean separation of concerns (layers treated as sub-jobs) - \u2705 No changes to agent code (backward compatible) - \u2705 RESTful API for layer management - \u2705 Comprehensive error handling</p> <p>The system is production-ready for distributed hashcat operations with increment mode.</p>"},{"location":"reference/architecture/job-completion-system/","title":"Automatic Job Completion System","text":""},{"location":"reference/architecture/job-completion-system/#overview","title":"Overview","text":"<p>KrakenHashes automatically detects when all hashes in a hashlist have been cracked and manages the lifecycle of related jobs to prevent failures and wasted resources.</p>"},{"location":"reference/architecture/job-completion-system/#the-problem","title":"The Problem","text":"<p>Hashcat's <code>--remove</code> option removes cracked hashes from input files during execution. When all hashes are cracked: - The hashlist file becomes empty - Subsequent jobs targeting that hashlist fail immediately - Resources are wasted attempting to process empty files - Users receive confusing error messages</p>"},{"location":"reference/architecture/job-completion-system/#the-solution","title":"The Solution","text":""},{"location":"reference/architecture/job-completion-system/#status-code-6-detection","title":"Status Code 6 Detection","text":"<p>The agent monitors hashcat's JSON status output for status code 6, which indicates \"all hashes cracked.\" This code is sent by hashcat when: - The input file has no remaining uncracked hashes - All work is complete for the given hashlist</p>"},{"location":"reference/architecture/job-completion-system/#trust-model","title":"Trust Model","text":"<p>The system trusts status code 6 as authoritative without database verification because: - Hashcat knows definitively when all hashes are cracked - Database verification would create race conditions - Status code 6 is a reliable signal from hashcat - Prevents complex synchronization issues</p>"},{"location":"reference/architecture/job-completion-system/#job-cleanup-process","title":"Job Cleanup Process","text":"<p>When status code 6 is received:</p> <ol> <li>Identify All Affected Jobs: Query for ALL jobs (any status) targeting the same hashlist</li> <li>Running Jobs:</li> <li>Send WebSocket stop signals to active agents</li> <li>Mark jobs as \"completed\" at 100% progress</li> <li>Send completion email notifications</li> <li>Pending Jobs:</li> <li>Delete jobs that haven't started yet</li> <li>No email notifications (jobs never ran)</li> <li>Prevention: New tasks for this hashlist won't be created</li> </ol>"},{"location":"reference/architecture/job-completion-system/#technical-implementation","title":"Technical Implementation","text":"<p>Components: - <code>HashlistCompletionService</code>: Handles job cleanup logic - <code>AllHashesCracked</code> flag in WebSocket messages - Background processing with 5-minute timeout</p> <p>Flow: <pre><code>Agent detects status code 6 \u2192 Sets AllHashesCracked flag \u2192\nBackend handler triggered \u2192 HashlistCompletionService runs async \u2192\nStop running tasks + Delete pending jobs \u2192 Send notifications\n</code></pre></p> <p>Code Location: <code>backend/internal/services/hashlist_completion_service.go</code></p>"},{"location":"reference/architecture/job-completion-system/#agent-side-implementation","title":"Agent-Side Implementation","text":""},{"location":"reference/architecture/job-completion-system/#detection","title":"Detection","text":"<p>In <code>agent/internal/jobs/hashcat_executor.go</code>: - Parses hashcat JSON status output - Checks for <code>status</code> field equal to 6 - Sets <code>AllHashesCracked</code> flag in progress update message - Flag sent with regular progress updates (no special message needed)</p>"},{"location":"reference/architecture/job-completion-system/#timing","title":"Timing","text":"<ul> <li>Detection occurs during normal progress monitoring</li> <li>No additional API calls required</li> <li>Flag transmitted with existing WebSocket infrastructure</li> </ul>"},{"location":"reference/architecture/job-completion-system/#backend-side-implementation","title":"Backend-Side Implementation","text":""},{"location":"reference/architecture/job-completion-system/#message-handling","title":"Message Handling","text":"<p>In <code>backend/internal/routes/websocket_with_jobs.go</code>: - Checks <code>AllHashesCracked</code> flag in job progress messages - Triggers before status-specific processing - Runs HashlistCompletionService asynchronously</p>"},{"location":"reference/architecture/job-completion-system/#service-logic","title":"Service Logic","text":"<p><code>HashlistCompletionService.HandleHashlistCompletion()</code>:</p> <ol> <li> <p>Query Affected Jobs:    <pre><code>SELECT * FROM job_executions\nWHERE hashlist_id = ?\nAND status IN ('pending', 'running', 'paused')\n</code></pre></p> </li> <li> <p>Process Running Jobs:</p> </li> <li>Find active tasks for each running job</li> <li>Send stop signals via WebSocket</li> <li>Update job status to 'completed'</li> <li>Set progress to 100%</li> <li> <p>Trigger email notifications</p> </li> <li> <p>Process Pending Jobs:</p> </li> <li>Delete jobs that haven't started</li> <li>Clean up any associated data</li> <li> <p>No notifications needed</p> </li> <li> <p>Update Job Priority:</p> </li> <li>Comprehensive processing regardless of priority</li> <li>Handles all affected jobs in single operation</li> </ol>"},{"location":"reference/architecture/job-completion-system/#processing-status-workflow","title":"Processing Status Workflow","text":""},{"location":"reference/architecture/job-completion-system/#overview_1","title":"Overview","text":"<p>To prevent jobs from completing prematurely before all crack batches are received and processed, KrakenHashes implements a \"processing\" status for both jobs and tasks. This ensures accurate completion emails and proper job state management.</p>"},{"location":"reference/architecture/job-completion-system/#the-challenge","title":"The Challenge","text":"<p>When hashcat finishes processing a task: 1. Agent sends final progress message with <code>Status=\"completed\"</code> and <code>CrackedCount</code> field 2. Agent begins sending crack batches asynchronously 3. Without processing status, job would complete immediately 4. Completion email would be sent before all cracks are received 5. Crack count in email would be inaccurate or zero</p>"},{"location":"reference/architecture/job-completion-system/#processing-status-solution","title":"Processing Status Solution","text":"<p>Task Processing Workflow:</p> <pre><code>Task Running \u2192 Final Progress Received \u2192 Task Processing \u2192 All Batches Received \u2192 Task Completed\n                                         (cracking_completed_at set)                (completed_at set)\n</code></pre> <ol> <li>Agent Sends Final Progress:</li> <li>Progress message includes <code>Status=\"completed\"</code></li> <li>Includes <code>CrackedCount</code> field with expected number of cracks</li> <li> <p>Or <code>AllHashesCracked=true</code> flag with crack count</p> </li> <li> <p>Backend Transitions to Processing:</p> </li> <li>Task status changes from <code>running</code> to <code>processing</code></li> <li><code>cracking_completed_at</code> timestamp set to current time (hashcat finished)</li> <li><code>expected_crack_count</code> field set from progress message</li> <li><code>received_crack_count</code> initialized to 0</li> <li> <p><code>batches_complete_signaled</code> set to false</p> </li> <li> <p>Agent Sends Crack Batches:</p> </li> <li>Agent sends one or more <code>crack_batch</code> messages</li> <li>Backend increments <code>received_crack_count</code> as batches arrive</li> <li> <p>Backend processes and stores each batch</p> </li> <li> <p>Agent Signals Completion:</p> </li> <li>Agent sends <code>crack_batches_complete</code> WebSocket message</li> <li>Backend sets <code>batches_complete_signaled</code> to true</li> <li> <p>Agent is now free to accept new work</p> </li> <li> <p>Backend Completes Task:</p> </li> <li>Backend checks: <code>received_crack_count &gt;= expected_crack_count AND batches_complete_signaled == true</code></li> <li>If true: Task transitions from <code>processing</code> to <code>completed</code></li> <li><code>completed_at</code> timestamp set to current time (all batches received)</li> <li>Agent busy status cleared</li> <li>Job completion check triggered</li> </ol> <p>Job Processing Workflow:</p> <pre><code>Job Running \u2192 All Tasks Processing \u2192 Job Processing \u2192 All Tasks Completed \u2192 Job Completed (Email Sent)\n                                     (cracking_completed_at set)           (completed_at set)\n</code></pre> <ol> <li>Job Enters Processing:</li> <li>When all tasks transition to <code>processing</code> status</li> <li>Job status changes from <code>running</code> to <code>processing</code></li> <li><code>cracking_completed_at</code> timestamp set (all tasks finished hashcat execution)</li> <li> <p>Progress shows 100% but job not yet complete</p> </li> <li> <p>Job Completes:</p> </li> <li>When all tasks reach <code>completed</code> status</li> <li>Job status changes from <code>processing</code> to <code>completed</code></li> <li><code>completed_at</code> timestamp set (job fully finished)</li> <li>Completion email notification sent with accurate crack count</li> </ol>"},{"location":"reference/architecture/job-completion-system/#email-notification-integration","title":"Email Notification Integration","text":"<p>Accurate Crack Counts: - <code>GetTotalCracksForJob()</code> sums <code>crack_count</code> from all tasks for the job - Provides per-job crack count instead of hashlist total - Prevents incorrect crack counts when multiple jobs target same hashlist</p> <p>Duplicate Email Prevention: - Job executions track <code>completion_email_sent</code> flag - Hashlist completion service email disabled to prevent duplicates - Only one email sent per job completion</p>"},{"location":"reference/architecture/job-completion-system/#database-fields","title":"Database Fields","text":"<p>job_executions: - <code>status</code> includes <code>'processing'</code> value - <code>cracking_completed_at</code> (TIMESTAMP WITH TIME ZONE) - When all tasks finished hashcat execution - <code>completion_email_sent</code> (BOOLEAN) - <code>completion_email_sent_at</code> (TIMESTAMP) - <code>completion_email_error</code> (TEXT)</p> <p>job_tasks: - <code>status</code> includes <code>'processing'</code> value - <code>cracking_completed_at</code> (TIMESTAMP WITH TIME ZONE) - When hashcat finished for this task (enters processing state) - <code>expected_crack_count</code> (INTEGER) - <code>received_crack_count</code> (INTEGER) - <code>batches_complete_signaled</code> (BOOLEAN)</p>"},{"location":"reference/architecture/job-completion-system/#timestamp-distinction","title":"Timestamp Distinction","text":"<p>The system uses two distinct completion timestamps:</p> Timestamp Scope Meaning <code>cracking_completed_at</code> Task When hashcat exited - task enters <code>processing</code> state <code>completed_at</code> Task When all crack batches received and processed <code>cracking_completed_at</code> Job When all tasks finished hashcat execution <code>completed_at</code> Job When job is fully complete and email sent <p>Why Two Timestamps?</p> <ol> <li>Accurate Duration Tracking: <code>cracking_completed_at - started_at</code> gives the actual GPU cracking time</li> <li>Processing Overhead Visibility: <code>completed_at - cracking_completed_at</code> shows batch processing time</li> <li>Debugging: Helps identify where delays occur (cracking vs. data transmission)</li> <li>Analytics: Enables reporting on actual GPU utilization vs. total job duration</li> </ol>"},{"location":"reference/architecture/job-completion-system/#repository-methods","title":"Repository Methods","text":"<p>JobTaskRepository: - <code>SetTaskProcessing(taskID, expectedCracks)</code> - Transition task to processing - <code>IncrementReceivedCrackCount(taskID, count)</code> - Track received batches - <code>MarkBatchesComplete(taskID)</code> - Signal all batches sent - <code>CheckTaskReadyToComplete(taskID)</code> - Verify completion conditions - <code>GetProcessingTasksForJob(jobExecutionID)</code> - Query processing tasks</p> <p>JobExecutionRepository: - <code>SetJobProcessing(jobExecutionID)</code> - Transition job to processing - <code>UpdateEmailStatus(jobExecutionID, sent, sentAt, error)</code> - Track email delivery</p>"},{"location":"reference/architecture/job-completion-system/#websocket-messages","title":"WebSocket Messages","text":"<p>From Agent to Backend:</p> <p><code>crack_batches_complete</code>: <pre><code>{\n  \"type\": \"crack_batches_complete\",\n  \"task_id\": \"uuid-here\"\n}\n</code></pre></p> <p>Signals that agent has finished sending all crack batches for the task.</p>"},{"location":"reference/architecture/job-completion-system/#configuration","title":"Configuration","text":"<p>No configuration required - this feature is always active.</p>"},{"location":"reference/architecture/job-completion-system/#benefits","title":"Benefits","text":"<ol> <li>Prevents Failures: No more failed jobs due to empty hashlist files</li> <li>Resource Efficiency: Stops wasting resources on completed hashlists</li> <li>User Experience: Automatic cleanup without manual intervention</li> <li>Accurate Notifications: Users receive completion emails with correct crack counts after all data is processed</li> <li>Clean State: Queue automatically cleaned of obsolete jobs</li> <li>Data Integrity: Processing status ensures all crack batches are received before job completion</li> <li>No Duplicate Emails: Each job sends exactly one completion email</li> </ol>"},{"location":"reference/architecture/job-completion-system/#error-handling","title":"Error Handling","text":""},{"location":"reference/architecture/job-completion-system/#timeout-protection","title":"Timeout Protection","text":"<ul> <li>5-minute timeout for cleanup operations</li> <li>Prevents hanging if service encounters issues</li> <li>Logged errors don't block agent progress reporting</li> </ul>"},{"location":"reference/architecture/job-completion-system/#transaction-safety","title":"Transaction Safety","text":"<ul> <li>Database operations use transactions</li> <li>Rollback on errors ensures consistency</li> <li>Agent continues normal operation regardless of cleanup success</li> </ul>"},{"location":"reference/architecture/job-completion-system/#websocket-errors","title":"WebSocket Errors","text":"<ul> <li>Gracefully handles disconnected agents</li> <li>Tasks marked for stop even if agent offline</li> <li>Agent reconnection triggers cleanup on next connection</li> </ul>"},{"location":"reference/architecture/job-completion-system/#limitations","title":"Limitations","text":"<ul> <li>Trusts hashcat status code 6 without verification</li> <li>Only handles jobs for the same hashlist (doesn't affect other hashlists)</li> <li>Requires agent to detect and report status code 6</li> <li>Depends on WebSocket connectivity for stop signals</li> </ul>"},{"location":"reference/architecture/job-completion-system/#testing","title":"Testing","text":"<p>Tested with hashlist 85: - 1 running job completed at 100% with stop signal sent - 2 pending jobs deleted (never started) - Email notifications triggered successfully - No errors in logs</p>"},{"location":"reference/architecture/job-completion-system/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"reference/architecture/job-completion-system/#log-messages","title":"Log Messages","text":"<p>Success: <pre><code>Successfully completed job [uuid] for hashlist [id]\nSuccessfully deleted pending job [uuid] for hashlist [id]\n</code></pre></p> <p>Errors: <pre><code>Failed to stop tasks for job [uuid]: [error]\nFailed to complete job [uuid]: [error]\n</code></pre></p>"},{"location":"reference/architecture/job-completion-system/#metrics","title":"Metrics","text":"<p>Track in monitoring: - Number of jobs auto-completed - Number of pending jobs cleaned up - Time taken for cleanup operations - Failed cleanup attempts</p>"},{"location":"reference/architecture/job-completion-system/#related-documentation","title":"Related Documentation","text":"<ul> <li>Crack Batching System - How crack batches are transmitted and the processing status integration</li> <li>Chunking System - How jobs are divided into chunks</li> <li>Job Update System - How keyspace updates work</li> <li>Jobs &amp; Workflows - User perspective on automatic completion and processing status</li> <li>Core Concepts - Understanding job execution flow</li> <li>Database Schema - Job executions and tasks table structure with processing status fields</li> </ul>"},{"location":"reference/architecture/job-completion-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements under consideration:</p> <ul> <li>Partial Completion Threshold: Complete jobs when X% of hashes cracked (configurable)</li> <li>Notification Customization: Per-client notification preferences</li> <li>Completion Hooks: Custom scripts triggered on hashlist completion</li> <li>Statistics Tracking: Historical data on completion rates and timing</li> <li>Manual Override: Allow users to force completion or prevent automatic cleanup</li> </ul>"},{"location":"reference/architecture/job-update-system/","title":"Job Update System","text":""},{"location":"reference/architecture/job-update-system/#overview","title":"Overview","text":"<p>The KrakenHashes Job Update System automatically recalculates job keyspaces when associated wordlists, rules, or potfiles change during execution. This is a \"going forward\" system - when files are updated, only undispatched work is affected. Already-assigned tasks continue with their original parameters, ensuring consistency while allowing jobs to benefit from updated resources.</p>"},{"location":"reference/architecture/job-update-system/#core-philosophy-forward-only-updates","title":"Core Philosophy: Forward-Only Updates","text":"<p>The system operates on these principles:</p> <ol> <li>No Deficit Tracking: The system doesn't track \"missed\" work from updates that occur after tasks are dispatched</li> <li>Current State Calculation: Keyspaces are recalculated based on the current file state and remaining work</li> <li>Non-Disruptive: Running tasks are never interrupted or restarted</li> <li>Automatic Adjustment: Jobs automatically adapt to file changes without user intervention</li> </ol>"},{"location":"reference/architecture/job-update-system/#how-it-works","title":"How It Works","text":""},{"location":"reference/architecture/job-update-system/#directory-monitoring","title":"Directory Monitoring","text":"<p>The system continuously monitors three key directories:</p> <ul> <li>Wordlists: <code>/data/krakenhashes/wordlists/</code></li> <li>Rules: <code>/data/krakenhashes/rules/</code></li> <li>Potfile: Special handling via staging mechanism</li> </ul> <p>Every 30 seconds (configurable), the directory monitor: 1. Calculates MD5 hashes of all monitored files 2. Compares with previous hashes to detect changes 3. Updates file metadata in the database 4. Triggers job updates for affected jobs</p>"},{"location":"reference/architecture/job-update-system/#change-detection-flow","title":"Change Detection Flow","text":"<pre><code>File Change \u2192 MD5 Hash Comparison \u2192 Metadata Update \u2192 Job Update Service \u2192 Keyspace Recalculation\n</code></pre>"},{"location":"reference/architecture/job-update-system/#wordlist-updates","title":"Wordlist Updates","text":"<p>When a wordlist file changes (words added or removed):</p>"},{"location":"reference/architecture/job-update-system/#for-jobs-without-rule-splitting","title":"For Jobs WITHOUT Rule Splitting","text":"<ol> <li>Base keyspace updates to new word count</li> <li>Effective keyspace recalculates:</li> <li>With rules: <code>new_wordlist_size \u00d7 multiplication_factor</code></li> <li>Without rules: <code>new_wordlist_size</code></li> </ol>"},{"location":"reference/architecture/job-update-system/#for-jobs-with-rule-splitting","title":"For Jobs WITH Rule Splitting","text":"<p>The system accounts for already-dispatched rule chunks:</p> <ol> <li>Calculates theoretical new effective keyspace</li> <li>Determines \"missed\" keyspace: <code>words_added \u00d7 rules_already_dispatched</code></li> <li>Actual effective keyspace: <code>theoretical - missed</code></li> </ol> <p>Example: <pre><code>Original: 1,000,000 words \u00d7 10,000 rules = 10 billion keyspace\nAfter 5,000 rules dispatched, add 100,000 words:\n- Theoretical: 1,100,000 \u00d7 10,000 = 11 billion\n- Missed: 100,000 \u00d7 5,000 = 500 million\n- Actual: 11 billion - 500 million = 10.5 billion\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#rule-updates","title":"Rule Updates","text":"<p>When a rule file changes (rules added or removed):</p>"},{"location":"reference/architecture/job-update-system/#jobs-without-tasks-yet","title":"Jobs Without Tasks Yet","text":"<ul> <li>Simple recalculation: <code>base_keyspace \u00d7 new_rule_count</code></li> <li>Multiplication factor updates to new rule count</li> </ul>"},{"location":"reference/architecture/job-update-system/#jobs-with-existing-tasks","title":"Jobs With Existing Tasks","text":"<p>For rule-splitting jobs: 1. Checks highest dispatched rule index 2. If new rule count \u2264 max dispatched: Job effectively complete 3. Otherwise: Updates multiplication factor and recalculates</p> <p>Example: <pre><code>Original: 10,000 rules, 5,000 dispatched\nRules reduced to 4,000: Job marked complete (all remaining rules gone)\nRules increased to 12,000: 7,000 rules remain to process\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#potfile-updates","title":"Potfile Updates","text":"<p>The potfile (collection of cracked passwords) has special handling:</p>"},{"location":"reference/architecture/job-update-system/#staging-mechanism","title":"Staging Mechanism","text":"<ol> <li>Cracked passwords accumulate in a staging table</li> <li>Periodic or manual refresh moves staged entries to potfile</li> <li>Potfile treated as a special wordlist for job purposes</li> </ol>"},{"location":"reference/architecture/job-update-system/#update-process","title":"Update Process","text":"<ol> <li>Manual Refresh: User triggers from frontend</li> <li>Staging Integration: Moves cracked passwords to main potfile</li> <li>Line Count Update: Updates wordlist metadata</li> <li>Job Updates: Triggers same update logic as regular wordlists</li> </ol>"},{"location":"reference/architecture/job-update-system/#key-differences","title":"Key Differences","text":"<ul> <li>Not monitored by directory monitor (excluded from scans)</li> <li>Updates via database staging, not file watching</li> <li>Requires explicit refresh action</li> <li>Always grows (passwords only added, never removed)</li> </ul>"},{"location":"reference/architecture/job-update-system/#keyspace-recalculation-logic","title":"Keyspace Recalculation Logic","text":""},{"location":"reference/architecture/job-update-system/#basic-formula","title":"Basic Formula","text":"<pre><code>Effective Keyspace = Base Keyspace \u00d7 Multiplication Factor\n</code></pre> <p>Where: - Base Keyspace: Current wordlist size - Multiplication Factor: Number of rules (or 1 if no rules)</p>"},{"location":"reference/architecture/job-update-system/#adjustments-for-dispatched-work","title":"Adjustments for Dispatched Work","text":"<p>For rule-splitting jobs with updates: <pre><code>Adjusted Keyspace = New Effective - (Change \u00d7 Dispatched Rules)\n</code></pre></p> <p>This ensures already-dispatched tasks aren't double-counted.</p>"},{"location":"reference/architecture/job-update-system/#real-world-examples","title":"Real-World Examples","text":""},{"location":"reference/architecture/job-update-system/#scenario-1-growing-wordlist","title":"Scenario 1: Growing Wordlist","text":"<p>Initial State: - Wordlist: 1 million words - Rules: 1,000 - No tasks dispatched yet</p> <p>After Adding 100,000 Words: - New base: 1.1 million - New effective: 1.1 billion - All future tasks use updated wordlist</p>"},{"location":"reference/architecture/job-update-system/#scenario-2-rule-file-expansion-during-execution","title":"Scenario 2: Rule File Expansion During Execution","text":"<p>Initial State: - Job using rule splitting - 10,000 rules, split into 100 chunks - 50 chunks already dispatched (5,000 rules)</p> <p>After Adding 2,000 Rules: - Total rules: 12,000 - Remaining: 7,000 rules (chunks 51-120) - Future chunks use expanded rule set</p>"},{"location":"reference/architecture/job-update-system/#scenario-3-potfile-growth","title":"Scenario 3: Potfile Growth","text":"<p>Initial State: - Potfile job with 1,000 existing passwords - Rules: 500 - Effective keyspace: 500,000</p> <p>After Cracking Campaign: - 200 new passwords cracked - Manual refresh triggered - New base: 1,200 passwords - New effective: 600,000</p>"},{"location":"reference/architecture/job-update-system/#configuration","title":"Configuration","text":""},{"location":"reference/architecture/job-update-system/#directory-monitor-settings","title":"Directory Monitor Settings","text":"<p>Located in backend configuration:</p> Setting Default Description Monitor Interval 30s How often to check for file changes MD5 Hash Check Enabled Method for detecting changes Concurrent Updates Enabled Allow parallel job updates"},{"location":"reference/architecture/job-update-system/#system-behavior-settings","title":"System Behavior Settings","text":"Setting Default Description Auto-update Jobs Enabled Automatically update affected jobs Update Lock Timeout 60s Maximum time to wait for job lock Staging Refresh Interval Manual Potfile staging refresh trigger"},{"location":"reference/architecture/job-update-system/#technical-implementation","title":"Technical Implementation","text":""},{"location":"reference/architecture/job-update-system/#components","title":"Components","text":"<ol> <li>DirectoryMonitorService: Detects file changes via MD5 hashing</li> <li>JobUpdateService: Handles keyspace recalculation logic</li> <li>PotfileService: Manages potfile staging and updates</li> <li>Repository Layer: Database operations for job updates</li> </ol>"},{"location":"reference/architecture/job-update-system/#database-tables-involved","title":"Database Tables Involved","text":"<ul> <li><code>job_executions</code>: Stores base_keyspace, effective_keyspace, multiplication_factor</li> <li><code>job_tasks</code>: Tracks dispatched work (rule_start_index, rule_end_index)</li> <li><code>wordlists</code>: Metadata including word_count, file_hash</li> <li><code>rules</code>: Metadata including rule_count, file_hash</li> <li><code>potfile_staging</code>: Temporary storage for cracked passwords</li> </ul>"},{"location":"reference/architecture/job-update-system/#locking-strategy","title":"Locking Strategy","text":"<p>The system uses per-job locks to prevent race conditions: <pre><code>// Lock specific job during updates\ns.lockJob(jobID)\ndefer s.unlockJob(jobID)\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/job-update-system/#for-users","title":"For Users","text":"<ol> <li>Expect Keyspace Changes: Don't be alarmed if keyspaces update during execution</li> <li>Manual Potfile Refresh: Remember to refresh potfile after cracking campaigns</li> <li>Monitor Progress: Check effective keyspace to understand total work</li> <li>Plan Updates: Large file changes can significantly affect running jobs</li> </ol>"},{"location":"reference/architecture/job-update-system/#for-administrators","title":"For Administrators","text":"<ol> <li>Monitor Disk Space: File updates may require temporary storage</li> <li>Adjust Check Intervals: Balance between responsiveness and system load</li> <li>Review Logs: Check for update failures or lock timeouts</li> <li>Database Maintenance: Ensure potfile staging table doesn't grow too large</li> </ol>"},{"location":"reference/architecture/job-update-system/#for-developers","title":"For Developers","text":"<ol> <li>Respect Forward-Only: Never try to retroactively update dispatched tasks</li> <li>Use Job Locks: Always lock jobs during updates to prevent races</li> <li>Handle Errors Gracefully: File update failures shouldn't crash jobs</li> <li>Test Edge Cases: Consider jobs with no tasks, completed tasks, etc.</li> </ol>"},{"location":"reference/architecture/job-update-system/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/job-update-system/#common-issues","title":"Common Issues","text":"<p>Keyspace Not Updating: - Verify file actually changed (MD5 hash different) - Check directory monitor is running - Ensure job is in eligible state (pending/running/paused)</p> <p>Incorrect Effective Keyspace: - Verify multiplication_factor is set correctly - Check if job uses rule splitting - Review calculation for \"missed\" keyspace</p> <p>Potfile Not Updating Jobs: - Ensure manual refresh was triggered - Check potfile staging has new entries - Verify job references potfile wordlist</p>"},{"location":"reference/architecture/job-update-system/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to trace update flow: <pre><code>DEBUG: Directory monitor detected change\nDEBUG: Handling wordlist update, old: 1000000, new: 1100000\nDEBUG: Updated job keyspace, effective: 1100000000\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#limitations","title":"Limitations","text":"<ol> <li>No Retroactive Updates: Already-dispatched work won't get new words/rules</li> <li>Forward Progress Only: System doesn't track or compensate for missed combinations</li> <li>Manual Potfile Refresh: Requires user action to trigger potfile updates</li> <li>File Lock Conflicts: Rapid file changes might cause temporary update delays</li> </ol>"},{"location":"reference/architecture/job-update-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements under consideration:</p> <ul> <li>Deficit Tracking: Optional mode to track missed combinations</li> <li>Automatic Potfile Refresh: Configurable automatic refresh intervals</li> <li>Smart Chunking: Re-chunk remaining work when files change significantly</li> <li>Update History: Track all keyspace changes for job audit trail</li> <li>Predictive Updates: Estimate impact before applying changes</li> </ul>"},{"location":"reference/architecture/job-update-system/#progressive-keyspace-refinement","title":"Progressive Keyspace Refinement","text":"<p>In addition to handling file changes, the job system implements progressive keyspace refinement to improve accuracy as tasks complete. This is especially important for rule-splitting jobs where hashlist size changes between task assignments.</p>"},{"location":"reference/architecture/job-update-system/#the-challenge","title":"The Challenge","text":"<p>Consider this scenario: <pre><code>Job Start: 10,000 hashes \u00d7 1,000 rules = 10,000,000 effective keyspace\nAfter 500 rules: 200 hashes cracked\nRemaining: 9,800 hashes \u00d7 500 rules = ?\n</code></pre></p> <p>The remaining work is NOT simply \"50% of original\" because the hashlist has changed.</p>"},{"location":"reference/architecture/job-update-system/#how-progressive-refinement-works","title":"How Progressive Refinement Works","text":"<p>The system continuously recalculates <code>effective_keyspace</code> using: <pre><code>Refined Keyspace = Actual Completed + Estimated Remaining\n</code></pre></p> <p>Actual Completed: Sum of <code>chunk_actual_keyspace</code> from finished tasks Estimated Remaining: Uses average keyspace-per-rule from completed tasks</p>"},{"location":"reference/architecture/job-update-system/#implementation","title":"Implementation","text":"<p>For Single-Task Jobs (No Rule Splitting):</p> <p>When the first task completes with actual keyspace from hashcat: <pre><code>if !job.UsesRuleSplitting &amp;&amp; task.ChunkNumber == 1 &amp;&amp; len(allTasks) == 1 {\n    // Update effective_keyspace to match actual total\n    newEffectiveKeyspace := chunkActualKeyspace\n    jobExecRepo.UpdateEffectiveKeyspace(ctx, jobID, newEffectiveKeyspace)\n}\n</code></pre></p> <p>For Multi-Task Jobs (Rule Splitting):</p> <p>After each task completion: <pre><code>// Calculate actual completed work\ntotalActualKeyspace := sum(completed_task.chunk_actual_keyspace)\ntotalActualRules := sum(completed_task.rule_count)\n\n// Estimate remaining work\navgKeyspacePerRule := totalActualKeyspace / totalActualRules\ncurrentHashCount := hashlistRepo.GetUncrackedHashCount(hashlistID)\nestimatedRemaining := avgKeyspacePerRule \u00d7 remainingRules\n\n// Update job's effective_keyspace\nnewEffectiveKeyspace := totalActualKeyspace + estimatedRemaining\njobExecRepo.UpdateEffectiveKeyspace(ctx, jobID, newEffectiveKeyspace)\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#benefits","title":"Benefits","text":"<ol> <li>Accurate Progress: Progress percentages reflect current reality, not initial estimates</li> <li>Adapts to Cracks: As hashes are cracked, estimates adjust automatically</li> <li>No Retroactive Changes: Only affects undispatched work</li> <li>Improves Over Time: More completed tasks = better estimates</li> </ol>"},{"location":"reference/architecture/job-update-system/#example-walkthrough","title":"Example Walkthrough","text":"<p>Initial State: - Job: 10,000 hashes \u00d7 1,000 rules (10 tasks of 100 rules each) - Estimated effective_keyspace: 10,000,000</p> <p>After Task 1 Completes: - Actual keyspace: 1,005,234 (not exactly 1,000,000 due to rule effectiveness) - Avg per rule: 1,005,234 / 100 = 10,052 - Current hash count: 9,950 (50 cracked) - Estimated remaining: 10,052 \u00d7 900 = 9,046,800 - New effective_keyspace: 1,005,234 + 9,046,800 = 10,052,034</p> <p>After Task 2 Completes: - Total actual: 2,008,123 (tasks 1-2) - Avg per rule: 2,008,123 / 200 = 10,040 - Current hash count: 9,880 (120 cracked total) - Estimated remaining: 10,040 \u00d7 800 = 8,032,000 - New effective_keyspace: 2,008,123 + 8,032,000 = 10,040,123</p> <p>After All Tasks Complete: - Total actual: 9,892,456 (sum of all actuals) - No remaining work - Final effective_keyspace: 9,892,456 (matches processed_keyspace)</p>"},{"location":"reference/architecture/job-update-system/#threshold-for-updates","title":"Threshold for Updates","text":"<p>To avoid unnecessary database writes for tiny changes: <pre><code>if absInt64(*job.EffectiveKeyspace - newEffectiveKeyspace) &gt; 1000 {\n    jobExecRepo.UpdateEffectiveKeyspace(ctx, jobID, newEffectiveKeyspace)\n}\n</code></pre></p> <p>Only updates if change exceeds 1,000 keyspace units.</p>"},{"location":"reference/architecture/job-update-system/#integration-with-other-systems","title":"Integration with Other Systems","text":"<p>Progressive refinement works alongside: - Benchmark System: Initial <code>effective_keyspace</code> from first benchmark or progress - Job Update System: File changes trigger immediate recalculation - Cross-Hashlist Sync: Hashlist changes detected via current hash count queries</p>"},{"location":"reference/architecture/job-update-system/#debugging-progressive-refinement","title":"Debugging Progressive Refinement","text":"<p>Enable debug logging to see refinement in action: <pre><code>DEBUG: Progressive refinement for job abc-123:\n  actual=2,008,123 (from 200 rules),\n  estimated=8,032,000 (for 800 rules with 9,880 hashes),\n  total=10,040,123\nDEBUG: Updated job abc-123 effective_keyspace from 10,000,000 to 10,040,123\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#edge-cases","title":"Edge Cases","text":"<p>All Tasks Complete Before Refinement: - Final update sets <code>effective_keyspace = processed_keyspace</code> - Ensures 100% progress at completion</p> <p>Large Hashlist Changes During Execution: - Refinement uses CURRENT hash count from database - Automatically adapts to cross-hashlist crack propagation</p> <p>First Task Has Unusual Keyspace: - Subsequent tasks smooth out the average - More data = better estimates</p>"},{"location":"reference/architecture/job-update-system/#summary","title":"Summary","text":"<p>The Job Update System ensures KrakenHashes jobs remain accurate and efficient as resources change. By following a forward-only philosophy combined with progressive keyspace refinement, it provides a balance between consistency for running tasks and adaptability for future work. Understanding this system helps explain why job keyspaces may change during execution and how the system maintains integrity without disrupting active cracking operations.</p>"},{"location":"reference/architecture/lm-ntlm-linking/","title":"LM/NTLM Linking Architecture","text":""},{"location":"reference/architecture/lm-ntlm-linking/#overview","title":"Overview","text":"<p>KrakenHashes v1.2.1+ introduces comprehensive support for LM (LAN Manager) and NTLM hash linking, enabling intelligent processing of pwdump-format files and advanced Windows password cracking workflows. This document details the technical architecture, database schema, processing pipeline, and design decisions.</p>"},{"location":"reference/architecture/lm-ntlm-linking/#architectural-layers","title":"Architectural Layers","text":"<p>The LM/NTLM linking system operates across three database layers:</p> <ol> <li>Hashlist-to-Hashlist Links (<code>linked_hashlists</code>): High-level relationship between entire hashlists</li> <li>Hash-to-Hash Links (<code>linked_hashes</code>): Individual hash pair relationships</li> <li>LM Metadata (<code>lm_hash_metadata</code>): Partial crack tracking for LM hashes</li> </ol> <p>This layered approach enables: - Flexible linking strategies (not limited to LM/NTLM) - Efficient analytics calculations - Partial crack tracking without impacting other hash types - Clean separation of concerns</p>"},{"location":"reference/architecture/lm-ntlm-linking/#database-schema","title":"Database Schema","text":""},{"location":"reference/architecture/lm-ntlm-linking/#linked_hashlists-table","title":"linked_hashlists Table","text":"<p>Manages relationships between entire hashlists (e.g., LM hashlist \u2194 NTLM hashlist).</p> <pre><code>CREATE TABLE linked_hashlists (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    hashlist_id_1 BIGINT NOT NULL REFERENCES hashlists(id) ON DELETE CASCADE,\n    hashlist_id_2 BIGINT NOT NULL REFERENCES hashlists(id) ON DELETE CASCADE,\n    link_type VARCHAR(50) NOT NULL,  -- 'lm_ntlm', extensible for future types\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\n    CONSTRAINT unique_hashlist_link UNIQUE (hashlist_id_1, hashlist_id_2),\n    CONSTRAINT no_self_link CHECK (hashlist_id_1 != hashlist_id_2)\n);\n\nCREATE INDEX idx_linked_hashlists_id2 ON linked_hashlists(hashlist_id_2);\nCREATE INDEX idx_linked_hashlists_type ON linked_hashlists(link_type);\n</code></pre> <p>Design Decisions: - Bidirectional Uniqueness: Prevents both <code>(A, B)</code> and <code>(B, A)</code> from existing - Generic link_type: Enables future link types (e.g., <code>sha1_ntlm</code> for hash type correlations) - CASCADE DELETE: When a hashlist is deleted, links are automatically removed - Reverse Index: <code>idx_linked_hashlists_id2</code> enables efficient bidirectional lookups</p> <p>Use Cases: - Track which LM and NTLM hashlists were created from the same pwdump file - Calculate effective hashlist count in analytics (linked pairs count as ONE) - Determine when to create individual hash-to-hash links</p>"},{"location":"reference/architecture/lm-ntlm-linking/#linked_hashes-table","title":"linked_hashes Table","text":"<p>Manages relationships between individual hash records (e.g., specific LM hash \u2194 specific NTLM hash for same user).</p> <pre><code>CREATE TABLE linked_hashes (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    hash_id_1 UUID NOT NULL REFERENCES hashes(id) ON DELETE CASCADE,\n    hash_id_2 UUID NOT NULL REFERENCES hashes(id) ON DELETE CASCADE,\n    link_type VARCHAR(50) NOT NULL,  -- 'lm_ntlm'\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n\n    CONSTRAINT unique_hash_link UNIQUE (hash_id_1, hash_id_2),\n    CONSTRAINT no_self_link CHECK (hash_id_1 != hash_id_2)\n);\n\nCREATE INDEX idx_linked_hashes_id2 ON linked_hashes(hash_id_2);\nCREATE INDEX idx_linked_hashes_type ON linked_hashes(link_type);\n</code></pre> <p>Design Decisions: - Hash-Level Granularity: Links specific hash records, not just hashlists - Username/Domain Based: Links created by matching <code>username</code> and <code>domain</code> columns - Analytics Support: Enables \"Linked Hash Correlation\" statistics - Independent of Hashlists: Links persist even if hashlists are deleted (CASCADE handles cleanup)</p> <p>Use Cases: - Show correlation: \"Administrator's LM cracked but NTLM still unknown\" - Generate statistics: \"X linked pairs have both cracked\" - Enable domain-filtered correlation analysis</p>"},{"location":"reference/architecture/lm-ntlm-linking/#lm_hash_metadata-table","title":"lm_hash_metadata Table","text":"<p>Tracks partial crack status for LM hashes (mode 3000 only).</p> <pre><code>CREATE TABLE lm_hash_metadata (\n    hash_id UUID PRIMARY KEY REFERENCES hashes(id) ON DELETE CASCADE,\n    first_half_cracked BOOLEAN NOT NULL DEFAULT FALSE,\n    second_half_cracked BOOLEAN NOT NULL DEFAULT FALSE,\n    first_half_password VARCHAR(7),     -- Max 7 chars (LM first half)\n    second_half_password VARCHAR(7),    -- Max 7 chars (LM second half)\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_lm_metadata_crack_status\n    ON lm_hash_metadata(first_half_cracked, second_half_cracked);\nCREATE INDEX idx_lm_metadata_hash_id ON lm_hash_metadata(hash_id);\n</code></pre> <p>Design Decisions: - Hash-Specific: Only created for LM hashes (type 3000), zero impact on other types - Separate Password Storage: Stores 7-char fragments, not full password (assembled on demand) - Composite Index: <code>(first_half_cracked, second_half_cracked)</code> enables fast partial crack queries - VARCHAR(7) Limit: Enforces LM's 7-character half constraint at database level</p> <p>Use Cases: - Track partial crack status: \"First half cracked, second half pending\" - Analytics: \"X LM hashes are partially cracked\" - Strategic intelligence: \"Known half reduces keyspace by factor of 68 trillion\"</p>"},{"location":"reference/architecture/lm-ntlm-linking/#upload-flow","title":"Upload Flow","text":""},{"location":"reference/architecture/lm-ntlm-linking/#pwdump-format-detection","title":"Pwdump Format Detection","text":"<p>When a user uploads a hashlist file:</p> <ol> <li>File Selection: User selects file via upload dialog</li> <li>Automatic Detection: Frontend calls <code>/api/hashlists/detect-linked</code> endpoint</li> <li>Backend Analysis:</li> <li>Reads first 1000 lines (sample)</li> <li>Checks for pwdump format: <code>DOMAIN\\user:RID:LM:NTLM:::</code></li> <li>Counts LM hashes, NTLM hashes, blank LM hashes</li> <li>User Dialog: If both types found, present options:</li> <li>\"Upload as Single List\"</li> <li>\"Create Linked Lists\"</li> </ol> <p>Detection Endpoint (<code>POST /api/hashlists/detect-linked</code>):</p> <pre><code>Request (multipart/form-data):\n{\n  \"file\": &lt;uploaded file&gt;\n}\n\nResponse (if both types found):\n{\n  \"has_both_types\": true,\n  \"lm_count\": 1428,\n  \"ntlm_count\": 1500,\n  \"blank_lm_count\": 72\n}\n\nResponse (if only one type):\n{\n  \"has_both_types\": false\n}\n</code></pre> <p>Design Decision: Detection is client-side initiated to provide immediate feedback without committing to upload.</p>"},{"location":"reference/architecture/lm-ntlm-linking/#linked-hashlist-creation","title":"Linked Hashlist Creation","text":"<p>When user chooses \"Create Linked Lists\":</p> <ol> <li>Upload Request: Frontend sends <code>create_linked=true</code> parameter</li> <li>Hashlist Creation:</li> <li>Create LM hashlist: <code>{original_name}-LM</code> (hash_type_id: 3000)</li> <li>Create NTLM hashlist: <code>{original_name}-NTLM</code> (hash_type_id: 1000)</li> <li>Hashlist Link: Insert record into <code>linked_hashlists</code> table</li> <li>Processing: Both hashlists enter processing queue independently</li> <li>Hash Linking: After processing completes, create individual hash-to-hash links</li> </ol> <p>API Endpoint (<code>POST /api/hashlists</code>):</p> <pre><code>Parameters:\n- name: Original hashlist name\n- hash_type_id: (ignored if create_linked=true)\n- client_id: Optional client association\n- file: Pwdump format file\n- create_linked: \"true\" to enable linked creation\n</code></pre> <p>Processing Flow: <pre><code>1. Create LM hashlist record\n2. Create NTLM hashlist record\n3. Create linked_hashlists entry (lm_id, ntlm_id, 'lm_ntlm')\n4. Enqueue LM hashlist for processing\n5. Enqueue NTLM hashlist for processing\n6. (Background) Process LM hashes\n7. (Background) Process NTLM hashes\n8. (Background) Create hash-to-hash links\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"reference/architecture/lm-ntlm-linking/#hashlist-processing","title":"Hashlist Processing","text":"<p>Standard Processing (non-LM): 1. Read file line by line 2. Extract hash values and metadata 3. Batch insert into <code>hashes</code> table 4. Create <code>hashlist_hashes</code> join entries</p> <p>LM-Specific Processing: 1. Read file line by line 2. Extract LM hash (32 hex chars) 3. Skip blank LM constant: If hash equals <code>aad3b435b51404eeaad3b435b51404ee</code>, skip line 4. Store full 32-char hash in <code>hashes.hash_value</code> 5. Create <code>lm_hash_metadata</code> entry (all fields FALSE/NULL initially) 6. Create <code>hashlist_hashes</code> join entry</p> <p>Code Location: <code>backend/internal/processor/hashlist_processor.go</code></p> <p>Blank LM Filtering Logic: <pre><code>if hashType.ID == 3000 {\n    upperHashValue := strings.ToUpper(hashValue)\n    if upperHashValue == \"AAD3B435B51404EEAAD3B435B51404EE\" {\n        debug.Debug(\"[Processor:%d] Line %d: Skipping blank LM hash\", hashlistID, lineNumber)\n        totalHashes-- // Don't count blank LM hashes\n        continue\n    }\n}\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#hash-to-hash-linking","title":"Hash-to-Hash Linking","text":"<p>After both linked hashlists complete processing:</p> <ol> <li>Retrieve Hashes: Get all hashes from both hashlists with username/domain</li> <li>Build NTLM Map: <code>map[string]*models.Hash</code> keyed by <code>{domain}\\{username}</code></li> <li>Match LM to NTLM: For each LM hash, lookup NTLM hash by username/domain</li> <li>Batch Insert Links: Create <code>linked_hashes</code> entries for all matches</li> </ol> <p>Matching Logic: <pre><code>func makeUserDomainKey(username, domain *string) string {\n    user := \"\"\n    if username != nil {\n        user = *username\n    }\n\n    dom := \"\"\n    if domain != nil {\n        dom = *domain\n    }\n\n    if dom != \"\" {\n        return fmt.Sprintf(\"%s\\\\%s\", dom, user)\n    }\n    return user\n}\n</code></pre></p> <p>Batch Linking: <pre><code>INSERT INTO linked_hashes (hash_id_1, hash_id_2, link_type)\nVALUES\n    ($1, $2, 'lm_ntlm'),\n    ($3, $4, 'lm_ntlm'),\n    ...\nON CONFLICT (hash_id_1, hash_id_2) DO NOTHING;\n</code></pre></p> <p>Design Decision: Links created by username/domain match, not by RID, to handle domain migrations and account renames.</p>"},{"location":"reference/architecture/lm-ntlm-linking/#agent-download","title":"Agent Download","text":""},{"location":"reference/architecture/lm-ntlm-linking/#standard-hash-download","title":"Standard Hash Download","text":"<p>For most hash types, agents download via <code>GET /api/hashlists/{id}/uncracked</code>:</p> <pre><code>Response (text/plain):\n5f4dcc3b5cd84097a65d1633f5c74f5e\n098f6bcd4621d373cade4e832627b4f6\n1a1dc91c907325c69271ddf0c944bc72\n...\n</code></pre>"},{"location":"reference/architecture/lm-ntlm-linking/#lm-hash-half-streaming","title":"LM Hash Half Streaming","text":"<p>For LM hashlists (hash_type_id 3000), special processing occurs:</p> <p>Backend Processing (<code>routes/hashlist.go</code>): <pre><code>if hashlist.HashTypeID == 3000 {\n    // Stream unique 16-char halves instead of full 32-char hashes\n    err = h.hashRepo.StreamUncrackedLMHashHalvesForHashlist(ctx, hashlist.ID, func(hashHalf string) error {\n        fmt.Fprintln(w, hashHalf)  // Write 16-char half\n        return nil\n    })\n}\n</code></pre></p> <p>SQL Query (<code>repository/hash_repository.go</code>): <pre><code>SELECT DISTINCT half\nFROM (\n    SELECT SUBSTRING(h.hash_value, 1, 16) AS half\n    FROM hashes h\n    INNER JOIN hashlist_hashes hh ON h.id = hh.hash_id\n    WHERE hh.hashlist_id = $1 AND h.is_cracked = FALSE\n    UNION\n    SELECT SUBSTRING(h.hash_value, 17, 16) AS half\n    FROM hashes h\n    INNER JOIN hashlist_hashes hh ON h.id = hh.hash_id\n    WHERE hh.hashlist_id = $1 AND h.is_cracked = FALSE\n) AS halves\nORDER BY half\n</code></pre></p> <p>Example Output: <pre><code>01FC5A6BE7BC6929  \u2190 First half of hash 1\n5F4DCC3B5CD84097  \u2190 First half of hash 2\nAAD3B435B51404EE  \u2190 Blank constant (appears once despite multiple occurrences)\nC3B435B51404EE89  \u2190 Second half of hash 1\n...\n</code></pre></p> <p>Why This Approach: - Hashcat Requirement: Mode 3000 expects 16-char halves, not 32-char full hashes - Deduplication: DISTINCT ensures common halves appear only once - Efficiency: Blank constant <code>aad3b435b51404ee</code> sent once instead of hundreds of times - Parallel Capability: Agents can crack different halves simultaneously</p>"},{"location":"reference/architecture/lm-ntlm-linking/#crack-handling","title":"Crack Handling","text":""},{"location":"reference/architecture/lm-ntlm-linking/#lm-partial-crack-flow","title":"LM Partial Crack Flow","text":"<p>When an agent reports a cracked LM hash half:</p> <ol> <li>Agent Reports Crack: Sends 16-char hash half + password to backend</li> <li>Identify Full Hashes: Find all 32-char LM hashes containing this 16-char half</li> <li>Determine Position: Check if half matches LEFT(hash, 16) or RIGHT(hash, 16)</li> <li>Update Metadata:</li> <li>If first half: Set <code>first_half_cracked = TRUE</code>, <code>first_half_password = &lt;password&gt;</code></li> <li>If second half: Set <code>second_half_cracked = TRUE</code>, <code>second_half_password = &lt;password&gt;</code></li> <li>Check Completion: If both halves now cracked, assemble full password</li> <li>Mark Complete: If both halves cracked, update <code>hashes.is_cracked = TRUE</code></li> </ol> <p>Repository Method (<code>repository/lm_hash_repository.go</code>): <pre><code>func (r *LMHashRepository) UpdateLMHalfCrack(ctx context.Context, tx *sql.Tx, hashID uuid.UUID, halfPosition string, password string) error {\n    // halfPosition: \"first\" or \"second\"\n    query := `\n        INSERT INTO lm_hash_metadata (hash_id, {half}_cracked, {half}_password, updated_at)\n        VALUES ($1, TRUE, $2, $3)\n        ON CONFLICT (hash_id) DO UPDATE\n        SET {half}_cracked = TRUE, {half}_password = $2, updated_at = $3\n    `\n    // ...\n}\n</code></pre></p> <p>Full Password Assembly: <pre><code>func (r *LMHashRepository) CheckAndFinalizeLMCrack(ctx context.Context, tx *sql.Tx, hashID uuid.UUID) (bool, string, error) {\n    // Check if both halves are cracked\n    query := `\n        SELECT (first_half_cracked AND second_half_cracked) AS both_cracked,\n               first_half_password, second_half_password\n        FROM lm_hash_metadata\n        WHERE hash_id = $1\n    `\n\n    if bothCracked {\n        fullPassword = firstHalfPwd + secondHalfPwd\n        return true, fullPassword, nil\n    }\n    return false, \"\", nil\n}\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#cross-hashlist-propagation","title":"Cross-Hashlist Propagation","text":"<p>LM hash cracks propagate across all hashlists (standard behavior):</p> <ol> <li>Crack Reported: Agent cracks 16-char LM half</li> <li>Find All Matching: Identify all 32-char LM hashes containing this half</li> <li>Update All: Update metadata for every matching hash</li> <li>Regenerate Files: Regenerate all affected hashlist files</li> <li>Notify Agents: Mark agent copies as outdated</li> </ol> <p>This ensures that cracking one LM half benefits all hashlists containing hashes with that half.</p>"},{"location":"reference/architecture/lm-ntlm-linking/#analytics-integration","title":"Analytics Integration","text":""},{"location":"reference/architecture/lm-ntlm-linking/#windows-hash-statistics","title":"Windows Hash Statistics","text":"<p>Overview Count Calculation: <pre><code>-- Get effective count (linked pairs count as ONE)\nSELECT\n    COUNT(DISTINCT CASE\n        WHEN lh.id IS NOT NULL THEN\n            CASE WHEN h.hash_type_id = 3000 THEN lh.id ELSE NULL END\n        ELSE h.id\n    END) AS total_windows,\n    COUNT(DISTINCT CASE\n        WHEN h.is_cracked AND lh.id IS NOT NULL THEN\n            CASE WHEN h.hash_type_id = 3000 THEN lh.id ELSE NULL END\n        ELSE CASE WHEN h.is_cracked THEN h.id ELSE NULL END\n    END) AS cracked_windows\nFROM hashes h\nLEFT JOIN linked_hashes lh ON (h.id = lh.hash_id_1 OR h.id = lh.hash_id_2)\n    AND lh.link_type = 'lm_ntlm'\nWHERE ...\n</code></pre></p> <p>Individual Hash Type Counts: - Use raw counts (don't adjust for linking) to show actual hash quantities - Example: 1500 NTLM hashes and 1428 LM hashes displayed separately</p> <p>Linked Pair Count: <pre><code>SELECT COUNT(*) FROM linked_hashes WHERE link_type = 'lm_ntlm'\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#linked-hash-correlation","title":"Linked Hash Correlation","text":"<p>Query Structure: <pre><code>SELECT\n    COUNT(*) AS total_pairs,\n    COUNT(CASE WHEN lm.is_cracked AND ntlm.is_cracked THEN 1 END) AS both_cracked,\n    COUNT(CASE WHEN NOT lm.is_cracked AND ntlm.is_cracked THEN 1 END) AS only_ntlm,\n    COUNT(CASE WHEN lm.is_cracked AND NOT ntlm.is_cracked THEN 1 END) AS only_lm,\n    COUNT(CASE WHEN NOT lm.is_cracked AND NOT ntlm.is_cracked THEN 1 END) AS neither\nFROM linked_hashes lh\nINNER JOIN hashes lm ON lh.hash_id_1 = lm.id\nINNER JOIN hashes ntlm ON lh.hash_id_2 = ntlm.id\nWHERE lh.link_type = 'lm_ntlm' AND ...\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#lm-partial-crack-query","title":"LM Partial Crack Query","text":"<p>Find Partially Cracked LM Hashes: <pre><code>SELECT\n    h.id, h.username, h.domain,\n    lm.first_half_cracked, lm.first_half_password,\n    lm.second_half_cracked, lm.second_half_password,\n    hl.name AS hashlist_name\nFROM lm_hash_metadata lm\nINNER JOIN hashes h ON lm.hash_id = h.id\nINNER JOIN hashlist_hashes hlh ON h.id = hlh.hash_id\nINNER JOIN hashlists hl ON hlh.hashlist_id = hl.id\nWHERE (lm.first_half_cracked OR lm.second_half_cracked)\n  AND NOT (lm.first_half_cracked AND lm.second_half_cracked)\n  AND hlh.hashlist_id = ANY($1)\nORDER BY h.username\nLIMIT 50;\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/architecture/lm-ntlm-linking/#index-strategy","title":"Index Strategy","text":"<p>Critical Indexes: 1. <code>idx_linked_hashlists_id2</code>: Enables bidirectional hashlist lookup 2. <code>idx_linked_hashes_id2</code>: Enables bidirectional hash lookup 3. <code>idx_lm_metadata_crack_status</code>: Fast partial crack queries 4. <code>idx_lm_metadata_hash_id</code>: Foreign key lookup</p> <p>Query Optimization: - Composite index on <code>(first_half_cracked, second_half_cracked)</code> enables single-scan partial crack detection - DISTINCT in LM half streaming handled by PostgreSQL with UNION optimization</p>"},{"location":"reference/architecture/lm-ntlm-linking/#memory-usage","title":"Memory Usage","text":"<p>LM Half Streaming: - No full dataset loaded into memory - Cursor-based streaming from database - Backpressure via HTTP chunked transfer encoding - Typical memory: &lt;100MB for 1M+ hashes</p> <p>Hash Linking: - In-memory map: <code>map[string]*models.Hash</code> for NTLM hashes - Typical size: ~200 bytes per hash \u00d7 count - Example: 100K hashes = ~20MB - Batch insert: 1000 links at a time to limit transaction size</p>"},{"location":"reference/architecture/lm-ntlm-linking/#scalability","title":"Scalability","text":"<p>Tested Performance: - Pwdump files up to 1M lines: &lt;30 seconds processing - Hash linking 100K pairs: &lt;5 seconds - Analytics with linked pairs: &lt;10 seconds for 1M+ hashes - LM half streaming: Line-speed (network bound, not CPU/DB bound)</p>"},{"location":"reference/architecture/lm-ntlm-linking/#future-extensibility","title":"Future Extensibility","text":"<p>The generic design enables future enhancements:</p> <p>Potential Link Types: - <code>sha1_ntlm</code>: Link SHA1 and NTLM hashes for same user (multi-platform analysis) - <code>old_new</code>: Link old and new password hashes for password change analysis - <code>service_user</code>: Link service account hashes across systems</p> <p>Metadata Tables: - Similar to <code>lm_hash_metadata</code>, could add:   - <code>kerberos_metadata</code>: etype information, ticket details   - <code>netntlm_metadata</code>: challenge/response pair tracking   - <code>custom_metadata</code>: User-defined fields for special analyses</p> <p>Analytics Extensions: - Password aging analysis (old_new links) - Cross-platform password reuse (sha1_ntlm links) - Service account proliferation tracking</p>"},{"location":"reference/architecture/lm-ntlm-linking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/lm-ntlm-linking/#common-issues","title":"Common Issues","text":"<p>Issue: Hash links not created after upload - Cause: Username/domain mismatch between LM and NTLM entries - Solution: Verify username extraction logic handles special characters - Check: <code>SELECT username, domain FROM hashes WHERE hashlist_id IN (...)</code></p> <p>Issue: Partial cracks not appearing in analytics - Cause: <code>lm_hash_metadata</code> entries not created during processing - Solution: Verify LM hashlist has <code>hash_type_id = 3000</code> - Check: <code>SELECT COUNT(*) FROM lm_hash_metadata WHERE hash_id IN (...)</code></p> <p>Issue: Duplicate links created - Cause: Bidirectional uniqueness constraint prevents this, but check for manual SQL - Solution: Constraints automatically prevent duplicates</p> <p>Issue: Analytics show wrong linked pair count - Cause: May be counting hashlist links instead of hash links - Solution: Verify query uses <code>linked_hashes</code> not <code>linked_hashlists</code></p>"},{"location":"reference/architecture/lm-ntlm-linking/#debugging-queries","title":"Debugging Queries","text":"<p>Check Hashlist Linkage: <pre><code>SELECT * FROM linked_hashlists WHERE hashlist_id_1 = X OR hashlist_id_2 = X;\n</code></pre></p> <p>Check Hash Linkage: <pre><code>SELECT COUNT(*) FROM linked_hashes WHERE link_type = 'lm_ntlm';\n</code></pre></p> <p>Find Orphaned Metadata: <pre><code>SELECT lm.* FROM lm_hash_metadata lm\nLEFT JOIN hashes h ON lm.hash_id = h.id\nWHERE h.id IS NULL;\n-- Should return 0 rows (CASCADE DELETE should prevent orphans)\n</code></pre></p> <p>Verify LM Half Streaming: <pre><code># Download LM hashlist, count unique halves\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  http://localhost:8080/api/hashlists/{id}/uncracked | sort -u | wc -l\n</code></pre></p>"},{"location":"reference/architecture/lm-ntlm-linking/#references","title":"References","text":"<ul> <li>Hashlists User Guide</li> <li>Analytics Reports</li> <li>Hash Types Reference</li> <li>Database Schema</li> </ul>"},{"location":"reference/architecture/rule-splitting/","title":"Rule Splitting Implementation Summary","text":""},{"location":"reference/architecture/rule-splitting/#current-status","title":"Current Status","text":"<p>The rule splitting feature has been fully implemented but requires testing with a fresh job. The implementation includes:</p>"},{"location":"reference/architecture/rule-splitting/#backend-components","title":"Backend Components","text":"<ol> <li>Database Schema (\u2713 Complete)</li> <li>Added columns to <code>job_executions</code>: <code>uses_rule_splitting</code>, <code>rule_split_count</code>, <code>base_keyspace</code>, <code>effective_keyspace</code>, <code>multiplication_factor</code></li> <li> <p>Added columns to <code>job_tasks</code>: <code>is_rule_split_task</code>, <code>rule_chunk_path</code>, <code>rule_start_index</code>, <code>rule_end_index</code></p> </li> <li> <p>Rule Split Manager (\u2713 Complete)</p> </li> <li><code>RuleSplitManager</code> service that handles splitting rule files into chunks</li> <li>Creates temporary chunk files in <code>/data/krakenhashes/temp/rule_chunks/</code></li> <li> <p>Supports counting rules and creating evenly distributed chunks</p> </li> <li> <p>Job Execution Service (\u2713 Complete)</p> </li> <li><code>calculateEffectiveKeyspace</code> - Calculates virtual keyspace for rules/combination attacks</li> <li><code>determineRuleSplitting</code> - Decides if a job should use rule splitting based on thresholds</li> <li><code>InitializeRuleSplitting</code> - Creates rule chunk tasks when job starts</li> <li> <p><code>GetNextRuleSplitTask</code> - Assigns rule chunks to agents</p> </li> <li> <p>Job Scheduling Service (\u2713 Complete)</p> </li> <li>Enhanced to detect rule-split jobs and initialize splitting on first assignment</li> <li>Routes rule-split jobs through special task assignment logic</li> <li> <p>Syncs rule chunk files to agents</p> </li> <li> <p>WebSocket Integration (\u2713 Complete)</p> </li> <li>Sends rule chunk paths to agents as <code>rules/chunks/&lt;filename&gt;</code></li> <li>Properly handles file sync for rule chunks</li> </ol>"},{"location":"reference/architecture/rule-splitting/#frontend-components","title":"Frontend Components","text":"<ol> <li>Keyspace Display (\u2713 Complete)</li> <li>Shows effective keyspace with multiplication factor badge</li> <li>Tooltips explain virtual keyspace calculation</li> <li> <p>Proper handling of snake_case field names from backend</p> </li> <li> <p>Admin Settings (\u2713 Complete)</p> </li> <li>Rule splitting configuration in Job Execution settings</li> <li>Controls for threshold, min rules, max chunks</li> </ol>"},{"location":"reference/architecture/rule-splitting/#known-issues","title":"Known Issues","text":"<ol> <li>Existing Jobs: Jobs created before the implementation don't have:</li> <li>Effective keyspace calculated</li> <li>Rule splitting decision made</li> <li> <p>This causes them to run with full rule files</p> </li> <li> <p>Rule File Path Resolution: The <code>calculateEffectiveKeyspace</code> method may fail to count rules if the file path resolution doesn't match the actual file location.</p> </li> </ol>"},{"location":"reference/architecture/rule-splitting/#how-rule-splitting-works","title":"How Rule Splitting Works","text":"<ol> <li>When a job is created with attack mode 0 (straight) and rules:</li> <li>Calculate effective keyspace (wordlist \u00d7 rules)</li> <li> <p>If effective keyspace &gt; threshold \u00d7 chunk_duration \u00d7 benchmark_speed AND rules &gt; min_rules:</p> <ul> <li>Mark job with <code>uses_rule_splitting = true</code></li> <li>Calculate optimal number of chunks</li> </ul> </li> <li> <p>When first agent picks up the job:</p> </li> <li><code>InitializeRuleSplitting</code> is called</li> <li>Rule file is split into N chunks</li> <li> <p>N tasks are created, each with a chunk file path</p> </li> <li> <p>Agents receive tasks with:</p> </li> <li>Full wordlist keyspace (no skip/limit)</li> <li>Rule chunk file instead of full rule file</li> <li> <p>Progress tracked per chunk</p> </li> <li> <p>Progress aggregation accounts for:</p> </li> <li>Each chunk processes full wordlist with subset of rules</li> <li>Total progress = sum of (chunk_progress \u00d7 rules_in_chunk)</li> </ol>"},{"location":"reference/architecture/rule-splitting/#testing-the-implementation","title":"Testing the Implementation","text":"<p>To test rule splitting with a new job:</p> <ol> <li>Create a preset job with:</li> <li>Attack mode 0 (straight)</li> <li>A wordlist (e.g., crackstation.txt with 1.2B words)</li> <li> <p>A large rule file (e.g., _nsakey.v2.dive.rule with 123K rules)</p> </li> <li> <p>Create a job execution from this preset</p> </li> <li>The job should be marked with <code>uses_rule_splitting = true</code></li> <li> <p><code>rule_split_count</code> should be calculated (e.g., 415 chunks)</p> </li> <li> <p>When agent picks up the job:</p> </li> <li>Check logs for \"Initializing rule splitting\"</li> <li>Verify chunk files created in temp directory</li> <li>Agent should receive <code>rules/chunks/job_*_chunk_*.rule</code> path</li> </ol>"},{"location":"reference/architecture/rule-splitting/#configuration","title":"Configuration","text":"<p>Current settings (in <code>system_settings</code> table): - <code>rule_split_enabled</code>: true - <code>rule_split_threshold</code>: 2.0 (split if job takes &gt; 2x chunk duration) - <code>rule_split_min_rules</code>: 100 (only split if &gt; 100 rules) - <code>rule_split_max_chunks</code>: 1000 (maximum chunks to create) - <code>rule_chunk_temp_dir</code>: /data/krakenhashes/temp/rule_chunks</p>"},{"location":"reference/architecture/rule-splitting/#agent-expectations","title":"Agent Expectations","text":"<p>Agents expect rule chunks to be available at: - <code>&lt;agent_data_dir&gt;/rules/chunks/&lt;chunk_filename&gt;</code></p> <p>The backend WebSocket integration correctly formats the path as <code>rules/chunks/&lt;filename&gt;</code> when sending task assignments for rule-split jobs.</p>"},{"location":"reference/architecture/username-extraction/","title":"Username and Domain Extraction Architecture","text":"<p>Version: 1.1+</p>"},{"location":"reference/architecture/username-extraction/#overview","title":"Overview","text":"<p>KrakenHashes implements automatic username and domain extraction from password hash formats that contain identity information. This system enables:</p> <ul> <li>User tracking: Identify which accounts have been cracked</li> <li>Domain mapping: Understand organizational structure</li> <li>Prioritization: Focus on high-value accounts (administrators, machine accounts)</li> <li>Reporting: Generate client-facing reports with username context</li> <li>Statistics: Analyze crack rates by user, domain, or account type</li> </ul>"},{"location":"reference/architecture/username-extraction/#architecture-components","title":"Architecture Components","text":""},{"location":"reference/architecture/username-extraction/#1-database-schema","title":"1. Database Schema","text":"<p>Migration 000070 adds domain support to the hashes table:</p> <pre><code>ALTER TABLE hashes\nADD COLUMN domain TEXT;\n\nCREATE INDEX idx_hashes_domain ON hashes(domain) WHERE domain IS NOT NULL;\nCREATE INDEX idx_hashes_username ON hashes(username) WHERE username IS NOT NULL;\n</code></pre> <p>Schema Fields: - <code>username</code> (TEXT, nullable): Extracted username - <code>domain</code> (TEXT, nullable): Extracted domain/realm - <code>original_hash</code> (TEXT): Complete hash line as uploaded - <code>hash_value</code> (TEXT): Canonical hash for hashcat processing</p>"},{"location":"reference/architecture/username-extraction/#2-extraction-pipeline","title":"2. Extraction Pipeline","text":"<pre><code>Upload \u2192 Type Detection \u2192 Line Parse \u2192 Extract Username/Domain \u2192 Store \u2192 Display\n</code></pre> <p>Detailed Flow:</p> <ol> <li>Upload: User uploads hashlist via API (<code>POST /api/hashlists</code>)</li> <li>Type Selection: User specifies hash type (e.g., 1000 for NTLM)</li> <li>Async Processing: Background worker processes file line-by-line</li> <li>Extraction: Each line processed by appropriate extractor</li> <li>Storage: Username, domain, and hash stored in database</li> <li>Indexing: Indexed for fast filtering and searching</li> <li>Display: Surfaced in UI tables and reports</li> </ol>"},{"location":"reference/architecture/username-extraction/#3-extractor-system","title":"3. Extractor System","text":"<p>Location: <code>backend/pkg/hashutils/processing.go</code></p> <p>Main Function: <pre><code>func ExtractUsernameAndDomain(rawHash string, hashTypeID int) *UsernameAndDomain\n</code></pre></p> <p>Type-Specific Extractors:</p> Hash Type Function Pattern 1000 (NTLM) <code>extractNTLM()</code> <code>DOMAIN\\user:sid:LM:NT:::</code> 1100 (DCC) <code>extractDCC()</code> <code>hash:username</code> 5500 (NetNTLMv1) <code>extractNetNTLMv1()</code> <code>user::domain:chal:resp</code> 5600 (NetNTLMv2) <code>extractNetNTLMv2()</code> <code>user::domain:chal:resp</code> 18200 (Kerberos) <code>extractKerberos()</code> <code>$krb5asrep$23$user@domain:hash</code> 6800 (LastPass) <code>extractLastPass()</code> <code>hash:iterations:email</code> <p>Priority Order: 1. Custom type-specific extractor (if available) 2. Heuristic fallback extractor 3. Return <code>nil</code> if no match</p>"},{"location":"reference/architecture/username-extraction/#4-domain-parsing","title":"4. Domain Parsing","text":"<p>Function: <code>ParseDomainUsername(input string) (username, domain string)</code></p> <p>Handles two standard formats:</p> <p>Windows/NetBIOS Format: <pre><code>DOMAIN\\username    \u2192    (\"username\", \"DOMAIN\")\nCORP\\alice         \u2192    (\"alice\", \"CORP\")\n</code></pre></p> <p>Kerberos/UPN Format: <pre><code>user@domain        \u2192    (\"user\", \"domain\")\njohn@CORP.LOCAL    \u2192    (\"john\", \"CORP.LOCAL\")\n</code></pre></p> <p>Edge Cases: - Multiple <code>\\</code> characters: Only first is domain separator - Multiple <code>@</code> characters: Only last is domain separator - No separator found: Domain returns as empty string</p>"},{"location":"reference/architecture/username-extraction/#5-machine-account-handling","title":"5. Machine Account Handling","text":"<p>Identification: Usernames ending with <code>$</code> character</p> <p>Examples: <pre><code>COMPUTERNAME$\nDC01$\nWKS-001$\nSERVER-WEB$\n</code></pre></p> <p>Special Handling: - <code>$</code> is NOT treated as a separator character - Trailing <code>$</code> is preserved in username field - Enables identification of computer accounts vs user accounts</p> <p>Security Significance: - Computer accounts often have elevated privileges - Can be used for lateral movement in domain environments - Important to track in penetration testing engagements</p>"},{"location":"reference/architecture/username-extraction/#extractor-implementation-details","title":"Extractor Implementation Details","text":""},{"location":"reference/architecture/username-extraction/#ntlm-extractor-mode-1000","title":"NTLM Extractor (Mode 1000)","text":"<p>Format: <code>DOMAIN\\username:sid:LM_hash:NT_hash:::</code></p> <pre><code>func extractNTLM(rawHash string) *UsernameAndDomain {\n    parts := strings.Split(rawHash, \":\")\n    if len(parts) &gt;= 4 {\n        // This is pwdump format\n        userPart := parts[0]\n        username, domain := ParseDomainUsername(userPart)\n        return &amp;UsernameAndDomain{\n            Username: &amp;username,\n            Domain:   ptrIfNotEmpty(domain),\n        }\n    }\n    return nil\n}\n</code></pre> <p>Example: <pre><code>Input:  CONTOSO\\Administrator:500:aad3b...:8846f7...:::\nOutput: username=\"Administrator\", domain=\"CONTOSO\"\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#kerberos-extractor-mode-18200","title":"Kerberos Extractor (Mode 18200)","text":"<p>Format: <code>$krb5asrep$23$user@domain.com:hash_data</code></p> <p>Challenge: Preserving <code>$</code> in machine account names while parsing format delimiters</p> <p>Original Bug (Fixed in this branch): <pre><code>// \u274c BUGGY: Splits on ALL $ characters\nparts := strings.Split(rawHash, \"$\")\n// Input: $krb5asrep$23$WKS01$@CORP.COM:...\n// Result: [\"\", \"krb5asrep\", \"23\", \"WKS01\", \"@CORP.COM:...\"]\n//         loses the $ in WKS01$!\n</code></pre></p> <p>Fixed Implementation: <pre><code>// \u2705 FIXED: Find exactly the first 3 $ delimiters\nidx1 := strings.Index(rawHash, \"$\")\nidx2 := strings.Index(rawHash[idx1+1:], \"$\") + idx1 + 1\nidx3 := strings.Index(rawHash[idx2+1:], \"$\") + idx2 + 1\nuserDomainPart := rawHash[idx3+1:]  // Everything after 3rd $\n\n// Now extract user@domain\ncolonIdx := strings.Index(userDomainPart, \":\")\nif colonIdx != -1 {\n    userDomainPart = userDomainPart[:colonIdx]\n}\n\n// Input: $krb5asrep$23$WKS01$@CORP.COM:hash...\n// userDomainPart: \"WKS01$@CORP.COM\"\n// Result: username=\"WKS01$\", domain=\"CORP.COM\" \u2713\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#netntlmv2-extractor-mode-5600","title":"NetNTLMv2 Extractor (Mode 5600)","text":"<p>Format: <code>username::domain:server_challenge:HMAC_MD5</code></p> <pre><code>func extractNetNTLMv2(rawHash string) *UsernameAndDomain {\n    parts := strings.Split(rawHash, \":\")\n    if len(parts) &gt;= 3 &amp;&amp; parts[1] == \"\" {\n        username := parts[0]\n        domain := ptrIfNotEmpty(parts[2])\n        return &amp;UsernameAndDomain{\n            Username: &amp;username,\n            Domain:   domain,\n        }\n    }\n    return nil\n}\n</code></pre> <p>Examples: <pre><code>alice::ENTERPRISE:1122...   \u2192  username=\"alice\", domain=\"ENTERPRISE\"\nWKS99$::DOMAIN:abcd...      \u2192  username=\"WKS99$\", domain=\"DOMAIN\"\ntestuser:::fedcba...        \u2192  username=\"testuser\", domain=NULL\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#lastpass-extractor-mode-6800","title":"LastPass Extractor (Mode 6800)","text":"<p>Format: <code>hash:iterations:email</code></p> <pre><code>func extractLastPass(rawHash string) *UsernameAndDomain {\n    parts := strings.Split(rawHash, \":\")\n    if len(parts) == 3 {\n        email := parts[2]\n        return &amp;UsernameAndDomain{\n            Username: &amp;email,\n            Domain:   nil, // LastPass doesn't have a domain concept\n        }\n    }\n    return nil\n}\n</code></pre> <p>Examples: <pre><code>a1b2c3...:500:john@example.com      \u2192  username=\"john@example.com\", domain=NULL\nb2c3d4...:1000:alice@corp.com       \u2192  username=\"alice@corp.com\", domain=NULL\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#dccms-cache-extractor-mode-1100","title":"DCC/MS Cache Extractor (Mode 1100)","text":"<p>Format: <code>hash:username</code></p> <pre><code>func extractDCC(rawHash string) *UsernameAndDomain {\n    parts := strings.Split(rawHash, \":\")\n    if len(parts) == 2 {\n        username := parts[1]\n        return &amp;UsernameAndDomain{\n            Username: &amp;username,\n            Domain:   nil,\n        }\n    }\n    return nil\n}\n</code></pre> <p>Examples: <pre><code>a1b2c3...:jsmith              \u2192  username=\"jsmith\", domain=NULL\nb2c3d4...:administrator       \u2192  username=\"administrator\", domain=NULL\nc3d4e5...:WKS01$              \u2192  username=\"WKS01$\", domain=NULL\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#heuristic-fallback-extractor","title":"Heuristic Fallback Extractor","text":"<p>Used when no type-specific extractor matches:</p> <p>Strategy: 1. Look for DOMAIN\\username pattern first 2. Look for username@domain pattern second 3. Look for hash:username pattern 4. Check if line starts/ends with username patterns 5. Preserve <code>$</code> suffix (don't treat as separator)</p> <p>Code Excerpt: <pre><code>func extractUsernameHeuristic(rawHash string) *UsernameAndDomain {\n    // Try DOMAIN\\username\n    if idx := strings.Index(rawHash, \"\\\\\"); idx &gt; 0 {\n        username, domain := ParseDomainUsername(rawHash[:...])\n        return &amp;UsernameAndDomain{...}\n    }\n\n    // Try username@domain\n    if strings.Contains(rawHash, \"@\") {\n        parts := strings.Split(rawHash, \":\")\n        if parts[0] contains \"@\" {\n            username, domain := ParseDomainUsername(parts[0])\n            return &amp;UsernameAndDomain{...}\n        }\n    }\n\n    // Try hash:username format\n    parts := strings.Split(rawHash, \":\")\n    if len(parts) == 2 &amp;&amp; looks like hash format {\n        return &amp;UsernameAndDomain{Username: &amp;parts[1]}\n    }\n\n    return nil\n}\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#database-integration","title":"Database Integration","text":""},{"location":"reference/architecture/username-extraction/#storage-operations","title":"Storage Operations","text":"<p>Batch Insert with Username/Domain:</p> <pre><code>INSERT INTO hashes (hash_value, original_hash, username, domain, hash_type_id, is_cracked, password)\nVALUES ($1, $2, $3, $4, $5, $6, $7)\nON CONFLICT (original_hash, hash_type_id) DO UPDATE SET\n    username = COALESCE(hashes.username, EXCLUDED.username),\n    domain = COALESCE(hashes.domain, EXCLUDED.domain),\n    is_cracked = EXCLUDED.is_cracked OR hashes.is_cracked,\n    password = COALESCE(EXCLUDED.password, hashes.password)\n</code></pre> <p>Query Patterns:</p> <pre><code>-- Filter by domain\nSELECT * FROM hashes WHERE domain = 'CONTOSO';\n\n-- Filter by username\nSELECT * FROM hashes WHERE username LIKE 'admin%';\n\n-- Machine accounts only\nSELECT * FROM hashes WHERE username LIKE '%$';\n\n-- Cracked domain admins\nSELECT * FROM hashes\nWHERE domain = 'ENTERPRISE'\n  AND username = 'Administrator'\n  AND is_cracked = true;\n</code></pre>"},{"location":"reference/architecture/username-extraction/#performance-considerations","title":"Performance Considerations","text":"<p>Indexes Created: <pre><code>CREATE INDEX idx_hashes_domain ON hashes(domain) WHERE domain IS NOT NULL;\nCREATE INDEX idx_hashes_username ON hashes(username) WHERE username IS NOT NULL;\n</code></pre></p> <p>Query Optimization: - Domain and username fields are indexed - Partial indexes only index non-NULL values - Enables fast filtering in UI and API endpoints</p>"},{"location":"reference/architecture/username-extraction/#api-integration","title":"API Integration","text":""},{"location":"reference/architecture/username-extraction/#hashlist-hash-endpoint","title":"Hashlist Hash Endpoint","text":"<p>Endpoint: <code>GET /api/hashlists/{id}/hashes</code></p> <p>Query Parameters: - <code>limit</code>: Number of results (default: 500, max: 2000, -1 for all) - <code>offset</code>: Pagination offset - <code>search</code>: Filter across all fields</p> <p>Response includes domain: <pre><code>{\n  \"hashes\": [\n    {\n      \"id\": \"uuid\",\n      \"hash_value\": \"8846f7eaee8fb117ad06bdd830b7586c\",\n      \"original_hash\": \"CONTOSO\\\\Administrator:500:aad3...:8846...\",\n      \"username\": \"Administrator\",\n      \"domain\": \"CONTOSO\",\n      \"is_cracked\": true,\n      \"password\": \"Password123!\"\n    }\n  ],\n  \"total\": 1000,\n  \"limit\": 500,\n  \"offset\": 0\n}\n</code></pre></p> <p>Sorting: Results sorted by <code>is_cracked DESC, id</code> (cracked hashes first)</p> <p>Pagination Limits: - Default: 500 hashes per page - Maximum: 2000 hashes per page - Special: -1 for unlimited (all hashes)</p>"},{"location":"reference/architecture/username-extraction/#frontend-integration","title":"Frontend Integration","text":""},{"location":"reference/architecture/username-extraction/#hashdetail-interface","title":"HashDetail Interface","text":"<pre><code>interface HashDetail {\n  id: string;\n  hash_value: string;\n  original_hash: string;\n  username?: string;\n  domain?: string;          // Added in v1.1+\n  hash_type_id: number;\n  is_cracked: boolean;\n  password?: string;\n  last_updated: string;\n}\n</code></pre>"},{"location":"reference/architecture/username-extraction/#display-components","title":"Display Components","text":"<p>HashlistHashesTable Component: - Displays username, domain, and password columns - Filters by username or domain via search - Copy button copies password (if cracked) or hash - Color-coded status chips</p> <p>Component Location: <code>frontend/src/components/hashlist/HashlistHashesTable.tsx</code></p> <p>Features: - Paginated table with customizable page sizes - Real-time search across all fields - Automatic sorting (cracked hashes first) - Copy-to-clipboard integration - Dynamic responsive layout</p>"},{"location":"reference/architecture/username-extraction/#testing","title":"Testing","text":""},{"location":"reference/architecture/username-extraction/#test-coverage","title":"Test Coverage","text":"<p>Created comprehensive test files in <code>/tmp/kh-test-hashes/</code>:</p> <p>Hash Types Tested: - NTLM pwdump format (7 cases) - NTLM various formats (6 cases) - NetNTLMv1 (6 cases) - NetNTLMv2 (6 cases) - Kerberos AS-REP (6 cases) - LastPass (6 cases) - DCC/MS Cache (6 cases) - Machine accounts across types (6 cases) - Edge cases (10 cases)</p> <p>Total Test Cases: 59</p> <p>Test Results: 100% passing after Kerberos bug fix</p>"},{"location":"reference/architecture/username-extraction/#test-examples","title":"Test Examples","text":"<p>NTLM Pwdump Format: <pre><code># Test 1: Standard pwdump with domain\nCONTOSO\\Administrator:500:aad3b435b51404eeaad3b435b51404ee:8846f7eaee8fb117ad06bdd830b7586c:::\nExpected: username=\"Administrator\", domain=\"CONTOSO\"\n\n# Test 2: Plain NTLM without username\n8846f7eaee8fb117ad06bdd830b7586c\nExpected: username=NULL, domain=NULL\n</code></pre></p> <p>Kerberos Machine Account: <pre><code># Test with machine account\n$krb5asrep$23$WKS01$@ENTERPRISE.LOCAL:3e156a5f5e5e5e5e5e5e5e5e5e5e5e5e$a1b2c3d4e5f6\nExpected: username=\"WKS01$\", domain=\"ENTERPRISE.LOCAL\"\n</code></pre></p> <p>NetNTLMv2 with Empty Domain: <pre><code># Test with no domain\ntestuser:::fedcba9876543210:b2b2b2b2b2b2b2b2b2b2b2b2b2b2b2b20101000000000000\nExpected: username=\"testuser\", domain=NULL\n</code></pre></p>"},{"location":"reference/architecture/username-extraction/#known-issues-and-fixes","title":"Known Issues and Fixes","text":""},{"location":"reference/architecture/username-extraction/#issue-kerberos-machine-account-bug","title":"Issue: Kerberos Machine Account Bug","text":"<p>Problem: Machine accounts with <code>$</code> in username corrupted during extraction</p> <p>Example Failure: <pre><code>Input:  $krb5asrep$23$WKS01$@ENTERPRISE.LOCAL:hash...\nExpect: username=\"WKS01$\", domain=\"ENTERPRISE.LOCAL\"\nActual: username=\"WKS01\", domain=NULL \u274c\n</code></pre></p> <p>Root Cause: <code>strings.Split(rawHash, \"$\")</code> split on ALL <code>$</code> characters, including the one in the username</p> <p>Fix: Implemented targeted delimiter finding to locate exactly the first 3 <code>$</code> separators</p> <p>Status: \u2705 Fixed in feature branch <code>feature/enhanced-username-domain-extraction</code></p> <p>Impact: All 6 Kerberos test cases now passing</p>"},{"location":"reference/architecture/username-extraction/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/username-extraction/#for-administrators","title":"For Administrators","text":"<ol> <li>Verify Hash Type: Always select correct hash type when uploading</li> <li>Check Extraction: Review username/domain extraction in detail view</li> <li>Filter by Domain: Use domain field to focus on specific organizations</li> <li>Prioritize Targets: Focus on administrative accounts and machine accounts</li> </ol>"},{"location":"reference/architecture/username-extraction/#for-developers","title":"For Developers","text":"<ol> <li>Test Edge Cases: Always test with machine accounts and special characters</li> <li>Preserve Original: Never lose information from original hash line</li> <li>Handle NULLs: Domain may be NULL for many hash types</li> <li>Index Appropriately: Use partial indexes on nullable fields</li> <li>Document Formats: Clearly document expected input formats</li> </ol>"},{"location":"reference/architecture/username-extraction/#for-penetration-testers","title":"For Penetration Testers","text":"<ol> <li>Identify High-Value Targets: Look for Administrator, Domain Admin accounts</li> <li>Track Machine Accounts: Computer accounts can provide lateral movement</li> <li>Domain Mapping: Use domain field to understand organizational structure</li> <li>Password Reuse: Check if same passwords used across different domains</li> </ol>"},{"location":"reference/architecture/username-extraction/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/username-extraction/#file-locations","title":"File Locations","text":"<p>Backend: - <code>backend/pkg/hashutils/processing.go</code> - Extraction functions - <code>backend/internal/repository/hash_repository.go</code> - Database queries - <code>backend/internal/processor/hashlist_processor.go</code> - Upload processing - <code>backend/db/migrations/000070_add_domain_to_hashes.up.sql</code> - Schema migration</p> <p>Frontend: - <code>frontend/src/components/hashlist/HashlistHashesTable.tsx</code> - Table component - <code>frontend/src/components/hashlist/HashlistDetailView.tsx</code> - Detail page - <code>frontend/src/types/</code> - TypeScript interfaces</p>"},{"location":"reference/architecture/username-extraction/#code-changes-summary","title":"Code Changes Summary","text":"<p>Migration 000070: - Added <code>domain</code> TEXT column to <code>hashes</code> table - Created partial indexes on <code>domain</code> and <code>username</code></p> <p>Processing Pipeline: - Modified <code>hashlist_processor.go</code> to call extraction functions - Updated batch insert to include username and domain - Maintained backward compatibility</p> <p>Repository Layer: - Updated all SQL queries to include domain field - Added COALESCE logic to preserve existing values - Added sorting by cracked status</p> <p>API Layer: - Increased pagination limits (500 default, 2000 max) - Added support for <code>-1</code> (unlimited results) - Included domain in JSON responses</p> <p>Frontend: - Created new paginated table component - Added domain column to table display - Implemented copy-to-clipboard for passwords</p>"},{"location":"reference/architecture/username-extraction/#future-enhancements","title":"Future Enhancements","text":"<p>Implemented Enhancements (v1.2+): - \u2705 Domain-based analytics reports - Comprehensive analytics with domain filtering (see Analytics Reports) - \u2705 Statistics by domain - Pre-calculated analytics for each domain - \u2705 Export reports grouped by domain - Domain-specific analytics sections</p> <p>Potential Improvements: - Automatic detection of privileged accounts (admin, root, etc.) - Domain hierarchy visualization - Cross-hashlist username tracking - Machine account identification in UI - LDAP/Active Directory integration for username validation</p> <p>API Enhancements: - Filter endpoint by username or domain - Aggregation queries (count by domain, etc.) - Export functionality with username/domain fields</p> <p>UI Enhancements: - Domain tree view for enterprise environments - User-specific crack statistics - Highlight privileged accounts in UI - Filter presets (e.g., \"Show only machine accounts\")</p>"},{"location":"reference/architecture/username-extraction/#references","title":"References","text":"<ul> <li>Hashcat hash mode reference: https://hashcat.net/wiki/doku.php?id=example_hashes</li> <li>Active Directory machine accounts: https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/active-directory-accounts</li> <li>Kerberos authentication: https://datatracker.ietf.org/doc/html/rfc4120</li> <li>NTLM authentication: https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-nlmp/</li> </ul> <p>Last Updated: October 2025 Feature Branch: <code>feature/enhanced-username-domain-extraction</code> Migration: 000070 Version: 1.1+</p>"},{"location":"troubleshooting/potfile-issues/","title":"Troubleshooting Potfile Issues","text":""},{"location":"troubleshooting/potfile-issues/#potfile-preset-job-not-created","title":"Potfile Preset Job Not Created","text":""},{"location":"troubleshooting/potfile-issues/#symptoms","title":"Symptoms","text":"<ul> <li>Potfile wordlist exists in Resources \u2192 Wordlists</li> <li>No \"Potfile Run\" job in Admin \u2192 Preset Jobs</li> <li>Logs show \"No binary versions found\" messages</li> </ul>"},{"location":"troubleshooting/potfile-issues/#cause","title":"Cause","text":"<p>The potfile preset job requires a hashcat binary to be uploaded. On fresh installations, no binaries exist, so the preset job cannot be created due to database constraints (binary_version_id is NOT NULL).</p>"},{"location":"troubleshooting/potfile-issues/#solution","title":"Solution","text":"<ol> <li>Upload a hashcat binary via Admin \u2192 Binary Management</li> <li>Wait for verification to complete</li> <li>Check Admin \u2192 Preset Jobs - \"Potfile Run\" should appear within 5-10 seconds</li> <li>If not visible after 30 seconds, restart the backend service</li> </ol>"},{"location":"troubleshooting/potfile-issues/#prevention","title":"Prevention","text":"<p>Always upload a hashcat binary as the first step after installation. This is documented in the Quick Start and First Crack guides.</p>"},{"location":"troubleshooting/potfile-issues/#verification","title":"Verification","text":"<p>Check system logs for confirmation: <pre><code>docker logs krakenhashes 2&gt;&amp;1 | grep -i \"potfile preset job\"\n</code></pre></p> <p>You should see one of these messages: - \"Successfully created pot-file preset job with ID: [uuid]\" - Success - \"Waiting for binary versions to be added before creating pot-file preset job\" - Still waiting - \"Found existing pot-file preset job with ID\" - Already exists</p>"},{"location":"troubleshooting/potfile-issues/#technical-details","title":"Technical Details","text":"<p>The potfile system initializes in two stages:</p> <ol> <li>Potfile Wordlist Creation (always succeeds):</li> <li>Creates <code>/data/krakenhashes/wordlists/custom/potfile.txt</code></li> <li>Adds wordlist entry to database with <code>is_potfile = true</code></li> <li> <p>Sets system setting <code>potfile_wordlist_id</code></p> </li> <li> <p>Preset Job Creation (requires binary):</p> </li> <li>Attempts to create \"Potfile Run\" preset job</li> <li>Requires <code>binary_version_id</code> (NOT NULL constraint)</li> <li>If no binaries exist, starts background monitor</li> <li>Monitor checks every 5 seconds for binary availability</li> <li>Creates preset job once binary is uploaded</li> </ol>"},{"location":"troubleshooting/potfile-issues/#potfile-not-being-updated","title":"Potfile Not Being Updated","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_1","title":"Symptoms","text":"<ul> <li>Cracked passwords not appearing in potfile</li> <li>Potfile size not increasing</li> <li>Staging table has entries but potfile is unchanged</li> </ul>"},{"location":"troubleshooting/potfile-issues/#common-causes-and-solutions","title":"Common Causes and Solutions","text":"<ol> <li>Potfile Disabled</li> <li>Check: <code>SELECT value FROM system_settings WHERE key = 'potfile_enabled';</code></li> <li>Fix: <code>UPDATE system_settings SET value = 'true' WHERE key = 'potfile_enabled';</code></li> <li> <p>Restart backend service</p> </li> <li> <p>Background Worker Stopped</p> </li> <li>Check logs: <code>docker logs krakenhashes 2&gt;&amp;1 | grep \"pot-file service\"</code></li> <li>Look for: \"Pot-file service started\" vs error messages</li> <li> <p>Fix: Restart backend service</p> </li> <li> <p>Staging Table Processing Issues</p> </li> <li>Check staging count: <code>SELECT COUNT(*) FROM potfile_staging;</code></li> <li>Check for old entries: <code>SELECT COUNT(*) FROM potfile_staging WHERE created_at &lt; NOW() - INTERVAL '1 hour';</code></li> <li> <p>If stuck, manually clear: <code>DELETE FROM potfile_staging WHERE created_at &lt; NOW() - INTERVAL '1 hour';</code></p> </li> <li> <p>File Permission Issues</p> </li> <li>Check file exists: <code>ls -la /data/krakenhashes/wordlists/custom/potfile.txt</code></li> <li>Check permissions allow writing</li> <li>Fix: <code>chmod 644 /data/krakenhashes/wordlists/custom/potfile.txt</code></li> </ol>"},{"location":"troubleshooting/potfile-issues/#potfile-wordlist-shows-wrong-count","title":"Potfile Wordlist Shows Wrong Count","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_2","title":"Symptoms","text":"<ul> <li>Wordlist entry shows incorrect word count</li> <li>Keyspace calculations are wrong</li> <li>Jobs using potfile have incorrect progress</li> </ul>"},{"location":"troubleshooting/potfile-issues/#solution_1","title":"Solution","text":"<p>The system should automatically update counts during batch processing. If not:</p> <ol> <li> <p>Check recent batch processing:    <pre><code>SELECT * FROM wordlists WHERE is_potfile = true;\n</code></pre></p> </li> <li> <p>Manually trigger recount (requires backend restart):    <pre><code>UPDATE wordlists \nSET word_count = (SELECT COUNT(*) FROM potfile_lines),\n    updated_at = NOW()\nWHERE is_potfile = true;\n</code></pre></p> </li> <li> <p>Update preset job keyspace:    <pre><code>UPDATE preset_jobs \nSET keyspace = (SELECT word_count FROM wordlists WHERE is_potfile = true)\nWHERE name = 'Potfile Run';\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/potfile-issues/#duplicate-passwords-in-potfile","title":"Duplicate Passwords in Potfile","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_3","title":"Symptoms","text":"<ul> <li>Same password appears multiple times</li> <li>File size larger than expected</li> <li>Performance degradation</li> </ul>"},{"location":"troubleshooting/potfile-issues/#causes","title":"Causes","text":"<ul> <li>Manual editing of potfile while system is running</li> <li>Database/filesystem sync issues</li> <li>Processing errors</li> </ul>"},{"location":"troubleshooting/potfile-issues/#solution_2","title":"Solution","text":"<ol> <li>Stop the backend service</li> <li>Back up the current potfile</li> <li>Remove duplicates:    <pre><code>sort -u /data/krakenhashes/wordlists/custom/potfile.txt &gt; /tmp/potfile_clean.txt\nmv /tmp/potfile_clean.txt /data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre></li> <li>Update database:    <pre><code>UPDATE wordlists \nSET word_count = (SELECT COUNT(*) FROM potfile_lines),\n    file_size = pg_stat_file('/data/krakenhashes/wordlists/custom/potfile.txt').size,\n    updated_at = NOW()\nWHERE is_potfile = true;\n</code></pre></li> <li>Restart backend service</li> </ol>"},{"location":"troubleshooting/potfile-issues/#monitor-not-creating-preset-job","title":"Monitor Not Creating Preset Job","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_4","title":"Symptoms","text":"<ul> <li>Binary uploaded but preset job still missing</li> <li>Logs show monitor is running but not creating job</li> <li>System seems stuck waiting</li> </ul>"},{"location":"troubleshooting/potfile-issues/#debugging-steps","title":"Debugging Steps","text":"<ol> <li> <p>Check if monitor is running:    <pre><code>docker logs krakenhashes 2&gt;&amp;1 | grep \"monitor for binary versions\"\n</code></pre></p> </li> <li> <p>Check for any binaries:    <pre><code>SELECT id, binary_type, is_active, verification_status \nFROM binary_versions;\n</code></pre></p> </li> <li> <p>Check system settings:    <pre><code>SELECT * FROM system_settings \nWHERE key IN ('potfile_wordlist_id', 'potfile_preset_job_id');\n</code></pre></p> </li> <li> <p>Force retry by clearing preset job ID:    <pre><code>UPDATE system_settings \nSET value = NULL \nWHERE key = 'potfile_preset_job_id';\n</code></pre>    Then restart backend</p> </li> </ol>"},{"location":"troubleshooting/potfile-issues/#manual-creation-last-resort","title":"Manual Creation (Last Resort)","text":"<p>If automatic creation fails, manually create the preset job:</p> <pre><code>-- Get the wordlist ID\nSELECT id FROM wordlists WHERE is_potfile = true;\n\n-- Get a binary version ID\nSELECT id FROM binary_versions WHERE is_active = true LIMIT 1;\n\n-- Create the preset job (replace IDs)\nINSERT INTO preset_jobs (\n    id, name, wordlist_ids, rule_ids, attack_mode, \n    priority, chunk_size_seconds, status_updates_enabled,\n    allow_high_priority_override, binary_version_id, keyspace\n) VALUES (\n    gen_random_uuid(),\n    'Potfile Run',\n    '[\"WORDLIST_ID\"]'::jsonb,  -- Replace WORDLIST_ID\n    '[]'::jsonb,\n    0,  -- Dictionary attack\n    1000,  -- Max priority\n    1200,  -- 20 minute chunks\n    true,\n    true,\n    BINARY_ID,  -- Replace BINARY_ID\n    1  -- Initial keyspace\n);\n\n-- Update system settings with the new ID\nUPDATE system_settings \nSET value = (SELECT id::text FROM preset_jobs WHERE name = 'Potfile Run')\nWHERE key = 'potfile_preset_job_id';\n</code></pre>"},{"location":"troubleshooting/potfile-issues/#best-practices","title":"Best Practices","text":"<ol> <li>Always upload a binary first during initial setup</li> <li>Don't manually edit the potfile while the system is running</li> <li>Monitor staging table size - large backlogs indicate processing issues</li> <li>Check logs regularly for potfile-related errors</li> <li>Keep batch intervals reasonable (30-60 seconds recommended)</li> <li>Archive old potfiles if they grow beyond 1GB</li> </ol>"},{"location":"troubleshooting/potfile-issues/#related-documentation","title":"Related Documentation","text":"<ul> <li>Potfile Management Guide</li> <li>Binary Management</li> </ul>"},{"location":"user-api/","title":"KrakenHashes User API Documentation","text":"<p>This directory contains comprehensive documentation for the KrakenHashes User API.</p>"},{"location":"user-api/#quick-start","title":"Quick Start","text":"<ol> <li>Get your API key: Log into KrakenHashes \u2192 Profile Settings \u2192 Generate API Key</li> <li>Read the User Guide: USER_GUIDE.md</li> <li>Try the examples: See <code>examples/</code> directory</li> </ol>"},{"location":"user-api/#documentation-files","title":"Documentation Files","text":"<ul> <li>USER_GUIDE.md - Complete user guide with workflows and best practices</li> <li>openapi.yaml - OpenAPI 3.0 specification</li> <li>examples/python_client.py - Python client library and examples</li> <li>examples/curl_examples.sh - cURL command examples</li> </ul>"},{"location":"user-api/#api-endpoints","title":"API Endpoints","text":""},{"location":"user-api/#authentication","title":"Authentication","text":"<p>All endpoints require <code>X-User-Email</code> and <code>X-API-Key</code> headers.</p>"},{"location":"user-api/#available-endpoints","title":"Available Endpoints","text":"Category Endpoint Description Health <code>GET /health</code> API health check Clients <code>GET /clients</code> List clients <code>POST /clients</code> Create client <code>GET /clients/{id}</code> Get client <code>PATCH /clients/{id}</code> Update client <code>DELETE /clients/{id}</code> Delete client Hashlists <code>GET /hashlists</code> List hashlists <code>POST /hashlists</code> Upload hashlist <code>GET /hashlists/{id}</code> Get hashlist <code>DELETE /hashlists/{id}</code> Delete hashlist Agents <code>POST /agents/vouchers</code> Generate voucher <code>GET /agents</code> List agents <code>GET /agents/{id}</code> Get agent <code>PATCH /agents/{id}</code> Update agent <code>DELETE /agents/{id}</code> Disable agent Jobs <code>GET /jobs</code> List jobs <code>POST /jobs</code> Create job <code>GET /jobs/{id}</code> Get job <code>PATCH /jobs/{id}</code> Update job <code>GET /jobs/{id}/layers</code> Get job layers (increment mode) <code>GET /jobs/{id}/layers/{layer_id}</code> Get tasks for a layer Metadata <code>GET /hash-types</code> List hash types <code>GET /workflows</code> List workflows <code>GET /preset-jobs</code> List preset jobs"},{"location":"user-api/#dynamic-validation","title":"Dynamic Validation","text":"<p>Some API validation rules are configured via system settings:</p> Setting Default Description <code>max_job_priority</code> 1000 Maximum allowed job priority value <code>require_client_for_hashlist</code> false Whether <code>client_id</code> is required for hashlist uploads <code>default_data_retention_months</code> null Default retention period for new clients <p>These settings can be adjusted by administrators through the system settings interface.</p>"},{"location":"user-api/#examples","title":"Examples","text":""},{"location":"user-api/#quick-test","title":"Quick Test","text":"<pre><code># Set your credentials\nexport KRAKEN_EMAIL=\"user@example.com\"\nexport KRAKEN_API_KEY=\"your-64-character-api-key\"\n\n# Health check (no auth required)\ncurl http://localhost:31337/api/v1/health\n\n# List clients\ncurl http://localhost:31337/api/v1/clients \\\n  -H \"X-User-Email: $KRAKEN_EMAIL\" \\\n  -H \"X-API-Key: $KRAKEN_API_KEY\"\n</code></pre>"},{"location":"user-api/#python","title":"Python","text":"<pre><code>from python_client import KrakenHashesClient\n\nclient = KrakenHashesClient(\n    base_url='http://localhost:31337/api/v1',\n    email='user@example.com',\n    api_key='your-api-key'\n)\n\n# Create a client\nclient_obj = client.create_client(name=\"My Client\")\nprint(f\"Created: {client_obj['id']}\")\n\n# List hash types\nhash_types = client.list_hash_types(enabled_only=True)\nprint(f\"Available hash types: {hash_types['total']}\")\n</code></pre>"},{"location":"user-api/#viewing-openapi-documentation","title":"Viewing OpenAPI Documentation","text":""},{"location":"user-api/#with-swagger-ui","title":"With Swagger UI","text":"<pre><code>docker run -p 8080:8080 \\\n  -e SWAGGER_JSON=/specs/openapi.yaml \\\n  -v $(pwd)/openapi.yaml:/specs/openapi.yaml \\\n  swaggerapi/swagger-ui\n</code></pre> <p>Open: http://localhost:8080</p>"},{"location":"user-api/#with-redoc","title":"With Redoc","text":"<pre><code>docker run -p 8080:80 \\\n  -e SPEC_URL=openapi.yaml \\\n  -v $(pwd)/openapi.yaml:/usr/share/nginx/html/openapi.yaml \\\n  redocly/redoc\n</code></pre> <p>Open: http://localhost:8080</p>"},{"location":"user-api/#import-to-postman","title":"Import to Postman","text":"<ol> <li>Open Postman</li> <li>Import \u2192 Upload File \u2192 <code>openapi.yaml</code></li> <li>Set environment variables for <code>baseUrl</code>, <code>userEmail</code>, <code>apiKey</code></li> </ol>"},{"location":"user-api/#current-status","title":"Current Status","text":"<p>All User API endpoints are fully implemented:</p> <ul> <li>Client management (CRUD with data retention settings)</li> <li>Hashlist management (upload, list, delete)</li> <li>Agent management (vouchers, list, update, delete)</li> <li>Job management (create, list, update, layers)</li> <li>Metadata endpoints (hash types, workflows, preset jobs)</li> </ul> <p>Planned Enhancements: - WebSocket support for real-time job status updates</p>"},{"location":"user-api/#support","title":"Support","text":"<ul> <li>Issues: https://github.com/ZerkerEOD/krakenhashes/issues</li> <li>Main Documentation: See the documentation site</li> <li>User Guide: USER_GUIDE.md</li> </ul>"},{"location":"user-api/#version","title":"Version","text":"<p>Current API Version: v1.0.0</p> <p>Last Updated: November 2025</p>"},{"location":"user-api/USER_GUIDE/","title":"KrakenHashes User API - User Guide","text":""},{"location":"user-api/USER_GUIDE/#overview","title":"Overview","text":"<p>The KrakenHashes User API provides programmatic access to core KrakenHashes functionality, allowing you to integrate password cracking workflows into your own tools and automation.</p> <p>Base URL: <code>https://your-domain.com/api/v1</code> (or <code>http://localhost:31337/api/v1</code> for development)</p>"},{"location":"user-api/USER_GUIDE/#authentication","title":"Authentication","text":"<p>All API requests require two headers:</p> <ul> <li><code>X-User-Email</code>: Your KrakenHashes account email</li> <li><code>X-API-Key</code>: Your 64-character API key</li> </ul>"},{"location":"user-api/USER_GUIDE/#getting-your-api-key","title":"Getting Your API Key","text":"<ol> <li>Log into the KrakenHashes web interface</li> <li>Navigate to Profile Settings \u2192 API Keys</li> <li>Click Generate API Key</li> <li>Copy and save the key immediately - it will only be shown once!</li> </ol>"},{"location":"user-api/USER_GUIDE/#example-authentication","title":"Example Authentication","text":"<pre><code>curl -X GET https://your-domain.com/api/v1/health \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-64-character-api-key-here\"\n</code></pre>"},{"location":"user-api/USER_GUIDE/#core-concepts","title":"Core Concepts","text":""},{"location":"user-api/USER_GUIDE/#1-clients","title":"1. Clients","text":"<p>Clients represent organizations or engagements. Hashlists can optionally be associated with a client.</p> <p>Key Points: - Client names must be unique - Clients can only be deleted if they have no associated hashlists - Client assignment to hashlists may be required based on system settings - All clients are accessible to all authenticated users</p>"},{"location":"user-api/USER_GUIDE/#2-hashlists","title":"2. Hashlists","text":"<p>Hashlists are collections of password hashes to crack.</p> <p>Key Points: - Upload via multipart form-data - Must specify hash type ID (Hashcat mode number) - Processing happens in background after upload - Check <code>/hash-types</code> endpoint for supported types - Client assignment may be optional or required based on <code>require_client_for_hashlist</code> setting</p>"},{"location":"user-api/USER_GUIDE/#3-agents","title":"3. Agents","text":"<p>Compute agents perform the actual cracking work.</p> <p>Key Points: - Register agents using voucher codes - Vouchers can be single-use or continuous - Agents are associated with the user who generated the voucher - Monitor agent status and hardware via API</p>"},{"location":"user-api/USER_GUIDE/#4-jobs","title":"4. Jobs","text":"<p>Jobs define cracking tasks using preset configurations.</p> <p>Key Points: - Jobs link hashlists to preset job configurations - Control priority (higher = processed first) - Set <code>max_agents</code> to limit concurrent agent allocation - Monitor progress via layers endpoint for increment mode jobs - Priority maximum is configurable via <code>max_job_priority</code> system setting (default: 1000)</p> <p>Job Status Values: - <code>pending</code> - Job created, waiting to be scheduled - <code>running</code> - Job is actively being processed - <code>paused</code> - Job manually paused - <code>completed</code> - All hashes cracked or exhausted - <code>failed</code> - Job failed due to error</p> <p>Increment Mode: - <code>off</code> - Standard attack (single layer) - <code>enabled</code> - Increment mode enabled - <code>enabled_with_brain</code> - Increment mode with brain feature</p>"},{"location":"user-api/USER_GUIDE/#common-workflows","title":"Common Workflows","text":""},{"location":"user-api/USER_GUIDE/#workflow-1-upload-and-crack-hashes","title":"Workflow 1: Upload and Crack Hashes","text":"<pre><code># 1. Create a client (optional, based on system settings)\nCLIENT_ID=$(curl -s -X POST https://your-domain.com/api/v1/clients \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\":\"Pentest 2024\",\"description\":\"Annual pentest engagement\"}' \\\n  | jq -r .id)\n\n# 2. Check available hash types\ncurl -s -X GET \"https://your-domain.com/api/v1/hash-types?enabled_only=true\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '.hash_types[] | {id, name}'\n\n# 3. Upload hashlist\nHASHLIST_ID=$(curl -s -X POST https://your-domain.com/api/v1/hashlists \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -F \"file=@hashes.txt\" \\\n  -F \"name=Domain Hashes\" \\\n  -F \"client_id=$CLIENT_ID\" \\\n  -F \"hash_type_id=1000\" \\\n  | jq -r .id)\n\n# 4. Check available preset jobs\ncurl -s -X GET https://your-domain.com/api/v1/preset-jobs \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '.preset_jobs[] | {id, name}'\n\n# 5. Create a job\nJOB_ID=$(curl -s -X POST https://your-domain.com/api/v1/jobs \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"hashlist_id\\\": $HASHLIST_ID,\n    \\\"preset_job_id\\\": 1,\n    \\\"name\\\": \\\"Domain Hash Attack\\\",\n    \\\"priority\\\": 100,\n    \\\"max_agents\\\": 5\n  }\" \\\n  | jq -r .id)\n\n# 6. Monitor job progress\ncurl -s -X GET \"https://your-domain.com/api/v1/jobs/$JOB_ID\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '{status, progress, cracked_count, total_hashes}'\n</code></pre>"},{"location":"user-api/USER_GUIDE/#workflow-2-agent-registration","title":"Workflow 2: Agent Registration","text":"<pre><code># 1. Generate a registration voucher\nVOUCHER=$(curl -s -X POST https://your-domain.com/api/v1/agents/vouchers \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"is_continuous\": false\n  }' \\\n  | jq -r .code)\n\necho \"Voucher code: $VOUCHER\"\n\n# 2. On the agent machine, register with the voucher\n./agent --host your-domain.com:31337 --claim $VOUCHER\n\n# 3. Monitor agent status\ncurl -s -X GET https://your-domain.com/api/v1/agents \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '.agents[] | {id, name, status, gpus: .hardware.gpus | length}'\n</code></pre>"},{"location":"user-api/USER_GUIDE/#workflow-3-job-management","title":"Workflow 3: Job Management","text":"<pre><code># List all jobs with filtering\ncurl -s -X GET \"https://your-domain.com/api/v1/jobs?status=running&amp;page=1&amp;page_size=20\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq .\n\n# Update job priority (boost to run sooner)\ncurl -s -X PATCH \"https://your-domain.com/api/v1/jobs/$JOB_ID\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"priority\": 500}'\n\n# Get job layers (for increment mode jobs)\ncurl -s -X GET \"https://your-domain.com/api/v1/jobs/$JOB_ID/layers\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '.layers[] | {id, name, status, progress}'\n\n# Get tasks within a specific layer\ncurl -s -X GET \"https://your-domain.com/api/v1/jobs/$JOB_ID/layers/1\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\" \\\n  | jq '.tasks[] | {id, status, agent_name, progress}'\n</code></pre>"},{"location":"user-api/USER_GUIDE/#workflow-4-batch-operations","title":"Workflow 4: Batch Operations","text":"<pre><code>from krakenhashes import KrakenHashesClient\nimport glob\nimport os\n\nclient = KrakenHashesClient(\n    base_url='https://your-domain.com/api/v1',\n    email='user@example.com',\n    api_key='your-api-key'\n)\n\n# Create a client for this batch\nclient_obj = client.create_client(name=\"Batch Upload 2024\")\nclient_id = client_obj['id']\n\n# Get preset job for potfile attack\npreset_jobs = client.list_preset_jobs()\npotfile_preset_id = next(\n    p['id'] for p in preset_jobs['preset_jobs']\n    if 'potfile' in p['name'].lower()\n)\n\n# Upload all hash files and create jobs\nfor hash_file in glob.glob('hashes/*.txt'):\n    filename = os.path.basename(hash_file)\n    print(f\"Uploading {filename}...\")\n\n    hashlist = client.create_hashlist(\n        name=filename,\n        client_id=client_id,\n        hash_type_id=1000,  # NTLM\n        file_path=hash_file\n    )\n    print(f\"  Created hashlist ID: {hashlist['id']}\")\n\n    # Create job for this hashlist\n    job = client.create_job(\n        name=f\"Attack {filename}\",\n        hashlist_id=hashlist['id'],\n        preset_job_id=potfile_preset_id,\n        priority=100,\n        max_agents=3\n    )\n    print(f\"  Created job ID: {job['id']}\")\n\nprint(\"Batch upload complete!\")\n</code></pre>"},{"location":"user-api/USER_GUIDE/#api-reference","title":"API Reference","text":""},{"location":"user-api/USER_GUIDE/#pagination","title":"Pagination","text":"<p>List endpoints support pagination via query parameters:</p> <ul> <li><code>page</code>: Page number (1-indexed, default: 1)</li> <li><code>page_size</code>: Items per page (1-100, default: 20)</li> </ul> <p>Example:</p> <pre><code>curl \"https://your-domain.com/api/v1/clients?page=2&amp;page_size=50\" \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"clients\": [...],\n  \"page\": 2,\n  \"page_size\": 50,\n  \"total\": 127\n}\n</code></pre>"},{"location":"user-api/USER_GUIDE/#dynamic-validation","title":"Dynamic Validation","text":"<p>Some API validation rules are configured via system settings:</p> Setting Default Description <code>max_job_priority</code> 1000 Maximum allowed job priority value <code>require_client_for_hashlist</code> false Whether <code>client_id</code> is required for hashlist uploads <code>default_data_retention_months</code> null Default retention period for new clients"},{"location":"user-api/USER_GUIDE/#error-handling","title":"Error Handling","text":"<p>All errors return JSON with this structure:</p> <pre><code>{\n  \"error\": \"Human-readable error message\",\n  \"code\": \"MACHINE_READABLE_ERROR_CODE\"\n}\n</code></pre> <p>Common Error Codes:</p> Code Description HTTP Status <code>VALIDATION_ERROR</code> Invalid request data 400 <code>AUTH_REQUIRED</code> Missing or invalid credentials 401 <code>RESOURCE_ACCESS_DENIED</code> Not authorized to access resource 403 <code>RESOURCE_NOT_FOUND</code> Resource does not exist 404 <code>CLIENT_HAS_HASHLISTS</code> Cannot delete client with hashlists 409 <code>HASHLIST_HAS_ACTIVE_JOBS</code> Cannot delete hashlist with active jobs 409 <code>CLIENT_REQUIRED</code> Client is required (based on system setting) 400 <code>INTERNAL_ERROR</code> Server error 500"},{"location":"user-api/USER_GUIDE/#rate-limiting","title":"Rate Limiting","text":"<p>The User API does not currently implement rate limiting. However, be mindful of:</p> <ul> <li>File upload sizes (hashlists)</li> <li>Background processing capacity</li> <li>Database connection limits</li> </ul>"},{"location":"user-api/USER_GUIDE/#best-practices","title":"Best Practices","text":""},{"location":"user-api/USER_GUIDE/#security","title":"Security","text":"<ol> <li>Protect your API key like a password</li> <li>Never commit API keys to version control</li> <li>Use environment variables: <code>export KRAKEN_API_KEY=\"...\"</code></li> <li> <p>Rotate keys periodically</p> </li> <li> <p>Use HTTPS in production</p> </li> <li>API keys are sent in headers, not encrypted by default</li> <li> <p>Always use TLS/SSL for production deployments</p> </li> <li> <p>Scope access appropriately</p> </li> <li>Each user has separate API keys</li> <li>API keys have same permissions as user account</li> </ol>"},{"location":"user-api/USER_GUIDE/#performance","title":"Performance","text":"<ol> <li> <p>Use pagination for large result sets    <pre><code># Good: Paginate through results\npage = 1\nwhile True:\n    result = client.list_clients(page=page, page_size=100)\n    if not result['clients']:\n        break\n    process(result['clients'])\n    page += 1\n\n# Bad: Try to load everything at once (may timeout)\nall_clients = client.list_clients(page_size=10000)  # Don't do this\n</code></pre></p> </li> <li> <p>Check processing status before proceeding    <pre><code># Upload hashlist\nhashlist = client.create_hashlist(...)\n\n# Wait for processing to complete\nimport time\nwhile True:\n    status = client.get_hashlist(hashlist['id'])\n    if status.get('status') == 'ready':\n        break\n    time.sleep(5)\n</code></pre></p> </li> <li> <p>Reuse HTTP connections <pre><code># Good: Client maintains session\nclient = KrakenHashesClient(...)\nfor i in range(100):\n    client.create_client(...)  # Reuses connection\n\n# Bad: Creating new connections each time\nfor i in range(100):\n    client = KrakenHashesClient(...)  # New connection overhead\n    client.create_client(...)\n</code></pre></p> </li> </ol>"},{"location":"user-api/USER_GUIDE/#error-handling_1","title":"Error Handling","text":"<p>Always handle errors gracefully:</p> <pre><code>try:\n    client.create_client(name=\"Test\")\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 400:\n        error_data = e.response.json()\n        if error_data.get('code') == 'VALIDATION_ERROR':\n            print(f\"Validation error: {error_data.get('error')}\")\n        else:\n            raise\n    elif e.response.status_code == 409:\n        print(\"Client already exists, continuing...\")\n    else:\n        raise\n</code></pre>"},{"location":"user-api/USER_GUIDE/#examples","title":"Examples","text":""},{"location":"user-api/USER_GUIDE/#python","title":"Python","text":"<p>See <code>examples/python_client.py</code> for a complete Python client implementation with examples.</p> <p>Quick Start:</p> <pre><code>pip install requests\npython examples/python_client.py\n</code></pre>"},{"location":"user-api/USER_GUIDE/#curl","title":"cURL","text":"<p>See <code>examples/curl_examples.sh</code> for comprehensive cURL examples.</p> <p>Quick Start:</p> <pre><code># Edit the script to set your credentials\nvim examples/curl_examples.sh\n\n# Run examples\nbash examples/curl_examples.sh\n</code></pre>"},{"location":"user-api/USER_GUIDE/#postman","title":"Postman","text":"<p>Import the OpenAPI specification (<code>openapi.yaml</code>) into Postman:</p> <ol> <li>Open Postman</li> <li>Import \u2192 Upload File \u2192 Select <code>openapi.yaml</code></li> <li>Configure environment variables:</li> <li><code>baseUrl</code>: <code>https://your-domain.com/api/v1</code></li> <li><code>userEmail</code>: Your email</li> <li><code>apiKey</code>: Your API key</li> </ol>"},{"location":"user-api/USER_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-api/USER_GUIDE/#authentication-fails","title":"Authentication Fails","text":"<p>Symptom: 401 Unauthorized errors</p> <p>Solutions: 1. Verify headers are set correctly:    <pre><code>curl -v https://your-domain.com/api/v1/health \\\n  -H \"X-User-Email: user@example.com\" \\\n  -H \"X-API-Key: your-api-key\"\n</code></pre> 2. Check for typos in email or API key 3. Regenerate API key if needed (invalidates old key) 4. Ensure you're using the correct base URL</p>"},{"location":"user-api/USER_GUIDE/#hashlist-upload-fails","title":"Hashlist Upload Fails","text":"<p>Symptom: 400 Bad Request on hashlist upload</p> <p>Solutions: 1. If <code>CLIENT_REQUIRED</code> error, check if system requires client assignment:    <pre><code># Either provide a client_id or ask admin to disable requirement\n</code></pre> 2. Verify client exists (if provided):    <pre><code>curl https://your-domain.com/api/v1/clients/CLIENT_ID \\\n  -H \"X-User-Email: ...\" -H \"X-API-Key: ...\"\n</code></pre> 3. Check hash type is valid:    <pre><code>curl \"https://your-domain.com/api/v1/hash-types?enabled_only=true\" \\\n  -H \"X-User-Email: ...\" -H \"X-API-Key: ...\"\n</code></pre> 4. Verify file format (one hash per line, plain text) 5. Check file size limits</p>"},{"location":"user-api/USER_GUIDE/#job-creation-fails","title":"Job Creation Fails","text":"<p>Symptom: 400 Bad Request on job creation</p> <p>Solutions: 1. Verify hashlist exists and is ready:    <pre><code>curl https://your-domain.com/api/v1/hashlists/HASHLIST_ID \\\n  -H \"X-User-Email: ...\" -H \"X-API-Key: ...\"\n</code></pre> 2. Verify preset job exists:    <pre><code>curl https://your-domain.com/api/v1/preset-jobs \\\n  -H \"X-User-Email: ...\" -H \"X-API-Key: ...\"\n</code></pre> 3. Check priority is within allowed range (default max: 1000) 4. Ensure <code>max_agents</code> is a positive integer</p>"},{"location":"user-api/USER_GUIDE/#agent-not-appearing","title":"Agent Not Appearing","text":"<p>Symptom: Agent registers but doesn't appear in list</p> <p>Solutions: 1. Verify voucher was generated by your account:    <pre><code>curl https://your-domain.com/api/v1/agents \\\n  -H \"X-User-Email: ...\" -H \"X-API-Key: ...\"\n</code></pre> 2. Check agent logs for connection errors 3. Verify network connectivity to backend 4. Ensure voucher is still active</p>"},{"location":"user-api/USER_GUIDE/#openapi-specification","title":"OpenAPI Specification","text":"<p>The complete API specification is available in <code>openapi.yaml</code>. This can be used with:</p> <ul> <li>Swagger UI: Interactive API documentation</li> <li>Postman: Import for testing</li> <li>Code generators: Generate client libraries in various languages</li> </ul> <p>To view the specification in Swagger UI:</p> <pre><code>docker run -p 8080:8080 \\\n  -e SWAGGER_JSON=/specs/openapi.yaml \\\n  -v $(pwd)/openapi.yaml:/specs/openapi.yaml \\\n  swaggerapi/swagger-ui\n</code></pre> <p>Then open: http://localhost:8080</p>"},{"location":"user-api/USER_GUIDE/#support","title":"Support","text":""},{"location":"user-api/USER_GUIDE/#documentation","title":"Documentation","text":"<ul> <li>OpenAPI Spec: <code>openapi.yaml</code></li> <li>Python Examples: <code>examples/python_client.py</code></li> <li>cURL Examples: <code>examples/curl_examples.sh</code></li> <li>Main Docs: See the documentation site</li> </ul>"},{"location":"user-api/USER_GUIDE/#issues","title":"Issues","text":"<p>Report issues at: https://github.com/ZerkerEOD/krakenhashes/issues</p> <p>When reporting issues, include: - API endpoint being called - Request headers (redact API key!) - Request body - Response status and body - Expected vs. actual behavior</p>"},{"location":"user-api/USER_GUIDE/#future-enhancements","title":"Future Enhancements","text":"<p>The following features are planned for future releases:</p> <ul> <li>WebSocket Support: Real-time job progress updates</li> <li>Bulk Operations: Batch create/update/delete endpoints</li> <li>Export Endpoints: Download cracked passwords</li> <li>Statistics API: Aggregate cracking statistics</li> <li>Webhook Support: Event notifications for job completion</li> </ul> <p>Stay tuned for updates!</p>"},{"location":"user-guide/","title":"User Guide","text":"<p>Learn how to effectively use KrakenHashes for your password auditing needs.</p>"},{"location":"user-guide/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Core Concepts</p> <p>Understand the fundamental concepts and terminology</p> </li> <li> <p> Managing Hashlists</p> <p>Import, organize, and manage your password hashes</p> </li> <li> <p> Jobs &amp; Workflows</p> <p>Create and manage cracking jobs with preset workflows</p> </li> <li> <p> Wordlists &amp; Rules</p> <p>Work with wordlists and transformation rules</p> </li> <li> <p> Analyzing Results</p> <p>Understanding POT files and analyzing cracking results</p> </li> <li> <p> Analytics Reports</p> <p>Generate comprehensive analytics reports with domain-based filtering</p> </li> <li> <p> Troubleshooting</p> <p>Common issues and how to resolve them</p> </li> </ul> <p> KrakenHashes dashboard overview featuring the left navigation sidebar, hashlist overview table showing crack progress, and jobs management interface in the dark theme</p>"},{"location":"user-guide/#quick-navigation","title":"Quick Navigation","text":""},{"location":"user-guide/#new-users","title":"New Users","text":"<ol> <li>Start with Core Concepts to understand the system</li> <li>Learn about Managing Hashlists</li> <li>Create your first job with Jobs &amp; Workflows</li> </ol>"},{"location":"user-guide/#common-tasks","title":"Common Tasks","text":"<ul> <li>Upload a hashlist</li> <li>Create a cracking job</li> <li>View cracked passwords</li> <li>Export results</li> <li>Generate analytics report</li> <li>Filter analytics by domain</li> </ul>"},{"location":"user-guide/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Custom attack workflows</li> <li>Rule chaining strategies</li> <li>Performance optimization</li> </ul>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<p>Pro Tips</p> <ul> <li>Always start with fast hash types to quickly identify weak passwords</li> <li>Use preset workflows for common scenarios</li> <li>Monitor job progress and adjust priorities as needed</li> <li>Regularly clean up old hashlists to maintain performance</li> </ul>"},{"location":"user-guide/#getting-support","title":"Getting Support","text":"<ul> <li> Review the Troubleshooting Guide</li> <li> Ask questions on Discord</li> <li> Report bugs on GitHub</li> </ul>"},{"location":"user-guide/analytics-reports/","title":"Analytics Reports","text":""},{"location":"user-guide/analytics-reports/#overview","title":"Overview","text":"<p>Analytics Reports provide comprehensive statistical analysis of cracked passwords across multiple dimensions. These reports help security professionals identify password patterns, assess organizational security posture, and provide evidence-based recommendations to clients.</p>"},{"location":"user-guide/analytics-reports/#key-features","title":"Key Features","text":"<ul> <li>13 Analytics Sections: From length distribution to strength metrics</li> <li>Domain-Based Filtering: Analyze password patterns by domain in multi-domain environments</li> <li>Custom Pattern Detection: Define and track organization-specific password patterns</li> <li>Pre-Calculated Analytics: Fast report generation with no performance impact during analysis</li> <li>Client-Specific Reports: Generate reports for specific clients or across multiple engagements</li> <li>Time-Based Filtering: Analyze trends over specific time periods</li> </ul>"},{"location":"user-guide/analytics-reports/#when-to-use-analytics-reports","title":"When to Use Analytics Reports","text":"<ul> <li>Client Reporting: Generate comprehensive password analysis for security assessments</li> <li>Trend Analysis: Track password security improvements over time</li> <li>Multi-Domain Environments: Compare password practices across different organizational units</li> <li>Security Posture Assessment: Identify weaknesses and improvement areas</li> <li>Compliance Reporting: Document password policy compliance</li> </ul>"},{"location":"user-guide/analytics-reports/#generating-analytics-reports","title":"Generating Analytics Reports","text":""},{"location":"user-guide/analytics-reports/#creating-a-new-report","title":"Creating a New Report","text":"<ol> <li>Navigate to Analytics</li> <li>Go to Clients in the main menu</li> <li>Select the client you want to analyze</li> <li> <p>Click Generate Analytics Report</p> </li> <li> <p>Select Hashlists</p> </li> <li>Choose which hashlists to include in the analysis</li> <li>Reports can analyze one or multiple hashlists</li> <li> <p>Hashlists must be from the same client</p> </li> <li> <p>Generate Report</p> </li> <li>Click Generate Report</li> <li>The system will calculate all analytics sections</li> <li> <p>Generation typically takes 5-15 seconds depending on dataset size</p> </li> <li> <p>View Report</p> </li> <li>Once generated, the report appears in the analytics list</li> <li>Click View Report to see the full analysis</li> <li>Reports are saved and can be re-viewed at any time</li> </ol>"},{"location":"user-guide/analytics-reports/#report-scope-options","title":"Report Scope Options","text":""},{"location":"user-guide/analytics-reports/#client-specific-reports","title":"Client-Specific Reports","text":"<ul> <li>Analyze all hashlists for a specific client</li> <li>Compare password practices across different engagements</li> <li>Track improvements over time for the same organization</li> </ul>"},{"location":"user-guide/analytics-reports/#hashlist-specific-reports","title":"Hashlist-Specific Reports","text":"<ul> <li>Focus analysis on a specific hashlist</li> <li>Useful for targeted assessments (e.g., executive accounts only)</li> <li>Compare different departments or organizational units</li> </ul>"},{"location":"user-guide/analytics-reports/#understanding-analytics-sections","title":"Understanding Analytics Sections","text":""},{"location":"user-guide/analytics-reports/#overview-statistics","title":"Overview Statistics","text":"<p>The Overview section provides high-level metrics:</p> <ul> <li>Total Hashes: Number of hashes analyzed</li> <li>Total Cracked: Number of successfully cracked passwords</li> <li>Crack Rate: Percentage of hashes cracked</li> <li>Hash Mode Breakdown: Distribution of hash types (NTLM, NetNTLMv2, etc.)</li> </ul> <p>Use Case: Executive summary statistics for client reports</p>"},{"location":"user-guide/analytics-reports/#domain-based-filtering","title":"Domain-Based Filtering \ud83c\udd95","text":"<p>New in v1.2+: Analytics reports now support domain-based filtering for multi-domain environments.</p>"},{"location":"user-guide/analytics-reports/#how-it-works","title":"How It Works","text":"<p>When your analyzed hashlists contain hashes with domain information (e.g., NetNTLMv2, NTLM pwdump, Kerberos), the system:</p> <ol> <li>Automatically extracts domains from hash formats</li> <li>Creates dynamic tabs at the top of the Overview section:</li> <li>\"All\" tab: Shows aggregated statistics across all domains</li> <li>Domain tabs: One tab per unique domain found (e.g., <code>acme.local</code>, <code>example.com</code>)</li> <li>Filters all analytics sections when a domain is selected</li> </ol>"},{"location":"user-guide/analytics-reports/#domain-breakdown-table","title":"Domain Breakdown Table","text":"<p>When the \"All\" tab is selected, a domain breakdown table appears showing:</p> Domain Total Hashes Cracked Percentage acme.local 14,638 14,638 100.00% contoso.local 14,638 14,638 100.00% example.com 14,638 14,638 100.00%"},{"location":"user-guide/analytics-reports/#filtering-by-domain","title":"Filtering by Domain","text":"<p>To filter analytics by domain:</p> <ol> <li>Click a domain tab (e.g., <code>acme.local</code>)</li> <li>All sections automatically update to show only that domain's data:</li> <li>Overview statistics show only the domain's hashes</li> <li>Hash Mode Breakdown shows only hash types present in that domain</li> <li>All other sections (Length, Complexity, etc.) filter accordingly</li> <li>Click \"All\" to return to aggregated view</li> </ol>"},{"location":"user-guide/analytics-reports/#use-cases-for-domain-filtering","title":"Use Cases for Domain Filtering","text":"<p>Multi-Domain Active Directory Environments: <pre><code>Scenario: Client has acquired multiple companies, each with their own AD domain\nUse Domain Filtering to:\n- Compare password security between legacy vs. new domains\n- Identify which organizational units need security training\n- Report on password patterns specific to each business unit\n</code></pre></p> <p>Department/Location Analysis: <pre><code>Scenario: Large organization with geographic domains (us.corp.com, eu.corp.com, apac.corp.com)\nUse Domain Filtering to:\n- Compare regional password practices\n- Identify location-specific security issues\n- Target training to specific regions\n</code></pre></p> <p>Client Reporting: <pre><code>Scenario: Security assessment covering multiple subsidiaries\nUse Domain Filtering to:\n- Generate per-subsidiary security reports\n- Show executives domain-specific vulnerabilities\n- Provide targeted recommendations per organizational unit\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#length-distribution","title":"Length Distribution","text":"<p>Analyzes password lengths to identify patterns:</p> <ul> <li>Distribution Chart: Visual representation of password lengths</li> <li>Average Length: Mean password length across cracked passwords</li> <li>Most Common Length: Mode of the distribution</li> <li>Length Range: Minimum and maximum password lengths</li> </ul> <p>Key Metrics: - Percentage of passwords under 8 characters - Percentage meeting typical complexity requirements (12+ characters) - Distribution curve shape (indicates policy enforcement)</p> <p>Example Findings: - \"78% of passwords are exactly 8 characters, indicating a minimum-length-only policy\" - \"No passwords exceed 12 characters, suggesting users choose minimum-required lengths\"</p>"},{"location":"user-guide/analytics-reports/#complexity-analysis","title":"Complexity Analysis","text":"<p>Evaluates password composition and character usage:</p> <ul> <li>Character Class Usage:</li> <li>Lowercase only</li> <li>Uppercase only</li> <li>Numbers only</li> <li>Mixed alphanumeric</li> <li> <p>Special characters</p> </li> <li> <p>Complexity Metrics:</p> </li> <li>Percentage meeting corporate policies</li> <li>Common character substitutions (e.g., <code>@</code> for <code>a</code>, <code>3</code> for <code>e</code>)</li> <li>Entropy calculations</li> </ul> <p>Example Findings: - \"45% of passwords use only lowercase letters\" - \"92% of passwords with special characters use <code>!</code> as the final character\"</p>"},{"location":"user-guide/analytics-reports/#positional-analysis","title":"Positional Analysis","text":"<p>Examines where different character types appear in passwords:</p> <ul> <li>First Character Analysis: Common starting characters (capital letters, digits)</li> <li>Last Character Analysis: Common ending patterns (special chars, digits, years)</li> <li>Middle Patterns: Character placement in password body</li> </ul> <p>Example Findings: - \"67% of passwords start with an uppercase letter (indicates capital-first policy)\" - \"84% of passwords end with a digit or exclamation point\" - \"Year patterns (2023, 2024) commonly appear at password end\"</p>"},{"location":"user-guide/analytics-reports/#pattern-detection","title":"Pattern Detection","text":"<p>Identifies common password patterns and structures:</p> <ul> <li>Keyboard Patterns: <code>qwerty</code>, <code>asdf</code>, <code>12345</code></li> <li>Common Sequences: <code>abc123</code>, <code>password1</code></li> <li>Name + Number: <code>John2024</code>, <code>Alice123</code></li> <li>Base Word + Modification: <code>Password!</code>, <code>Welcome1</code></li> </ul> <p>Pattern Categories: - Dictionary words - Keyboard walks - Repeating characters - Sequential characters - Common substitutions</p> <p>Example Findings: - \"234 passwords follow the pattern [Name][Year]\" - \"156 passwords use keyboard walks (qwerty, asdfgh)\"</p>"},{"location":"user-guide/analytics-reports/#username-correlation","title":"Username Correlation","text":"<p>Analyzes relationships between usernames and passwords:</p> <ul> <li>Username in Password: Passwords containing the username</li> <li>Partial Username Match: Password contains part of username</li> <li>Name-Based Passwords: FirstName/LastName in password</li> <li>Common Variations: Username + season/year</li> </ul> <p>Example Findings: - \"23% of passwords contain the user's first or last name\" - \"89 users have password that equals their username\"</p>"},{"location":"user-guide/analytics-reports/#password-reuse","title":"Password Reuse","text":"<p>Identifies password reuse across accounts:</p> <ul> <li>Reuse Count: Number of accounts sharing the same password</li> <li>Most Reused Passwords: Top passwords by usage count</li> <li>Reuse Percentage: Percentage of users with non-unique passwords</li> </ul> <p>Example Findings: - \"The password 'Welcome123' is used by 45 different accounts\" - \"34% of all accounts share passwords with at least one other account\"</p>"},{"location":"user-guide/analytics-reports/#temporal-patterns","title":"Temporal Patterns","text":"<p>Examines time-based patterns in passwords:</p> <ul> <li>Season References: <code>Summer2024</code>, <code>Winter23</code></li> <li>Month Names: <code>January</code>, <code>March2024</code></li> <li>Year Patterns: Current year, previous years</li> <li>Date Formats: <code>01012024</code>, <code>2024-01-01</code></li> </ul> <p>Example Findings: - \"78% of passwords containing years use the current year (2024)\" - \"Seasonal passwords most commonly reference 'Summer' and 'Fall'\"</p>"},{"location":"user-guide/analytics-reports/#mask-analysis","title":"Mask Analysis","text":"<p>Shows password structure patterns using hashcat mask format:</p> <ul> <li>Top Masks: Most common password structures</li> <li>Mask Distribution: Frequency of each pattern</li> <li>Complexity by Mask: Strength assessment per structure</li> </ul> <p>Mask Format: - <code>?l</code> = lowercase letter - <code>?u</code> = uppercase letter - <code>?d</code> = digit - <code>?s</code> = special character</p> <p>Example Masks: <pre><code>?u?l?l?l?l?l?l?d    = Capital + 6 lowercase + 1 digit (Welcome1)\n?u?l?l?l?l?l?l?l?d?d = Capital + 7 lowercase + 2 digits (Password24)\n</code></pre></p> <p>Example Findings: - \"Most common mask: ?u?l?l?l?l?l?l?l?d (43% of passwords)\" - \"Top 5 masks account for 87% of all cracked passwords\"</p>"},{"location":"user-guide/analytics-reports/#custom-patterns","title":"Custom Patterns","text":"<p>Track organization-specific password patterns:</p> <ul> <li>Company Name Usage: Passwords containing company name</li> <li>Product Names: References to company products/services</li> <li>Department Names: IT, HR, Finance references</li> <li>Custom Keywords: Any administrator-defined patterns</li> </ul> <p>Configuration: Administrators can define custom patterns to track in the Admin interface.</p> <p>Example Findings: - \"123 passwords contain the company name 'Acme'\" - \"34% of IT department passwords reference 'admin' or 'root'\"</p>"},{"location":"user-guide/analytics-reports/#strength-metrics","title":"Strength Metrics","text":"<p>Overall password strength assessment:</p> <ul> <li>Weak Passwords: Crackable in under 1 minute</li> <li>Moderate Passwords: Crackable in 1 minute to 1 hour</li> <li>Strong Passwords: Crackable in over 1 hour</li> <li>Very Strong: Not yet cracked despite extensive attacks</li> </ul> <p>Strength Calculation Factors: - Password length - Character diversity - Pattern presence - Dictionary word usage - Brute-force resistance estimate</p> <p>Example Findings: - \"89% of cracked passwords classified as 'Weak'\" - \"Only 2% of passwords show 'Strong' resistance to attacks\"</p>"},{"location":"user-guide/analytics-reports/#top-passwords","title":"Top Passwords","text":"<p>List of most frequently used passwords:</p> <ul> <li>Top 10/25/50: Configurable list size</li> <li>Usage Count: How many accounts use each password</li> <li>Strength Rating: Security assessment of each password</li> <li>Pattern Type: Classification (dictionary, keyboard walk, etc.)</li> </ul> <p>Example Top Passwords: 1. <code>Welcome123</code> - 45 accounts 2. <code>Password1!</code> - 38 accounts 3. <code>Summer2024</code> - 29 accounts 4. <code>Compan!</code> - 23 accounts</p>"},{"location":"user-guide/analytics-reports/#windows-hash-analytics-v121","title":"Windows Hash Analytics (v1.2.1+)","text":"<p>Comprehensive statistics for Windows-related hash types, providing detailed analysis of enterprise authentication security.</p> <p>Supported Hash Types: - NTLM (mode 1000): Current Windows password hashes - LM (mode 3000): Legacy LAN Manager hashes - NetNTLMv1 (modes 5500, 27000): Network authentication challenges - NetNTLMv2 (modes 5600, 27100): Improved network authentication - DCC/MS Cache (mode 1100): Domain Cached Credentials - DCC2/MS Cache 2 (mode 2100): Improved cached credentials - Kerberos (modes 7500, 13100, 18200, 19600, 19700): Domain authentication tickets</p>"},{"location":"user-guide/analytics-reports/#overview-card","title":"Overview Card","text":"<p>The overview section provides enterprise-wide Windows authentication metrics:</p> <p>Key Metrics: - Total Hash Records: Count of all Windows hash entries (includes LM, NTLM, and other types) - Unique Users: Distinct usernames across all Windows hashes - Cracked: Number of successfully cracked Windows hashes - Success Rate: Percentage of cracked Windows hashes - Linked Pairs: Number of LM/NTLM pairs linked during upload (if applicable)</p> <p>Important Note on Linked Pairs: When hashlists are created as linked LM/NTLM pairs (from pwdump format files), the system counts them as ONE hashlist entry in the overview statistics. This prevents double-counting the same user's credentials. Individual hash type cards show raw counts, but the overview reflects the effective count.</p> <p>Example: <pre><code>Overview:\n- Total Hash Records: 15,000\n- Unique Users: 5,000 (distinct usernames)\n- Cracked: 12,500\n- Success Rate: 83.33%\n- Linked Pairs: 4,800 (LM/NTLM pairs from pwdump upload)\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#hash-type-cards","title":"Hash Type Cards","text":"<p>Individual cards for each hash type show:</p> <p>Standard Metrics: - Total count - Cracked count - Crack percentage</p> <p>LM-Specific Metrics: - Length Distribution: Passwords \u22647 chars vs 8-14 chars (based on hash structure) - Partially Cracked: Count of hashes with only one half cracked (see LM Partial Cracks)</p> <p>Kerberos Breakdown: - etype 23 (RC4): Weak encryption, vulnerable to brute-force - etype 17 (AES128): Stronger encryption - etype 18 (AES256): Strongest encryption</p>"},{"location":"user-guide/analytics-reports/#linked-hash-correlation","title":"Linked Hash Correlation","text":"<p>For hashlists created as linked LM/NTLM pairs, this section shows correlation statistics:</p> <p>Correlation States: - Both Cracked: Both LM and NTLM hashes cracked for the user - NTLM Only: NTLM cracked (implies LM can be derived from password) - LM Only: LM cracked but NTLM still unknown (use LM-to-NTLM masks) - Neither Cracked: Both hashes still uncracked</p> <p>Use Case: Identify which users have partial compromise and prioritize follow-up attacks. When LM is cracked but NTLM is not, use the LM-to-NTLM mask generation feature to create targeted attacks.</p>"},{"location":"user-guide/analytics-reports/#security-recommendations","title":"Security Recommendations","text":"<p>The system generates automatic recommendations based on Windows hash analysis:</p> <p>CRITICAL Severity: - LM Hashes Detected: Presence of LM hashes indicates legacy authentication enabled - Recommendation: Disable LM hash storage via Group Policy immediately</p> <p>HIGH Severity: - NetNTLMv1 Detected: Vulnerable to relay attacks - Recommendation: Upgrade to NetNTLMv2 minimum authentication level</p> <p>MEDIUM Severity: - Kerberos RC4 (etype 23): Weak encryption - Recommendation: Enable AES256/AES128 Kerberos encryption</p> <p>Use Case: Provide these recommendations in client reports as evidence-based security improvements.</p>"},{"location":"user-guide/analytics-reports/#hash-reuse-analysis-v121","title":"Hash Reuse Analysis (v1.2.1+)","text":"<p>Detects when the same hash value appears across multiple user accounts, indicating shared passwords.</p> <p>Difference from Password Reuse: - Password Reuse: Same plaintext password used by one user for multiple accounts - Hash Reuse: Same hash value (same password) shared by multiple different users - Hash reuse detection works on cracked AND uncracked hashes</p>"},{"location":"user-guide/analytics-reports/#key-metrics","title":"Key Metrics","text":"<p>Overview Statistics: - Total Reused: Number of hash values appearing 2+ times - Percentage Reused: Percentage of total unique hashes that are reused - Total Unique: Count of distinct hash values in the dataset</p> <p>Hash Reuse Details Table: Sorted by occurrence count (most reused first), showing:</p> <ul> <li>Hash Value: The actual hash (truncated for display)</li> <li>Hash Type: Algorithm (e.g., NTLM, LM)</li> <li>Password: Cracked plaintext (if available)</li> <li>User Count: Number of distinct users with this hash</li> <li>Total Occurrences: Number of times hash appears across all hashlists</li> <li>Users: List of affected usernames with domain information</li> </ul> <p>Example: <pre><code>Top Reused Hash:\nHash: 5f4dcc3b...\nType: NTLM\nPassword: password (cracked)\nUsers: 47 distinct users\nOccurrences: 52 (some users appear in multiple hashlists)\n\nAffected Users:\n- DOMAIN\\user1\n- DOMAIN\\user2\n- CORP\\testaccount\n- ...\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#domain-filtering","title":"Domain Filtering","text":"<p>When domain filtering is active, hash reuse analysis filters to show only hashes belonging to the selected domain. This enables:</p> <ul> <li>Department-Specific Analysis: Identify password sharing within organizational units</li> <li>Multi-Domain Comparison: Compare reuse rates between different domains</li> <li>Targeted Remediation: Focus password reset efforts on specific domains</li> </ul>"},{"location":"user-guide/analytics-reports/#security-implications","title":"Security Implications","text":"<p>High-Risk Scenarios: - Default Passwords: Same hash across many accounts indicates default/template passwords - Service Accounts: Shared credentials for system accounts - IT Admin Practices: Multiple admins using same password (bad practice) - Compromise Blast Radius: One cracked password = multiple compromised accounts</p> <p>Use in Client Reports: Highlight top reused hashes as critical findings. If hash is cracked, all affected accounts are compromised. If uncracked, cracking one reveals all.</p>"},{"location":"user-guide/analytics-reports/#lm-partial-cracks-v121","title":"LM Partial Cracks (v1.2.1+)","text":"<p>Analyzes LM hashes where only one of the two 7-character halves has been cracked, providing actionable intelligence for completing the crack.</p> <p>Background: LM hashes consist of two independent 16-character halves (each representing up to 7 characters of the password). Each half can be cracked independently, resulting in \"partial cracks\" where one half is known but the other is not.</p>"},{"location":"user-guide/analytics-reports/#key-metrics_1","title":"Key Metrics","text":"<p>Overview Statistics: - Total Partial: Count of LM hashes with exactly one half cracked - First Half Only: Hashes with first 7 characters known - Second Half Only: Hashes with last 7 characters known - Percentage Partial: Percentage of total LM hashes that are partially cracked</p> <p>Partial Crack Details Table:</p> <p>Shows each partially cracked hash with:</p> <ul> <li>Username: Account name (if available)</li> <li>Domain: Domain/workgroup (if available)</li> <li>First Half Status: \u2713 cracked or \u2717 pending</li> <li>First Half Password: Up to 7 characters (if cracked)</li> <li>Second Half Status: \u2713 cracked or \u2717 pending</li> <li>Second Half Password: Up to 7 characters (if cracked)</li> <li>Hashlist: Which hashlist contains this hash</li> </ul> <p>Example: <pre><code>User: Administrator\nDomain: CORP\nFirst Half: \u2713 PASSWORD (cracked)\nSecond Half: \u2717 (pending)\nHashlist: Domain-Controller-LM\n\nStrategic Value: Keyspace reduced from 95^14 to 95^7 combinations\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#strategic-value","title":"Strategic Value","text":"<p>Why Partial Cracks Matter:</p> <ol> <li>Keyspace Reduction:</li> <li>Full 14-char LM: ~95^14 combinations (~4.7 \u00d7 10^27)</li> <li>One half known: ~95^7 combinations (~6.9 \u00d7 10^13)</li> <li> <p>Reduction factor: ~68 trillion times faster</p> </li> <li> <p>Pattern Recognition:</p> </li> <li>First half often reveals password pattern (e.g., <code>WELCOME</code>)</li> <li>Likely second half: digits/special chars (e.g., <code>123!</code>)</li> <li> <p>Inform targeted wordlists for the unknown half</p> </li> <li> <p>LM-to-NTLM Intelligence:</p> </li> <li>Partial LM knowledge helps generate NTLM masks</li> <li>See LM-to-NTLM Masks section</li> </ol>"},{"location":"user-guide/analytics-reports/#recommended-actions","title":"Recommended Actions","text":"<p>For First Half Known: - Generate wordlists for second half (common suffixes: <code>123</code>, <code>2024</code>, <code>!</code>) - Use mask attacks: <code>?d?d?d?d</code> (4 digits), <code>?d?d?d?s</code> (3 digits + special char) - Check for keyboard patterns continuing from first half</p> <p>For Second Half Known: - Generate wordlists for first half (common prefixes: <code>WELCOME</code>, <code>PASSWORD</code>) - Use mask attacks based on organizational patterns - Check for common words + known suffix pattern</p> <p>Domain Filtering: Partial crack analysis respects domain filtering, allowing targeted analysis of specific organizational units.</p>"},{"location":"user-guide/analytics-reports/#lm-to-ntlm-masks-v121","title":"LM-to-NTLM Masks (v1.2.1+)","text":"<p>Leverages cracked LM passwords to generate hashcat masks for attacking stronger NTLM hashes, exploiting the relationship between LM (uppercase, max 14 chars) and NTLM (case-sensitive, case-preserving) password representations.</p> <p>Concept: When an LM password is cracked, it reveals the uppercase version of the original password (e.g., LM: <code>PASSWORD123</code>). The original NTLM password likely uses mixed case (e.g., <code>Password123</code>, <code>PassWord123</code>, <code>PASSWORD123</code>). By analyzing patterns in cracked LM passwords, the system generates targeted masks to crack NTLM efficiently.</p>"},{"location":"user-guide/analytics-reports/#key-metrics_2","title":"Key Metrics","text":"<p>Overview Statistics: - Total LM Cracked: Count of fully cracked LM passwords - Total Masks Generated: Number of unique mask patterns created - Total Estimated Keyspace: Sum of all mask keyspaces (attempts needed)</p> <p>Mask Generation Table:</p> <p>For each generated mask, shows:</p> <ul> <li>Mask: Hashcat mask format (e.g., <code>?u?l?l?l?l?l?l?l?d?d?d</code>)</li> <li>LM Pattern: Pattern derived from LM analysis (e.g., <code>AAAAAADDD</code>)</li> <li>Count: Number of LM passwords matching this pattern</li> <li>Percentage: Percentage of total LM passwords following this pattern</li> <li>Match Percentage: Estimated NTLM match rate for this mask</li> <li>Estimated Keyspace: Number of combinations to try</li> <li>Example LM: Sample LM password showing the pattern</li> </ul> <p>Example: <pre><code>Mask: ?u?l?l?l?l?l?l?l?d?d?d\nLM Pattern: PASSWORD123\nCount: 342 LM passwords\nPercentage: 23.4% of LM hashes\nEstimated Keyspace: 1,757,600,000 combinations\nExample: PASSWORD123 \u2192 likely Password123, PassWord123, etc.\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#mask-format","title":"Mask Format","text":"<p>Hashcat Mask Characters: - <code>?l</code> = lowercase letter (a-z) - <code>?u</code> = uppercase letter (A-Z) - <code>?d</code> = digit (0-9) - <code>?s</code> = special character (!@#$%^&amp;*) - <code>?a</code> = any character (all of the above)</p> <p>Common Patterns: <pre><code>?u?l?l?l?l?l?l?l?d?d        = \"Welcome24\" pattern\n?u?l?l?l?l?l?l?l?d?d?d?d    = \"Password2024\" pattern\n?u?l?l?l?l?l?l?l?s          = \"Password!\" pattern\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#how-masks-are-generated","title":"How Masks Are Generated","text":"<ol> <li>Analyze LM Password: Extract pattern from cracked LM (e.g., <code>WELCOME2024</code>)</li> <li>Identify Components:</li> <li>First character: uppercase (LM is always uppercase)</li> <li>Next 6 characters: likely lowercase in NTLM</li> <li>Digits: remain digits</li> <li>Special chars: remain special chars (if present)</li> <li>Generate Mask: Create hashcat mask representation</li> <li>Calculate Keyspace: Estimate attempts needed (26^lowercase \u00d7 10^digits)</li> </ol> <p>Pattern Intelligence: - First char usually uppercase (corporate password policies) - Remaining letters typically lowercase - Digits and special chars maintain position</p>"},{"location":"user-guide/analytics-reports/#using-generated-masks","title":"Using Generated Masks","text":"<p>Export Masks for Hashcat:</p> <pre><code># Use top mask from analytics\nhashcat -m 1000 -a 3 ntlm_hashes.txt '?u?l?l?l?l?l?l?l?d?d?d'\n\n# Try top 5 masks in sequence\nhashcat -m 1000 -a 3 ntlm_hashes.txt '?u?l?l?l?l?l?l?l?d?d?d'\nhashcat -m 1000 -a 3 ntlm_hashes.txt '?u?l?l?l?l?l?l?l?d?d?d?d'\nhashcat -m 1000 -a 3 ntlm_hashes.txt '?u?l?l?l?l?l?l?l?s'\n</code></pre> <p>Strategic Workflow:</p> <ol> <li>Crack LM hashes first (much faster)</li> <li>Generate analytics report to produce LM-to-NTLM masks</li> <li>Export top masks from analytics section</li> <li>Run mask attacks on NTLM hashes using generated patterns</li> <li>Refine based on results - adjust masks if needed</li> </ol>"},{"location":"user-guide/analytics-reports/#domain-filtering_1","title":"Domain Filtering","text":"<p>When domain filtering is active, masks are generated only from LM passwords in the selected domain, enabling:</p> <ul> <li>Domain-Specific Patterns: Different organizational units may have different password patterns</li> <li>Targeted Attacks: Use domain-specific masks for maximum efficiency</li> <li>Comparative Analysis: See which domains have weaker password patterns</li> </ul>"},{"location":"user-guide/analytics-reports/#performance-considerations","title":"Performance Considerations","text":"<p>Keyspace Analysis: The system estimates keyspace for each mask to help prioritize:</p> <ul> <li>Small Keyspace (&lt;1 billion): Fast to crack, try first</li> <li>Medium Keyspace (1B - 100B): Moderate time, worth attempting</li> <li>Large Keyspace (&gt;100B): Consider refining mask or using wordlists</li> </ul> <p>Match Percentage: Indicates estimated success rate for each mask based on pattern frequency. Higher match percentage = more likely to crack NTLM hashes.</p>"},{"location":"user-guide/analytics-reports/#recommendations","title":"Recommendations","text":"<p>Automated security recommendations based on analysis:</p> <ul> <li>Policy Improvements: Suggested password policy changes</li> <li>Training Topics: Areas where user education is needed</li> <li>Technical Controls: MFA, password managers, monitoring</li> <li>Specific Findings: Critical issues requiring immediate attention</li> </ul> <p>Example Recommendations: - \"Implement minimum 12-character requirement (current avg: 8.3)\" - \"Prohibit use of company name in passwords (found in 34% of passwords)\" - \"Deploy password manager to reduce reuse (45% reuse rate detected)\"</p>"},{"location":"user-guide/analytics-reports/#using-analytics-in-client-reports","title":"Using Analytics in Client Reports","text":""},{"location":"user-guide/analytics-reports/#best-practices-for-client-reporting","title":"Best Practices for Client Reporting","text":""},{"location":"user-guide/analytics-reports/#1-executive-summary","title":"1. Executive Summary","text":"<ul> <li>Focus on Overall Statistics and Strength Metrics</li> <li>Use percentages rather than raw numbers for impact</li> <li>Highlight top 3-5 critical findings</li> <li>Provide clear, actionable recommendations</li> </ul>"},{"location":"user-guide/analytics-reports/#2-technical-details","title":"2. Technical Details","text":"<ul> <li>Include all relevant analytics sections</li> <li>Use visualizations from Length Distribution and Complexity Analysis</li> <li>Reference specific patterns and examples (anonymized if needed)</li> <li>Document methodology and tools used</li> </ul>"},{"location":"user-guide/analytics-reports/#3-comparative-analysis","title":"3. Comparative Analysis","text":"<ul> <li>Use domain filtering to show differences between business units</li> <li>Compare against industry benchmarks</li> <li>Track improvements if this is a follow-up assessment</li> <li>Show before/after if policies were changed</li> </ul>"},{"location":"user-guide/analytics-reports/#exporting-analytics-data","title":"Exporting Analytics Data","text":"<p>Export Options: - PDF Export: Generate printable client reports (Coming Soon) - CSV Export: Export raw analytics data for further analysis (Coming Soon) - Screenshots: Capture charts and tables for presentations</p> <p>Privacy Considerations: - Never include actual passwords in client reports - Anonymize usernames if required by engagement scope - Use password examples only with explicit permission - Aggregate sensitive findings to prevent individual identification</p>"},{"location":"user-guide/analytics-reports/#presentation-tips","title":"Presentation Tips","text":"<p>For Executive Audiences: - Lead with crack rate percentage and risk level - Use domain filtering to show business-unit-specific issues - Focus on business impact and compliance - Provide clear ROI for recommendations</p> <p>For Technical Audiences: - Include detailed pattern analysis and mask distributions - Show specific examples of vulnerable configurations - Reference technical controls and implementation steps - Provide timeline and resource estimates for remediation</p>"},{"location":"user-guide/analytics-reports/#technical-details","title":"Technical Details","text":""},{"location":"user-guide/analytics-reports/#pre-calculated-analytics","title":"Pre-Calculated Analytics","text":"<p>Analytics reports use pre-calculation to ensure fast performance:</p> <ol> <li>Report Generation: All analytics calculated when report is created</li> <li>Domain Analytics: Separate analytics calculated for each domain</li> <li>Storage: Complete analytics stored in database as JSONB</li> <li>Retrieval: Instant loading when viewing saved reports</li> </ol> <p>Performance Characteristics: - Generation: 5-15 seconds for 100,000+ hashes - Viewing: &lt;1 second for any report size - Domain Filtering**: Client-side switching (instant)</p>"},{"location":"user-guide/analytics-reports/#domain-extraction-process","title":"Domain Extraction Process","text":"<p>Domains are automatically extracted during hashlist upload:</p> <p>Supported Hash Formats: - NetNTLMv2 (5600): <code>username::domain:challenge:response</code> - NTLM pwdump (1000): <code>DOMAIN\\username:sid:lm:nt:::</code> - Kerberos (18200): <code>$krb5asrep$23$user@domain.com:hash</code></p> <p>Extraction Details: - Domains stored in <code>hashes.domain</code> column - Indexed for fast filtering - See Username Extraction Architecture for full details</p>"},{"location":"user-guide/analytics-reports/#database-schema","title":"Database Schema","text":"<p>Analytics reports are stored in the <code>analytics_reports</code> table:</p> <pre><code>CREATE TABLE analytics_reports (\n    id UUID PRIMARY KEY,\n    client_id UUID REFERENCES clients(id),\n    name VARCHAR(255),\n    hashlist_ids INTEGER[],\n    analytics_data JSONB,\n    total_hashlists INTEGER,\n    total_hashes INTEGER,\n    total_cracked INTEGER,\n    crack_percentage NUMERIC,\n    created_at TIMESTAMP,\n    created_by UUID REFERENCES users(id)\n);\n</code></pre> <p>Analytics Data Structure: <pre><code>{\n  \"overview\": {...},\n  \"length_distribution\": {...},\n  \"complexity_analysis\": {...},\n  \"domain_analytics\": [\n    {\n      \"domain\": \"acme.local\",\n      \"overview\": {...},\n      \"length_distribution\": {...},\n      ...\n    }\n  ]\n}\n</code></pre></p>"},{"location":"user-guide/analytics-reports/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/analytics-reports/#report-generation-issues","title":"Report Generation Issues","text":"<p>Problem: Report generation fails or times out</p> <p>Solutions: - Ensure all selected hashlists are in \"ready\" status - Try generating with fewer hashlists - Check backend logs for errors - Verify database connectivity</p> <p>Problem: Domain tabs don't appear</p> <p>Causes: - No hashes in the hashlists contain domain information - Only hash types without domain support (e.g., MD5, SHA1, bcrypt) - Domains weren't extracted during upload</p> <p>Solutions: - Verify hash format supports domains (NetNTLMv2, NTLM, Kerberos) - Check if <code>domain</code> column is populated for hashes - Re-upload hashlist to trigger domain extraction</p>"},{"location":"user-guide/analytics-reports/#data-accuracy-issues","title":"Data Accuracy Issues","text":"<p>Problem: Analytics don't match expected values</p> <p>Checks: - Verify correct hashlists are selected - Check if potfile was imported from external source - Ensure time range filter is appropriate - Verify no hashes were manually modified</p>"},{"location":"user-guide/analytics-reports/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/analytics-reports/#generating-meaningful-reports","title":"Generating Meaningful Reports","text":"<ol> <li>Select Appropriate Scope: Include only relevant hashlists for the analysis</li> <li>Use Domain Filtering: Leverage domain separation for multi-domain environments</li> <li>Document Context: Note any custom patterns defined before generation</li> <li>Regular Cadence: Generate reports periodically to track improvements</li> <li>Combine with POT Data: Cross-reference with raw POT exports for detailed analysis</li> </ol>"},{"location":"user-guide/analytics-reports/#security-and-privacy","title":"Security and Privacy","text":"<ol> <li>Access Control: Restrict report access to authorized personnel</li> <li>Data Retention: Delete old reports per data retention policies</li> <li>Anonymization: Remove identifying information from client-facing reports</li> <li>Secure Export: Encrypt exported analytics data</li> <li>Audit Trail: Track who generates and views reports</li> </ol>"},{"location":"user-guide/analytics-reports/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Hashlist Selection: Don't include unnecessary hashlists</li> <li>Archival: Archive old reports that are no longer needed</li> <li>Storage Monitoring: Monitor database growth from analytics data</li> <li>Cleanup: Implement retention policies for old analytics reports</li> </ol>"},{"location":"user-guide/analytics-reports/#summary","title":"Summary","text":"<p>Analytics Reports provide comprehensive password analysis across 13 metrics with domain-based filtering for multi-domain environments. By pre-calculating analytics during report generation, the system delivers instant results while enabling detailed security assessments for client reporting and organizational security improvement.</p> <p>For additional analysis capabilities, see: - Analyzing Results - POT file analysis and export - Username Extraction - Domain extraction details - Client Management - Client configuration</p>"},{"location":"user-guide/analyzing-results/","title":"Analyzing Password Cracking Results","text":"<p>This guide covers how to view, analyze, and export password cracking results in KrakenHashes. The system provides comprehensive tools for examining cracked passwords at different levels and exporting data for reporting purposes.</p>"},{"location":"user-guide/analyzing-results/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding POT Files and Results</li> <li>Viewing Results at Different Levels</li> <li>Filtering and Searching Results</li> <li>Export Options and Formats</li> <li>Analyzing Patterns and Statistics</li> <li>Using Results for Reporting</li> <li>Best Practices for Handling Sensitive Data</li> </ul>"},{"location":"user-guide/analyzing-results/#understanding-pot-files-and-results","title":"Understanding POT Files and Results","text":""},{"location":"user-guide/analyzing-results/#what-is-a-pot-file","title":"What is a POT File?","text":"<p>A POT file (short for \"potfile\") is a standard format used by Hashcat and other password cracking tools to store successfully cracked hashes. KrakenHashes maintains a centralized POT database that tracks:</p> <ul> <li>Original Hash: The hash value as it appeared in the source data</li> <li>Password: The plaintext password that was recovered</li> <li>Username: Associated username (if available from the hash format)</li> <li>Hash Type: The algorithm used (MD5, SHA1, NTLM, etc.)</li> <li>Metadata: When the hash was cracked, which job cracked it, etc.</li> </ul>"},{"location":"user-guide/analyzing-results/#how-results-are-stored","title":"How Results are Stored","text":"<p>KrakenHashes stores cracked passwords in a PostgreSQL database rather than traditional POT files. This provides:</p> <ul> <li>Deduplication: Each unique hash is stored only once</li> <li>Performance: Fast queries across millions of cracked hashes</li> <li>Security: Database-level access controls and encryption</li> <li>Rich Metadata: Track which client, hashlist, and job produced each crack</li> </ul>"},{"location":"user-guide/analyzing-results/#viewing-results-at-different-levels","title":"Viewing Results at Different Levels","text":"<p>KrakenHashes provides three levels of result viewing, each offering different perspectives on your cracked passwords:</p>"},{"location":"user-guide/analyzing-results/#1-master-pot-view","title":"1. Master POT View","text":"<p>The Master POT view shows all cracked hashes across your entire KrakenHashes instance.</p> <p>Access: Navigate to POT in the main menu</p> <p> Master POT view showing all cracked hashes with filter options for different export formats</p> <p>Use Cases: - Overall password analysis across all engagements - Building universal password dictionaries - Identifying common passwords across different clients</p>"},{"location":"user-guide/analyzing-results/#2-client-level-results","title":"2. Client-Level Results","text":"<p>View all cracked passwords for a specific client/engagement.</p> <p>Access:  1. Go to Clients 2. Select a client 3. Click View POT or the POT icon</p> <p> Filter results to show only cracked passwords for a specific client or engagement</p> <p>Use Cases: - Client-specific reporting - Understanding password patterns within an organization - Compliance reporting for specific engagements</p>"},{"location":"user-guide/analyzing-results/#3-hashlist-level-results","title":"3. Hashlist-Level Results","text":"<p>View cracked passwords for a specific hashlist.</p> <p>Access: 1. Go to Hashlists 2. Select a hashlist 3. Click View POT in the actions menu</p> <p> View cracked passwords from a particular hashlist with detailed crack information</p> <p>Use Cases: - Analyzing results from specific sources (AD dump, database, etc.) - Comparing crack rates between different hash types - Focused analysis on specific user groups</p>"},{"location":"user-guide/analyzing-results/#4-job-level-results-coming-soon","title":"4. Job-Level Results (Coming Soon)","text":"<p>Future releases will add job-specific result viewing to track which attack strategies were most effective.</p>"},{"location":"user-guide/analyzing-results/#filtering-and-searching-results","title":"Filtering and Searching Results","text":""},{"location":"user-guide/analyzing-results/#search-functionality","title":"Search Functionality","text":"<p>Each POT view includes a search bar that filters results in real-time:</p> <ul> <li>Hash Search: Find specific hash values</li> <li>Password Search: Look for passwords containing specific strings</li> <li>Username Search: Filter by username patterns</li> </ul> <p>Example Searches: <pre><code>admin          # Find all results with \"admin\" in hash, password, or username\npassword123    # Find all instances of this specific password\n@company.com   # Find all email-based usernames from a domain\n</code></pre></p>"},{"location":"user-guide/analyzing-results/#pagination-options","title":"Pagination Options","text":"<p>Control how many results are displayed:</p> <ul> <li>500 (Default): Good balance of performance and visibility</li> <li>1000, 1500, 2000: Larger result sets for analysis</li> <li>All: Load entire dataset (use with caution on large sets)</li> </ul> <p>Warning: Loading all results may impact browser performance with datasets over 50,000 entries.</p>"},{"location":"user-guide/analyzing-results/#export-options-and-formats","title":"Export Options and Formats","text":"<p>KrakenHashes supports multiple export formats optimized for different use cases:</p>"},{"location":"user-guide/analyzing-results/#available-export-formats","title":"Available Export Formats","text":""},{"location":"user-guide/analyzing-results/#1-hashpass-format-filename-h-plst","title":"1. Hash:Pass Format (<code>filename-h-p.lst</code>)","text":"<p><pre><code>5f4dcc3b5aa765d61d8327deb882cf99:password\ne10adc3949ba59abbe56e057f20f883e:123456\n</code></pre> Use Cases:  - Re-importing into other tools - Hash verification - Building POT files</p>"},{"location":"user-guide/analyzing-results/#2-userpass-format-filename-u-plst","title":"2. User:Pass Format (<code>filename-u-p.lst</code>)","text":"<p><pre><code>admin:password123\njohn.doe:Summer2024!\nmary.smith:Welcome123\n</code></pre> Use Cases: - Password spraying lists - User-specific analysis - Account takeover testing</p>"},{"location":"user-guide/analyzing-results/#3-username-only-filename-ulst","title":"3. Username Only (<code>filename-u.lst</code>)","text":"<p><pre><code>admin\njohn.doe\nmary.smith\n</code></pre> Use Cases: - Username enumeration - Account existence validation - User list generation</p>"},{"location":"user-guide/analyzing-results/#4-password-only-filename-plst","title":"4. Password Only (<code>filename-p.lst</code>)","text":"<p><pre><code>password123\nSummer2024!\nWelcome123\n</code></pre> Use Cases: - Wordlist generation - Password frequency analysis - Rule generation input</p>"},{"location":"user-guide/analyzing-results/#export-scope","title":"Export Scope","text":"<p>Exports can be performed at different levels: - Current View: Export only visible/filtered results - Full Dataset: Export all results for the current context (client/hashlist/master)</p>"},{"location":"user-guide/analyzing-results/#practical-export-examples","title":"Practical Export Examples","text":""},{"location":"user-guide/analyzing-results/#building-a-client-specific-wordlist","title":"Building a Client-Specific Wordlist","text":"<ol> <li>Navigate to client POT view</li> <li>Click Download \u2192 Password</li> <li>Use the exported list as input for future attacks</li> </ol>"},{"location":"user-guide/analyzing-results/#creating-credential-lists-for-testing","title":"Creating Credential Lists for Testing","text":"<ol> <li>Go to hashlist POT view</li> <li>Ensure hashlist contains usernames</li> <li>Click Download \u2192 User:Pass</li> <li>Import into your testing tools</li> </ol>"},{"location":"user-guide/analyzing-results/#analytics-reports","title":"Analytics Reports","text":"<p>For comprehensive password analysis across multiple metrics, KrakenHashes provides an Analytics Reports feature that generates detailed statistical breakdowns. Analytics reports support:</p> <ul> <li>Domain-based filtering: Analyze password patterns by domain in multi-domain environments</li> <li>13 analytics sections: From length distribution to strength metrics</li> <li>Custom pattern detection: Define and track organization-specific password patterns</li> <li>Visual dashboards: Interactive charts and graphs for data visualization</li> </ul> <p>For complete details on generating and using analytics reports, see the Analytics Reports Guide.</p>"},{"location":"user-guide/analyzing-results/#analyzing-patterns-and-statistics","title":"Analyzing Patterns and Statistics","text":""},{"location":"user-guide/analyzing-results/#current-statistics","title":"Current Statistics","text":"<p>Each view displays: - Total Cracked: Number of successfully cracked hashes - Crack Rate: Percentage of hashes cracked (shown in hashlist view) - Result Distribution: Visual breakdown by hash type</p>"},{"location":"user-guide/analyzing-results/#pattern-analysis-techniques","title":"Pattern Analysis Techniques","text":""},{"location":"user-guide/analyzing-results/#1-password-complexity-analysis","title":"1. Password Complexity Analysis","text":"<p>Export passwords and analyze with external tools: <pre><code># Length distribution\ncat client-p.lst | awk '{print length}' | sort -n | uniq -c\n\n# Character set analysis\ncat client-p.lst | grep -E '^[a-z]+$' | wc -l  # lowercase only\ncat client-p.lst | grep -E '^[A-Za-z0-9]+$' | wc -l  # alphanumeric\n</code></pre></p>"},{"location":"user-guide/analyzing-results/#2-common-password-identification","title":"2. Common Password Identification","text":"<pre><code># Top 20 most common passwords\ncat client-p.lst | sort | uniq -c | sort -rn | head -20\n</code></pre>"},{"location":"user-guide/analyzing-results/#3-username-password-relationship","title":"3. Username-Password Relationship","text":"<pre><code># Find passwords containing username\nwhile IFS=: read user pass; do\n  echo \"$pass\" | grep -i \"$user\" &amp;&amp; echo \"$user:$pass\"\ndone &lt; client-u-p.lst\n</code></pre>"},{"location":"user-guide/analyzing-results/#advanced-analysis-with-sql","title":"Advanced Analysis with SQL","text":"<p>For advanced users with database access:</p> <pre><code>-- Password length distribution\nSELECT LENGTH(password) as pwd_length, COUNT(*) as count\nFROM hashes\nWHERE is_cracked = true\nGROUP BY pwd_length\nORDER BY pwd_length;\n\n-- Most common passwords by client\nSELECT h.password, COUNT(*) as usage_count\nFROM hashes h\nJOIN hashlist_hashes hh ON h.id = hh.hash_id\nJOIN hashlists hl ON hh.hashlist_id = hl.id\nWHERE hl.client_id = 'CLIENT_UUID_HERE'\n  AND h.is_cracked = true\nGROUP BY h.password\nORDER BY usage_count DESC\nLIMIT 50;\n</code></pre>"},{"location":"user-guide/analyzing-results/#using-results-for-reporting","title":"Using Results for Reporting","text":""},{"location":"user-guide/analyzing-results/#executive-summary-data","title":"Executive Summary Data","text":"<p>Extract key metrics for reports:</p> <ol> <li>Overall Statistics</li> <li>Total unique passwords cracked</li> <li>Crack rate percentage</li> <li> <p>Time to first/last crack</p> </li> <li> <p>Risk Indicators</p> </li> <li>Passwords meeting complexity requirements</li> <li>Default/common passwords found</li> <li> <p>Password reuse across accounts</p> </li> <li> <p>Compliance Metrics</p> </li> <li>Accounts with weak passwords</li> <li>Policy compliance rates</li> <li>High-privilege accounts with poor passwords</li> </ol>"},{"location":"user-guide/analyzing-results/#generating-report-data","title":"Generating Report Data","text":""},{"location":"user-guide/analyzing-results/#powershell-script-example","title":"PowerShell Script Example","text":"<pre><code># Import cracked credentials\n$creds = Import-Csv \"client-u-p.lst\" -Delimiter \":\" -Header \"Username\",\"Password\"\n\n# Analyze password policies\n$results = $creds | ForEach-Object {\n    [PSCustomObject]@{\n        Username = $_.Username\n        Length = $_.Password.Length\n        HasUppercase = $_.Password -cmatch '[A-Z]'\n        HasLowercase = $_.Password -cmatch '[a-z]'\n        HasNumbers = $_.Password -match '\\d'\n        HasSpecial = $_.Password -match '[^a-zA-Z0-9]'\n        MeetsPolicy = (\n            $_.Password.Length -ge 8 -and\n            $_.Password -cmatch '[A-Z]' -and\n            $_.Password -cmatch '[a-z]' -and\n            $_.Password -match '\\d'\n        )\n    }\n}\n\n# Generate statistics\n$stats = @{\n    TotalAccounts = $results.Count\n    MeetsPolicy = ($results | Where-Object MeetsPolicy).Count\n    AverageLength = ($results | Measure-Object -Property Length -Average).Average\n}\n</code></pre>"},{"location":"user-guide/analyzing-results/#visualization-recommendations","title":"Visualization Recommendations","text":"<p>Create impactful visualizations using exported data:</p> <ol> <li>Password Length Distribution - Bar chart</li> <li>Crack Timeline - Line graph showing cracks over time</li> <li>Complexity Heatmap - Visual grid of password characteristics</li> <li>Top 10 Passwords - Horizontal bar chart</li> </ol>"},{"location":"user-guide/analyzing-results/#best-practices-for-handling-sensitive-data","title":"Best Practices for Handling Sensitive Data","text":""},{"location":"user-guide/analyzing-results/#security-considerations","title":"Security Considerations","text":"<ol> <li>Access Control</li> <li>Limit POT access to authorized personnel only</li> <li>Use role-based permissions for different result levels</li> <li> <p>Audit all exports and downloads</p> </li> <li> <p>Data Handling</p> </li> <li>Never store exported credentials in unencrypted formats</li> <li>Use secure deletion for temporary export files</li> <li> <p>Implement retention policies for cracked passwords</p> </li> <li> <p>Client Confidentiality</p> </li> <li>Keep client results strictly separated</li> <li>Never mix credentials between engagements</li> <li>Sanitize data before sharing externally</li> </ol>"},{"location":"user-guide/analyzing-results/#operational-security","title":"Operational Security","text":""},{"location":"user-guide/analyzing-results/#secure-export-workflow","title":"Secure Export Workflow","text":"<pre><code># 1. Export to encrypted container\nkrakenhashes-export | gpg -c &gt; results.gpg\n\n# 2. Verify encryption\ngpg --list-packets results.gpg\n\n# 3. Secure transfer\nscp results.gpg user@secure-host:/encrypted/\n\n# 4. Secure deletion\nshred -vfz original-export.lst\n</code></pre>"},{"location":"user-guide/analyzing-results/#audit-trail-maintenance","title":"Audit Trail Maintenance","text":"<ul> <li>Log all POT access with timestamps</li> <li>Track export operations with user attribution</li> <li>Monitor for unusual access patterns</li> </ul>"},{"location":"user-guide/analyzing-results/#compliance-considerations","title":"Compliance Considerations","text":"<ol> <li>Data Retention</li> <li>Define retention periods per client agreement</li> <li>Implement automatic purging of old results</li> <li> <p>Document retention policies</p> </li> <li> <p>Reporting Requirements</p> </li> <li>Redact sensitive information in reports</li> <li>Use password hashes in documentation, not plaintexts</li> <li> <p>Implement need-to-know access controls</p> </li> <li> <p>Cross-Border Considerations</p> </li> <li>Be aware of data residency requirements</li> <li>Implement geographic access controls if needed</li> <li>Consider encryption-at-rest requirements</li> </ol>"},{"location":"user-guide/analyzing-results/#incident-response","title":"Incident Response","text":"<p>If credentials are accidentally exposed:</p> <ol> <li>Immediate Actions</li> <li>Revoke access to affected systems</li> <li>Force password resets for exposed accounts</li> <li> <p>Document the incident</p> </li> <li> <p>Investigation</p> </li> <li>Review access logs</li> <li>Identify scope of exposure</li> <li> <p>Determine root cause</p> </li> <li> <p>Remediation</p> </li> <li>Update access controls</li> <li>Implement additional monitoring</li> <li>Review and update procedures</li> </ol>"},{"location":"user-guide/analyzing-results/#summary","title":"Summary","text":"<p>Effective analysis of password cracking results requires understanding the available tools, choosing appropriate export formats, and maintaining strict security controls. KrakenHashes provides flexible viewing and export options while maintaining the security and segregation necessary for professional password auditing services.</p> <p>Remember that cracked passwords represent sensitive security information that must be handled with appropriate care and professionalism. Always follow your organization's data handling policies and client agreements when working with password cracking results.</p>"},{"location":"user-guide/core-concepts/","title":"KrakenHashes Core Concepts Guide","text":""},{"location":"user-guide/core-concepts/#overview","title":"Overview","text":"<p>KrakenHashes is a distributed password cracking management system that orchestrates multiple agents running hashcat to efficiently crack password hashes. This guide explains the fundamental concepts and terminology you need to understand to effectively use the system.</p>"},{"location":"user-guide/core-concepts/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Key Terminology</li> <li>System Architecture</li> <li>Job Execution Flow</li> <li>Priority System</li> <li>Chunking and Distribution</li> <li>File Management</li> <li>Result Handling</li> </ul>"},{"location":"user-guide/core-concepts/#key-terminology","title":"Key Terminology","text":""},{"location":"user-guide/core-concepts/#hashlist","title":"Hashlist","text":"<p>A hashlist is a collection of password hashes uploaded to the system for cracking. Each hashlist: - Contains one or more individual hashes of the same type (MD5, NTLM, SHA1, etc.) - Has a lifecycle: <code>uploading</code> \u2192 <code>processing</code> \u2192 <code>ready</code> (or <code>error</code>) - Can be associated with a client for engagement tracking - Tracks total hashes and cracked count - May include usernames in formats like <code>username:hash</code></p>"},{"location":"user-guide/core-concepts/#hash","title":"Hash","text":"<p>A hash is an individual password hash within a hashlist. Each hash: - Contains the encrypted password value to be cracked - May include an associated username (preserved from original line format) - Tracks whether it has been cracked and its plaintext value - Can appear in multiple hashlists (deduplication handled by full line, not just hash value) - Cross-hashlist updates: When a hash value is cracked, ALL hashes with that same value are updated across all hashlists</p>"},{"location":"user-guide/core-concepts/#preset-job","title":"Preset Job","text":"<p>A preset job is a pre-configured attack strategy that defines: - Which wordlists to use - Which rule files to apply - The hashcat attack mode (dictionary, brute-force, hybrid, association, etc.) - Priority level (0-1000, higher = more important) - Chunk duration (how long each task should run) - Maximum number of agents allowed to work on it - The specific hashcat binary version to use</p>"},{"location":"user-guide/core-concepts/#job-workflow","title":"Job Workflow","text":"<p>A job workflow is a named sequence of preset jobs executed in order. Workflows enable: - Systematic attack progression (e.g., common passwords \u2192 rules \u2192 brute force) - Reusable attack strategies across different hashlists - Automated execution of multiple attack phases</p>"},{"location":"user-guide/core-concepts/#job-execution","title":"Job Execution","text":"<p>A job execution is an actual running instance of a preset job against a specific hashlist. It: - Tracks the overall status: <code>pending</code>, <code>running</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code>, <code>interrupted</code> - Manages keyspace progress (how much of the search space has been processed) - Supports dynamic rule splitting for large rule files - Can be interrupted by higher priority jobs and automatically resumed - Tracks which user created it and when</p>"},{"location":"user-guide/core-concepts/#job-interruption-behavior","title":"Job Interruption Behavior","text":"<p>When a job is interrupted by a higher priority job: - Status changes from <code>running</code> to <code>pending</code> (not <code>paused</code>) - All progress is preserved and saved - Job automatically returns to the queue with the same priority - When agents become available, the job resumes from where it stopped - No manual intervention required - the system handles everything automatically</p>"},{"location":"user-guide/core-concepts/#job-task-chunk","title":"Job Task (Chunk)","text":"<p>A job task or chunk is an individual unit of work assigned to an agent. Tasks are: - Time-based chunks (e.g., 10-minute segments of work) - Defined by keyspace ranges (start/end positions in the search space) - Self-contained with the complete hashcat command - Tracked for progress and can be retried on failure - For rule-based attacks, may represent a subset of rules</p>"},{"location":"user-guide/core-concepts/#agent","title":"Agent","text":"<p>An agent is a compute node that executes hashcat commands. Agents: - Run on systems with GPUs or CPUs capable of password cracking - Connect via WebSocket for real-time communication - Report hardware capabilities (GPU types, benchmark speeds) - Can be scheduled for specific time windows - Are authenticated via API keys and claim codes - Can be owned by specific users or shared within teams</p>"},{"location":"user-guide/core-concepts/#pot-file","title":"Pot File","text":"<p>A pot file (short for \"potfile\") is hashcat's database of cracked hashes. In KrakenHashes: - Each successful crack is immediately synchronized to the backend - Results are shared across all agents and jobs - The system maintains a centralized view of all cracked hashes - Previously cracked hashes are automatically filtered from new jobs</p>"},{"location":"user-guide/core-concepts/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Web Frontend  \u2502     \u2502  Backend API    \u2502     \u2502   PostgreSQL    \u2502\n\u2502   (React/TS)    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   (Go REST)     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Database      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    WebSocket    \u2502\n                    Connection   \u2502\n                                 \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Distributed Agents \u2502\n                    \u2502    (Go + Hashcat)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/core-concepts/#component-responsibilities","title":"Component Responsibilities","text":"<ol> <li>Frontend: User interface for uploading hashlists, creating jobs, monitoring progress</li> <li>Backend: Orchestrates job scheduling, manages data, handles agent communication</li> <li>Database: Stores all persistent data (users, hashlists, jobs, results)</li> <li>Agents: Execute hashcat commands and report results back to the backend</li> </ol>"},{"location":"user-guide/core-concepts/#job-execution-flow","title":"Job Execution Flow","text":""},{"location":"user-guide/core-concepts/#1-job-creation-phase","title":"1. Job Creation Phase","text":"<pre><code>User uploads hashlist \u2192 System processes hashes \u2192 Hashlist marked as \"ready\"\n                                    \u2193\nUser creates job execution \u2192 Selects preset job \u2192 Sets priority\n                                    \u2193\nSystem calculates keyspace \u2192 Creates job execution record\n</code></pre>"},{"location":"user-guide/core-concepts/#2-task-generation-phase","title":"2. Task Generation Phase","text":"<pre><code>Job execution created \u2192 System analyzes attack parameters\n                               \u2193\n        Calculate total keyspace (wordlist \u00d7 rules)\n                               \u2193\n    Generate time-based chunks using agent benchmarks\n                               \u2193\n          Create job tasks in \"pending\" status\n</code></pre>"},{"location":"user-guide/core-concepts/#3-assignment-phase","title":"3. Assignment Phase","text":"<pre><code>Agent requests work \u2192 Scheduler checks available jobs (priority order)\n                                    \u2193\n              Find highest priority job with pending tasks\n                                    \u2193\n          Check agent capabilities and assign compatible task\n                                    \u2193\n               Agent downloads required files if needed\n</code></pre>"},{"location":"user-guide/core-concepts/#4-execution-phase","title":"4. Execution Phase","text":"<pre><code>Agent receives task \u2192 Builds hashcat command \u2192 Starts execution\n                                \u2193\n                    Reports progress every 5 seconds\n                                \u2193\n            Backend updates task and job execution progress\n                                \u2193\n                  Cracked hashes synchronized in real-time\n</code></pre>"},{"location":"user-guide/core-concepts/#5-completion-phase","title":"5. Completion Phase","text":"<pre><code>Task completes \u2192 Agent reports final status \u2192 Backend updates records\n                                \u2193\n                Check if more tasks remain for job\n                                \u2193\n    If no tasks remain \u2192 Mark job execution as completed\n                                \u2193\n            Update hashlist cracked count\n</code></pre>"},{"location":"user-guide/core-concepts/#priority-system","title":"Priority System","text":"<p>KrakenHashes uses a priority scale from 0 to 1000, where: - 1000 = Highest priority (urgent/critical jobs) - 500 = Normal priority (default for most jobs) - 0 = Lowest priority (background/research jobs)</p>"},{"location":"user-guide/core-concepts/#priority-behavior","title":"Priority Behavior","text":"<ol> <li>Job Selection: When agents request work, jobs are assigned in priority order</li> <li>FIFO Within Priority: Jobs with the same priority follow First-In-First-Out</li> <li>Job Interruption: Higher priority jobs can interrupt lower priority running jobs</li> <li>Priority-Based Agent Allocation: How many agents your job receives depends on priority</li> <li>Automatic Resumption: Interrupted jobs automatically resume when resources are available</li> </ol>"},{"location":"user-guide/core-concepts/#priority-based-agent-allocation","title":"Priority-Based Agent Allocation","text":"<p>The priority system uses intelligent allocation to ensure critical jobs get the resources they need:</p> <p>Higher Priority Jobs Override max_agents: - When your job has higher priority than all other jobs, it receives ALL available agents - The max_agents setting is ignored for highest priority jobs - Ensures critical work completes as fast as possible</p> <p>Same Priority Jobs Respect max_agents: - When multiple jobs share the same priority, each respects its max_agents limit - Overflow agents (beyond max_agents) are distributed based on overflow allocation mode:   - FIFO mode (default): Oldest job gets all overflow agents   - Round-robin mode: Overflow agents distributed evenly across all jobs</p> <p>Lower Priority Jobs Wait: - Jobs with lower priority wait for higher priority jobs to complete or release agents - Automatically start when resources become available</p> <p>Examples:</p> <p>Scenario 1: Different Priorities <pre><code>Job A: Priority 100, max_agents = 3\nJob B: Priority 50,  max_agents = 5\n15 agents available\n\nResult:\n- Job A: 15 agents (highest priority, gets ALL agents)\n- Job B: 0 agents (lower priority, waits)\n</code></pre></p> <p>Scenario 2: Same Priority with FIFO Mode <pre><code>Job A: Priority 50, max_agents = 2 (created first)\nJob B: Priority 50, max_agents = 2 (created second)\nJob C: Priority 50, max_agents = 2 (created third)\n15 agents available\n\nResult:\n- Job A: 11 agents (2 max + 9 overflow, oldest job)\n- Job B: 2 agents (max_agents only)\n- Job C: 2 agents (max_agents only)\n</code></pre></p> <p>Scenario 3: Same Priority with Round-Robin Mode <pre><code>Job A: Priority 50, max_agents = 2\nJob B: Priority 50, max_agents = 2\nJob C: Priority 50, max_agents = 2\n15 agents available\n\nResult:\n- Job A: 5 agents (2 max + 3 overflow)\n- Job B: 5 agents (2 max + 3 overflow)\n- Job C: 5 agents (2 max + 3 overflow)\n</code></pre></p> <p>Key Takeaways: - Set higher priority for time-critical jobs to get all available resources - Use max_agents to control resource usage for jobs at the same priority - Overflow allocation mode (FIFO vs round-robin) affects same-priority job distribution - Administrators configure overflow mode via system settings</p>"},{"location":"user-guide/core-concepts/#priority-guidelines","title":"Priority Guidelines","text":"<ul> <li>900-1000: Time-critical engagements, incident response</li> <li>600-899: Active client engagements with deadlines</li> <li>400-599: Standard testing and assessments</li> <li>100-399: Research and development tasks</li> <li>0-99: Background processing, long-running attacks</li> </ul>"},{"location":"user-guide/core-concepts/#chunking-and-distribution","title":"Chunking and Distribution","text":""},{"location":"user-guide/core-concepts/#time-based-chunking","title":"Time-Based Chunking","text":"<p>KrakenHashes uses time-based chunks rather than fixed keyspace divisions:</p> <pre><code>Total Keyspace: 1,000,000,000 candidates\nAgent Benchmark: 10,000,000 hashes/second\nChunk Duration: 600 seconds (10 minutes)\n\nChunk Size = Benchmark \u00d7 Duration = 6,000,000,000 candidates\nNumber of Chunks = Total Keyspace \u00f7 Chunk Size = 167 chunks\n</code></pre>"},{"location":"user-guide/core-concepts/#benefits-of-time-based-chunks","title":"Benefits of Time-Based Chunks","text":"<ol> <li>Predictable Duration: Each chunk runs for approximately the same time</li> <li>Fair Distribution: Fast and slow agents get appropriately sized work</li> <li>Better Scheduling: Easier to estimate completion times</li> <li>Checkpoint Recovery: Regular checkpoints minimize lost work</li> </ol>"},{"location":"user-guide/core-concepts/#rule-splitting","title":"Rule Splitting","text":"<p>For attacks using large rule files:</p> <pre><code>Wordlist: 10,000 words\nRules: 100,000 rules\nEffective Keyspace: 1,000,000,000 (words \u00d7 rules)\n\nInstead of processing all rules at once:\n- Split into chunks of 1,000 rules each\n- Create 100 separate tasks\n- Each task processes: 10,000 words \u00d7 1,000 rules\n</code></pre>"},{"location":"user-guide/core-concepts/#file-management","title":"File Management","text":""},{"location":"user-guide/core-concepts/#storage-hierarchy","title":"Storage Hierarchy","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/          # Hashcat executables (multiple versions)\n\u251c\u2500\u2500 wordlists/         # Dictionary files\n\u2502   \u251c\u2500\u2500 general/       # Common password lists\n\u2502   \u251c\u2500\u2500 specialized/   # Domain-specific lists\n\u2502   \u2514\u2500\u2500 custom/        # User-uploaded lists\n\u251c\u2500\u2500 rules/             # Rule files for mutations\n\u2502   \u251c\u2500\u2500 hashcat/       # Hashcat-format rules\n\u2502   \u2514\u2500\u2500 custom/        # User-created rules\n\u251c\u2500\u2500 hashlists/         # Uploaded hash files\n\u2502   \u2514\u2500\u2500 {hashlist_id}/ # Organized by hashlist ID\n\u2514\u2500\u2500 temp/              # Temporary files (rule chunks, etc.)\n</code></pre>"},{"location":"user-guide/core-concepts/#file-synchronization","title":"File Synchronization","text":"<ol> <li>Lazy Sync: Agents download files only when needed</li> <li>Hash Verification: MD5 checksums ensure file integrity</li> <li>Local Caching: Agents cache files to avoid re-downloading</li> <li>Automatic Cleanup: Temporary files removed after job completion</li> </ol>"},{"location":"user-guide/core-concepts/#result-handling","title":"Result Handling","text":""},{"location":"user-guide/core-concepts/#real-time-crack-synchronization","title":"Real-Time Crack Synchronization","text":"<pre><code>Agent cracks hash \u2192 Sends result to backend \u2192 Backend updates database\n                            \u2193\n        Broadcast to other agents working on same hashlist\n                            \u2193\n            Update hashlist statistics\n                            \u2193\n        Notify connected web clients\n</code></pre>"},{"location":"user-guide/core-concepts/#result-storage","title":"Result Storage","text":"<p>Each cracked hash stores: - Original hash value - Plaintext password - Username (if available) - Crack timestamp - Which job/task found it - Position in keyspace where found</p>"},{"location":"user-guide/core-concepts/#pot-file-management","title":"Pot File Management","text":"<ul> <li>Centralized Pot: Backend maintains master record of all cracks</li> <li>Agent Sync: Agents receive relevant cracks for their current hashlist</li> <li>Deduplication by Line: Each unique input line preserved; duplicates by hash value automatically updated when cracked</li> <li>Cross-Hashlist: Cracks are automatically applied to all hashlists containing that hash value</li> <li>Username Preservation: Multiple users with same password (e.g., \"Administrator\", \"Administrator1\") tracked separately</li> </ul>"},{"location":"user-guide/core-concepts/#result-access","title":"Result Access","text":"<p>Users can access results through: 1. Hashlist View: See all cracked hashes for a specific hashlist 2. Pot File Export: Download results in hashcat pot format 3. Client Reports: Generate reports filtered by client/engagement 4. Real-Time Updates: Live view of cracks as they happen</p>"},{"location":"user-guide/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts/#job-design","title":"Job Design","text":"<ol> <li>Start with fast attacks (common passwords, small rules)</li> <li>Progress to more intensive attacks (large wordlists, complex rules)</li> <li>Use workflows to automate multi-stage attacks</li> <li>Set appropriate priorities based on urgency</li> </ol>"},{"location":"user-guide/core-concepts/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Use appropriate chunk durations (5-15 minutes typically)</li> <li>Limit max agents for jobs that don't scale well</li> <li>Schedule large jobs during off-peak hours</li> <li>Monitor agent efficiency and adjust benchmarks</li> </ol>"},{"location":"user-guide/core-concepts/#resource-management","title":"Resource Management","text":"<ol> <li>Organize wordlists by effectiveness</li> <li>Test and optimize custom rules</li> <li>Regular cleanup of old hashlists (retention policies)</li> <li>Monitor storage usage</li> </ol>"},{"location":"user-guide/core-concepts/#conclusion","title":"Conclusion","text":"<p>Understanding these core concepts enables you to effectively use KrakenHashes for distributed password cracking. The system handles the complexity of distributing work, managing results, and coordinating agents, allowing you to focus on designing effective attack strategies and analyzing results.</p> <p>For more detailed information on specific features, refer to the appropriate sections of the user guide.</p>"},{"location":"user-guide/hashlists/","title":"Hashlists","text":"<p>Hashlists are fundamental to KrakenHashes, representing collections of hashes uploaded for cracking jobs or analysis. This document outlines how hashlists are uploaded, processed, and managed within the system.</p>"},{"location":"user-guide/hashlists/#overview","title":"Overview","text":"<ul> <li>Definition: A hashlist is a file containing multiple lines, where each line typically represents a single hash (and potentially its cracked password).</li> <li>Association: Each hashlist is associated with a specific Hash Type (e.g., NTLM, SHA1) and can optionally be linked to a Client/Engagement.</li> <li>Processing: Uploaded hashlists undergo an asynchronous background processing workflow to ingest the hashes into the central database.</li> <li>Storage: Hashlist files are stored on the backend server in a configured directory.</li> </ul>"},{"location":"user-guide/hashlists/#uploading-hashlists","title":"Uploading Hashlists","text":"<p>Hashlists are typically uploaded through the frontend UI.</p> <ol> <li>Navigate: Go to the \"Hashlists\" section of the dashboard.</li> </ol> <p> Hashlist Management page showing the upload interface with UPLOAD HASHLIST button and data table displaying hashlist details including Name, Client, Status, Total Hashes, Cracked percentages, and Created dates</p> <ol> <li>Initiate Upload: Click the \"Upload Hashlist\" button.</li> <li>Fill Details: In the dialog, provide:<ul> <li>Name: A descriptive name for the hashlist.</li> <li>Hash Type: Select the correct hash type from the dropdown. This list is populated from the <code>hash_types</code> table in the database (see Hash Types below). The format displayed is <code>ID - Name</code> (e.g., <code>1000 - NTLM</code>).</li> <li>Client: (Optional) Select an existing client to associate this hashlist with, or create a new one on the fly.</li> <li>File: Choose the hashlist file from your local machine.</li> </ul> </li> <li>Submit: Click the upload button in the dialog.</li> </ol>"},{"location":"user-guide/hashlists/#api-endpoint","title":"API Endpoint","text":"<p>The frontend interacts with the <code>POST /api/hashlists</code> endpoint. This endpoint expects a <code>multipart/form-data</code> request containing the fields mentioned above (name, hash_type_id, client_id) and the hashlist file itself.</p>"},{"location":"user-guide/hashlists/#file-storage","title":"File Storage","text":"<ul> <li>Uploaded hashlist files are stored on the backend server.</li> <li>The base directory for uploads is configured via the <code>KH_DATA_DIR</code> environment variable.</li> <li>Within the data directory, hashlists are stored in a specific subdirectory, typically <code>hashlist_uploads</code>, but configurable via <code>KH_HASH_UPLOAD_DIR</code>.</li> <li>The maximum allowed upload size is determined by the <code>KH_MAX_UPLOAD_SIZE_MB</code> environment variable (default: 32 MiB).</li> </ul>"},{"location":"user-guide/hashlists/#lmntlm-linked-hashlists-v121","title":"LM/NTLM Linked Hashlists (v1.2.1+)","text":"<p>When uploading hashlist files in pwdump format (commonly from Windows domain exports), KrakenHashes can automatically detect and create linked LM/NTLM hashlist pairs.</p>"},{"location":"user-guide/hashlists/#automatic-detection","title":"Automatic Detection","text":"<p>When you select a file containing pwdump-format hashes, the system automatically:</p> <ol> <li>Analyzes the file to detect both LM and NTLM hashes</li> <li>Counts hash types:</li> <li>Non-blank LM hashes (mode 3000)</li> <li>NTLM hashes (mode 1000)</li> <li>Blank LM hashes (constant: <code>aad3b435b51404eeaad3b435b51404ee</code>)</li> <li>Presents a dialog if both hash types are found</li> </ol> <p>Example pwdump format: <pre><code>DOMAIN\\Administrator:500:01FC5A6BE7BC6929AAD3B435B51404EE:0CB6948805F797BF2A82807973B89537:::\nDOMAIN\\Guest:501:AAD3B435B51404EEAAD3B435B51404EE:31D6CFE0D16AE931B73C59D7E0C089C0:::\nDOMAIN\\User1:1001:E52CAC67419A9A224A3B108F3FA6CB6D:8846F7EAEE8FB117AD06BDD830B7586C:::\n</code></pre></p>"},{"location":"user-guide/hashlists/#linked-hashlist-creation-dialog","title":"Linked Hashlist Creation Dialog","text":"<p>When pwdump format is detected, you'll see a dialog showing:</p> <ul> <li>LM hash count: Number of non-blank LM hashes found</li> <li>NTLM hash count: Number of NTLM hashes found</li> <li>Blank LM count: Number of blank LM hashes (will be skipped)</li> <li>Two options:</li> <li>Upload as Single List: Process as-is with your selected hash type</li> <li>Create Linked Lists: Create two separate, linked hashlists</li> </ul> <p>Example: <pre><code>Detected pwdump format file with:\n- 1,428 LM hashes (non-blank)\n- 1,500 NTLM hashes\n- 72 blank LM hashes (empty password - will be skipped)\n\nWould you like to create two linked hashlists for separate cracking workflows?\n\nThis will create \"DomainDump-LM\" and \"DomainDump-NTLM\" hashlists that are linked together.\n</code></pre></p>"},{"location":"user-guide/hashlists/#benefits-of-linked-hashlists","title":"Benefits of Linked Hashlists","text":"<p>Separate Attack Strategies: - LM hashes are much weaker (uppercase, 7-char halves) and crack faster - NTLM hashes are stronger but can be informed by cracked LM hashes - Run different wordlists/rules optimized for each hash type</p> <p>Username/Domain Linking: - System automatically links hashes by matching username and domain - View correlation: which users have both LM and NTLM cracked - Analytics show linked pair statistics</p> <p>Progress Tracking: - Each hashlist shows independent progress - Linked pairs counted as ONE hashlist in analytics overview - See correlation statistics (both cracked, only LM, only NTLM)</p> <p>Analytics Advantages: - Windows Hash Analytics section shows linked correlation - LM Partial Cracks tracked separately - LM-to-NTLM Mask Generation uses cracked LM patterns to attack NTLM - Domain filtering works across both linked hashlists</p>"},{"location":"user-guide/hashlists/#blank-lm-hash-filtering","title":"Blank LM Hash Filtering","text":"<p>LM hashes with the constant <code>aad3b435b51404eeaad3b435b51404ee</code> indicate: - Password was blank/empty when LM hash was created, OR - LM hash storage was disabled (Windows Vista+ default)</p> <p>These hashes are automatically filtered during processing: - Not counted toward total hash count - Not sent to agents for cracking - Appear in upload dialog count for awareness</p>"},{"location":"user-guide/hashlists/#when-to-use-linked-hashlists","title":"When to Use Linked Hashlists","text":"<p>Use Linked Hashlists When: - Uploading pwdump format files (LM + NTLM) - Want to track LM and NTLM progress separately - Plan different attack strategies for each hash type - Need analytics on LM/NTLM correlation</p> <p>Use Single Hashlist When: - File contains only one hash type - Prefer simpler management - Don't need separate progress tracking</p>"},{"location":"user-guide/hashlists/#hashlist-processing","title":"Hashlist Processing","text":"<p>Once a hashlist file is uploaded and initial metadata is saved, it enters an asynchronous processing queue.</p>"},{"location":"user-guide/hashlists/#status-workflow","title":"Status Workflow","text":"<p>A hashlist progresses through the following statuses:</p> <ol> <li><code>uploading</code>: Initial state when the upload request is received.</li> <li><code>processing</code>: The backend worker has picked up the hashlist and is actively reading the file and ingesting hashes.</li> <li><code>ready</code>: Processing completed successfully. All valid lines have been processed and stored. The hashlist is now available for use in cracking jobs.</li> <li><code>ready_with_errors</code>: Processing finished, but one or more lines in the file could not be processed correctly (e.g., invalid format for the selected hash type). Valid lines were still ingested. Check backend logs for details on specific line errors. (Not fully implemented)    `</li> <li><code>error</code>: A fatal error occurred during processing (e.g., file unreadable, database error during batch insert). The <code>error_message</code> field on the hashlist provides a general reason. Check backend logs for more details.</li> </ol>"},{"location":"user-guide/hashlists/#processing-steps","title":"Processing Steps","text":"<p>The backend processor performs the following steps:</p> <ol> <li>Fetch Details: Retrieves the hashlist metadata (ID, file path, hash type ID) from the database.</li> <li>Open File: Opens the stored hashlist file.</li> <li>Scan Line by Line: Reads the file line by line.<ul> <li>Empty lines are skipped.</li> <li>Lines starting with <code>#</code> are treated as comments and skipped.</li> </ul> </li> <li>Extract Hash/Password:<ul> <li>Default: Checks for a colon (<code>:</code>) separator. If found, the part before the colon is treated as the hash, and the part after is treated as the pre-cracked password (<code>is_cracked</code> = true). If no colon is found, the entire line is treated as the hash (<code>is_cracked</code> = false).</li> <li>Type-Specific Processing: For certain hash types (e.g., <code>1000 - NTLM</code>), specific processing logic might be applied to extract the canonical hash format from more complex lines (like <code>user:sid:LM:NT:::</code>). This logic is determined by the <code>needs_processing</code> flag and potentially the <code>processing_logic</code> field in the <code>hash_types</code> table.</li> </ul> </li> <li>Batching: Hashes are collected into batches (size configured by <code>KH_HASHLIST_BATCH_SIZE</code>, default: 1000).</li> <li>Database Insertion with Deduplication: Each batch is processed:<ul> <li>Deduplication Strategy: The system deduplicates by <code>original_hash</code> (the complete input line), not just by <code>hash_value</code>. This ensures that different users with the same password hash are preserved as separate entries.<ul> <li>Example: Lines like <code>Administrator:...:hash123</code>, <code>Administrator1:...:hash123</code>, and <code>Administrator2:...:hash123</code> are all stored as distinct hash records.</li> </ul> </li> <li>The system checks if any hashes in the batch already exist in the central <code>hashes</code> table (based on <code>original_hash</code> and hash type ID).</li> <li>New, unique hashes are inserted into the <code>hashes</code> table with both <code>hash_value</code> and <code>original_hash</code>.</li> <li>Entries are created in the <code>hashlist_hashes</code> join table to link both new and existing hashes from the batch to the current hashlist.</li> <li>Cross-Hashlist Crack Propagation: When a hash is cracked, ALL hashes with the same <code>hash_value</code> (across all hashlists) are automatically marked as cracked. Additionally, ALL hashlist files containing that hash are automatically regenerated to remove the cracked hash. This ensures:<ul> <li>If \"Administrator\", \"Administrator1\", and \"Administrator2\" share the same password across different hashlists, cracking one updates all three</li> <li>All affected hashlist files are regenerated with only uncracked hashes remaining</li> <li>Agents automatically download updated hashlist files on their next task</li> <li>The <code>cracked_hashes</code> counter is incremented for each affected hashlist</li> </ul> </li> <li>If a hash being added includes a pre-cracked password, the corresponding record in the <code>hashes</code> table is updated (<code>is_cracked</code>=true, <code>password</code>=...).</li> </ul> </li> <li>Update Status: Once the entire file is processed, the hashlist status is updated to <code>ready</code>, <code>ready_with_errors</code>, or <code>error</code>, along with the final <code>total_hashes</code> and <code>cracked_hashes</code> counts.</li> </ol>"},{"location":"user-guide/hashlists/#efficient-hashcat-processing","title":"Efficient Hashcat Processing","text":"<p>When generating hashlist files for hashcat: *   DISTINCT Query: The system uses a DISTINCT query on <code>hash_value</code> to prevent sending duplicate password hashes to hashcat. Even if multiple users share the same password, hashcat only needs to crack it once. *   Ordering: Results are ordered by <code>hash_value</code> for stable, consistent output.</p>"},{"location":"user-guide/hashlists/#lm-hash-special-processing-v121","title":"LM Hash Special Processing (v1.2.1+)","text":"<p>LM hashes (hash type 3000) require special handling due to their unique structure. LM hashes consist of two 7-character DES-encrypted halves, resulting in a 32-character hex string.</p>"},{"location":"user-guide/hashlists/#how-lm-processing-works","title":"How LM Processing Works","text":"<p>Structure: - Full LM hash: 32 hex characters (e.g., <code>01FC5A6BE7BC6929AAD3B435B51404EE</code>) - First half: Characters 1-16 (represents first 7 chars of password) - Second half: Characters 17-32 (represents next 7 chars of password) - Maximum password length: 14 characters (7 + 7)</p> <p>Agent Download Behavior: When agents download LM hashlists for cracking:</p> <ol> <li>Backend splits hashes: Each 32-char LM hash is split into two 16-char halves</li> <li>Unique halves streamed: System sends unique 16-char halves to agents (not full 32-char hashes)</li> <li>Automatic deduplication: Common halves (like blank constant <code>aad3b435b51404ee</code>) appear only once</li> <li>Hashcat processes independently: Each half is cracked as a separate 16-char hash</li> </ol> <p>Why This Approach: - LM's DES encryption processes each half independently - Hashcat can't crack 32-char LM hashes; it expects 16-char halves - Deduplication reduces redundant work (many passwords share common halves) - Partial cracks are possible (one half cracked, not the other)</p>"},{"location":"user-guide/hashlists/#partial-crack-tracking","title":"Partial Crack Tracking","text":"<p>When agents crack LM hash halves, the system tracks partial crack status:</p> <p>Partial Crack States: - First half cracked: First 7 characters known (e.g., <code>PASSWOR</code>) - Second half cracked: Next 7 characters known (e.g., <code>D123</code>) - Fully cracked: Both halves known, full password assembled (e.g., <code>PASSWORD123</code>)</p> <p>Database Tracking: - <code>lm_hash_metadata</code> table stores first/second half crack status - Each half stores its 7-character password fragment - When both halves crack, full password is assembled and hash marked complete - Partial cracks visible in hash table view and analytics</p> <p>Strategic Value: Knowing one half of an LM password significantly reduces the keyspace for the other half: - Full 14-char LM keyspace: ~95^14 combinations - One half known: Reduces to ~95^7 combinations - See Analytics Reports for partial crack analysis</p>"},{"location":"user-guide/hashlists/#blank-lm-hash-constant","title":"Blank LM Hash Constant","text":"<p>The blank LM hash constant <code>aad3b435b51404eeaad3b435b51404ee</code> appears when: - Password was empty when LM hash was created - LM hash storage was disabled (Windows Vista+ default) - Account created after LM storage was disabled</p> <p>Processing Behavior: - Automatically filtered during hashlist processing - Not counted in total hash count - Not sent to agents for cracking - Appears in upload detection dialog for awareness</p> <p>See Hash Types Reference for more details on LM hash structure and security implications.</p>"},{"location":"user-guide/hashlists/#association-wordlists-v140","title":"Association Wordlists (v1.4.0+)","text":"<p>Association wordlists enable targeted password attacks where each candidate is tested against a specific hash in line-order correspondence.</p>"},{"location":"user-guide/hashlists/#what-is-an-association-wordlist","title":"What is an Association Wordlist?","text":"<p>An association wordlist contains password candidates where: - Line 1 is tested against hash 1 in your hashlist - Line 2 is tested against hash 2 in your hashlist - And so on...</p> <p>This is useful when you have specific password intelligence for each user, such as: - Previously cracked passwords from other systems - Password hints or personal information - Known password patterns per user</p>"},{"location":"user-guide/hashlists/#uploading-association-wordlists","title":"Uploading Association Wordlists","text":"<ol> <li>Navigate to the hashlist detail page</li> <li>Find the Association Wordlists section</li> <li>Click Upload Association Wordlist</li> <li>Select your file</li> <li>The system validates that line count matches hash count</li> </ol>"},{"location":"user-guide/hashlists/#requirements-and-validation","title":"Requirements and Validation","text":"<p>Line Count Matching: - The association wordlist MUST have exactly the same number of lines as there are hashes in the hashlist - Upload will fail if counts don't match - Example: Hashlist with 5,000 hashes requires association wordlist with 5,000 lines</p> <p>Mixed Work Factor Warning: For hash types with variable computational cost (e.g., bcrypt with different cost parameters), association attacks are blocked because: - Hash order must match wordlist order - Mixed work factors mean hashes may be reordered during processing - The 1:1 correspondence would be broken</p> <p>Work Factor Compatibility</p> <p>Association attacks are not available for hashlists that contain hashes with mixed work factors. The hashlist detail page will show a warning if this applies to your hashlist.</p>"},{"location":"user-guide/hashlists/#managing-association-wordlists","title":"Managing Association Wordlists","text":"<ul> <li>View: See all association wordlists for a hashlist on the detail page</li> <li>Delete: Remove association wordlists you no longer need</li> <li>Reuse: The same association wordlist can be used across multiple jobs on the same hashlist</li> </ul>"},{"location":"user-guide/hashlists/#creating-association-attack-jobs","title":"Creating Association Attack Jobs","text":"<ol> <li>Go to Jobs and click Create Job</li> <li>Select your hashlist</li> <li>Choose Attack Mode 9 (Association)</li> <li>Select your association wordlist from the dropdown</li> <li>Optionally add rules for password variations</li> <li>Set priority and other options</li> <li>Submit the job</li> </ol> <p>For more details on association attacks, see Association Attacks in the Jobs &amp; Workflows guide.</p>"},{"location":"user-guide/hashlists/#supported-input-formats","title":"Supported Input Formats","text":"<p>The processor primarily expects:</p> <ul> <li>One hash per line.</li> <li>Optional: <code>hash:password</code> format for lines containing already cracked hashes.</li> <li>Lines starting with <code>#</code> are ignored.</li> <li>Empty lines are ignored.</li> <li>Specific formats handled by type-specific processors (e.g., NTLM).</li> </ul>"},{"location":"user-guide/hashlists/#hash-types","title":"Hash Types","text":"<ul> <li>Supported hash types are defined in the <code>hash_types</code> database table.</li> <li>This table is populated by a database migration (<code>000016_add_hashcat_hash_types.up.sql</code>) which includes common types and examples sourced from the Hashcat wiki.</li> <li>Each type has an ID (corresponding to the Hashcat mode), Name, Description (optional), Example (optional), and flags indicating if it needs special processing (<code>needs_processing</code>, <code>processing_logic</code>) or is enabled (<code>is_enabled</code>).</li> <li>The frontend uses the <code>GET /api/hashtypes</code> endpoint (filtered by <code>is_enabled=true</code> by default) to populate the selection dropdown during hashlist upload.</li> </ul>"},{"location":"user-guide/hashlists/#managing-hashlists","title":"Managing Hashlists","text":"<ul> <li>Viewing: The \"Hashlists\" dashboard provides a sortable and filterable view of all accessible hashlists, showing Name, Client, Status, Progress (% Cracked), and Creation Date.</li> </ul> <p> Detailed view of a hashlist named 'Test' showing ready status, crack progress indicator, and sample hashes section with individual hash entries and their crack status -   Downloading: Use the download icon on the dashboard or the <code>GET /api/hashlists/{id}/download</code> endpoint to retrieve the original uploaded hashlist file. -   Deleting:     *   Use the delete button in the hashlist detail view (with confirmation dialog) or the <code>DELETE /api/hashlists/{id}</code> endpoint.     *   Deleting a hashlist removes its entry from the <code>hashlists</code> table and removes associated entries from the <code>hashlist_hashes</code> table.     *   The original hashlist file is securely deleted from backend storage (overwritten with random data before removal).     *   Individual hashes in the central <code>hashes</code> table are not deleted if they are referenced by other hashlists.     *   Orphaned hashes (not linked to any hashlist) are automatically cleaned up.</p>"},{"location":"user-guide/hashlists/#viewing-individual-hashes","title":"Viewing Individual Hashes","text":"<p>The hashlist detail page provides a comprehensive paginated table view of all hashes:</p> <p> Paginated table showing individual hashes with username, domain, original hash, cracked password, and status</p> <p>Table Features: - Automatic Sorting: Cracked hashes appear first for easy review - Flexible Pagination: Choose 500, 1000, 1500, 2000, or view all hashes at once - Search Functionality: Filter hashes across all fields in real-time - Quick Copy: Click the copy icon to copy cracked passwords (or hash if not yet cracked) - Status Indicators: Color-coded chips show crack status at a glance</p> <p>Table Columns: - Original Hash: The complete hash line as it was uploaded - Username: Automatically extracted username (when available) - Domain: Automatically extracted domain information (when available) - Password: The cracked plaintext password (displayed only when cracked) - Status: Visual indicator showing \"Cracked\" or \"Pending\" - Actions: Copy button for quick clipboard access</p> <p>Username and Domain Extraction:</p> <p>KrakenHashes automatically extracts username and domain information from supported hash formats:</p> <ul> <li>NTLM (1000): Parses pwdump format <code>DOMAIN\\username:sid:LM:NT:::</code></li> <li>NetNTLMv1/v2 (5500, 5600): Extracts from <code>username::domain:challenge:response</code></li> <li>Kerberos (18200): Parses <code>$krb5asrep$23$user@domain.com:hash</code></li> <li>LastPass (6800): Extracts email from <code>hash:iterations:email</code></li> <li>DCC/MS Cache (1100): Extracts from <code>hash:username</code></li> </ul> <p>Machine accounts (with <code>$</code> suffix) are fully preserved: <code>COMPUTER01$</code>, <code>WKS01$</code>, etc.</p>"},{"location":"user-guide/hashlists/#hashlist-file-synchronization","title":"Hashlist File Synchronization","text":""},{"location":"user-guide/hashlists/#automatic-updates-after-cracks","title":"Automatic Updates After Cracks","text":"<p>When hashes are cracked during job execution, KrakenHashes automatically maintains file consistency across all affected hashlists:</p> <p>Update Process: 1. Agent reports cracked hashes via crack batch mechanism 2. Backend marks ALL matching hashes as cracked (by <code>hash_value</code>) 3. System identifies ALL hashlists containing the cracked hashes 4. Each affected hashlist file is regenerated with only uncracked hashes 5. Agents are notified that their local copies are outdated 6. On next task assignment, agents automatically download fresh files</p> <p>Example Scenario: <pre><code>Initial State:\n- Hashlist A: 10,000 hashes (Administrator, User1, Guest, ...)\n- Hashlist B: 5,000 hashes (john@corp.com, admin@corp.com, ...)\n- Both contain hash \"5F4DCC3B...\" (password: \"password123\")\n\nAfter Crack:\n1. Agent cracks \"5F4DCC3B...\" while working on Hashlist A\n2. Backend marks hash as cracked in central database\n3. System finds that BOTH Hashlist A and B contain this hash\n4. BOTH hashlist files are regenerated without the cracked hash\n5. Hashlist A: 9,999 hashes remaining\n6. Hashlist B: 4,999 hashes remaining\n7. All agents with either hashlist are marked for update\n</code></pre></p> <p>Benefits: - No Duplicate Work: Agents never attempt already-cracked hashes - Consistency: All hashlists remain synchronized - Efficiency: File sizes shrink as cracks accumulate - Automatic: No manual intervention required</p> <p>User Experience: - You may notice hashlist file sizes decreasing as cracks occur - Progress percentages update across all affected hashlists - Download hashlist via UI to get current uncracked hashes - Original uploaded file preserved if needed for audit purposes</p>"},{"location":"user-guide/hashlists/#cross-hashlist-impact","title":"Cross-Hashlist Impact","text":"<p>If you upload multiple hashlists that share common hashes (e.g., same organization, different departments):</p> <p>Advantages: - Cracking work in one hashlist benefits all others - Faster overall completion across all hashlists - Reduced computational cost (each unique hash cracked only once)</p> <p>Considerations: - More hashlists = more file regeneration operations per crack - Performance impact minimal for typical deployments (&lt; 100 hashlists) - Monitor backend disk I/O during high-volume cracking sessions</p> <p>Best Practices: 1. Group related hashlists (same organization/campaign) 2. Separate unrelated hashlists for clearer progress tracking 3. Use client associations to organize hashlist collections</p> <p>For technical details on the cross-hashlist synchronization system, see Cross-Hashlist Sync Architecture.</p>"},{"location":"user-guide/hashlists/#data-retention","title":"Data Retention","text":"<p>Uploaded hashlists and their associated data are subject to the system's data retention policies. Old hashlists may be automatically purged based on client-specific or default retention settings configured by an administrator. See Admin Settings documentation for details. </p>"},{"location":"user-guide/jobs-workflows/","title":"Understanding Jobs and Workflows","text":""},{"location":"user-guide/jobs-workflows/#quick-overview","title":"Quick Overview","text":"<p>KrakenHashes uses a two-tier system to organize password cracking attacks:</p> <ol> <li>Preset Jobs: Individual attack configurations (like a single recipe)</li> <li>Job Workflows: Collections of preset jobs that run in sequence (like a cookbook)</li> </ol> <p>This system ensures consistent, efficient password auditing across your organization.</p>"},{"location":"user-guide/jobs-workflows/#how-it-works","title":"How It Works","text":""},{"location":"user-guide/jobs-workflows/#the-password-cracking-process","title":"The Password Cracking Process","text":"<p>When you submit a hashlist for cracking, KrakenHashes can apply a workflow that: 1. Tries the most likely passwords first (common passwords) 2. Progressively tries more complex attacks 3. Ensures no time is wasted on inefficient approaches</p>"},{"location":"user-guide/jobs-workflows/#example-workflow-in-action","title":"Example Workflow in Action","text":"<p>Imagine you're auditing passwords for a company. A typical workflow might:</p> <ol> <li>First: Check against known leaked passwords (fast, high success rate)</li> <li>Next: Try common passwords with variations (password1, Password123!)</li> <li>Then: Combine company terms with numbers (CompanyName2024)</li> <li>Finally: Attempt more exhaustive searches if needed</li> </ol> <p>"},{"location":"user-guide/jobs-workflows/#benefits","title":"Benefits","text":""},{"location":"user-guide/jobs-workflows/#consistency","title":"Consistency","text":"<ul> <li>Every password audit follows the same proven methodology</li> <li>No steps are accidentally skipped</li> <li>New team members can run expert-level audits immediately</li> </ul>"},{"location":"user-guide/jobs-workflows/#efficiency","title":"Efficiency","text":"<ul> <li>Fast attacks run first, finding easy passwords quickly</li> <li>Resource-intensive attacks only run when necessary</li> <li>Priority system ensures optimal resource usage</li> </ul>"},{"location":"user-guide/jobs-workflows/#customization","title":"Customization","text":"<ul> <li>Different workflows for different scenarios:</li> <li>Quick compliance checks</li> <li>Thorough security audits  </li> <li>Industry-specific patterns</li> <li>Post-breach assessments</li> </ul>"},{"location":"user-guide/jobs-workflows/#common-attack-types","title":"Common Attack Types","text":""},{"location":"user-guide/jobs-workflows/#dictionary-attacks","title":"Dictionary Attacks","text":"<p>Uses lists of known passwords: - Common passwords (password, 123456) - Previously leaked passwords - Industry-specific terms</p>"},{"location":"user-guide/jobs-workflows/#rule-based-attacks","title":"Rule-Based Attacks","text":"<p>Applies transformations to dictionary words: - Capitalize first letter - Add numbers at the end - Replace letters with symbols (@ for a, 3 for e)</p>"},{"location":"user-guide/jobs-workflows/#hybrid-attacks","title":"Hybrid Attacks","text":"<p>Combines dictionaries with patterns: - Dictionary word + 4 digits (password2024) - Year + dictionary word (2024password)</p>"},{"location":"user-guide/jobs-workflows/#brute-force","title":"Brute Force","text":"<p>Tries all possible combinations for a pattern: - All 4-digit PINs (0000-9999) - All 6-character lowercase (aaaaaa-zzzzzz)</p>"},{"location":"user-guide/jobs-workflows/#increment-mode-attacks","title":"Increment Mode Attacks","text":"<p>Systematically tests patterns of increasing (or decreasing) length: - Starts with shortest pattern, progresses to longest - Each length is processed as a separate \"layer\" - Useful for PIN attacks (2-8 digit), password length exploration - Works with Brute Force and Hybrid attack modes</p> <p>Example: A mask <code>?d?d?d?d?d?d</code> with increment mode (min=4, max=6) tests:</p> <ol> <li><code>?d?d?d?d</code> (4 digits: 0000-9999)</li> <li><code>?d?d?d?d?d</code> (5 digits: 00000-99999)</li> <li><code>?d?d?d?d?d?d</code> (6 digits: 000000-999999)</li> </ol> <p>This is more efficient than creating three separate preset jobs!</p>"},{"location":"user-guide/jobs-workflows/#association-attacks-v140","title":"Association Attacks (v1.4.0+)","text":"<p>Association attacks (hashcat mode <code>-a 9</code>) test password candidates against specific hashes in a 1:1 mapping: - Hash on line N is tested against candidate password on line N - Requires an association wordlist with exactly the same line count as your hashlist - Useful for targeted attacks with user-specific password hints</p> <p>Common Use Cases: - Testing known passwords from previous breaches against current accounts - Password reuse detection across systems - Targeted attacks using personal information (birthdays, pet names, etc.) - Testing variations of previously cracked passwords</p> <p>Requirements: - Association wordlist line count must exactly match hashlist hash count - Original hash order is preserved during the attack - Rules can be combined with association attacks for password variations</p> <p>Example: You have 1,000 user hashes and corresponding password hints:</p> <pre><code>Hashlist (1,000 lines):\n$ntlm$abc123...  (user1)\n$ntlm$def456...  (user2)\n\nAssociation Wordlist (1,000 lines):\nuser1birthday2020\nuser2petname!\n</code></pre> <p>Hash 1 is tested against \"user1birthday2020\", hash 2 against \"user2petname!\", etc.</p> <p>Combining with Rules</p> <p>Association attacks support rules. If you use a rule file with 100 rules, each hash-candidate pair will be tested with 100 rule transformations applied to the candidate password.</p> <p>See Association Wordlists for how to upload and manage association wordlists.</p>"},{"location":"user-guide/jobs-workflows/#understanding-priorities","title":"Understanding Priorities","text":"<p>Jobs within workflows run in priority order: - Critical Priority (90-100): Emergency response, security incidents - High Priority (70-89): Time-sensitive audits, compliance deadlines - Normal Priority (40-69): Standard security assessments - Low Priority (10-39): Background processing, research tasks - Minimal Priority (0-9): Non-urgent, opportunistic processing</p>"},{"location":"user-guide/jobs-workflows/#how-priority-affects-your-jobs","title":"How Priority Affects Your Jobs","text":"<p>Priority determines how many agents your job receives and when it runs:</p> <p>1. Agent Allocation Based on Priority: - Higher priority jobs: Get ALL available agents (max_agents setting is overridden) - Same priority jobs: Respect max_agents limit, share overflow agents based on allocation mode - Lower priority jobs: Wait until higher priority jobs complete or release agents</p> <p>2. Execution Order: - Jobs start in priority order (highest first) - Within the same priority, older jobs start first (FIFO)</p> <p>3. Resource Control: - max_agents setting: Controls resource usage for jobs at the same priority - Overflow allocation mode: Determines how extra agents are distributed (FIFO or round-robin) - Priority override: Higher priority jobs ignore max_agents and take all resources</p> <p>Real-World Example:</p> <p>You have 10 agents available and submit two jobs: <pre><code>Job A: Priority 90, max_agents = 5 (urgent client deadline)\nJob B: Priority 50, max_agents = 10 (background research)\n\nResult:\n- Job A gets ALL 10 agents (higher priority overrides max_agents)\n- Job B waits until Job A completes\n</code></pre></p> <p>If both jobs have priority 50: <pre><code>Job A: Priority 50, max_agents = 5\nJob B: Priority 50, max_agents = 5\n10 agents available\n\nFIFO Mode (default):\n- Job A: 10 agents (created first, gets overflow)\n- Job B: 0 agents (waits for Job A)\n\nRound-Robin Mode:\n- Job A: 5 agents\n- Job B: 5 agents\n</code></pre></p> <p>Key Takeaway: Use higher priority for time-critical jobs to get maximum resources immediately. For jobs at the same priority, max_agents controls resource sharing.</p> <p>"},{"location":"user-guide/jobs-workflows/#job-interruption-and-resumption","title":"Job Interruption and Resumption","text":"<p>KrakenHashes includes an intelligent job interruption system that ensures critical tasks get the resources they need without losing work on other jobs.</p>"},{"location":"user-guide/jobs-workflows/#how-job-interruption-works","title":"How Job Interruption Works","text":"<p>When a high-priority job needs immediate attention:</p> <ol> <li>Automatic Detection: The system identifies when critical jobs are waiting</li> <li>Smart Interruption: Only interrupts lower priority jobs when necessary</li> <li>Progress Preservation: All completed work is saved before interruption</li> <li>Seamless Resumption: Interrupted jobs automatically continue when resources are available</li> </ol>"},{"location":"user-guide/jobs-workflows/#what-this-means-for-your-jobs","title":"What This Means for Your Jobs","text":"<ul> <li>No Lost Work: If your job is interrupted, it will resume from where it stopped</li> <li>Transparent Process: You'll see the job status change from \"running\" to \"pending\" and back</li> <li>Fair Resource Sharing: The system balances urgent needs with ongoing work</li> <li>Automatic Management: No manual intervention required for resumption</li> </ul>"},{"location":"user-guide/jobs-workflows/#understanding-job-statuses","title":"Understanding Job Statuses","text":"<p> The Jobs Management interface showing active password cracking jobs with status filtering (ALL, PENDING, RUNNING, COMPLETED, FAILED). The table displays job details including name, hashlist, progress, keyspace, cracked count, agents assigned, priority level, and available actions.</p> <ul> <li>Pending: Job is waiting for available agents</li> <li>Running: Job is actively being processed by agents</li> <li>Processing: Job execution has finished but system is receiving cracked passwords from agents</li> <li>Completed: Job finished successfully and all results have been processed</li> <li>Failed: Job encountered an error</li> <li>Paused: Job was manually paused or interrupted for a higher priority task</li> </ul>"},{"location":"user-guide/jobs-workflows/#priority-best-practices","title":"Priority Best Practices","text":"<p>To ensure optimal performance:</p> <ol> <li>Use Appropriate Priorities: Don't mark everything as high priority</li> <li>Plan for Interruptions: Expect that low-priority jobs may pause for critical work</li> <li>Monitor Progress: Check job status regularly for time-sensitive tasks</li> <li>Communicate Urgency: Work with administrators to set correct priorities for critical audits</li> </ol>"},{"location":"user-guide/jobs-workflows/#automatic-job-completion","title":"Automatic Job Completion","text":"<p>KrakenHashes automatically detects when all hashes in a hashlist have been cracked and manages the lifecycle of related jobs to prevent failures and wasted resources.</p>"},{"location":"user-guide/jobs-workflows/#how-it-works_1","title":"How It Works","text":"<p>When an agent reports hashcat status code 6 (all hashes cracked):</p> <ol> <li>Detection: Backend receives status code 6 from hashcat's JSON status output</li> <li>Trust Model: Status code 6 is trusted as authoritative (no database verification needed)</li> <li>Running Jobs: Currently executing jobs are stopped and marked as completed at 100%</li> <li>Pending Jobs: Jobs that haven't started yet are automatically deleted</li> <li>Notifications: Email notifications sent for completed jobs (if configured)</li> </ol>"},{"location":"user-guide/jobs-workflows/#why-this-matters","title":"Why This Matters","text":"<p>Hashcat's <code>--remove</code> option removes cracked hashes from the input file during execution. If all hashes are cracked, the file becomes empty, causing subsequent jobs to fail. This automatic detection prevents those failures.</p>"},{"location":"user-guide/jobs-workflows/#what-youll-see","title":"What You'll See","text":"<ul> <li>Job progress reaches 100% when hashcat finishes processing</li> <li>Job status may briefly show \"processing\" while cracked passwords are being received</li> <li>Job status changes to \"completed\" once all results are processed</li> <li>Email notification sent when job truly completes (not during processing)</li> <li>Related pending jobs for the same hashlist disappear from the queue</li> </ul> <p>The \"processing\" status ensures that completion emails contain accurate crack counts and that all discovered passwords are properly stored before the job is marked as complete.</p> <p>This ensures your workflow doesn't encounter errors when your cracking campaign is successful!</p>"},{"location":"user-guide/jobs-workflows/#technical-details","title":"Technical Details","text":"<p>For administrators and developers interested in the implementation: - Status code 6 detection occurs in the agent's hashcat output parser - The <code>AllHashesCracked</code> flag is transmitted via WebSocket to the backend - <code>HashlistCompletionService</code> handles the cleanup asynchronously - See the Job Completion System architecture documentation for full details</p>"},{"location":"user-guide/jobs-workflows/#monitoring-job-execution","title":"Monitoring Job Execution","text":""},{"location":"user-guide/jobs-workflows/#the-job-details-page","title":"The Job Details Page","text":"<p>Once you've submitted a job, you can monitor its progress in real-time through the Job Details page. Access it by clicking on any job in the Jobs list or navigating to <code>/jobs/{job-id}</code>.</p> <p>The Job Details page shows real-time progress, assigned agents, and crack results</p>"},{"location":"user-guide/jobs-workflows/#real-time-updates","title":"Real-Time Updates","text":"<p>The Job Details page provides live updates for active jobs:</p>"},{"location":"user-guide/jobs-workflows/#auto-refresh","title":"Auto-Refresh","text":"<ul> <li>Automatic Updates: The page refreshes every 5 seconds for running jobs (configurable by administrators)</li> <li>Smart Refresh: Auto-refresh pauses when you're editing settings to prevent data loss</li> <li>Manual Refresh: Click the refresh button for immediate updates</li> <li>Status-Based: Auto-refresh only active for pending, running, or paused jobs</li> </ul>"},{"location":"user-guide/jobs-workflows/#progress-visualization","title":"Progress Visualization","text":"<p>The new progress bar provides at-a-glance job status: - Visual Progress: Color-coded bar showing completion percentage - Keyspace Coverage: Shows processed vs. total keyspace - Time Estimates: Estimated time remaining based on current speed - Agent Distribution: See how work is distributed across agents</p>"},{"location":"user-guide/jobs-workflows/#information-available","title":"Information Available","text":""},{"location":"user-guide/jobs-workflows/#job-summary","title":"Job Summary","text":"<ul> <li>Status: Current job state (pending, running, completed, failed)</li> <li>Priority: Job priority level and queue position</li> <li>Hashlist: Target hashlist being processed</li> <li>Workflow: Associated workflow and current preset job</li> </ul>"},{"location":"user-guide/jobs-workflows/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Hash Rate: Combined speed across all agents (H/s, MH/s, GH/s)</li> <li>Keyspace Progress: Amount of keyspace processed</li> <li>Cracks Found: Real-time count of cracked passwords</li> <li>Efficiency: Cracks per billion attempts</li> </ul>"},{"location":"user-guide/jobs-workflows/#agent-assignment","title":"Agent Assignment","text":"<ul> <li>Active Agents: List of agents currently working on the job</li> <li>Agent Performance: Individual agent hash rates and progress</li> <li>Task Distribution: How chunks are distributed</li> <li>Agent Status: Online/offline status of assigned agents</li> </ul>"},{"location":"user-guide/jobs-workflows/#crack-results","title":"Crack Results","text":"<ul> <li>Real-Time Cracks: See passwords as they're cracked</li> <li>Crack Positions: Where in the attack the crack occurred</li> <li>Plain Text: Recovered passwords (if permissions allow)</li> <li>Export Options: Download results in various formats</li> </ul>"},{"location":"user-guide/jobs-workflows/#completed-tasks-history","title":"Completed Tasks History","text":"<p>The Job Details page maintains a comprehensive history of all completed tasks, providing valuable insights into job execution:</p>"},{"location":"user-guide/jobs-workflows/#information-displayed","title":"Information Displayed","text":"<p>For each completed task, you can view: - Agent ID: The specific agent that processed the task - Task ID: Unique identifier for reference and troubleshooting - Completion Time: When the task finished processing - Keyspace Range: The exact portion of keyspace that was processed - Final Progress: The percentage of the task that was completed - Average Speed: Hash rate achieved during task execution - Cracks Found: Number of passwords cracked (click to view details in the POT)</p>"},{"location":"user-guide/jobs-workflows/#organization-and-navigation","title":"Organization and Navigation","text":"<ul> <li>Automatic Sorting: Tasks are sorted by completion time, with most recent first</li> <li>Pagination Controls: Navigate through large task lists with configurable page sizes:</li> <li>25 items per page (default)</li> <li>50, 100, or 200 items for larger views</li> <li>Persistent History: Completed tasks remain visible even after job completion</li> </ul>"},{"location":"user-guide/jobs-workflows/#use-cases","title":"Use Cases","text":"<p>The completed tasks history helps with: - Performance Analysis: Compare hash rates across different agents to identify performance variations - Crack Distribution: See which keyspace ranges yielded the most cracks - Troubleshooting: Identify if specific agents or keyspace ranges had issues - Audit Trail: Maintain a complete record of job execution for compliance or review - Resource Planning: Analyze task completion patterns to optimize future job configurations</p> <p>Performance Insights</p> <p>Use the completed tasks table to identify your most efficient agents and optimize task distribution in future jobs.</p>"},{"location":"user-guide/jobs-workflows/#interactive-controls","title":"Interactive Controls","text":"<p>While monitoring your job, you can:</p>"},{"location":"user-guide/jobs-workflows/#adjust-settings","title":"Adjust Settings","text":"<ul> <li>Change Priority: Modify job priority to speed up or slow down execution</li> <li>Agent Limits: Adjust maximum agents assigned to the job</li> <li>Pause/Resume: Temporarily halt job execution</li> </ul>"},{"location":"user-guide/jobs-workflows/#job-actions","title":"Job Actions","text":"<ul> <li>Stop Job: Terminate the job (progress is saved)</li> <li>Clone Job: Create a new job with the same settings</li> <li>View Logs: Access detailed execution logs</li> <li>Export Results: Download crack results and reports</li> </ul>"},{"location":"user-guide/jobs-workflows/#understanding-progress-indicators","title":"Understanding Progress Indicators","text":""},{"location":"user-guide/jobs-workflows/#keyspace-progress","title":"Keyspace Progress","text":"<p>The keyspace represents the total search space: - Linear Progress: Steady advancement through wordlist attacks - Chunk-Based: Progress jumps as chunks complete - Rule Multiplication: Progress may seem slow with large rule sets - Accurate Tracking: Progress values are captured directly from hashcat for precision</p> <p>Accurate Progress Tracking</p> <p>KrakenHashes captures actual keyspace values directly from hashcat (<code>progress[1]</code>), ensuring that progress percentages accurately reflect the real search progress, especially for jobs with rules or combination attacks where estimation can be inaccurate.</p>"},{"location":"user-guide/jobs-workflows/#increment-mode-progress","title":"Increment Mode Progress","text":"<p>For increment mode jobs, progress represents the aggregate across all layers:</p> <ul> <li>Each mask length runs as a separate layer</li> <li>The Job Details page shows an Increment Layers table with per-layer progress</li> <li>Overall progress aggregates all layers: (processed across all layers) / (total keyspace)</li> </ul> <p>Layer-Based Execution</p> <p>KrakenHashes decomposes increment mode into discrete layers for distributed processing. Multiple agents can work on different layers simultaneously, maximizing efficiency. See Increment Mode Architecture for technical details.</p>"},{"location":"user-guide/jobs-workflows/#time-estimates","title":"Time Estimates","text":"<p>Estimated completion times are based on: - Current hash rate - Remaining keyspace - Historical performance - Agent availability - Actual keyspace values from hashcat</p> <p>Estimate Accuracy</p> <p>Time estimates become more accurate as the job progresses and the system learns the actual performance characteristics. After the first benchmark or progress update, the system uses actual keyspace values from hashcat instead of estimates.</p>"},{"location":"user-guide/jobs-workflows/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Check Early Progress: Verify the job started correctly in the first few minutes</li> <li>Monitor Agent Assignment: Ensure sufficient agents are assigned</li> <li>Watch for Stalls: If progress stops, check agent status</li> <li>Review Partial Results: Examine cracked passwords as they appear</li> <li>Adjust Priority if Needed: Increase priority for time-sensitive jobs</li> </ol>"},{"location":"user-guide/jobs-workflows/#troubleshooting-job-issues","title":"Troubleshooting Job Issues","text":""},{"location":"user-guide/jobs-workflows/#job-stuck-in-pending","title":"Job Stuck in Pending","text":"<ul> <li>Check if agents are available</li> <li>Verify agent scheduling settings</li> <li>Review job priority relative to other jobs</li> </ul>"},{"location":"user-guide/jobs-workflows/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check assigned agent count</li> <li>Review chunk size settings</li> <li>Verify network connectivity</li> <li>Consider increasing job priority</li> </ul>"},{"location":"user-guide/jobs-workflows/#no-cracks-found","title":"No Cracks Found","text":"<ul> <li>Normal for strong passwords</li> <li>Review attack methodology</li> <li>Consider different workflows</li> <li>Check hashlist format</li> </ul>"},{"location":"user-guide/jobs-workflows/#real-world-applications","title":"Real-World Applications","text":""},{"location":"user-guide/jobs-workflows/#compliance-auditing","title":"Compliance Auditing","text":"<p>Verify passwords meet policy requirements: - Minimum length checks - Complexity requirements - Banned password lists</p>"},{"location":"user-guide/jobs-workflows/#security-assessments","title":"Security Assessments","text":"<p>Identify weak passwords before attackers do: - Test against current attack techniques - Benchmark password strength - Provide actionable reports</p>"},{"location":"user-guide/jobs-workflows/#incident-response","title":"Incident Response","text":"<p>Quickly assess breach impact: - Check if compromised passwords are reused - Identify affected accounts - Prioritize password resets</p>"},{"location":"user-guide/jobs-workflows/#what-this-means-for-you","title":"What This Means for You","text":"<p>As a user, the preset jobs and workflows system: - Ensures thorough password testing - Provides consistent results - Optimizes cracking time - Delivers actionable insights</p> <p>Your administrators have configured these workflows based on: - Industry best practices - Your organization's specific needs - Current threat landscape - Compliance requirements</p>"},{"location":"user-guide/jobs-workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Review your hashlist results to understand which attacks succeeded</li> <li>Work with your security team to address found passwords</li> <li>Consider implementing stronger password policies</li> <li>Schedule regular password audits using these workflows</li> </ul> <p>For more detailed information about creating and managing workflows, see the administrator documentation.</p>"},{"location":"user-guide/troubleshooting/","title":"KrakenHashes Troubleshooting Guide","text":"<p>This guide helps you resolve common issues when using KrakenHashes. If your issue isn't covered here, please contact support with relevant error messages and logs.</p>"},{"location":"user-guide/troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation Issues</li> <li>Login and Authentication Problems</li> <li>Job Creation and Execution Issues</li> <li>Agent Connection Problems</li> <li>Performance Issues</li> <li>File Upload Errors</li> <li>Results Not Appearing</li> <li>How to Check Logs and Get Help</li> </ol>"},{"location":"user-guide/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"user-guide/troubleshooting/#docker-compose-fails-to-start","title":"Docker Compose Fails to Start","text":"<p>Error: <code>docker-compose: command not found</code> - Solution: Install Docker Compose following the official documentation for your OS</p> <p>Error: <code>Cannot connect to the Docker daemon</code> - Solution:    - Ensure Docker is running: <code>sudo systemctl start docker</code>   - Add your user to the docker group: <code>sudo usermod -aG docker $USER</code>   - Log out and back in for group changes to take effect</p> <p>Error: <code>Port 8080 is already in use</code> - Solution:   - Check what's using the port: <code>sudo lsof -i :8080</code>   - Stop the conflicting service or change KrakenHashes ports in <code>docker-compose.yml</code></p>"},{"location":"user-guide/troubleshooting/#database-migration-failures","title":"Database Migration Failures","text":"<p>Error: <code>pq: password authentication failed for user</code> - Solution:    - Check your <code>DATABASE_URL</code> environment variable   - Ensure PostgreSQL credentials match in <code>docker-compose.yml</code>   - Try resetting with: <code>docker-compose down -v</code> then <code>docker-compose up -d</code></p> <p>Error: <code>migration failed: table already exists</code> - Solution:   - Reset migrations: <code>docker-compose exec backend make migrate-down</code>   - Clean database: <code>docker-compose down -v</code>   - Restart: <code>docker-compose up -d</code></p>"},{"location":"user-guide/troubleshooting/#ssltls-certificate-issues","title":"SSL/TLS Certificate Issues","text":"<p>Error: <code>NET::ERR_CERT_AUTHORITY_INVALID</code> - Solution:   - For self-signed certificates, follow the installation guide in <code>docs/SSL_TLS_SETUP.md</code>   - Import the CA certificate to your browser/OS trust store   - For production, use proper certificates with <code>KH_TLS_MODE=provided</code></p>"},{"location":"user-guide/troubleshooting/#login-and-authentication-problems","title":"Login and Authentication Problems","text":""},{"location":"user-guide/troubleshooting/#cannot-log-in","title":"Cannot Log In","text":"<p>Error: <code>Invalid credentials</code> - Solution:   - Verify username and password are correct   - Check if account is active (admin can verify)   - Try resetting password through admin</p> <p>Error: <code>Token expired</code> - Solution:   - Clear browser cookies/localStorage   - Log in again   - If persistent, check system time synchronization</p>"},{"location":"user-guide/troubleshooting/#multi-factor-authentication-issues","title":"Multi-Factor Authentication Issues","text":"<p>Error: <code>Invalid TOTP code</code> - Solution:   - Ensure device time is synchronized   - Verify you're using the correct authenticator app   - Try backup codes if available   - Contact admin to reset MFA</p> <p>Error: <code>Email verification code not received</code> - Solution:   - Check spam/junk folder   - Verify email address is correct in profile   - Check backend logs for SMTP errors   - Contact admin to check email configuration</p>"},{"location":"user-guide/troubleshooting/#session-timeout","title":"Session Timeout","text":"<p>Issue: Logged out unexpectedly - Solution:   - Check JWT token expiration settings   - Enable \"Remember Me\" during login   - Check for network connectivity issues</p>"},{"location":"user-guide/troubleshooting/#job-creation-and-execution-issues","title":"Job Creation and Execution Issues","text":""},{"location":"user-guide/troubleshooting/#cannot-create-job","title":"Cannot Create Job","text":"<p>Error: <code>No hashlist selected</code> - Solution:   - Upload a hashlist first via Hashlists page   - Ensure hashlist contains valid hashes   - Check hashlist format matches selected hash type</p> <p>Error: <code>No available agents</code> - Solution:   - Verify at least one agent is connected   - Check agent status on Agents page   - Ensure agents have required capabilities</p> <p>Error: <code>Invalid workflow configuration</code> - Solution:   - Verify all required fields are filled   - Check attack mode parameters are valid   - Ensure selected wordlists/rules exist</p>"},{"location":"user-guide/troubleshooting/#job-stuck-in-pending","title":"Job Stuck in Pending","text":"<p>Issue: Job never starts - Causes:   - No agents available with required capabilities   - Agent offline or disconnected   - Resource constraints on agent - Solution:   - Check agent status and capabilities   - Verify agent GPU requirements match job   - Check agent logs for errors</p> <p>Note (v1.2.1+): If an agent crashes while working on your job, the system automatically returns the unfinished work to the pool and reassigns it to another available agent. No manual intervention is required.</p>"},{"location":"user-guide/troubleshooting/#job-failed-immediately","title":"Job Failed Immediately","text":"<p>Error: <code>Hashcat execution failed</code> - Solution:   - Check job logs for specific hashcat errors   - Verify hash format matches selected type   - Ensure wordlists/rules are accessible   - Check agent has sufficient disk space</p>"},{"location":"user-guide/troubleshooting/#agent-connection-problems","title":"Agent Connection Problems","text":""},{"location":"user-guide/troubleshooting/#agent-wont-connect","title":"Agent Won't Connect","text":"<p>Error: <code>websocket: bad handshake</code> - Solution:   - Verify backend URL in agent config   - Check firewall allows WebSocket connections   - Ensure SSL certificates are trusted by agent</p> <p>Error: <code>Invalid API key</code> - Solution:   - Regenerate API key from agent settings   - Update agent configuration with new key   - Restart agent after configuration change</p> <p>Error: <code>Claim code invalid or expired</code> - Solution:   - Generate new claim code from UI   - Use claim code within 15 minutes   - Ensure claim code hasn't been used already</p>"},{"location":"user-guide/troubleshooting/#agent-keeps-disconnecting","title":"Agent Keeps Disconnecting","text":"<p>Issue: Frequent reconnections - Causes:   - Network instability   - Firewall/proxy timeout settings   - Backend overload - Solution:   - Check network connectivity   - Increase WebSocket timeout settings   - Monitor backend resource usage</p> <p>Note (v1.2.1+): Agent disconnections are now handled gracefully. If an agent crashes during high-volume cracking, the crack batching system prevents data loss and automatically recovers the connection when the agent reconnects. Your job progress will be preserved.</p>"},{"location":"user-guide/troubleshooting/#agent-not-detecting-gpus","title":"Agent Not Detecting GPUs","text":"<p>Error: <code>No GPUs detected</code> - Solution:   - Verify GPU drivers are installed:     - NVIDIA: <code>nvidia-smi</code>     - AMD: <code>rocm-smi</code>     - Intel: Check oneAPI installation   - Run agent with sudo if needed   - Check GPU is not in exclusive mode</p>"},{"location":"user-guide/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"user-guide/troubleshooting/#slow-hash-cracking","title":"Slow Hash Cracking","text":"<p>Issue: Lower than expected hash rates - Causes:   - Thermal throttling   - Incorrect workload tuning   - CPU bottleneck - Solution:   - Monitor GPU temperature   - Adjust workload profile in job settings   - Ensure adequate cooling   - Check hashcat benchmark mode</p>"},{"location":"user-guide/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Issue: System running out of memory - Solution:   - Reduce wordlist buffer size   - Split large hashlists   - Use rule-based attacks instead of large wordlists   - Monitor with <code>docker stats</code></p>"},{"location":"user-guide/troubleshooting/#database-performance","title":"Database Performance","text":"<p>Issue: Slow query responses - Solution:   - Check database indexes are created   - Monitor with <code>docker-compose logs postgres</code>   - Consider increasing PostgreSQL resources   - Clean up old completed jobs</p>"},{"location":"user-guide/troubleshooting/#file-upload-errors","title":"File Upload Errors","text":""},{"location":"user-guide/troubleshooting/#upload-fails","title":"Upload Fails","text":"<p>Error: <code>Request entity too large</code> - Solution:   - Check file size limits in nginx config   - Split large files into smaller chunks   - Use compression for text files</p> <p>Error: <code>Invalid file format</code> - Solution:   - Verify file format matches expected type:     - Hashlists: One hash per line     - Wordlists: Plain text, one word per line     - Rules: Hashcat rule format   - Remove any special characters or BOM</p> <p>Error: <code>Permission denied</code> - Solution:   - Check backend data directory permissions   - Ensure Docker volumes are writable   - Verify disk space available</p>"},{"location":"user-guide/troubleshooting/#files-not-appearing","title":"Files Not Appearing","text":"<p>Issue: Uploaded files not visible - Solution:   - Refresh the page   - Check upload completed successfully   - Verify file processing logs   - Check file storage directory</p>"},{"location":"user-guide/troubleshooting/#results-not-appearing","title":"Results Not Appearing","text":""},{"location":"user-guide/troubleshooting/#cracked-passwords-not-showing","title":"Cracked Passwords Not Showing","text":"<p>Issue: Job shows progress but no results - Causes:   - Results not yet synced   - Database write issues   - Display filtering - Solution:   - Wait for sync interval (usually 30 seconds)   - Check job logs for errors   - Verify database connectivity   - Check results filter settings</p>"},{"location":"user-guide/troubleshooting/#export-not-working","title":"Export Not Working","text":"<p>Error: <code>Export failed</code> - Solution:   - Check browser download permissions   - Try different export format   - Verify results exist to export   - Check browser console for errors</p>"},{"location":"user-guide/troubleshooting/#statistics-incorrect","title":"Statistics Incorrect","text":"<p>Issue: Progress/statistics don't match - Solution:   - Force refresh the page   - Check for duplicate hashes   - Verify job status is updated   - Review calculation logic in logs</p>"},{"location":"user-guide/troubleshooting/#how-to-check-logs-and-get-help","title":"How to Check Logs and Get Help","text":""},{"location":"user-guide/troubleshooting/#accessing-logs","title":"Accessing Logs","text":""},{"location":"user-guide/troubleshooting/#docker-logs","title":"Docker Logs","text":"<pre><code># View all logs\ndocker-compose logs\n\n# View specific service logs\ndocker-compose logs -f backend    # Backend logs\ndocker-compose logs -f postgres   # Database logs\ndocker-compose logs -f app        # Frontend/nginx logs\n\n# Save logs to file\ndocker-compose logs backend &gt; backend.log\n</code></pre>"},{"location":"user-guide/troubleshooting/#log-file-locations","title":"Log File Locations","text":"<ul> <li>Backend: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/backend/</code></li> <li>PostgreSQL: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/postgres/</code></li> <li>Nginx: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/nginx/</code></li> </ul>"},{"location":"user-guide/troubleshooting/#agent-logs","title":"Agent Logs","text":"<pre><code># On agent machine\ntail -f /var/log/krakenhashes-agent.log\n\n# Or check systemd\njournalctl -u krakenhashes-agent -f\n</code></pre>"},{"location":"user-guide/troubleshooting/#what-to-include-when-reporting-issues","title":"What to Include When Reporting Issues","text":"<ol> <li>Error Messages</li> <li>Exact error text</li> <li>Screenshot if UI issue</li> <li> <p>Time when error occurred</p> </li> <li> <p>Environment Information</p> </li> <li>KrakenHashes version</li> <li>Browser type and version</li> <li>Operating system</li> <li> <p>Docker version</p> </li> <li> <p>Steps to Reproduce</p> </li> <li>What you were trying to do</li> <li>Exact steps taken</li> <li> <p>Expected vs actual behavior</p> </li> <li> <p>Relevant Logs</p> </li> <li>Error entries from logs</li> <li>Stack traces if available</li> <li>Related warning messages</li> </ol>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Check Documentation</li> <li>Review relevant guides in <code>/docs</code></li> <li> <p>Review API documentation</p> </li> <li> <p>Search Known Issues</p> </li> <li>Check GitHub issues</li> <li>Review release notes</li> <li> <p>Search error messages</p> </li> <li> <p>Contact Support</p> </li> <li>Email: support@krakenhashes.com</li> <li>Include issue report with details above</li> <li>Provide job IDs if relevant</li> </ol>"},{"location":"user-guide/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for more details: <pre><code># Backend\nexport LOG_LEVEL=debug\n\n# Agent\nkrakenhashes-agent --debug\n\n# Frontend\n# Open browser developer console\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#common-quick-fixes","title":"Common Quick Fixes","text":"<ol> <li> <p>Restart Services <pre><code>docker-compose restart backend\n</code></pre></p> </li> <li> <p>Clear Browser Cache</p> </li> <li>Hard refresh: Ctrl+Shift+R (Cmd+Shift+R on Mac)</li> <li> <p>Clear site data in browser settings</p> </li> <li> <p>Reset Database Connection <pre><code>docker-compose restart postgres backend\n</code></pre></p> </li> <li> <p>Verify Connectivity <pre><code># Test backend\ncurl -k https://localhost:8080/api/v1/health\n\n# Test database\ndocker-compose exec postgres pg_isready\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#prevention-tips","title":"Prevention Tips","text":"<ol> <li>Regular Maintenance</li> <li>Monitor disk space</li> <li>Clean old job data</li> <li>Update regularly</li> <li> <p>Backup database</p> </li> <li> <p>Performance Monitoring</p> </li> <li>Use <code>docker stats</code></li> <li>Monitor agent resources</li> <li> <p>Set up alerting</p> </li> <li> <p>Security Best Practices</p> </li> <li>Keep software updated</li> <li>Use strong passwords</li> <li>Enable MFA</li> <li>Regular security audits</li> </ol>"},{"location":"user-guide/wordlists-rules/","title":"Wordlists and Rules User Guide","text":"<p>This guide explains how to use the wordlist and rule management features in KrakenHashes.</p>"},{"location":"user-guide/wordlists-rules/#overview","title":"Overview","text":"<p>KrakenHashes provides comprehensive management of wordlists and rules used for password cracking operations. These resources are essential for effective password cracking jobs.</p>"},{"location":"user-guide/wordlists-rules/#accessing-the-management-interface","title":"Accessing the Management Interface","text":"<ol> <li>Log in to the KrakenHashes web interface</li> <li>Navigate to the \"Resources\" section in the main menu</li> <li>Select either \"Wordlists\" or \"Rules\" to manage the respective resources</li> </ol>"},{"location":"user-guide/wordlists-rules/#wordlists-management","title":"Wordlists Management","text":""},{"location":"user-guide/wordlists-rules/#viewing-wordlists","title":"Viewing Wordlists","text":"<p>The wordlists page displays all available wordlists with the following information: - Name - Description - Type (General, Specialized, Targeted, Custom) - Format (Plaintext, Compressed) - Size - Word count - Tags - Status</p> <p> Wordlist Management interface showing category filtering, wordlist details table, and upload functionality</p> <p>You can sort and filter the list by any of these attributes.</p>"},{"location":"user-guide/wordlists-rules/#uploading-a-wordlist","title":"Uploading a Wordlist","text":"<p>To upload a new wordlist:</p> <ol> <li>Click the \"Upload Wordlist\" button</li> <li>Fill in the following information:</li> <li>Name (optional - defaults to filename)</li> <li>Description (optional)</li> <li>Type (General, Specialized, Targeted, Custom)</li> <li>Tags (optional)</li> <li>Select the file to upload</li> <li>Click \"Upload\"</li> </ol> <p>The system will: - Check if a file with the same name already exists - Calculate the MD5 hash of the file - Count the words in the file - Store the file in the appropriate directory based on the selected type</p> <p>For large files, the word counting process may take some time. The wordlist will be available with a \"pending\" status until counting completes.</p>"},{"location":"user-guide/wordlists-rules/#downloading-a-wordlist","title":"Downloading a Wordlist","text":"<p>To download a wordlist:</p> <ol> <li>Find the wordlist in the list</li> <li>Click the \"Download\" button</li> <li>The file will be downloaded to your computer</li> </ol>"},{"location":"user-guide/wordlists-rules/#deleting-a-wordlist","title":"Deleting a Wordlist","text":"<p>When deleting a wordlist, the system checks for dependencies and handles them appropriately:</p> <p>Simple Deletion (no dependencies): 1. Find the wordlist in the list 2. Click the \"Delete\" button 3. Confirm the deletion in the dialog</p> <p>Cascade Deletion (has dependencies):</p> <p>If the wordlist is used by jobs, preset jobs, or workflows, the deletion dialog will show: - Impact Preview: A summary of all affected items - Affected Items List: Details of jobs, preset jobs, and workflow steps that will be deleted - ID Confirmation: You must type the wordlist ID to confirm cascade deletion</p> <p>What Gets Deleted in a Cascade: - Non-completed jobs (pending, running, failed) using this wordlist - Preset jobs that reference this wordlist - Workflow steps using those preset jobs - Workflows that become empty after step removal</p> <p>What Is Preserved: - Completed jobs (historical records) - The physical file on disk (must be manually removed)</p> <p>Note: Deleting a wordlist removes it from the database but does not delete the file from the filesystem.</p>"},{"location":"user-guide/wordlists-rules/#rules-management","title":"Rules Management","text":""},{"location":"user-guide/wordlists-rules/#viewing-rules","title":"Viewing Rules","text":"<p>The rules page displays all available rules with the following information: - Name - Description - Type (Hashcat, John, Custom) - Size - Rule count - Tags - Status</p> <p> Rule Management page showing the uploaded _nsakey.v2.dive rule file with verification status, type (Hashcat), file size (1.19 MB), and rule count (123,289 rules). Filter tabs allow sorting by rule type.</p> <p>You can sort and filter the list by any of these attributes.</p>"},{"location":"user-guide/wordlists-rules/#uploading-a-rule","title":"Uploading a Rule","text":"<p>To upload a new rule:</p> <ol> <li>Click the \"Upload Rule\" button</li> <li>Fill in the following information:</li> <li>Name (optional - defaults to filename)</li> <li>Description (optional)</li> <li>Type (Hashcat, John, Custom)</li> <li>Tags (optional)</li> <li>Select the file to upload</li> <li>Click \"Upload\"</li> </ol> <p>The system will: - Check if a file with the same name already exists - Calculate the MD5 hash of the file - Count the rules in the file - Store the file in the appropriate directory based on the selected type</p>"},{"location":"user-guide/wordlists-rules/#downloading-a-rule","title":"Downloading a Rule","text":"<p>To download a rule:</p> <ol> <li>Find the rule in the list</li> <li>Click the \"Download\" button</li> <li>The file will be downloaded to your computer</li> </ol>"},{"location":"user-guide/wordlists-rules/#deleting-a-rule","title":"Deleting a Rule","text":"<p>When deleting a rule, the system checks for dependencies and handles them appropriately:</p> <p>Simple Deletion (no dependencies): 1. Find the rule in the list 2. Click the \"Delete\" button 3. Confirm the deletion in the dialog</p> <p>Cascade Deletion (has dependencies):</p> <p>If the rule is used by jobs, preset jobs, or workflows, the deletion dialog will show: - Impact Preview: A summary of all affected items - Affected Items List: Details of jobs, preset jobs, and workflow steps that will be deleted - ID Confirmation: You must type the rule ID to confirm cascade deletion</p> <p>What Gets Deleted in a Cascade: - Non-completed jobs (pending, running, failed) using this rule - Preset jobs that reference this rule - Workflow steps using those preset jobs - Workflows that become empty after step removal</p> <p>What Is Preserved: - Completed jobs (historical records) - The physical file on disk (must be manually removed)</p> <p>Note: Deleting a rule removes it from the database but does not delete the file from the filesystem.</p>"},{"location":"user-guide/wordlists-rules/#managing-tags","title":"Managing Tags","text":"<p>Tags help organize and categorize wordlists and rules.</p>"},{"location":"user-guide/wordlists-rules/#adding-tags","title":"Adding Tags","text":"<p>To add a tag to a wordlist or rule:</p> <ol> <li>Find the item in the list</li> <li>Click the \"Edit\" button</li> <li>Add the tag in the tags field</li> <li>Click \"Save\"</li> </ol>"},{"location":"user-guide/wordlists-rules/#removing-tags","title":"Removing Tags","text":"<p>To remove a tag:</p> <ol> <li>Find the item in the list</li> <li>Click the \"Edit\" button</li> <li>Remove the tag from the tags field</li> <li>Click \"Save\"</li> </ol>"},{"location":"user-guide/wordlists-rules/#duplicate-handling","title":"Duplicate Handling","text":"<p>The system handles duplicate files intelligently:</p> <ul> <li>If you upload a file with the same name as an existing file and the content is identical (same MD5 hash), the system will recognize it as a duplicate and return the existing entry.</li> <li>If you upload a file with the same name but different content, the system will update the existing file with the new content.</li> <li>If you upload a file with different name but identical content to an existing file, the system will store both files separately.</li> </ul> <p>This approach ensures that: - Files are not unnecessarily duplicated - Updates to existing files are properly tracked - You can maintain multiple versions of similar files with different names</p>"},{"location":"user-guide/wordlists-rules/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive filenames: Choose clear, descriptive filenames that indicate the content and purpose of the file.</li> <li>Add meaningful descriptions: Include detailed descriptions to help other users understand the purpose and content of the wordlist or rule.</li> <li>Use appropriate types: Select the correct type for each wordlist or rule to ensure proper organization.</li> <li>Apply relevant tags: Use tags to categorize and make resources easier to find.</li> <li>Organize by purpose: Use the specialized and targeted categories for wordlists with specific purposes. </li> </ol>"}]}