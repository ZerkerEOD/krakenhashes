{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KrakenHashes","text":"Distributed Password Cracking System for Security Professionals"},{"location":"#what-is-krakenhashes","title":"What is KrakenHashes?","text":"<p>KrakenHashes is a powerful distributed password cracking platform that coordinates GPU and CPU resources across multiple agents to perform high-speed hash cracking. Built for security professionals, penetration testers, and red teams, it provides a secure web interface for managing complex password auditing operations.</p> <ul> <li> <p> High Performance</p> <p>Leverage distributed GPU/CPU resources across multiple agents for maximum cracking speed</p> </li> <li> <p> Enterprise Security</p> <p>JWT authentication, MFA support, role-based access control, and encrypted communications</p> </li> <li> <p> Modern Web Interface</p> <p>Intuitive React-based UI for job management, real-time monitoring, and result analysis</p> </li> <li> <p> Scalable Architecture</p> <p>Add agents dynamically, schedule resources, and manage workloads efficiently</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get KrakenHashes running in under 5 minutes with Docker!</p> <p>Requirements: Docker Engine 19.03.0+ and Docker Compose v2.0+</p> <pre><code># Download and run\nmkdir krakenhashes &amp;&amp; cd krakenhashes\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/docker-compose.yml\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/.env.example\ncp .env.example .env # Edit .env to set passwords\n</code></pre> <p>Once the .env has been edited you can run the following <pre><code>docker compose up -d  # Note: use 'docker compose' not 'docker-compose'\n</code></pre></p> <p>Get Started  Installation Guide</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#intelligent-job-management","title":"Intelligent Job Management","text":"<ul> <li>Preset job templates and workflows for common attack patterns</li> <li>Priority-based scheduling with adaptive load balancing</li> <li>Automatic job chunking for optimal distribution</li> <li>Real-time progress tracking and ETA calculations</li> </ul>"},{"location":"#comprehensive-hash-support","title":"Comprehensive Hash Support","text":"<ul> <li>Support for 300+ hash types via Hashcat</li> <li>Manual hash type selection with detailed metadata</li> <li>Bulk hash import and management</li> <li>Client-based organization for engagements</li> </ul>"},{"location":"#resource-management","title":"Resource Management","text":"<ul> <li>Centralized wordlist and rule file management</li> <li>Automatic file synchronization to agents</li> <li>Binary version management for Hashcat</li> <li>Efficient storage with deduplication</li> </ul>"},{"location":"#real-time-monitoring","title":"Real-Time Monitoring","text":"<ul> <li>Live job progress visualization</li> <li>Agent health and performance metrics</li> <li>GPU/CPU temperature and usage tracking</li> <li>WebSocket-based real-time updates</li> </ul>"},{"location":"#multi-user-collaboration","title":"Multi-User Collaboration","text":"<ul> <li>Role-based access control (Admin/User)</li> <li>Client and engagement management</li> <li>Audit logging for compliance</li> <li>Data retention policies</li> </ul>"},{"location":"#enterprise-security","title":"Enterprise Security","text":"<ul> <li>Multi-factor authentication (TOTP, Email, Backup codes)</li> <li>TLS/SSL support with multiple certificate options</li> <li>API key authentication for agents</li> <li>JWT-based session management with refresh tokens</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li> <p>Penetration Testing</p> <p>Coordinate password audits across multiple client engagements with proper data isolation</p> </li> <li> <p>Security Assessments</p> <p>Validate password policies by testing organizational hash dumps against common patterns</p> </li> <li> <p>Incident Response</p> <p>Quickly crack passwords during forensic investigations and evidence recovery</p> </li> <li> <p>Security Research</p> <p>Analyze hash algorithm vulnerabilities and benchmark cracking performance</p> </li> </ul>"},{"location":"#system-components","title":"System Components","text":"<pre><code>graph LR\n    A[Web Interface] --&gt;|HTTPS| B[Backend API]\n    B --&gt;|WebSocket| C[Agent Pool]\n    B --&gt;|SQL| D[(PostgreSQL)]\n    C --&gt;|Executes| E[Hashcat]\n    B --&gt;|Stores| F[File Storage]</code></pre> <ul> <li>Backend Service - Go-based API server with job scheduling and resource management</li> <li>Web Interface - React frontend with Material-UI for intuitive user experience  </li> <li>Agent System - Distributed agents that execute Hashcat with hardware optimization</li> <li>PostgreSQL Database - Reliable storage for jobs, results, and system configuration</li> </ul>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<ul> <li> <p> Getting Started</p> <p>New to KrakenHashes? Start here for installation and your first crack</p> </li> <li> <p> User Guide</p> <p>Learn how to create jobs, manage hashlists, and analyze results</p> </li> <li> <p> Admin Guide</p> <p>System configuration, user management, and operational procedures</p> </li> <li> <p> Deployment</p> <p>Production deployment, Docker setup, and update procedures</p> </li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Alpha Software</p> <p>KrakenHashes is currently in alpha development (v0.1.0). While core functionality is implemented, expect:</p> <ul> <li>Breaking changes between versions</li> <li>Incomplete features and documentation</li> <li>No migration path for data until v1.0</li> <li>Active development with frequent updates</li> </ul> <p>See our GitHub repository for the latest development status and roadmap.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li> Documentation - You're here! Browse the guides</li> <li> GitHub Issues - Report bugs and request features</li> <li> Discord Community - Join our Discord server</li> </ul>"},{"location":"#license","title":"License","text":"<p>KrakenHashes is open source software licensed under the GNU Affero General Public License v3.0.</p>"},{"location":"admin-guide/","title":"Administrator Guide","text":"<p>Comprehensive guide for KrakenHashes system administrators.</p>"},{"location":"admin-guide/#in-this-section","title":"In This Section","text":"<ul> <li> <p> System Setup</p> <p>Configure KrakenHashes for your environment</p> </li> <li> <p> Resource Management</p> <p>Manage binaries, wordlists, rules, and storage</p> </li> <li> <p> Operations</p> <p>User management, monitoring, and maintenance</p> </li> <li> <p> Advanced Features</p> <p>Presets, chunking, and performance optimization</p> </li> <li> <p> Security Guide</p> <p>Security considerations, data retention, and best practices</p> </li> </ul>"},{"location":"admin-guide/#first-time-setup-sequence","title":"First-Time Setup Sequence","text":"<p>When setting up KrakenHashes for the first time, follow this sequence:</p> <ol> <li>Upload Hashcat Binary (Required First)</li> <li>Navigate to Admin \u2192 Binary Management</li> <li>Upload a compressed hashcat binary (.7z, .zip, .tar.gz)</li> <li>Wait for verification to complete</li> <li> <p>This triggers creation of system preset jobs including the potfile job</p> </li> <li> <p>Verify Potfile Initialization</p> </li> <li>Check Resources \u2192 Wordlists for \"Pot-file\" entry</li> <li>Check Admin \u2192 Preset Jobs for \"Potfile Run\" job</li> <li> <p>Both should exist after binary upload</p> </li> <li> <p>Continue with Standard Setup</p> </li> <li>Upload wordlists</li> <li>Configure agents</li> <li>Create hashlists</li> </ol>"},{"location":"admin-guide/#quick-links","title":"Quick Links","text":""},{"location":"admin-guide/#initial-setup","title":"Initial Setup","text":"<ol> <li>System Configuration</li> <li>SSL/TLS Setup</li> <li>Email Configuration</li> <li>Authentication Settings</li> </ol>"},{"location":"admin-guide/#daily-operations","title":"Daily Operations","text":"<ul> <li>User Management</li> <li>Agent Management</li> <li>Job Execution Settings</li> <li>System Monitoring</li> <li>Potfile Management</li> <li>Data Retention</li> <li>Backup Procedures</li> </ul>"},{"location":"admin-guide/#optimization","title":"Optimization","text":"<ul> <li>Performance Tuning</li> <li>Job Chunking</li> <li>Storage Management</li> </ul>"},{"location":"admin-guide/#administrative-tasks","title":"Administrative Tasks","text":""},{"location":"admin-guide/#system-maintenance","title":"System Maintenance","text":"<ul> <li> Regular updates and patches</li> <li> Database maintenance and optimization</li> <li> File system cleanup and organization</li> <li> Performance monitoring and tuning</li> </ul>"},{"location":"admin-guide/#security-management","title":"Security Management","text":"<ul> <li> User access control and auditing</li> <li> Security policy enforcement</li> <li> Certificate management and renewal</li> <li> Incident response procedures</li> <li> Secure data deletion and retention</li> <li> Security Best Practices</li> </ul>"},{"location":"admin-guide/#resource-management","title":"Resource Management","text":"<ul> <li>:material-gpu: Agent capacity planning</li> <li> Storage allocation and cleanup</li> <li> Wordlist and rule curation</li> <li> Binary version management</li> </ul>"},{"location":"admin-guide/#best-practices","title":"Best Practices","text":"<p>Security First</p> <ul> <li>Enable MFA for all administrative accounts</li> <li>Regularly rotate API keys and passwords</li> <li>Monitor system logs for suspicious activity</li> <li>Keep all components updated</li> </ul> <p>Performance</p> <ul> <li>Schedule intensive jobs during off-peak hours</li> <li>Monitor agent resource utilization</li> <li>Implement data retention policies</li> <li>Optimize database indexes regularly</li> </ul>"},{"location":"admin-guide/#need-help","title":"Need Help?","text":"<ul> <li> Check specific guides in this section</li> <li>:material-discord: Join our Discord #admin channel</li> <li> Contact support for enterprise assistance</li> </ul>"},{"location":"admin-guide/security/","title":"Security Guide","text":"<p>This guide covers security considerations and best practices for KrakenHashes deployment and operation.</p>"},{"location":"admin-guide/security/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Security</li> <li>Data Retention Security</li> <li>Authentication &amp; Access Control</li> <li>Network Security</li> <li>Agent Security</li> <li>Database Security</li> <li>File System Security</li> <li>Audit &amp; Compliance</li> </ol>"},{"location":"admin-guide/security/#data-security","title":"Data Security","text":""},{"location":"admin-guide/security/#password-hash-protection","title":"Password Hash Protection","text":"<p>KrakenHashes handles sensitive password hash data. Follow these practices:</p> <ul> <li>Access Control: Limit access to hashlists based on roles and teams</li> <li>Client Isolation: Hashlists are associated with specific clients for data segregation</li> <li>Secure Storage: Hash files stored with restricted permissions in <code>/var/lib/krakenhashes/hashlists/</code></li> <li>No Plaintext Logging: System never logs recovered passwords or sensitive hash values</li> </ul>"},{"location":"admin-guide/security/#cracked-password-handling","title":"Cracked Password Handling","text":"<ul> <li>Passwords stored only in database after successful crack</li> <li>No automatic export of cracked passwords</li> <li>Access to cracked passwords requires authentication</li> <li>Potfile managed separately with controlled access</li> </ul> <p>Potfile Security</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) contains plaintext passwords from ALL cracked hashes and is NOT affected by the retention system. This means: - Passwords remain in the potfile even after hashlists are deleted - The potfile grows indefinitely unless manually managed - May conflict with data protection compliance requirements - Must be secured with strict file permissions and access controls</p>"},{"location":"admin-guide/security/#data-retention-security","title":"Data Retention Security","text":""},{"location":"admin-guide/security/#secure-data-deletion","title":"Secure Data Deletion","text":"<p>The retention system implements multiple layers of secure deletion to prevent data recovery:</p>"},{"location":"admin-guide/security/#1-file-system-security","title":"1. File System Security","text":"<p>When files are deleted due to retention policies:</p> <ul> <li>Random Overwrite: Files are overwritten with random data before deletion</li> <li>Multiple Passes: Sensitive data overwritten in 4KB chunks</li> <li>Immediate Removal: Files removed from filesystem after overwrite</li> <li>No Recovery: Prevents standard file recovery tools from retrieving data</li> </ul> <p>Implementation: <pre><code>// Files are overwritten with random data\nrandomData := make([]byte, 4096)\nfor written &lt; fileInfo.Size() {\n    rand.Read(randomData)\n    file.Write(randomData)\n}\nos.Remove(filePath)\n</code></pre></p>"},{"location":"admin-guide/security/#2-database-security","title":"2. Database Security","text":"<p>Database records are protected against recovery:</p> <ul> <li>Transaction Safety: All deletions occur within database transactions</li> <li>CASCADE Deletion: Related records automatically removed via foreign key constraints</li> <li>Orphan Cleanup: Hashes not linked to any hashlist are deleted</li> <li>VACUUM Operations: PostgreSQL VACUUM ANALYZE prevents recovery from:</li> <li>Dead tuples in heap files</li> <li>Write-Ahead Log (WAL) entries</li> <li>Transaction logs</li> </ul> <p>Affected tables during retention purge: - <code>hashlists</code> (primary target) - <code>hashlist_hashes</code> (associations) - <code>hashes</code> (orphaned entries) - <code>agent_hashlists</code> (distribution records) - <code>job_executions</code> (job history) - <code>job_tasks</code> (task assignments)</p>"},{"location":"admin-guide/security/#3-agent-side-cleanup","title":"3. Agent-Side Cleanup","text":"<p>Agents automatically clean temporary files:</p> <ul> <li>3-Day Retention: Temporary files removed after 3 days</li> <li>Automatic Process: Runs every 6 hours</li> <li>Protected Resources: Base files (binaries, wordlists, rules) never auto-deleted</li> <li>Storage Management: Prevents disk space exhaustion on compute nodes</li> </ul>"},{"location":"admin-guide/security/#retention-policy-configuration","title":"Retention Policy Configuration","text":"<ul> <li>System Default: Configurable default retention period for all data</li> <li>Client-Specific: Per-client retention overrides for compliance</li> <li>Audit Trail: All deletions logged with timestamp and affected records</li> <li>Manual Override: Administrators can trigger immediate purge if needed</li> </ul>"},{"location":"admin-guide/security/#potfile-exclusion-from-retention","title":"Potfile Exclusion from Retention","text":"<p>Critical Security Gap</p> <p>The potfile is NOT managed by the retention system and requires separate security procedures:</p> <p>Security Implications: - Plaintext passwords persist indefinitely in the potfile - Deleted hashlist passwords remain recoverable from potfile - No automatic cleanup when clients/hashlists are deleted - Potential compliance violation for GDPR/data protection laws</p> <p>Required Actions: 1. Implement manual potfile cleanup procedures 2. Create audit trail for potfile modifications 3. Consider encrypting the potfile at rest 4. Restrict potfile access to minimum required personnel 5. Document potfile retention policy separately for compliance</p>"},{"location":"admin-guide/security/#authentication-access-control","title":"Authentication &amp; Access Control","text":""},{"location":"admin-guide/security/#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>KrakenHashes supports multiple MFA methods:</p> <ul> <li>TOTP: Time-based One-Time Passwords via authenticator apps</li> <li>Email: Verification codes sent to registered email</li> <li>Backup Codes: Recovery codes for emergency access</li> </ul>"},{"location":"admin-guide/security/#password-security","title":"Password Security","text":"<ul> <li>Bcrypt Hashing: All passwords hashed with bcrypt</li> <li>Configurable Requirements: Min length, complexity rules</li> <li>Password History: Prevents password reuse</li> <li>Account Lockout: Automatic lockout after failed attempts</li> </ul>"},{"location":"admin-guide/security/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>System roles with increasing privileges:</p> <ol> <li>User: Standard access to assigned resources</li> <li>Admin: Full system administration</li> <li>Agent: Agent-specific operations only</li> <li>System: Internal system operations</li> </ol>"},{"location":"admin-guide/security/#jwt-token-security","title":"JWT Token Security","text":"<ul> <li>Short-Lived Access Tokens: 15-minute default expiry</li> <li>Refresh Token Rotation: New refresh token on each use</li> <li>Secure Storage: Tokens never logged or stored in plaintext</li> <li>Revocation Support: Immediate token invalidation</li> </ul>"},{"location":"admin-guide/security/#network-security","title":"Network Security","text":""},{"location":"admin-guide/security/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<p>Multiple TLS modes supported:</p> <ol> <li>Self-Signed Certificates</li> <li>Automatic generation with proper extensions</li> <li>Browser-compatible certificates</li> <li> <p>Full certificate chain support</p> </li> <li> <p>Provided Certificates</p> </li> <li>Custom certificate installation</li> <li>Certificate validation</li> <li> <p>Chain verification</p> </li> <li> <p>Let's Encrypt</p> </li> <li>Automatic renewal via ACME</li> <li>Production-ready certificates</li> </ol>"},{"location":"admin-guide/security/#api-security","title":"API Security","text":"<ul> <li>Rate Limiting: Prevents abuse and DoS attacks</li> <li>CORS Configuration: Controlled cross-origin access</li> <li>Request Validation: Input sanitization and validation</li> <li>API Key Authentication: Secure agent authentication</li> </ul>"},{"location":"admin-guide/security/#agent-security","title":"Agent Security","text":""},{"location":"admin-guide/security/#registration-security","title":"Registration Security","text":"<ul> <li>Claim Codes: One-time or continuous registration codes</li> <li>API Key Generation: Unique keys per agent</li> <li>Certificate Exchange: TLS certificate verification</li> <li>Voucher Expiration: Time-limited registration windows</li> </ul>"},{"location":"admin-guide/security/#communication-security","title":"Communication Security","text":"<ul> <li>WebSocket over TLS: Encrypted agent communication</li> <li>Heartbeat Monitoring: Detect disconnected agents</li> <li>Message Authentication: Signed messages prevent tampering</li> <li>Command Authorization: Agents verify command sources</li> </ul>"},{"location":"admin-guide/security/#file-synchronization","title":"File Synchronization","text":"<ul> <li>Checksum Verification: MD5/SHA validation of transferred files</li> <li>Partial Downloads: Resume support for large files</li> <li>Access Control: Agents access only assigned files</li> <li>Cleanup Policy: Automatic removal of unused files</li> </ul>"},{"location":"admin-guide/security/#database-security","title":"Database Security","text":""},{"location":"admin-guide/security/#connection-security","title":"Connection Security","text":"<ul> <li>TLS Connections: Encrypted database connections</li> <li>Connection Pooling: Limited concurrent connections</li> <li>Prepared Statements: SQL injection prevention</li> <li>Transaction Isolation: ACID compliance</li> </ul>"},{"location":"admin-guide/security/#data-protection","title":"Data Protection","text":"<ul> <li>No Soft Deletes: Hard deletion with CASCADE</li> <li>Audit Tables: Separate audit trail for critical operations</li> <li>UUID Primary Keys: Prevent sequential ID attacks</li> <li>JSONB Validation: Schema validation for JSON fields</li> </ul>"},{"location":"admin-guide/security/#backup-security","title":"Backup Security","text":"<ul> <li>Encrypted Backups: Optional backup encryption</li> <li>Offsite Storage: Remote backup recommendations</li> <li>Point-in-Time Recovery: Transaction log backups</li> <li>Test Restorations: Regular recovery testing</li> </ul>"},{"location":"admin-guide/security/#file-system-security","title":"File System Security","text":""},{"location":"admin-guide/security/#directory-permissions","title":"Directory Permissions","text":"<p>Recommended permissions:</p> <pre><code>/var/lib/krakenhashes/\n\u251c\u2500\u2500 binaries/     (755, krakenhashes:krakenhashes)\n\u251c\u2500\u2500 wordlists/    (755, krakenhashes:krakenhashes)\n\u251c\u2500\u2500 rules/        (755, krakenhashes:krakenhashes)\n\u2514\u2500\u2500 hashlists/    (750, krakenhashes:krakenhashes)  # Restricted\n</code></pre>"},{"location":"admin-guide/security/#path-sanitization","title":"Path Sanitization","text":"<ul> <li>Directory Traversal Prevention: Path validation</li> <li>Symlink Protection: Restricted symlink following</li> <li>Temporary File Security: Secure temp file creation</li> <li>Upload Validation: File type and size limits</li> </ul>"},{"location":"admin-guide/security/#audit-compliance","title":"Audit &amp; Compliance","text":""},{"location":"admin-guide/security/#audit-logging","title":"Audit Logging","text":"<p>Comprehensive audit trail for:</p> <ul> <li>User Actions: Login, logout, configuration changes</li> <li>Data Access: Hashlist views, downloads</li> <li>Administrative Actions: User management, system configuration</li> <li>Security Events: Failed logins, MFA failures, suspicious activity</li> <li>Retention Operations: All deletion operations with details</li> </ul>"},{"location":"admin-guide/security/#compliance-features","title":"Compliance Features","text":"<ul> <li>Data Retention Policies: Configurable per regulations</li> <li>Secure Deletion: Meets data destruction requirements</li> <li>Access Logs: Complete access trail for auditing</li> <li>Export Controls: Restricted data export capabilities</li> </ul>"},{"location":"admin-guide/security/#security-monitoring","title":"Security Monitoring","text":"<p>Monitor these key metrics:</p> <ol> <li>Failed Login Attempts: Potential brute force attacks</li> <li>API Rate Limit Hits: Possible abuse</li> <li>Agent Disconnections: Network or security issues</li> <li>Retention Purge Logs: Verify proper data deletion</li> <li>Database VACUUM Status: Ensure WAL cleanup</li> </ol>"},{"location":"admin-guide/security/#log-retention","title":"Log Retention","text":"<ul> <li>Security Logs: Retain for compliance period</li> <li>Audit Logs: Permanent retention recommended</li> <li>System Logs: Rotate based on size/age</li> <li>Backup Logs: Match backup retention period</li> </ul>"},{"location":"admin-guide/security/#security-best-practices","title":"Security Best Practices","text":""},{"location":"admin-guide/security/#regular-tasks","title":"Regular Tasks","text":"<ol> <li>Weekly</li> <li>Review security event logs</li> <li>Check failed login attempts</li> <li> <p>Verify agent connectivity</p> </li> <li> <p>Monthly</p> </li> <li>Audit user access and roles</li> <li>Review retention policy compliance</li> <li> <p>Test backup restoration</p> </li> <li> <p>Quarterly</p> </li> <li>Update TLS certificates</li> <li>Review and update passwords</li> <li>Security assessment</li> </ol>"},{"location":"admin-guide/security/#emergency-procedures","title":"Emergency Procedures","text":"<ol> <li>Suspected Breach</li> <li>Disable affected accounts</li> <li>Revoke all JWT tokens</li> <li>Review audit logs</li> <li> <p>Change system passwords</p> </li> <li> <p>Data Leak</p> </li> <li>Identify affected hashlists</li> <li>Trigger immediate retention purge</li> <li>Notify affected clients</li> <li> <p>Document incident</p> </li> <li> <p>Agent Compromise</p> </li> <li>Revoke agent API key</li> <li>Remove agent from system</li> <li>Audit agent's job history</li> <li>Regenerate claim codes</li> </ol>"},{"location":"admin-guide/security/#conclusion","title":"Conclusion","text":"<p>Security in KrakenHashes is multi-layered, from secure data deletion in the retention system to comprehensive authentication and audit trails. Regular monitoring and adherence to these security practices ensures data protection and compliance with security requirements.</p>"},{"location":"admin-guide/advanced/chunking/","title":"KrakenHashes Chunking System","text":""},{"location":"admin-guide/advanced/chunking/#overview","title":"Overview","text":"<p>KrakenHashes uses an intelligent chunking system to distribute password cracking workloads across multiple agents. This document explains how chunks are created, distributed, and tracked for different attack types.</p>"},{"location":"admin-guide/advanced/chunking/#what-is-chunking","title":"What is Chunking?","text":"<p>Chunking divides large password cracking jobs into smaller, manageable pieces that can be: - Distributed across multiple agents for parallel processing - Completed within a reasonable time frame (default: 20 minutes) - Resumed if interrupted or failed - Tracked for accurate progress reporting</p>"},{"location":"admin-guide/advanced/chunking/#how-chunking-works","title":"How Chunking Works","text":""},{"location":"admin-guide/advanced/chunking/#basic-chunking-no-rules","title":"Basic Chunking (No Rules)","text":"<p>For simple dictionary attacks without rules: 1. The system calculates the total keyspace (number of password candidates) 2. Based on agent benchmark speeds, it determines optimal chunk sizes 3. Each chunk processes a portion of the wordlist using hashcat's <code>--skip</code> and <code>--limit</code> parameters</p> <p>Example:  - Wordlist: 1,000,000 passwords - Agent speed: 1,000,000 H/s - Target chunk time: 1,200 seconds (20 minutes) - Chunk size: 1,200,000,000 candidates - Result: Single chunk processes entire wordlist</p>"},{"location":"admin-guide/advanced/chunking/#enhanced-chunking-with-rules","title":"Enhanced Chunking with Rules","text":"<p>When rules are applied, the effective keyspace multiplies:</p> <p>Effective Keyspace = Wordlist Size \u00d7 Number of Rules</p> <p>For example: - Wordlist: 1,000,000 passwords - Rules: 1,000 rules - Effective keyspace: 1,000,000,000 candidates</p>"},{"location":"admin-guide/advanced/chunking/#rule-splitting","title":"Rule Splitting","text":"<p>When a job with rules would take significantly longer than the target chunk time, KrakenHashes can split the rules:</p> <ol> <li>Detection: If estimated time &gt; 2\u00d7 target chunk time</li> <li>Splitting: Divides rules into smaller files</li> <li>Distribution: Each agent receives full wordlist + partial rules</li> <li>Progress: Tracks completion across all rule chunks</li> </ol> <p>Example: - Wordlist: 1,000,000 passwords - Rules: 10,000 rules - Agent speed: 1,000,000 H/s - Without splitting: 10,000 seconds (2.8 hours) per chunk - With splitting into 10 chunks: 1,000 rules each, ~1,000 seconds per chunk</p>"},{"location":"admin-guide/advanced/chunking/#combination-attacks","title":"Combination Attacks","text":"<p>For combination attacks (-a 1), the effective keyspace is:</p> <p>Effective Keyspace = Wordlist1 Size \u00d7 Wordlist2 Size</p> <p>The system tracks progress through the virtual keyspace while hashcat processes the first wordlist sequentially.</p>"},{"location":"admin-guide/advanced/chunking/#attack-mode-support","title":"Attack Mode Support","text":"Attack Mode Description Chunking Method 0 (Straight) Dictionary Wordlist position + optional rule splitting 1 (Combination) Two wordlists Virtual keyspace tracking 3 (Brute-force) Mask attack Mask position chunking 6 (Hybrid W+M) Wordlist + Mask Wordlist position chunking 7 (Hybrid M+W) Mask + Wordlist Mask position chunking 9 (Association) Per-hash rules Rule splitting when applicable"},{"location":"admin-guide/advanced/chunking/#progress-tracking","title":"Progress Tracking","text":""},{"location":"admin-guide/advanced/chunking/#standard-progress","title":"Standard Progress","text":"<ul> <li>Shows candidates tested vs total keyspace</li> <li>Updates in real-time via WebSocket</li> <li>Accurate percentage completion</li> </ul>"},{"location":"admin-guide/advanced/chunking/#with-rule-multiplication","title":"With Rule Multiplication","text":"<ul> <li>Display format: \"X / Y (\u00d7Z)\" where Z is the multiplication factor</li> <li>Accounts for all rules across all chunks</li> <li>Aggregates progress from distributed rule chunks</li> </ul>"},{"location":"admin-guide/advanced/chunking/#progress-bar-visualization","title":"Progress Bar Visualization","text":"<p>The progress bar always shows: - Green: Completed keyspace - Gray: Remaining keyspace - Percentage: Based on effective keyspace</p>"},{"location":"admin-guide/advanced/chunking/#configuration","title":"Configuration","text":"<p>Administrators can tune chunking behavior via system settings:</p> Setting Default Description <code>default_chunk_duration</code> 1200s Target time per chunk (20 minutes) <code>chunk_fluctuation_percentage</code> 20% Threshold for merging final chunks <code>rule_split_enabled</code> true Enable automatic rule splitting <code>rule_split_threshold</code> 2.0 Time multiplier to trigger splitting <code>rule_split_min_rules</code> 100 Minimum rules before considering split"},{"location":"admin-guide/advanced/chunking/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/advanced/chunking/#for-users","title":"For Users","text":"<ol> <li>Large Rule Files: Will automatically split for better distribution</li> <li>Multiple Rule Files: Multiplication is handled automatically</li> <li>Progress Monitoring: Check effective keyspace in job details</li> <li>Benchmarks: Ensure agents have current benchmarks for accurate chunking</li> </ol>"},{"location":"admin-guide/advanced/chunking/#for-administrators","title":"For Administrators","text":"<ol> <li>Chunk Duration: Balance between progress granularity and overhead</li> <li>Rule Splitting: Monitor temp directory space for large rule files</li> <li>Benchmarks: Configure benchmark validity period appropriately</li> <li>Resource Usage: Rule splitting creates temporary files</li> </ol>"},{"location":"admin-guide/advanced/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/chunking/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check if effective keyspace is much larger than expected</li> <li>Verify agent benchmarks are current</li> <li>Consider enabling rule splitting if disabled</li> </ul>"},{"location":"admin-guide/advanced/chunking/#uneven-distribution","title":"Uneven Distribution","text":"<ul> <li>Some chunks may be larger due to:</li> <li>Fluctuation threshold preventing small final chunks</li> <li>Rule count not evenly divisible</li> <li>Different agent speeds</li> </ul>"},{"location":"admin-guide/advanced/chunking/#rule-splitting-not-occurring","title":"Rule Splitting Not Occurring","text":"<p>Verify: - <code>rule_split_enabled</code> is true - Rule file has &gt; <code>rule_split_min_rules</code> rules - Estimated time exceeds threshold</p>"},{"location":"admin-guide/advanced/chunking/#technical-details","title":"Technical Details","text":""},{"location":"admin-guide/advanced/chunking/#keyspace-calculation","title":"Keyspace Calculation","text":"<pre><code>Attack Mode 0 (Dictionary):\n- Without rules: wordlist_size\n- With rules: wordlist_size \u00d7 total_rule_count\n\nAttack Mode 1 (Combination):\n- Always: wordlist1_size \u00d7 wordlist2_size\n\nAttack Mode 3 (Brute-force):\n- Calculated from mask: charset_size^length\n\nAttack Mode 6/7 (Hybrid):\n- Wordlist_size \u00d7 mask_keyspace\n</code></pre>"},{"location":"admin-guide/advanced/chunking/#chunk-assignment","title":"Chunk Assignment","text":"<ol> <li>Agent requests work</li> <li>System calculates optimal chunk size based on:</li> <li>Agent's benchmark speed</li> <li>Target chunk duration</li> <li>Remaining keyspace</li> <li>Chunk boundaries determined:</li> <li>Start position (skip)</li> <li>Chunk size (limit)</li> <li>Agent receives chunk assignment</li> <li>Progress tracked and aggregated</li> </ol>"},{"location":"admin-guide/advanced/chunking/#rule-chunk-files","title":"Rule Chunk Files","text":"<p>When rule splitting is active: - Temporary files created in configured directory - Named: <code>job_[ID]_chunk_[N].rule</code> - Automatically cleaned up after job completion - Synced to agents like normal rule files</p>"},{"location":"admin-guide/advanced/chunking/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Pre-calculation of optimal chunk distribution</li> <li>Dynamic chunk resizing based on actual speed</li> <li>Rule deduplication before splitting</li> <li>Compression for rule chunk transfers</li> </ul>"},{"location":"admin-guide/advanced/job-priority/","title":"Job Priority and Interruption System","text":""},{"location":"admin-guide/advanced/job-priority/#overview","title":"Overview","text":"<p>KrakenHashes implements a sophisticated priority system that ensures critical password auditing tasks receive the resources they need. The system supports priority-based scheduling, automatic job interruption, and intelligent resource allocation to optimize your password cracking operations.</p>"},{"location":"admin-guide/advanced/job-priority/#priority-system-fundamentals","title":"Priority System Fundamentals","text":""},{"location":"admin-guide/advanced/job-priority/#priority-scale","title":"Priority Scale","text":"<p>Jobs in KrakenHashes use a priority scale from 0 to 100:</p> <ul> <li>Critical Priority (90-100): Emergency response, security incidents</li> <li>High Priority (70-89): Time-sensitive audits, compliance deadlines</li> <li>Normal Priority (40-69): Standard security assessments</li> <li>Low Priority (10-39): Background processing, research tasks</li> <li>Minimal Priority (0-9): Non-urgent, opportunistic processing</li> </ul>"},{"location":"admin-guide/advanced/job-priority/#how-priority-affects-job-execution","title":"How Priority Affects Job Execution","text":"<ol> <li>Job Selection Order: Higher priority jobs are assigned to agents first</li> <li>Resource Allocation: High priority jobs can use more agents simultaneously</li> <li>Queue Position: Within the same priority level, jobs follow FIFO (First-In-First-Out)</li> <li>Interruption Rights: Jobs with high priority override can interrupt lower priority running jobs</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#high-priority-override-feature","title":"High Priority Override Feature","text":""},{"location":"admin-guide/advanced/job-priority/#what-is-high-priority-override","title":"What is High Priority Override?","text":"<p>The high priority override feature allows critical jobs to interrupt lower priority jobs that are currently running. This ensures that urgent tasks don't have to wait for long-running, low-priority jobs to complete.</p>"},{"location":"admin-guide/advanced/job-priority/#when-to-enable-high-priority-override","title":"When to Enable High Priority Override","text":"<p>Enable this feature for jobs that: - Respond to active security incidents - Have strict compliance deadlines - Require immediate results for business-critical decisions - Support time-sensitive investigations</p>"},{"location":"admin-guide/advanced/job-priority/#how-it-works","title":"How It Works","text":"<ol> <li>Trigger Condition: Interruption only occurs when:</li> <li>No agents are available for assignment</li> <li>A high-priority job with override enabled is waiting</li> <li> <p>Lower priority jobs are currently running</p> </li> <li> <p>Interruption Process:</p> </li> <li>System identifies the lowest priority running job</li> <li>Sends stop command to agents working on that job</li> <li>Moves interrupted job to \"pending\" status (not paused)</li> <li> <p>Assigns freed agents to the high-priority job</p> </li> <li> <p>Automatic Resumption:</p> </li> <li>Interrupted jobs automatically resume when agents become available</li> <li>Jobs maintain their progress and continue from where they stopped</li> <li>No manual intervention required</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#configuration","title":"Configuration","text":"<p>To enable high priority override for a preset job:</p> <ol> <li>Navigate to Jobs \u2192 Preset Jobs \u2192 [Job Name]</li> <li>In the Advanced Settings section, toggle \"Allow High Priority Override\"</li> <li>Set an appropriate priority level (typically 70+)</li> <li>Save the preset job</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#job-interruption-behavior","title":"Job Interruption Behavior","text":""},{"location":"admin-guide/advanced/job-priority/#status-transitions","title":"Status Transitions","text":"<p>When a job is interrupted: - Before: Status = \"running\" - During Interruption: Status changes to \"pending\" - After Resumption: Status returns to \"running\"</p>"},{"location":"admin-guide/advanced/job-priority/#what-happens-to-interrupted-jobs","title":"What Happens to Interrupted Jobs?","text":"<ol> <li>Progress Preserved: All completed work is saved</li> <li>Automatic Queue Return: Job returns to pending queue with same priority</li> <li>Smart Resumption: Job continues from last checkpoint, no work repeated</li> <li>Agent Cleanup: Agents properly release resources and become available</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#system-wide-interruption-control","title":"System-Wide Interruption Control","text":"<p>Administrators can enable or disable job interruption globally:</p> <ol> <li>Navigate to Admin Panel \u2192 System Settings</li> <li>Find \"Job Interruption Enabled\" setting</li> <li>Toggle to enable/disable interruption system-wide</li> </ol> <p>When disabled: - No jobs will be interrupted regardless of priority - High priority jobs wait in queue normally - System operates in strict FIFO mode within priority levels</p>"},{"location":"admin-guide/advanced/job-priority/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/advanced/job-priority/#setting-appropriate-priorities","title":"Setting Appropriate Priorities","text":""},{"location":"admin-guide/advanced/job-priority/#security-incident-response-priority-95-100","title":"Security Incident Response (Priority: 95-100)","text":"<pre><code>Priority: 100\nAllow High Priority Override: Yes\nMax Agents: Unlimited\nReason: Immediate threat mitigation required\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#compliance-audit-due-today-priority-80-90","title":"Compliance Audit - Due Today (Priority: 80-90)","text":"<pre><code>Priority: 85\nAllow High Priority Override: Yes\nMax Agents: 10\nReason: Regulatory deadline approaching\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#weekly-security-assessment-priority-50-60","title":"Weekly Security Assessment (Priority: 50-60)","text":"<pre><code>Priority: 55\nAllow High Priority Override: No\nMax Agents: 5\nReason: Routine scheduled assessment\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#research-project-priority-10-20","title":"Research Project (Priority: 10-20)","text":"<pre><code>Priority: 15\nAllow High Priority Override: No\nMax Agents: 2\nReason: Long-term analysis, no deadline\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#priority-strategy-guidelines","title":"Priority Strategy Guidelines","text":"<ol> <li>Reserve High Priorities: Don't use high priorities for routine tasks</li> <li>Consider Business Impact: Align priority with actual business urgency</li> <li>Plan for Interruptions: Design workflows assuming possible interruptions</li> <li>Monitor Resource Usage: Track how priority affects overall throughput</li> <li>Document Priority Decisions: Maintain a priority assignment guide for your team</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#avoiding-priority-inflation","title":"Avoiding Priority Inflation","text":"<p>To prevent \"priority creep\" where all jobs become high priority:</p> <ol> <li>Establish Clear Criteria: Document what qualifies for each priority level</li> <li>Regular Review: Audit priority assignments monthly</li> <li>Default to Normal: Start with priority 50 unless justified otherwise</li> <li>Limit Override Usage: Only enable override for truly critical jobs</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#priority-in-workflows","title":"Priority in Workflows","text":""},{"location":"admin-guide/advanced/job-priority/#workflow-priority-inheritance","title":"Workflow Priority Inheritance","text":"<p>When jobs are created from workflows: 1. Each preset job maintains its configured priority 2. Jobs execute in priority order within the workflow 3. Higher priority jobs from other workflows can interleave</p>"},{"location":"admin-guide/advanced/job-priority/#example-workflow-priority-design","title":"Example Workflow Priority Design","text":"<pre><code>Emergency Response Workflow:\n\u251c\u2500\u2500 Quick Dictionary (Priority: 95)\n\u251c\u2500\u2500 Common Patterns (Priority: 90)\n\u251c\u2500\u2500 Extended Dictionary (Priority: 85)\n\u2514\u2500\u2500 Brute Force Backup (Priority: 80)\n\nStandard Audit Workflow:\n\u251c\u2500\u2500 Leaked Passwords (Priority: 60)\n\u251c\u2500\u2500 Company Variations (Priority: 55)\n\u251c\u2500\u2500 Rule-Based Attack (Priority: 50)\n\u2514\u2500\u2500 Comprehensive Check (Priority: 45)\n</code></pre>"},{"location":"admin-guide/advanced/job-priority/#monitoring-priority-impact","title":"Monitoring Priority Impact","text":""},{"location":"admin-guide/advanced/job-priority/#key-metrics-to-track","title":"Key Metrics to Track","text":"<ol> <li>Interruption Frequency: How often jobs are interrupted</li> <li>Wait Time by Priority: Average wait time per priority level</li> <li>Completion Time Impact: Effect of interruptions on job completion</li> <li>Resource Utilization: Agent usage across priority levels</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#identifying-issues","title":"Identifying Issues","text":"<p>Watch for these warning signs: - Frequent interruptions of the same job - Low priority jobs never completing - All jobs set to high priority - Agents constantly switching between jobs</p>"},{"location":"admin-guide/advanced/job-priority/#advanced-scenarios","title":"Advanced Scenarios","text":""},{"location":"admin-guide/advanced/job-priority/#multi-tenant-environments","title":"Multi-Tenant Environments","text":"<p>For systems serving multiple teams or clients:</p> <ol> <li>Priority Ranges per Tenant: Assign priority bands to each tenant</li> <li>Fair Resource Sharing: Implement quotas alongside priorities</li> <li>Override Restrictions: Limit override capability to specific roles</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#scheduled-priority-changes","title":"Scheduled Priority Changes","text":"<p>For jobs that change priority over time:</p> <ol> <li>Escalation: Increase priority as deadlines approach</li> <li>De-escalation: Reduce priority after peak hours</li> <li>Time-Based Rules: Automate priority adjustments based on schedule</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#emergency-override-procedures","title":"Emergency Override Procedures","text":"<p>For critical incidents requiring immediate resources:</p> <ol> <li>Emergency Priority (100): Reserved for security incidents</li> <li>Administrative Override: Allow admins to force interrupt any job</li> <li>Audit Trail: Log all emergency overrides for review</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/job-priority/#job-not-interrupting-lower-priority-work","title":"Job Not Interrupting Lower Priority Work","text":"<p>Check: 1. Is \"Job Interruption Enabled\" in system settings? 2. Does the job have \"Allow High Priority Override\" enabled? 3. Are there actually lower priority jobs running? 4. Do the running jobs allow interruption?</p>"},{"location":"admin-guide/advanced/job-priority/#interrupted-job-not-resuming","title":"Interrupted Job Not Resuming","text":"<p>Verify: 1. Job status is \"pending\" not \"failed\" 2. Agents are available and online 3. No higher priority jobs in queue 4. Job hasn't exceeded retry limits</p>"},{"location":"admin-guide/advanced/job-priority/#excessive-interruptions","title":"Excessive Interruptions","text":"<p>Solutions: 1. Review and adjust priority assignments 2. Increase agent capacity 3. Implement scheduling to reduce contention 4. Consider priority bands to limit interruption cascades</p>"},{"location":"admin-guide/advanced/job-priority/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/advanced/job-priority/#impact-on-system-performance","title":"Impact on System Performance","text":"<ul> <li>Minimal Overhead: Priority checks are lightweight</li> <li>Interruption Cost: ~5-10 seconds to stop and reassign</li> <li>Progress Tracking: Checkpoint frequency affects resumption granularity</li> </ul>"},{"location":"admin-guide/advanced/job-priority/#optimizing-for-priority-systems","title":"Optimizing for Priority Systems","text":"<ol> <li>Appropriate Chunk Sizes: Smaller chunks (5-10 minutes) for better interruption response</li> <li>Checkpoint Frequency: Balance between progress saving and performance</li> <li>Agent Pool Size: More agents reduce need for interruptions</li> <li>Priority Distribution: Spread priorities to reduce conflicts</li> </ol>"},{"location":"admin-guide/advanced/job-priority/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"admin-guide/advanced/job-priority/#agent-scheduling","title":"Agent Scheduling","text":"<p>Priority system works with agent scheduling: - Scheduled agents only available during defined hours - Priority determines job selection within available windows - Interruptions respect scheduling boundaries</p>"},{"location":"admin-guide/advanced/job-priority/#max-agents-limits","title":"Max Agents Limits","text":"<p>Priority interacts with max agent settings: - High priority jobs reach max agents first - Lower priority jobs use remaining capacity - Override can free agents even from max-agent-limited jobs</p>"},{"location":"admin-guide/advanced/job-priority/#resource-management","title":"Resource Management","text":"<p>Priority affects resource allocation: - File sync prioritizes high-priority job requirements - Binary selection considers job priority - Wordlist/rule loading optimized for high-priority jobs</p>"},{"location":"admin-guide/advanced/job-priority/#summary","title":"Summary","text":"<p>The KrakenHashes priority and interruption system provides powerful tools for managing competing password auditing demands. By understanding and properly configuring priorities, you can ensure critical tasks complete quickly while maintaining efficient resource utilization across all jobs.</p> <p>Key takeaways: - Use priority levels that reflect actual business urgency - Enable high priority override only for critical jobs - Monitor interruption patterns to optimize settings - Design workflows with priority strategies in mind - Maintain clear documentation of priority policies</p>"},{"location":"admin-guide/advanced/performance/","title":"Performance Tuning Guide","text":"<p>This guide provides comprehensive performance optimization strategies for KrakenHashes deployments. All recommendations are based on the actual implementation details and system architecture.</p>"},{"location":"admin-guide/advanced/performance/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Database Performance</li> <li>Job Execution Optimization</li> <li>Agent Performance</li> <li>File I/O Optimization</li> <li>Network and WebSocket Tuning</li> <li>Storage Performance</li> <li>Monitoring and Benchmarking</li> <li>System Settings Reference</li> </ol>"},{"location":"admin-guide/advanced/performance/#database-performance","title":"Database Performance","text":""},{"location":"admin-guide/advanced/performance/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<p>The system uses PostgreSQL with connection pooling. Current default settings:</p> <pre><code>// From backend/internal/db/db.go\ndb.SetMaxOpenConns(25)      // Maximum number of open connections\ndb.SetMaxIdleConns(5)       // Maximum number of idle connections\ndb.SetConnMaxLifetime(5 * time.Minute)  // Connection lifetime\n</code></pre> <p>Optimization recommendations:</p> <ol> <li> <p>For small deployments (1-10 agents): <pre><code># Keep defaults\nMAX_OPEN_CONNS=25\nMAX_IDLE_CONNS=5\n</code></pre></p> </li> <li> <p>For medium deployments (10-50 agents): <pre><code># Increase connection pool\nMAX_OPEN_CONNS=50\nMAX_IDLE_CONNS=10\n</code></pre></p> </li> <li> <p>For large deployments (50+ agents): <pre><code># Use external connection pooler (PgBouncer)\nMAX_OPEN_CONNS=100\nMAX_IDLE_CONNS=20\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#index-optimization","title":"Index Optimization","text":"<p>The system uses extensive indexing. Key performance indexes:</p> <pre><code>-- Agent performance queries\nidx_agents_status\nidx_agents_last_heartbeat\nidx_agent_performance_metrics_device_lookup (composite)\n\n-- Job execution queries\nidx_job_tasks_job_chunk (composite)\nidx_job_tasks_status\nidx_job_executions_status\n\n-- Hash lookups\nidx_hashes_hash_value (unique)\nidx_hashlists_status\n</code></pre> <p>Monitoring index usage:</p> <pre><code>-- Check index usage statistics\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nORDER BY idx_scan DESC;\n\n-- Find missing indexes\nSELECT \n    schemaname,\n    tablename,\n    attname,\n    n_distinct,\n    most_common_vals\nFROM pg_stats\nWHERE schemaname = 'public'\nAND n_distinct &gt; 100\nAND tablename NOT IN (\n    SELECT tablename \n    FROM pg_indexes \n    WHERE schemaname = 'public'\n);\n</code></pre>"},{"location":"admin-guide/advanced/performance/#query-optimization","title":"Query Optimization","text":"<ol> <li> <p>Batch Processing Configuration: <pre><code># Environment variable\nexport KH_HASHLIST_BATCH_SIZE=1000  # Default: 1000\n\n# Recommendations:\n# - Small hashlists (&lt;100K): 500\n# - Medium hashlists (100K-1M): 1000\n# - Large hashlists (&gt;1M): 2000-5000\n</code></pre></p> </li> <li> <p>Pagination Optimization:</p> </li> <li>Use cursor-based pagination for large result sets</li> <li>Limit page sizes to 100-500 items</li> <li>Add appropriate indexes for ORDER BY columns</li> </ol>"},{"location":"admin-guide/advanced/performance/#job-execution-optimization","title":"Job Execution Optimization","text":""},{"location":"admin-guide/advanced/performance/#chunking-system-configuration","title":"Chunking System Configuration","text":"<p>The dynamic chunking system optimizes workload distribution based on agent performance.</p> <p>Key system settings:</p> <pre><code>-- Configure chunk behavior\nUPDATE system_settings SET value = '20' \nWHERE key = 'chunk_fluctuation_percentage';  -- Default: 20%\n\n-- Benchmark cache duration\nUPDATE system_settings SET value = '168' \nWHERE key = 'benchmark_cache_duration_hours';  -- Default: 168 (7 days)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#chunk-size-calculation","title":"Chunk Size Calculation","text":"<p>The system calculates chunks based on: 1. Agent benchmark speeds 2. Desired chunk duration 3. Attack mode modifiers 4. Available keyspace</p> <p>Optimization strategies:</p> <ol> <li> <p>For GPU clusters with similar performance: <pre><code>-- Larger chunks, less overhead\nUPDATE job_executions \nSET chunk_duration = 3600  -- 1 hour chunks\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>For mixed hardware environments: <pre><code>-- Smaller chunks for better distribution\nUPDATE job_executions \nSET chunk_duration = 900  -- 15 minute chunks\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>For time-sensitive jobs: <pre><code>-- Very small chunks for quick feedback\nUPDATE job_executions \nSET chunk_duration = 300  -- 5 minute chunks\nWHERE id = ?;\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#job-priority-and-scheduling","title":"Job Priority and Scheduling","text":"<p>The scheduler processes jobs based on priority and available agents:</p> <ol> <li>Priority levels:</li> <li>Critical: Process immediately</li> <li>High: Process within minutes</li> <li>Normal: Standard queue processing</li> <li> <p>Low: Process when resources available</p> </li> <li> <p>Scheduling optimization: <pre><code>-- Ensure proper job distribution\nUPDATE system_settings \nSET value = '30' \nWHERE key = 'scheduler_check_interval_seconds';\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#agent-performance","title":"Agent Performance","text":""},{"location":"admin-guide/advanced/performance/#hardware-detection-and-benchmarking","title":"Hardware Detection and Benchmarking","text":"<p>The system automatically detects GPU capabilities and runs benchmarks.</p> <p>Benchmark optimization:</p> <pre><code>-- Configure speedtest timeout\nUPDATE system_settings \nSET value = '180'  -- 3 minutes\nWHERE key = 'speedtest_timeout_seconds';\n\n-- For faster benchmarks (less accurate)\nUPDATE system_settings \nSET value = '60'  -- 1 minute\nWHERE key = 'speedtest_timeout_seconds';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#agent-configuration","title":"Agent Configuration","text":"<ol> <li> <p>GPU-specific optimizations: <pre><code># Agent config.yaml\nextra_parameters: \"--optimize-kernel-workload --force\"\n\n# For NVIDIA GPUs\nextra_parameters: \"-O -w 4\"\n\n# For AMD GPUs\nextra_parameters: \"-O -w 3\"\n</code></pre></p> </li> <li> <p>Memory management: <pre><code># Limit GPU memory usage\nextra_parameters: \"--gpu-memory-fraction=0.8\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#workload-distribution","title":"Workload Distribution","text":"<p>The system supports multiple distribution strategies:</p> <ol> <li>Round-robin: Equal distribution</li> <li>Performance-based: More work to faster agents</li> <li>Priority-based: Specific agents for specific jobs</li> </ol>"},{"location":"admin-guide/advanced/performance/#file-io-optimization","title":"File I/O Optimization","text":""},{"location":"admin-guide/advanced/performance/#hash-list-processing","title":"Hash List Processing","text":"<p>The system uses buffered I/O and batch processing for efficient file handling.</p> <p>Current implementation: - Buffered reading with <code>bufio.Scanner</code> - Configurable batch sizes - Streaming processing (no full file load)</p> <p>Optimization tips:</p> <ol> <li> <p>For NVMe storage: <pre><code>export KH_HASHLIST_BATCH_SIZE=5000  # Larger batches\n</code></pre></p> </li> <li> <p>For network storage: <pre><code>export KH_HASHLIST_BATCH_SIZE=500   # Smaller batches\n</code></pre></p> </li> <li> <p>File upload limits: <pre><code>export KH_MAX_UPLOAD_SIZE_MB=32     # Default: 32MB\n# Increase for trusted environments\nexport KH_MAX_UPLOAD_SIZE_MB=256    # 256MB\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#file-synchronization","title":"File Synchronization","text":"<p>The agent file sync system uses: - Chunk-based transfers - Resume capability - Integrity verification</p> <p>Optimization:</p> <ol> <li>LAN deployments:</li> <li>Increase chunk sizes</li> <li> <p>Disable compression</p> </li> <li> <p>WAN deployments:</p> </li> <li>Enable compression</li> <li>Smaller chunk sizes</li> <li>More aggressive retry policies</li> </ol>"},{"location":"admin-guide/advanced/performance/#network-and-websocket-tuning","title":"Network and WebSocket Tuning","text":""},{"location":"admin-guide/advanced/performance/#websocket-configuration","title":"WebSocket Configuration","text":"<p>The system uses WebSocket for real-time agent communication.</p> <p>Key optimizations:</p> <ol> <li>Message processing:</li> <li>Asynchronous handlers for non-blocking operation</li> <li>Goroutine-based processing for heavy operations</li> <li> <p>30-second timeout for async operations</p> </li> <li> <p>Heartbeat optimization: <pre><code>// Agent sends heartbeat every 30 seconds\n// Server expects heartbeat within 90 seconds\n</code></pre></p> </li> <li> <p>Connection management: <pre><code># Nginx configuration for WebSocket\nproxy_read_timeout 3600s;\nproxy_send_timeout 3600s;\nproxy_connect_timeout 60s;\n\n# Buffer sizes\nproxy_buffer_size 64k;\nproxy_buffers 8 32k;\nproxy_busy_buffers_size 128k;\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#tlsssl-performance","title":"TLS/SSL Performance","text":"<p>The system supports multiple TLS modes with configurable parameters:</p> <pre><code># Certificate configuration\nexport KH_CERT_KEY_SIZE=2048        # Faster handshakes\n# or\nexport KH_CERT_KEY_SIZE=4096        # Better security\n\n# For high-traffic deployments\nexport KH_TLS_SESSION_CACHE=on\nexport KH_TLS_SESSION_TIMEOUT=300\n</code></pre>"},{"location":"admin-guide/advanced/performance/#storage-performance","title":"Storage Performance","text":""},{"location":"admin-guide/advanced/performance/#directory-structure-optimization","title":"Directory Structure Optimization","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/      # Hashcat binaries (SSD recommended)\n\u251c\u2500\u2500 wordlists/     # Large wordlists (HDD acceptable)\n\u251c\u2500\u2500 rules/         # Rule files (SSD preferred)\n\u251c\u2500\u2500 hashlists/     # User hashlists (SSD recommended)\n\u2514\u2500\u2500 temp/          # Temporary files (RAM disk optimal)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#storage-recommendations","title":"Storage Recommendations","text":"<ol> <li>SSD for critical paths:</li> <li>Database files</li> <li>Hashcat binaries</li> <li>Active hashlists</li> <li> <p>Temporary processing</p> </li> <li> <p>HDD acceptable for:</p> </li> <li>Large wordlist storage</li> <li>Archived hashlists</li> <li> <p>Backup data</p> </li> <li> <p>RAM disk for temporary files: <pre><code># Create RAM disk for temp files\nsudo mkdir -p /mnt/ramdisk\nsudo mount -t tmpfs -o size=2G tmpfs /mnt/ramdisk\n\n# Link to KrakenHashes temp\nln -s /mnt/ramdisk /data/krakenhashes/temp\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#monitoring-and-benchmarking","title":"Monitoring and Benchmarking","text":""},{"location":"admin-guide/advanced/performance/#metrics-collection-and-retention","title":"Metrics Collection and Retention","text":"<p>The system includes automatic metrics aggregation:</p> <pre><code>-- Configure retention\nUPDATE system_settings \nSET value = '30'  -- Keep realtime data for 30 days\nWHERE key = 'metrics_retention_days';\n\n-- Enable/disable aggregation\nUPDATE system_settings \nSET value = 'true' \nWHERE key = 'enable_aggregation';\n</code></pre> <p>Aggregation levels: - Realtime \u2192 Daily (after 24 hours) - Daily \u2192 Weekly (after 7 days) - Cleanup runs daily at 2 AM</p>"},{"location":"admin-guide/advanced/performance/#performance-monitoring-queries","title":"Performance Monitoring Queries","text":"<pre><code>-- Agent performance overview\nSELECT \n    a.name,\n    a.status,\n    COUNT(DISTINCT jt.id) as active_tasks,\n    AVG(apm.hashes_per_second) as avg_speed,\n    MAX(apm.temperature) as max_temp\nFROM agents a\nLEFT JOIN job_tasks jt ON a.id = jt.agent_id AND jt.status = 'in_progress'\nLEFT JOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.created_at &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.id, a.name, a.status;\n\n-- Job execution performance\nSELECT \n    je.id,\n    je.status,\n    je.created_at,\n    je.completed_at,\n    je.progress,\n    COUNT(jt.id) as total_chunks,\n    COUNT(CASE WHEN jt.status = 'completed' THEN 1 END) as completed_chunks\nFROM job_executions je\nLEFT JOIN job_tasks jt ON je.id = jt.job_execution_id\nGROUP BY je.id\nORDER BY je.created_at DESC;\n</code></pre>"},{"location":"admin-guide/advanced/performance/#benchmarking-best-practices","title":"Benchmarking Best Practices","text":"<ol> <li>Initial benchmarking:</li> <li>Run comprehensive benchmarks on agent registration</li> <li>Test all hash types your organization uses</li> <li> <p>Store results for 7 days (default)</p> </li> <li> <p>Periodic re-benchmarking:</p> </li> <li>After driver updates</li> <li>After hardware changes</li> <li> <p>Monthly for consistency</p> </li> <li> <p>Benchmark commands: <pre><code># Force re-benchmark for specific agent\ncurl -X POST https://api.krakenhashes.com/agents/{id}/benchmark \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/advanced/performance/#system-settings-reference","title":"System Settings Reference","text":""},{"location":"admin-guide/advanced/performance/#performance-related-settings","title":"Performance-Related Settings","text":"Setting Key Default Description Optimization Range <code>chunk_fluctuation_percentage</code> 20 Threshold for merging small chunks 10-30% <code>benchmark_cache_duration_hours</code> 168 How long to cache benchmark results 24-720 hours <code>metrics_retention_days</code> 30 Realtime metrics retention 7-90 days <code>enable_aggregation</code> true Enable metrics aggregation true/false <code>speedtest_timeout_seconds</code> 180 Benchmark timeout 60-600 seconds <code>scheduler_check_interval_seconds</code> 30 Job scheduler interval 10-60 seconds"},{"location":"admin-guide/advanced/performance/#environment-variables","title":"Environment Variables","text":"Variable Default Description Optimization Tips <code>KH_HASHLIST_BATCH_SIZE</code> 1000 Database batch insert size 500-5000 based on hardware <code>KH_MAX_UPLOAD_SIZE_MB</code> 32 Maximum file upload size 32-1024 based on trust <code>DATABASE_MAX_OPEN_CONNS</code> 25 Max database connections 25-100 based on load <code>DATABASE_MAX_IDLE_CONNS</code> 5 Max idle connections 20% of max open"},{"location":"admin-guide/advanced/performance/#performance-troubleshooting","title":"Performance Troubleshooting","text":""},{"location":"admin-guide/advanced/performance/#common-bottlenecks","title":"Common Bottlenecks","text":"<ol> <li>Database connection exhaustion:</li> <li>Symptom: \"too many connections\" errors</li> <li> <p>Solution: Increase connection pool or use PgBouncer</p> </li> <li> <p>Slow hash imports:</p> </li> <li>Symptom: Hashlist processing takes hours</li> <li> <p>Solution: Increase batch size, use SSD storage</p> </li> <li> <p>Agent communication delays:</p> </li> <li>Symptom: Delayed job updates</li> <li> <p>Solution: Check network latency, adjust timeouts</p> </li> <li> <p>Memory exhaustion:</p> </li> <li>Symptom: OOM errors during processing</li> <li>Solution: Reduce batch sizes, add swap space</li> </ol>"},{"location":"admin-guide/advanced/performance/#performance-checklist","title":"Performance Checklist","text":"<ul> <li> Database indexes are being used (check pg_stat_user_indexes)</li> <li> Connection pool sized appropriately for agent count</li> <li> Batch sizes optimized for hardware</li> <li> Metrics retention configured</li> <li> Storage using appropriate media (SSD/HDD)</li> <li> Network timeouts adjusted for environment</li> <li> Benchmark cache duration set appropriately</li> <li> Chunk sizes appropriate for job types</li> </ul>"},{"location":"admin-guide/advanced/performance/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"admin-guide/advanced/performance/#small-deployment-1-10-agents","title":"Small Deployment (1-10 agents)","text":"<pre><code># Keep most defaults\nexport KH_HASHLIST_BATCH_SIZE=1000\nexport DATABASE_MAX_OPEN_CONNS=25\n# Use default chunk fluctuation (20%)\n</code></pre>"},{"location":"admin-guide/advanced/performance/#medium-deployment-10-50-agents","title":"Medium Deployment (10-50 agents)","text":"<pre><code>export KH_HASHLIST_BATCH_SIZE=2000\nexport DATABASE_MAX_OPEN_CONNS=50\nexport DATABASE_MAX_IDLE_CONNS=10\n# Adjust chunk fluctuation to 15%\nUPDATE system_settings SET value = '15' WHERE key = 'chunk_fluctuation_percentage';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#large-deployment-50-agents","title":"Large Deployment (50+ agents)","text":"<pre><code>export KH_HASHLIST_BATCH_SIZE=5000\nexport DATABASE_MAX_OPEN_CONNS=100\nexport DATABASE_MAX_IDLE_CONNS=20\n# Use PgBouncer for connection pooling\n# Adjust chunk fluctuation to 10%\nUPDATE system_settings SET value = '10' WHERE key = 'chunk_fluctuation_percentage';\n# Reduce metrics retention\nUPDATE system_settings SET value = '14' WHERE key = 'metrics_retention_days';\n</code></pre>"},{"location":"admin-guide/advanced/performance/#next-steps","title":"Next Steps","text":"<ol> <li>Review current system metrics</li> <li>Identify bottlenecks using monitoring queries</li> <li>Apply appropriate optimizations</li> <li>Monitor impact and adjust</li> <li>Document environment-specific settings</li> </ol> <p>For additional support, consult the System Overview documentation or contact the development team.</p>"},{"location":"admin-guide/advanced/presets/","title":"Preset Jobs and Job Workflows","text":""},{"location":"admin-guide/advanced/presets/#overview","title":"Overview","text":"<p>KrakenHashes provides two powerful features for standardizing and automating password cracking strategies:</p> <ul> <li>Preset Jobs: Pre-configured job templates that define specific attack strategies</li> <li>Job Workflows: Ordered sequences of preset jobs that execute systematically</li> </ul> <p>These features allow administrators to create reusable attack strategies that can be consistently applied across different hashlists, ensuring thorough and efficient password recovery.</p>"},{"location":"admin-guide/advanced/presets/#preset-jobs","title":"Preset Jobs","text":""},{"location":"admin-guide/advanced/presets/#what-are-preset-jobs","title":"What are Preset Jobs?","text":"<p>Preset jobs are templates that encapsulate all the parameters needed for a specific password cracking attack. Think of them as recipes that define: - Which wordlists to use - Which rules to apply - What attack mode to employ - How much priority the job should have - Various execution parameters</p>"},{"location":"admin-guide/advanced/presets/#creating-a-preset-job","title":"Creating a Preset Job","text":"<p>To create a new preset job:</p> <ol> <li>Navigate to Admin &gt; Preset Jobs</li> <li>Click the \"Create Preset Job\" button</li> </ol> <p> <ol> <li>Fill in the preset job details:</li> </ol>"},{"location":"admin-guide/advanced/presets/#basic-information","title":"Basic Information","text":"<ul> <li>Name: A unique, descriptive name (e.g., \"Common Passwords with Rules\")</li> <li>Attack Mode: Select the hashcat attack mode (see Attack Modes section below)</li> <li>Priority: Set execution priority (0-1000, higher = more important)</li> <li>Binary Version: Select the hashcat binary version to use</li> </ul> <p>"},{"location":"admin-guide/advanced/presets/#attack-specific-configuration","title":"Attack-Specific Configuration","text":"<p>Based on the selected attack mode, different fields will appear:</p> <ul> <li>Wordlists: Select one or more wordlists (depending on attack mode)</li> <li>Rules: Select rule files to apply transformations</li> <li>Mask: Define patterns for brute force attacks (e.g., <code>?d?d?d?d</code> for 4 digits)</li> </ul> <p>"},{"location":"admin-guide/advanced/presets/#advanced-options","title":"Advanced Options","text":"<ul> <li>Chunk Size: Time allocation per work unit (default: 900 seconds)</li> <li>Small Job: Check if this is a quick-running job</li> <li>Allow High Priority Override: Enable this job to interrupt lower priority running jobs (see details below)</li> <li>Status Updates: Enable real-time progress reporting</li> </ul>"},{"location":"admin-guide/advanced/presets/#high-priority-override-feature","title":"High Priority Override Feature","text":"<p>The Allow High Priority Override option gives a preset job special privileges in the scheduling system:</p>"},{"location":"admin-guide/advanced/presets/#what-it-does","title":"What It Does","text":"<p>When enabled, this preset job can interrupt lower priority jobs that are currently running, but only when: 1. No agents are available for assignment 2. This job has a higher priority than running jobs 3. The system-wide \"Job Interruption Enabled\" setting is active</p>"},{"location":"admin-guide/advanced/presets/#when-to-use-it","title":"When to Use It","text":"<p>Enable high priority override for jobs that: - Respond to security incidents or breaches - Have strict compliance or legal deadlines - Support time-sensitive investigations - Require immediate results for critical business decisions</p>"},{"location":"admin-guide/advanced/presets/#how-interruption-works","title":"How Interruption Works","text":"<ol> <li>Automatic Process: The system automatically identifies the lowest priority running job</li> <li>Graceful Interruption: Agents receive stop commands and save their progress</li> <li>Status Change: Interrupted jobs change from \"running\" to \"pending\" status</li> <li>Automatic Resumption: Interrupted jobs resume automatically when agents become available</li> <li>No Work Lost: All completed work is preserved and jobs continue from their last checkpoint</li> </ol>"},{"location":"admin-guide/advanced/presets/#best-practices","title":"Best Practices","text":"<ul> <li>Use Sparingly: Reserve this feature for truly critical jobs</li> <li>Set Appropriate Priority: Jobs with override should have priority 70+ to justify interruption</li> <li>Monitor Impact: Track how often jobs are interrupted to ensure system efficiency</li> <li>Document Usage: Maintain clear policies about when to enable this feature</li> </ul>"},{"location":"admin-guide/advanced/presets/#example-scenarios","title":"Example Scenarios","text":"<p>Emergency Incident Response: - Priority: 95 - Allow High Priority Override: \u2713 Yes - Justification: Active security breach requires immediate password analysis</p> <p>Routine Security Audit: - Priority: 50 - Allow High Priority Override: \u2717 No - Justification: Standard assessment with flexible timeline</p> <p>Compliance Deadline Today: - Priority: 85 - Allow High Priority Override: \u2713 Yes - Justification: Regulatory requirement with penalty for non-compliance</p>"},{"location":"admin-guide/advanced/presets/#attack-modes","title":"Attack Modes","text":"<p>KrakenHashes supports six different attack modes:</p>"},{"location":"admin-guide/advanced/presets/#1-straight-attack-mode-0","title":"1. Straight Attack (Mode 0)","text":"<p>The most common dictionary attack with optional rule transformations. - Requirements: 1 wordlist, 0 or more rules - Example: Using <code>rockyou.txt</code> with <code>best64.rule</code></p>"},{"location":"admin-guide/advanced/presets/#2-combination-attack-mode-1","title":"2. Combination Attack (Mode 1)","text":"<p>Combines words from two different wordlists. - Requirements: Exactly 2 wordlists, no rules - Example: Combining <code>firstnames.txt</code> with <code>years.txt</code> to get \"John2023\"</p> <p>"},{"location":"admin-guide/advanced/presets/#3-brute-force-attack-mode-3","title":"3. Brute Force Attack (Mode 3)","text":"<p>Generates passwords based on mask patterns. - Requirements: Mask pattern only, no wordlists - Common Masks:   - <code>?d?d?d?d</code> - 4 digits (0000-9999)   - <code>?l?l?l?l?l?l</code> - 6 lowercase letters   - <code>?u?l?l?l?d?d</code> - Capital + 3 lowercase + 2 digits</p> <p>"},{"location":"admin-guide/advanced/presets/#4-hybrid-wordlist-mask-mode-6","title":"4. Hybrid Wordlist + Mask (Mode 6)","text":"<p>Appends mask-generated characters to dictionary words. - Requirements: 1 wordlist and mask pattern - Example: <code>passwords.txt</code> + <code>?d?d?d</code> = \"password123\"</p>"},{"location":"admin-guide/advanced/presets/#5-hybrid-mask-wordlist-mode-7","title":"5. Hybrid Mask + Wordlist (Mode 7)","text":"<p>Prepends mask-generated characters to dictionary words. - Requirements: 1 wordlist and mask pattern - Example: <code>?d?d?d</code> + <code>passwords.txt</code> = \"123password\"</p>"},{"location":"admin-guide/advanced/presets/#6-association-attack-mode-9","title":"6. Association Attack (Mode 9)","text":"<p>Currently not implemented</p>"},{"location":"admin-guide/advanced/presets/#managing-preset-jobs","title":"Managing Preset Jobs","text":""},{"location":"admin-guide/advanced/presets/#viewing-preset-jobs","title":"Viewing Preset Jobs","text":"<p>The preset jobs list shows: - Name and attack mode - Wordlist and rule counts - Priority level - Binary version - Action buttons (Edit/Delete)</p> <p> Preset Job Management interface showing three configured preset jobs with their attack modes, priorities, agent limits, and resource assignments</p>"},{"location":"admin-guide/advanced/presets/#editing-preset-jobs","title":"Editing Preset Jobs","text":"<ol> <li>Click the Edit button on any preset job</li> <li>Modify the desired fields</li> <li>Click Update Preset Job</li> </ol>"},{"location":"admin-guide/advanced/presets/#deleting-preset-jobs","title":"Deleting Preset Jobs","text":"<ul> <li>Click the Delete button</li> <li>Confirm the deletion</li> <li>Note: You cannot delete preset jobs that are used in workflows</li> </ul>"},{"location":"admin-guide/advanced/presets/#job-workflows","title":"Job Workflows","text":""},{"location":"admin-guide/advanced/presets/#what-are-job-workflows","title":"What are Job Workflows?","text":"<p>Job workflows are ordered sequences of preset jobs that execute one after another. They allow you to: - Create comprehensive attack strategies - Ensure consistent methodology - Prioritize efficient attacks first - Automate complex multi-stage attacks</p>"},{"location":"admin-guide/advanced/presets/#creating-a-job-workflow","title":"Creating a Job Workflow","text":"<ol> <li>Navigate to Admin &gt; Job Workflows</li> <li>Click \"Create Job Workflow\"</li> </ol> <p> Job Workflows interface displaying a workflow with multiple jobs, priority indicators, and interruption capabilities</p> <ol> <li>Enter a workflow name (e.g., \"Standard Password Audit\")</li> <li>Add preset jobs to the workflow:</li> <li>Type in the search box to find preset jobs</li> <li>Click on a preset job to add it as a step</li> <li>Added jobs appear in the steps list below</li> </ol> <p> <ol> <li>Arrange the execution order:</li> <li>Steps are automatically sorted by priority (highest first)</li> <li> <p>Within the same priority, they execute in the order added</p> </li> <li> <p>Click Create Job Workflow</p> </li> </ol>"},{"location":"admin-guide/advanced/presets/#understanding-workflow-execution","title":"Understanding Workflow Execution","text":"<p>When a workflow runs: 1. All preset jobs are queued in priority order 2. Higher priority jobs execute first 3. Jobs with the same priority run in sequence 4. Each job completes before the next begins</p>"},{"location":"admin-guide/advanced/presets/#managing-workflows","title":"Managing Workflows","text":""},{"location":"admin-guide/advanced/presets/#viewing-workflows","title":"Viewing Workflows","text":"<p>The workflow list displays: - Workflow name - Number of steps - Creation date - Action buttons</p> <p>"},{"location":"admin-guide/advanced/presets/#workflow-details","title":"Workflow Details","text":"<p>Click on a workflow name to see: - Complete list of preset jobs in order - Priority of each step - Wordlists and rules for each step</p> <p>"},{"location":"admin-guide/advanced/presets/#editing-workflows","title":"Editing Workflows","text":"<ol> <li>Click Edit on any workflow</li> <li>Add or remove preset jobs</li> <li>Reorder steps as needed</li> <li>Click Update Job Workflow</li> </ol> <p>Note: When updating a workflow, all existing steps are replaced with the new configuration.</p>"},{"location":"admin-guide/advanced/presets/#best-practices_1","title":"Best Practices","text":""},{"location":"admin-guide/advanced/presets/#preset-job-design","title":"Preset Job Design","text":"<ol> <li>Start Simple: Begin with common, fast attacks</li> <li>Straight attack with common passwords</li> <li> <p>Small, targeted wordlists with effective rules</p> </li> <li> <p>Progressive Complexity: Order jobs from fast/likely to slow/exhaustive</p> </li> <li>Priority 100: Common passwords</li> <li>Priority 80: Leaked password lists</li> <li>Priority 60: Hybrid attacks</li> <li>Priority 40: Targeted brute force</li> <li> <p>Priority 20: Exhaustive searches</p> </li> <li> <p>Naming Conventions: Use descriptive names that indicate:</p> </li> <li>Attack type</li> <li>Target pattern</li> <li>Approximate runtime</li> </ol>"},{"location":"admin-guide/advanced/presets/#workflow-strategy","title":"Workflow Strategy","text":"<ol> <li>Standard Workflows: Create templates for common scenarios</li> <li>\"Quick Audit\" - Fast, high-probability attacks</li> <li>\"Comprehensive Audit\" - Thorough multi-day approach</li> <li> <p>\"Compliance Check\" - Specific policy violations</p> </li> <li> <p>Resource Management:</p> </li> <li>Group small jobs together</li> <li>Save intensive attacks for last</li> <li> <p>Use chunk sizes appropriate to job complexity</p> </li> <li> <p>Monitoring: Enable status updates for long-running jobs</p> </li> </ol>"},{"location":"admin-guide/advanced/presets/#examples","title":"Examples","text":""},{"location":"admin-guide/advanced/presets/#example-1-quick-security-audit-workflow","title":"Example 1: Quick Security Audit Workflow","text":"<p>Create these preset jobs: 1. \"Top 1000 Passwords\" (Priority: 100)    - Mode: Straight    - Wordlist: top1000.txt    - No rules</p> <ol> <li>\"Common with Basic Rules\" (Priority: 90)</li> <li>Mode: Straight</li> <li>Wordlist: common-passwords.txt</li> <li> <p>Rules: basic.rule</p> </li> <li> <p>\"4-6 Digit PINs\" (Priority: 80)</p> </li> <li>Mode: Brute Force</li> <li>Mask: ?d?d?d?d, ?d?d?d?d?d, ?d?d?d?d?d?d</li> </ol>"},{"location":"admin-guide/advanced/presets/#example-2-targeted-corporate-audit","title":"Example 2: Targeted Corporate Audit","text":"<ol> <li>\"Corporate Dictionary\" (Priority: 100)</li> <li>Mode: Straight</li> <li>Wordlist: corporate-terms.txt</li> <li> <p>Rules: best64.rule</p> </li> <li> <p>\"Names + Years\" (Priority: 90)</p> </li> <li>Mode: Combination</li> <li>Wordlist 1: employee-names.txt</li> <li> <p>Wordlist 2: years-2020-2024.txt</p> </li> <li> <p>\"Corporate + Numbers\" (Priority: 80)</p> </li> <li>Mode: Hybrid (Wordlist + Mask)</li> <li>Wordlist: corporate-terms.txt</li> <li>Mask: ?d?d?d</li> </ol>"},{"location":"admin-guide/advanced/presets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/advanced/presets/#common-issues","title":"Common Issues","text":"<ol> <li>\"Wordlist not found\": Ensure wordlists are uploaded before creating preset jobs</li> <li>\"Invalid mask pattern\": Check mask syntax (?d=digit, ?l=lowercase, ?u=uppercase, ?s=special)</li> <li>\"Priority exceeds maximum\": Check system settings for max priority value</li> </ol>"},{"location":"admin-guide/advanced/presets/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use smaller, targeted wordlists for initial attempts</li> <li>Apply rules selectively - more isn't always better</li> <li>Set appropriate chunk sizes (larger for simple attacks, smaller for complex)</li> <li>Monitor system resources when running multiple workflows</li> </ul>"},{"location":"admin-guide/advanced/presets/#future-enhancements","title":"Future Enhancements","text":"<p>The preset jobs and workflows system is designed to integrate with: - Automated job distribution to agents - Real-time progress monitoring - Success rate analytics - Dynamic workflow optimization</p> <p>As KrakenHashes evolves, these features will provide the foundation for intelligent, adaptive password auditing strategies.</p>"},{"location":"admin-guide/operations/agents/","title":"Agent Management Guide","text":"<p>This guide covers the comprehensive management of KrakenHashes agents, including registration, monitoring, scheduling, and troubleshooting.</p>"},{"location":"admin-guide/operations/agents/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Understanding Agents</li> <li>Agent Registration Process</li> <li>Managing Agent Connections</li> <li>Monitoring Agent Health and Performance</li> <li>Agent Scheduling and Availability</li> <li>Hardware Capabilities and Benchmarks</li> <li>Troubleshooting Agent Issues</li> </ol>"},{"location":"admin-guide/operations/agents/#understanding-agents","title":"Understanding Agents","text":""},{"location":"admin-guide/operations/agents/#what-are-agents","title":"What are Agents?","text":"<p>Agents are distributed compute nodes that execute password cracking tasks using hashcat. They connect to the KrakenHashes backend server via WebSocket and receive job assignments based on their availability and capabilities.</p>"},{"location":"admin-guide/operations/agents/#agent-architecture","title":"Agent Architecture","text":"<p>Each agent consists of: - Hardware Detection: Automatic detection of GPUs (NVIDIA, AMD, Intel) and CPUs - Performance Monitoring: Real-time tracking of resource utilization - File Synchronization: Automatic download of wordlists, rules, and hashlists - Job Execution: Running hashcat with specified attack parameters - Result Reporting: Real-time crack updates back to the server</p>"},{"location":"admin-guide/operations/agents/#agent-states","title":"Agent States","text":"<p>Agents can be in one of the following states:</p> <ul> <li><code>pending</code>: Initial registration state, awaiting activation</li> <li><code>active</code>: Connected and ready to receive jobs</li> <li><code>inactive</code>: Disconnected but previously active</li> <li><code>error</code>: Experiencing issues preventing normal operation</li> <li><code>disabled</code>: Administratively disabled</li> </ul>"},{"location":"admin-guide/operations/agents/#agent-registration-process","title":"Agent Registration Process","text":""},{"location":"admin-guide/operations/agents/#overview","title":"Overview","text":"<p>Agent registration uses a claim code (voucher) system to ensure only authorized agents can join the system.</p>"},{"location":"admin-guide/operations/agents/#creating-claim-codes","title":"Creating Claim Codes","text":"<ol> <li>Navigate to Admin Panel</li> <li> <p>Go to Settings \u2192 Agent Management \u2192 Claim Codes</p> </li> <li> <p>Generate New Claim Code <pre><code>Type: Single-use or Continuous\nOwner: Select user who will own the agents\n</code></pre></p> </li> <li> <p>Claim Code Types</p> </li> <li>Single-use: Can only be used once to register one agent</li> <li>Continuous: Can be used multiple times (useful for auto-scaling)</li> </ol>"},{"location":"admin-guide/operations/agents/#agent-registration-steps","title":"Agent Registration Steps","text":"<ol> <li> <p>Agent Installation <pre><code># On the agent machine\n./krakenhashes-agent install\n</code></pre></p> </li> <li> <p>Initial Registration <pre><code># Using the claim code\n./krakenhashes-agent --host your-server:31337 --claim XXXX-XXXX-XXXX\n</code></pre></p> </li> <li> <p>Certificate Download</p> </li> <li>Agent automatically downloads TLS certificates</li> <li> <p>Stores credentials in <code>~/.krakenhashes/agent/</code></p> </li> <li> <p>Connection Establishment</p> </li> <li>Agent connects via WebSocket using API key authentication</li> <li>Sends hardware information and capabilities</li> </ol>"},{"location":"admin-guide/operations/agents/#registration-security","title":"Registration Security","text":"<ul> <li>Claim codes are normalized (uppercase, no hyphens)</li> <li>API keys are generated using cryptographically secure random bytes</li> <li>TLS certificates ensure encrypted communication</li> <li>Agent ID and API key must match for authentication</li> </ul> <p> Agent Management interface showing active claim vouchers with different types and currently registered agents with their hardware configurations</p> <p> Detailed agent view displaying system information, hardware configuration, and GPU device management with enable/disable controls</p>"},{"location":"admin-guide/operations/agents/#managing-agent-connections","title":"Managing Agent Connections","text":""},{"location":"admin-guide/operations/agents/#websocket-communication","title":"WebSocket Communication","text":"<p>Agents maintain persistent WebSocket connections for: - Real-time job assignments - Status updates - Crack result reporting - Heartbeat monitoring</p>"},{"location":"admin-guide/operations/agents/#connection-parameters","title":"Connection Parameters","text":"<pre><code># Environment variables for connection tuning\nKH_WRITE_WAIT: \"10s\"      # Write timeout\nKH_PONG_WAIT: \"60s\"       # Time to wait for pong response\nKH_PING_PERIOD: \"54s\"     # Ping interval (must be &lt; pong wait)\n</code></pre>"},{"location":"admin-guide/operations/agents/#monitoring-connected-agents","title":"Monitoring Connected Agents","text":"<ol> <li>Dashboard View</li> <li>Real-time agent status on main dashboard</li> <li>Shows connected/disconnected agents</li> <li> <p>Current task assignments</p> </li> <li> <p>Agent List Page</p> </li> <li>Detailed view of all agents</li> <li>Filter by status, owner, or team</li> <li>Last heartbeat timestamps</li> </ol>"},{"location":"admin-guide/operations/agents/#managing-agent-settings","title":"Managing Agent Settings","text":"<pre><code>// PUT /api/agents/{id}\n{\n  \"isEnabled\": true,\n  \"ownerId\": \"user-uuid\",\n  \"extraParameters\": \"--custom-charset1=?l?u?d\"\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#disablingenabling-agents","title":"Disabling/Enabling Agents","text":"<ul> <li>Disabled agents remain connected but don't receive jobs</li> <li>Useful for maintenance or troubleshooting</li> <li>Preserves agent configuration and history</li> </ul>"},{"location":"admin-guide/operations/agents/#monitoring-agent-health-and-performance","title":"Monitoring Agent Health and Performance","text":""},{"location":"admin-guide/operations/agents/#real-time-metrics","title":"Real-time Metrics","text":"<p>Agents report metrics every 30 seconds:</p> <pre><code>{\n  \"cpu_usage\": 45.2,\n  \"memory_usage\": 62.8,\n  \"gpu_utilization\": 98.5,\n  \"gpu_temp\": 72.0,\n  \"gpu_metrics\": {\n    \"device_0\": {\n      \"temperature\": 72,\n      \"utilization\": 98.5,\n      \"memory_used\": 8192,\n      \"fan_speed\": 85\n    }\n  }\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#performance-monitoring-dashboard","title":"Performance Monitoring Dashboard","text":"<ol> <li>Agent Detail Page</li> <li>Historical performance graphs</li> <li>Temperature trends</li> <li>Utilization patterns</li> <li> <p>Hash rate performance</p> </li> <li> <p>Metrics Time Ranges</p> </li> <li>1 hour (default)</li> <li>24 hours</li> <li>7 days</li> <li>30 days</li> </ol>"},{"location":"admin-guide/operations/agents/#device-management","title":"Device Management","text":"<p>Each agent can have multiple devices (GPUs):</p> <pre><code>// GET /api/agents/{id}/devices\n[\n  {\n    \"id\": 1,\n    \"device_index\": 0,\n    \"device_type\": \"GPU\",\n    \"device_name\": \"NVIDIA GeForce RTX 4090\",\n    \"is_enabled\": true,\n    \"capabilities\": {\n      \"compute_capability\": \"8.9\",\n      \"memory\": 24576\n    }\n  }\n]\n</code></pre>"},{"location":"admin-guide/operations/agents/#enablingdisabling-devices","title":"Enabling/Disabling Devices","text":"<pre><code>// PUT /api/agents/{id}/devices/{deviceId}\n{\n  \"enabled\": false  // Disable specific GPU\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#agent-scheduling-and-availability","title":"Agent Scheduling and Availability","text":""},{"location":"admin-guide/operations/agents/#scheduling-overview","title":"Scheduling Overview","text":"<p>Agents support weekly scheduling to optimize resource usage and costs.</p>"},{"location":"admin-guide/operations/agents/#configuring-agent-schedules","title":"Configuring Agent Schedules","text":"<ol> <li> <p>Enable Scheduling <pre><code>// PUT /api/agents/{id}/scheduling-enabled\n{\n  \"enabled\": true,\n  \"timezone\": \"America/New_York\"\n}\n</code></pre></p> </li> <li> <p>Set Daily Schedules <pre><code>// POST /api/agents/{id}/schedules\n{\n  \"dayOfWeek\": 1,  // Monday (0=Sunday, 6=Saturday)\n  \"startTimeUTC\": \"22:00\",\n  \"endTimeUTC\": \"06:00\",\n  \"timezone\": \"America/New_York\",\n  \"isActive\": true\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/agents/#schedule-features","title":"Schedule Features","text":"<ul> <li>UTC Storage: All times stored in UTC for consistency</li> <li>Timezone Display: Shown in user's local timezone</li> <li>Overnight Support: Schedules can span midnight</li> <li>Bulk Updates: Update entire week at once</li> </ul>"},{"location":"admin-guide/operations/agents/#schedule-validation","title":"Schedule Validation","text":"<ul> <li>Start and end times must be different</li> <li>Day of week must be 0-6</li> <li>Times in HH:MM format</li> <li>Automatic handling of daylight saving time</li> </ul>"},{"location":"admin-guide/operations/agents/#availability-considerations","title":"Availability Considerations","text":"<p>When scheduling is enabled: - Agents only receive jobs during scheduled hours - Running jobs continue to completion - Agents remain connected outside schedule - Heartbeat monitoring continues</p>"},{"location":"admin-guide/operations/agents/#hardware-capabilities-and-benchmarks","title":"Hardware Capabilities and Benchmarks","text":""},{"location":"admin-guide/operations/agents/#hardware-detection","title":"Hardware Detection","text":"<p>Agents automatically detect:</p> <pre><code>{\n  \"hardware\": {\n    \"cpus\": [\n      {\n        \"model\": \"AMD Ryzen 9 7950X\",\n        \"cores\": 16,\n        \"threads\": 32,\n        \"frequency\": 4.5\n      }\n    ],\n    \"gpus\": [\n      {\n        \"vendor\": \"NVIDIA\",\n        \"model\": \"GeForce RTX 4090\",\n        \"memory\": 24576,\n        \"driver\": \"545.29.06\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"admin-guide/operations/agents/#benchmark-system","title":"Benchmark System","text":"<p>Agents can run benchmarks for different hash types:</p> <pre><code>-- Benchmark results stored per agent\nagent_benchmarks (\n  agent_id,\n  attack_mode,     -- 0=dictionary, 3=bruteforce, etc.\n  hash_type,       -- 0=MD5, 1000=NTLM, etc.\n  speed,           -- Hashes per second\n  created_at\n)\n</code></pre>"},{"location":"admin-guide/operations/agents/#performance-metrics","title":"Performance Metrics","text":"<p>Key metrics tracked: - Hash Rate: Speed for each hash type - GPU Temperature: Thermal monitoring - GPU Utilization: Processing efficiency - Memory Usage: VRAM consumption - Power Consumption: Wattage tracking</p>"},{"location":"admin-guide/operations/agents/#consecutive-failure-tracking","title":"Consecutive Failure Tracking","text":"<p>Agents track consecutive task failures: - Increments on task failure - Resets on successful completion - Can trigger automatic disabling - Helps identify problematic agents</p>"},{"location":"admin-guide/operations/agents/#agent-disconnection-and-task-recovery","title":"Agent Disconnection and Task Recovery","text":""},{"location":"admin-guide/operations/agents/#understanding-disconnection-scenarios","title":"Understanding Disconnection Scenarios","text":"<p>KrakenHashes handles three distinct agent disconnection scenarios, each with specific recovery mechanisms:</p>"},{"location":"admin-guide/operations/agents/#1-backend-restart-planned-or-unplanned","title":"1. Backend Restart (Planned or Unplanned)","text":"<p>When the backend server restarts while agents have running tasks:</p> <p>What Happens: - Tasks transition to <code>reconnect_pending</code> state - Agents continue processing and cache crack results locally - Agent attempts reconnection with exponential backoff - Upon reconnection, agent reports current task status</p> <p>Recovery Process: 1. Agent reconnects and sends <code>current_task_status</code> message 2. Backend validates the task belongs to this agent 3. Task transitions from <code>reconnect_pending</code> back to <code>running</code> 4. Cached crack results are processed 5. Job continues without losing progress</p> <p>Grace Period: Configured via Admin Panel \u2192 Settings \u2192 Job Execution \u2192 Reconnect Grace Period (default: 5 minutes)</p>"},{"location":"admin-guide/operations/agents/#2-graceful-agent-shutdown","title":"2. Graceful Agent Shutdown","text":"<p>When an agent is properly stopped (SIGTERM, Ctrl+C, or service stop):</p> <p>What Happens: - Agent sends <code>agent_shutdown</code> notification to backend - Backend immediately marks task as <code>pending</code> for reassignment - No retry count increment (not a failure) - Task becomes available for other agents</p> <p>Recovery Process: - Task is immediately available for reassignment - No grace period applies - Original agent can claim new tasks upon restart</p>"},{"location":"admin-guide/operations/agents/#3-agent-crash-or-network-failure","title":"3. Agent Crash or Network Failure","text":"<p>When an agent disconnects unexpectedly (crash, network loss, power failure):</p> <p>What Happens: - Backend detects lost WebSocket connection - Task enters <code>reconnect_pending</code> state - Grace period timer starts</p> <p>Recovery Process:</p> <p>If agent reconnects within grace period: - Agent reports it has no running task - Backend marks task as <code>pending</code> for reassignment - Agent becomes available for new tasks</p> <p>If grace period expires: - Task automatically transitions to <code>pending</code> - Available for any agent to claim - Retry count may increment based on configuration</p>"},{"location":"admin-guide/operations/agents/#task-state-transitions","title":"Task State Transitions","text":"<pre><code>running \u2192 reconnect_pending \u2192 running (agent reconnects with task)\nrunning \u2192 reconnect_pending \u2192 pending (agent reconnects without task)\nrunning \u2192 reconnect_pending \u2192 pending (grace period expires)\nrunning \u2192 pending (graceful shutdown)\n</code></pre>"},{"location":"admin-guide/operations/agents/#monitoring-disconnection-events","title":"Monitoring Disconnection Events","text":""},{"location":"admin-guide/operations/agents/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<ul> <li>Reconnection frequency: High frequency indicates network issues</li> <li>Grace period utilization: Tasks recovering vs. timing out</li> <li>Task reassignment rate: How often tasks move between agents</li> <li>Cached data volume: Amount of data agents cache during disconnections</li> </ul>"},{"location":"admin-guide/operations/agents/#log-indicators","title":"Log Indicators","text":"<p>Backend logs to monitor: <pre><code>INFO: Agent X: Task status - HasTask: true, TaskID: xxx\nINFO: Task can be recovered [task_id=xxx, status=reconnect_pending]\nINFO: Successfully recovered task\n</code></pre></p> <p>Agent logs to monitor: <pre><code>INFO: Sending current task status to backend\nINFO: Successfully sent current task status - HasTask: true\nINFO: Connection state: disconnected\nINFO: Reconnection attempt X - Waiting Ys before retry\n</code></pre></p>"},{"location":"admin-guide/operations/agents/#configuring-recovery-behavior","title":"Configuring Recovery Behavior","text":""},{"location":"admin-guide/operations/agents/#reconnect-grace-period","title":"Reconnect Grace Period","text":"<p>Adjust based on your environment: - Stable networks: 3-5 minutes - Cloud environments: 5-10 minutes - Unreliable networks: 10-15 minutes - Maintenance windows: 15-30 minutes</p>"},{"location":"admin-guide/operations/agents/#best-practices","title":"Best Practices","text":"<ol> <li>Set grace period based on recovery time: Consider how long it takes to:</li> <li>Restart backend services</li> <li>Complete rolling updates</li> <li> <p>Recover from network outages</p> </li> <li> <p>Monitor grace period effectiveness:</p> </li> <li>Track successful recoveries vs. timeouts</li> <li>Adjust if seeing excessive reassignments</li> <li> <p>Consider agent network stability</p> </li> <li> <p>Plan maintenance windows:</p> </li> <li>Increase grace period before maintenance</li> <li>Notify users of extended recovery time</li> <li> <p>Monitor agent reconnections post-maintenance</p> </li> <li> <p>Handle chronic disconnections:</p> </li> <li>Identify agents with frequent disconnections</li> <li>Check network path and stability</li> <li>Consider dedicated network routes for critical agents</li> </ol>"},{"location":"admin-guide/operations/agents/#troubleshooting-recovery-issues","title":"Troubleshooting Recovery Issues","text":""},{"location":"admin-guide/operations/agents/#agent-not-recovering-task-after-reconnection","title":"Agent Not Recovering Task After Reconnection","text":"<p>Symptoms: Agent reconnects but task is reassigned</p> <p>Common Causes: - Task ID mismatch between agent and backend - Database constraint violations (check backend logs) - Agent started fresh without cached state</p> <p>Solutions: 1. Check agent has persistent storage for state 2. Verify task ID in agent and backend logs match 3. Review backend logs for \"Failed to recover task\" errors 4. Ensure database migrations are up to date</p>"},{"location":"admin-guide/operations/agents/#tasks-stuck-in-reconnect_pending","title":"Tasks Stuck in Reconnect_Pending","text":"<p>Symptoms: Tasks remain in reconnect_pending after grace period</p> <p>Common Causes: - Backend service not running task cleanup - Database lock preventing state transition - Incorrect grace period configuration</p> <p>Solutions: 1. Verify job cleanup service is running 2. Check database for locks on job_tasks table 3. Manually transition stuck tasks if needed:    <pre><code>UPDATE job_tasks \nSET status = 'pending', updated_at = NOW() \nWHERE status = 'reconnect_pending' \nAND updated_at &lt; NOW() - INTERVAL '10 minutes';\n</code></pre></p>"},{"location":"admin-guide/operations/agents/#excessive-task-reassignments","title":"Excessive Task Reassignments","text":"<p>Symptoms: Tasks frequently moving between agents</p> <p>Common Causes: - Grace period too short - Network instability - Agents crashing frequently</p> <p>Solutions: 1. Increase reconnect grace period 2. Investigate network stability 3. Check agent system resources and logs 4. Consider agent health checks</p>"},{"location":"admin-guide/operations/agents/#troubleshooting-agent-issues","title":"Troubleshooting Agent Issues","text":""},{"location":"admin-guide/operations/agents/#common-connection-issues","title":"Common Connection Issues","text":"<ol> <li>Agent Won't Connect</li> <li>Check TLS certificates are valid</li> <li>Verify API key hasn't expired</li> <li>Ensure firewall allows WebSocket (port 8443)</li> <li> <p>Check agent logs for detailed errors</p> </li> <li> <p>Frequent Disconnections</p> </li> <li>Review ping/pong timeout settings</li> <li>Check network stability</li> <li>Monitor agent system resources</li> <li>Verify no proxy interference</li> </ol>"},{"location":"admin-guide/operations/agents/#authentication-problems","title":"Authentication Problems","text":"<ol> <li> <p>Invalid API Key <pre><code># Regenerate API key\n./krakenhashes-agent reregister --claim-code NEW-CODE\n</code></pre></p> </li> <li> <p>Certificate Issues</p> </li> <li>Check certificate expiration</li> <li>Verify CA certificate is trusted</li> <li>Ensure certificate matches server hostname</li> </ol>"},{"location":"admin-guide/operations/agents/#performance-issues","title":"Performance Issues","text":"<ol> <li>Low Hash Rates</li> <li>Check GPU driver versions</li> <li>Monitor thermal throttling</li> <li>Verify power settings</li> <li> <p>Review extra parameters</p> </li> <li> <p>High Failure Rate</p> </li> <li>Check hashcat binary compatibility</li> <li>Verify file synchronization</li> <li>Review job parameters</li> <li>Monitor system stability</li> </ol>"},{"location":"admin-guide/operations/agents/#debugging-tools","title":"Debugging Tools","text":"<ol> <li> <p>Agent Logs <pre><code># View agent logs\ntail -f ~/.krakenhashes/agent/logs/agent.log\n</code></pre></p> </li> <li> <p>Server-side Monitoring <pre><code>-- Check agent errors\nSELECT id, name, last_error, consecutive_failures\nFROM agents\nWHERE last_error IS NOT NULL\nORDER BY updated_at DESC;\n</code></pre></p> </li> <li> <p>WebSocket Messages</p> </li> <li>Enable debug logging for detailed messages</li> <li>Monitor heartbeat intervals</li> <li>Check message acknowledgments</li> </ol>"},{"location":"admin-guide/operations/agents/#recovery-procedures","title":"Recovery Procedures","text":"<ol> <li> <p>Reset Agent State <pre><code>-- Clear error state\nUPDATE agents \nSET status = 'inactive', \n    last_error = NULL,\n    consecutive_failures = 0\nWHERE id = ?;\n</code></pre></p> </li> <li> <p>Force Reconnection</p> </li> <li>Restart agent service</li> <li>Clear local cache</li> <li> <p>Verify network connectivity</p> </li> <li> <p>Complete Re-registration</p> </li> <li>Generate new claim code</li> <li>Remove agent from database</li> <li>Perform fresh registration</li> </ol>"},{"location":"admin-guide/operations/agents/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Set Up Alerts</li> <li>Agent offline &gt; 5 minutes</li> <li>Consecutive failures &gt; 3</li> <li>Temperature &gt; 85\u00b0C</li> <li> <p>Low hash rates</p> </li> <li> <p>Regular Maintenance</p> </li> <li>Update agent software</li> <li>Clean GPU fans</li> <li>Check thermal paste</li> <li> <p>Update drivers</p> </li> <li> <p>Capacity Planning</p> </li> <li>Monitor job queue depth</li> <li>Track agent utilization</li> <li>Plan for peak loads</li> <li>Consider scheduling optimization</li> </ol>"},{"location":"admin-guide/operations/agents/#advanced-topics","title":"Advanced Topics","text":""},{"location":"admin-guide/operations/agents/#agent-clustering","title":"Agent Clustering","text":"<ul> <li>Group agents by capability</li> <li>Assign specialized workloads</li> <li>Balance load across regions</li> <li>Implement failover strategies</li> </ul>"},{"location":"admin-guide/operations/agents/#security-hardening","title":"Security Hardening","text":"<ul> <li>Rotate API keys periodically</li> <li>Implement IP whitelisting</li> <li>Use dedicated agent VLANs</li> <li>Monitor for anomalous behavior</li> </ul>"},{"location":"admin-guide/operations/agents/#integration-points","title":"Integration Points","text":"<ul> <li>Export metrics to monitoring systems</li> <li>Webhook notifications for events</li> <li>API automation for scaling</li> <li>Custom scheduling algorithms</li> </ul>"},{"location":"admin-guide/operations/agents/#conclusion","title":"Conclusion","text":"<p>Effective agent management is crucial for maintaining a high-performance distributed cracking system. Regular monitoring, proper scheduling, and proactive troubleshooting ensure optimal resource utilization and job completion rates.</p> <p>For additional support or advanced configurations, consult the system administrator documentation or contact the development team.</p>"},{"location":"admin-guide/operations/backup/","title":"Backup and Restore Procedures","text":"<p>This guide provides comprehensive backup and restore procedures for KrakenHashes, covering database backups, file system backups, Docker volumes, and automated backup strategies.</p>"},{"location":"admin-guide/operations/backup/#overview","title":"Overview","text":"<p>KrakenHashes stores critical data in multiple locations that require regular backups:</p> <ol> <li>PostgreSQL Database - Contains all system metadata, user accounts, job configurations, and hash crack results</li> <li>File Storage - Binary files, wordlists, rules, and uploaded hashlists</li> <li>Docker Volumes - Persistent storage for containerized deployments</li> <li>Configuration Files - TLS certificates and system configuration</li> </ol>"},{"location":"admin-guide/operations/backup/#data-locations","title":"Data Locations","text":""},{"location":"admin-guide/operations/backup/#database","title":"Database","text":"<ul> <li>Container: <code>krakenhashes-postgres</code></li> <li>Volume: <code>krakenhashes_postgres_data</code></li> <li>Critical Tables:</li> <li><code>users</code> - User accounts and authentication</li> <li><code>agents</code> - Registered compute agents</li> <li><code>hashlists</code> - Hash collections metadata</li> <li><code>hashes</code> - Individual hash records and crack status</li> <li><code>clients</code> - Customer/engagement tracking</li> <li><code>job_executions</code> - Job execution history</li> <li><code>wordlists</code>, <code>rules</code> - Attack resource metadata</li> <li><code>auth_tokens</code> - Authentication tokens</li> <li><code>vouchers</code> - Agent registration codes</li> </ul>"},{"location":"admin-guide/operations/backup/#file-storage","title":"File Storage","text":"<p>Default location: <code>/var/lib/krakenhashes/</code> (configurable via <code>KH_DATA_DIR</code>)</p> <pre><code>/var/lib/krakenhashes/\n\u251c\u2500\u2500 binaries/          # Hashcat and other tool binaries\n\u251c\u2500\u2500 wordlists/         # Wordlist files\n\u2502   \u251c\u2500\u2500 general/\n\u2502   \u251c\u2500\u2500 specialized/\n\u2502   \u251c\u2500\u2500 targeted/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 rules/             # Rule files\n\u2502   \u251c\u2500\u2500 hashcat/\n\u2502   \u251c\u2500\u2500 john/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 hashlists/         # Uploaded hash files\n\u2514\u2500\u2500 hashlist_uploads/  # Temporary upload directory\n</code></pre>"},{"location":"admin-guide/operations/backup/#docker-volumes","title":"Docker Volumes","text":"<ul> <li><code>krakenhashes_postgres_data</code> - PostgreSQL data</li> <li><code>krakenhashes_app_data</code> - Application data files</li> </ul>"},{"location":"admin-guide/operations/backup/#backup-strategies","title":"Backup Strategies","text":""},{"location":"admin-guide/operations/backup/#1-postgresql-database-backup","title":"1. PostgreSQL Database Backup","text":""},{"location":"admin-guide/operations/backup/#manual-backup","title":"Manual Backup","text":"<pre><code># Create backup directory\nmkdir -p /backup/krakenhashes/postgres/$(date +%Y%m%d)\n\n# Backup using pg_dump (recommended for portability)\ndocker exec krakenhashes-postgres pg_dump \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --verbose \\\n  --format=custom \\\n  --file=/tmp/krakenhashes_$(date +%Y%m%d_%H%M%S).dump\n\n# Copy backup from container\ndocker cp krakenhashes-postgres:/tmp/krakenhashes_$(date +%Y%m%d_%H%M%S).dump \\\n  /backup/krakenhashes/postgres/$(date +%Y%m%d)/\n\n# Alternative: Direct backup with compression\ndocker exec krakenhashes-postgres pg_dump \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  | gzip &gt; /backup/krakenhashes/postgres/$(date +%Y%m%d)/krakenhashes_$(date +%Y%m%d_%H%M%S).sql.gz\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-database-backup-script","title":"Automated Database Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-db-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Database Backup Script\n# Run via cron for automated backups\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/postgres\"\nRETENTION_DAYS=30\nDB_CONTAINER=\"krakenhashes-postgres\"\nDB_NAME=\"krakenhashes\"\nDB_USER=\"krakenhashes\"\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Check if container is running\nif ! docker ps | grep -q \"${DB_CONTAINER}\"; then\n    log \"ERROR: Container ${DB_CONTAINER} is not running\"\n    exit 1\nfi\n\nlog \"Starting database backup...\"\n\n# Perform backup\nBACKUP_FILE=\"${BACKUP_DIR}/${DATE_DIR}/krakenhashes_${TIMESTAMP}.dump\"\nif docker exec \"${DB_CONTAINER}\" pg_dump \\\n    -U \"${DB_USER}\" \\\n    -d \"${DB_NAME}\" \\\n    --verbose \\\n    --format=custom \\\n    --file=\"/tmp/backup_${TIMESTAMP}.dump\"; then\n\n    # Copy backup from container\n    docker cp \"${DB_CONTAINER}:/tmp/backup_${TIMESTAMP}.dump\" \"${BACKUP_FILE}\"\n\n    # Clean up temp file in container\n    docker exec \"${DB_CONTAINER}\" rm \"/tmp/backup_${TIMESTAMP}.dump\"\n\n    # Compress backup\n    gzip \"${BACKUP_FILE}\"\n\n    log \"Database backup completed: ${BACKUP_FILE}.gz\"\n\n    # Calculate backup size\n    SIZE=$(du -h \"${BACKUP_FILE}.gz\" | cut -f1)\n    log \"Backup size: ${SIZE}\"\nelse\n    log \"ERROR: Database backup failed\"\n    exit 1\nfi\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f -name \"*.dump.gz\" -mtime +${RETENTION_DAYS} -delete\n\nlog \"Backup process completed\"\n</code></pre> <p>Make it executable: <pre><code>chmod +x /usr/local/bin/krakenhashes-db-backup.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#2-file-system-backup","title":"2. File System Backup","text":""},{"location":"admin-guide/operations/backup/#manual-backup_1","title":"Manual Backup","text":"<pre><code># Create backup directory\nmkdir -p /backup/krakenhashes/files/$(date +%Y%m%d)\n\n# Backup data directory with compression\ntar -czf /backup/krakenhashes/files/$(date +%Y%m%d)/krakenhashes_data_$(date +%Y%m%d_%H%M%S).tar.gz \\\n  -C /var/lib/krakenhashes \\\n  binaries wordlists rules hashlists hashlist_uploads\n\n# Backup with progress indicator\ntar -czf /backup/krakenhashes/files/$(date +%Y%m%d)/krakenhashes_data_$(date +%Y%m%d_%H%M%S).tar.gz \\\n  --checkpoint=1000 \\\n  --checkpoint-action=echo=\"Processed %{r}T files\" \\\n  -C /var/lib/krakenhashes \\\n  binaries wordlists rules hashlists hashlist_uploads\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-file-backup-script","title":"Automated File Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-file-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes File System Backup Script\n# Backs up all data files including binaries, wordlists, rules, and hashlists\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/files\"\nDATA_DIR=\"${KH_DATA_DIR:-/var/lib/krakenhashes}\"\nRETENTION_DAYS=30\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Check if data directory exists\nif [ ! -d \"${DATA_DIR}\" ]; then\n    log \"ERROR: Data directory ${DATA_DIR} does not exist\"\n    exit 1\nfi\n\nlog \"Starting file system backup...\"\nlog \"Source directory: ${DATA_DIR}\"\n\n# Calculate total size\nTOTAL_SIZE=$(du -sh \"${DATA_DIR}\" | cut -f1)\nlog \"Total data size: ${TOTAL_SIZE}\"\n\n# Create backup\nBACKUP_FILE=\"${BACKUP_DIR}/${DATE_DIR}/krakenhashes_data_${TIMESTAMP}.tar.gz\"\n\n# Backup with progress\ntar -czf \"${BACKUP_FILE}\" \\\n    --checkpoint=1000 \\\n    --checkpoint-action=echo=\"[$(date '+%Y-%m-%d %H:%M:%S')] Processed %{r}T files\" \\\n    -C \"${DATA_DIR}\" \\\n    binaries wordlists rules hashlists hashlist_uploads 2&gt;&amp;1 | while read line; do\n        log \"$line\"\n    done\n\nif [ ${PIPESTATUS[0]} -eq 0 ]; then\n    log \"File backup completed: ${BACKUP_FILE}\"\n\n    # Calculate backup size\n    SIZE=$(du -h \"${BACKUP_FILE}\" | cut -f1)\n    log \"Backup size: ${SIZE}\"\n\n    # Create checksum\n    sha256sum \"${BACKUP_FILE}\" &gt; \"${BACKUP_FILE}.sha256\"\n    log \"Checksum created: ${BACKUP_FILE}.sha256\"\nelse\n    log \"ERROR: File backup failed\"\n    exit 1\nfi\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f \\( -name \"*.tar.gz\" -o -name \"*.sha256\" \\) -mtime +${RETENTION_DAYS} -delete\n\nlog \"File backup process completed\"\n</code></pre> <p>Make it executable: <pre><code>chmod +x /usr/local/bin/krakenhashes-file-backup.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#3-docker-volume-backup","title":"3. Docker Volume Backup","text":""},{"location":"admin-guide/operations/backup/#manual-volume-backup","title":"Manual Volume Backup","text":"<pre><code># Stop services to ensure consistency\ndocker-compose down\n\n# Backup PostgreSQL volume\ndocker run --rm \\\n  -v krakenhashes_postgres_data:/source:ro \\\n  -v /backup/krakenhashes/volumes:/backup \\\n  alpine tar -czf /backup/postgres_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /source .\n\n# Backup application data volume\ndocker run --rm \\\n  -v krakenhashes_app_data:/source:ro \\\n  -v /backup/krakenhashes/volumes:/backup \\\n  alpine tar -czf /backup/app_data_$(date +%Y%m%d_%H%M%S).tar.gz -C /source .\n\n# Restart services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-volume-backup-script","title":"Automated Volume Backup Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-volume-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Docker Volume Backup Script\n# Backs up Docker volumes with minimal downtime\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes/volumes\"\nRETENTION_DAYS=30\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nDATE_DIR=$(date +%Y%m%d)\nCOMPOSE_FILE=\"/path/to/krakenhashes/docker-compose.yml\"\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}/${DATE_DIR}\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n}\n\n# Function to backup a volume\nbackup_volume() {\n    local volume_name=$1\n    local backup_name=$2\n\n    log \"Backing up volume: ${volume_name}\"\n\n    docker run --rm \\\n        -v \"${volume_name}:/source:ro\" \\\n        -v \"${BACKUP_DIR}/${DATE_DIR}:/backup\" \\\n        alpine tar -czf \"/backup/${backup_name}_${TIMESTAMP}.tar.gz\" -C /source .\n\n    if [ $? -eq 0 ]; then\n        log \"Volume backup completed: ${backup_name}_${TIMESTAMP}.tar.gz\"\n\n        # Create checksum\n        cd \"${BACKUP_DIR}/${DATE_DIR}\"\n        sha256sum \"${backup_name}_${TIMESTAMP}.tar.gz\" &gt; \"${backup_name}_${TIMESTAMP}.tar.gz.sha256\"\n    else\n        log \"ERROR: Failed to backup volume ${volume_name}\"\n        return 1\n    fi\n}\n\nlog \"Starting Docker volume backup...\"\n\n# For live backups without downtime (PostgreSQL only)\nif docker ps | grep -q \"krakenhashes-postgres\"; then\n    log \"Creating PostgreSQL checkpoint for consistent backup...\"\n    docker exec krakenhashes-postgres psql -U krakenhashes -c \"CHECKPOINT;\"\nfi\n\n# Backup volumes\nbackup_volume \"krakenhashes_postgres_data\" \"postgres_data\"\nbackup_volume \"krakenhashes_app_data\" \"app_data\"\n\n# Clean up old backups\nlog \"Cleaning up backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -type f \\( -name \"*.tar.gz\" -o -name \"*.sha256\" \\) -mtime +${RETENTION_DAYS} -delete\n\nlog \"Volume backup process completed\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#4-configuration-backup","title":"4. Configuration Backup","text":"<pre><code>#!/bin/bash\n\n# Backup configuration files\nBACKUP_DIR=\"/backup/krakenhashes/config/$(date +%Y%m%d)\"\nmkdir -p \"${BACKUP_DIR}\"\n\n# Backup TLS certificates\ntar -czf \"${BACKUP_DIR}/certs_$(date +%Y%m%d_%H%M%S).tar.gz\" \\\n  -C /etc/krakenhashes certs\n\n# Backup environment files\ncp /path/to/krakenhashes/.env \"${BACKUP_DIR}/env_$(date +%Y%m%d_%H%M%S)\"\n\n# Backup docker-compose configuration\ncp /path/to/krakenhashes/docker-compose.yml \"${BACKUP_DIR}/\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#automated-backup-schedule","title":"Automated Backup Schedule","text":"<p>Add to crontab (<code>crontab -e</code>):</p> <pre><code># KrakenHashes Automated Backups\n# Database backup - every 6 hours\n0 */6 * * * /usr/local/bin/krakenhashes-db-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# File system backup - daily at 2 AM\n0 2 * * * /usr/local/bin/krakenhashes-file-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# Volume backup - daily at 3 AM\n0 3 * * * /usr/local/bin/krakenhashes-volume-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n\n# Configuration backup - weekly on Sunday at 4 AM\n0 4 * * 0 /usr/local/bin/krakenhashes-config-backup.sh &gt;&gt; /var/log/krakenhashes-backup.log 2&gt;&amp;1\n</code></pre>"},{"location":"admin-guide/operations/backup/#restore-procedures","title":"Restore Procedures","text":""},{"location":"admin-guide/operations/backup/#1-database-restore","title":"1. Database Restore","text":""},{"location":"admin-guide/operations/backup/#from-pg_dump-backup","title":"From pg_dump backup","text":"<pre><code># Stop the application\ndocker-compose stop krakenhashes\n\n# Restore database\ndocker exec -i krakenhashes-postgres pg_restore \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --clean \\\n  --if-exists \\\n  --verbose \\\n  &lt; /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.dump\n\n# Or from compressed SQL\ngunzip -c /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.sql.gz | \\\n  docker exec -i krakenhashes-postgres psql -U krakenhashes -d krakenhashes\n\n# Restart application\ndocker-compose start krakenhashes\n</code></pre>"},{"location":"admin-guide/operations/backup/#full-database-recreation","title":"Full database recreation","text":"<pre><code># Stop all services\ndocker-compose down\n\n# Remove old database volume\ndocker volume rm krakenhashes_postgres_data\n\n# Recreate and start database\ndocker-compose up -d postgres\n\n# Wait for database to be ready\nsleep 10\n\n# Create database and user\ndocker exec krakenhashes-postgres psql -U postgres -c \"CREATE DATABASE krakenhashes;\"\ndocker exec krakenhashes-postgres psql -U postgres -c \"CREATE USER krakenhashes WITH PASSWORD 'your-password';\"\ndocker exec krakenhashes-postgres psql -U postgres -c \"GRANT ALL PRIVILEGES ON DATABASE krakenhashes TO krakenhashes;\"\n\n# Restore from backup\ndocker exec -i krakenhashes-postgres pg_restore \\\n  -U krakenhashes \\\n  -d krakenhashes \\\n  --verbose \\\n  &lt; /backup/krakenhashes/postgres/20240101/krakenhashes_20240101_120000.dump\n\n# Start all services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#2-file-system-restore","title":"2. File System Restore","text":"<pre><code># Create data directory if it doesn't exist\nmkdir -p /var/lib/krakenhashes\n\n# Extract backup\ntar -xzf /backup/krakenhashes/files/20240101/krakenhashes_data_20240101_120000.tar.gz \\\n  -C /var/lib/krakenhashes\n\n# Verify checksum\ncd /backup/krakenhashes/files/20240101\nsha256sum -c krakenhashes_data_20240101_120000.tar.gz.sha256\n\n# Fix permissions\nchown -R 1000:1000 /var/lib/krakenhashes\nchmod -R 750 /var/lib/krakenhashes\n</code></pre>"},{"location":"admin-guide/operations/backup/#3-docker-volume-restore","title":"3. Docker Volume Restore","text":"<pre><code># Stop services\ndocker-compose down\n\n# Remove existing volumes\ndocker volume rm krakenhashes_postgres_data krakenhashes_app_data\n\n# Recreate volumes\ndocker volume create krakenhashes_postgres_data\ndocker volume create krakenhashes_app_data\n\n# Restore PostgreSQL volume\ndocker run --rm \\\n  -v krakenhashes_postgres_data:/target \\\n  -v /backup/krakenhashes/volumes/20240101:/backup:ro \\\n  alpine tar -xzf /backup/postgres_data_20240101_120000.tar.gz -C /target\n\n# Restore application data volume\ndocker run --rm \\\n  -v krakenhashes_app_data:/target \\\n  -v /backup/krakenhashes/volumes/20240101:/backup:ro \\\n  alpine tar -xzf /backup/app_data_20240101_120000.tar.gz -C /target\n\n# Start services\ndocker-compose up -d\n</code></pre>"},{"location":"admin-guide/operations/backup/#backup-verification","title":"Backup Verification","text":""},{"location":"admin-guide/operations/backup/#automated-verification-script","title":"Automated Verification Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-verify-backup.sh</code>:</p> <pre><code>#!/bin/bash\n\n# KrakenHashes Backup Verification Script\n# Verifies backup integrity and tests restore procedures\n\nset -e\n\n# Configuration\nBACKUP_DIR=\"/backup/krakenhashes\"\nTEST_RESTORE_DIR=\"/tmp/krakenhashes-restore-test\"\nVERIFICATION_LOG=\"/var/log/krakenhashes-backup-verification.log\"\n\n# Function to log messages\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" | tee -a \"${VERIFICATION_LOG}\"\n}\n\n# Function to verify file integrity\nverify_file() {\n    local file=$1\n    local checksum_file=\"${file}.sha256\"\n\n    if [ -f \"${checksum_file}\" ]; then\n        if sha256sum -c \"${checksum_file}\" &gt; /dev/null 2&gt;&amp;1; then\n            log \"\u2713 Checksum verified: $(basename ${file})\"\n            return 0\n        else\n            log \"\u2717 Checksum failed: $(basename ${file})\"\n            return 1\n        fi\n    else\n        log \"\u26a0 No checksum file for: $(basename ${file})\"\n        return 2\n    fi\n}\n\n# Function to test database restore\ntest_db_restore() {\n    local backup_file=$1\n\n    log \"Testing database restore from: $(basename ${backup_file})\"\n\n    # Create test database\n    docker exec krakenhashes-postgres psql -U postgres -c \"CREATE DATABASE krakenhashes_test;\"\n\n    # Attempt restore\n    if gunzip -c \"${backup_file}\" | docker exec -i krakenhashes-postgres psql -U postgres -d krakenhashes_test &gt; /dev/null 2&gt;&amp;1; then\n        # Verify some data\n        USERS=$(docker exec krakenhashes-postgres psql -U postgres -d krakenhashes_test -t -c \"SELECT COUNT(*) FROM users;\")\n        log \"\u2713 Database restore successful. Found ${USERS// /} users.\"\n\n        # Clean up\n        docker exec krakenhashes-postgres psql -U postgres -c \"DROP DATABASE krakenhashes_test;\"\n        return 0\n    else\n        log \"\u2717 Database restore failed\"\n        docker exec krakenhashes-postgres psql -U postgres -c \"DROP DATABASE IF EXISTS krakenhashes_test;\"\n        return 1\n    fi\n}\n\n# Main verification process\nlog \"Starting backup verification...\"\n\n# Find latest backups\nLATEST_DB_BACKUP=$(find \"${BACKUP_DIR}/postgres\" -name \"*.dump.gz\" -type f -mtime -1 | sort -r | head -1)\nLATEST_FILE_BACKUP=$(find \"${BACKUP_DIR}/files\" -name \"*.tar.gz\" -type f -mtime -1 | sort -r | head -1)\nLATEST_VOLUME_BACKUP=$(find \"${BACKUP_DIR}/volumes\" -name \"postgres_data_*.tar.gz\" -type f -mtime -1 | sort -r | head -1)\n\n# Verify database backup\nif [ -n \"${LATEST_DB_BACKUP}\" ]; then\n    verify_file \"${LATEST_DB_BACKUP}\"\n    test_db_restore \"${LATEST_DB_BACKUP}\"\nelse\n    log \"\u26a0 No recent database backup found\"\nfi\n\n# Verify file backup\nif [ -n \"${LATEST_FILE_BACKUP}\" ]; then\n    verify_file \"${LATEST_FILE_BACKUP}\"\n\n    # Test extraction\n    mkdir -p \"${TEST_RESTORE_DIR}\"\n    if tar -tzf \"${LATEST_FILE_BACKUP}\" &gt; /dev/null 2&gt;&amp;1; then\n        log \"\u2713 File backup archive is valid\"\n    else\n        log \"\u2717 File backup archive is corrupted\"\n    fi\n    rm -rf \"${TEST_RESTORE_DIR}\"\nelse\n    log \"\u26a0 No recent file backup found\"\nfi\n\n# Verify volume backup\nif [ -n \"${LATEST_VOLUME_BACKUP}\" ]; then\n    verify_file \"${LATEST_VOLUME_BACKUP}\"\nelse\n    log \"\u26a0 No recent volume backup found\"\nfi\n\nlog \"Backup verification completed\"\n</code></pre>"},{"location":"admin-guide/operations/backup/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":""},{"location":"admin-guide/operations/backup/#recovery-time-objectives-rto","title":"Recovery Time Objectives (RTO)","text":"<ul> <li>Database: 30 minutes</li> <li>File System: 1 hour</li> <li>Full System: 2 hours</li> </ul>"},{"location":"admin-guide/operations/backup/#recovery-point-objectives-rpo","title":"Recovery Point Objectives (RPO)","text":"<ul> <li>Database: 6 hours (based on backup frequency)</li> <li>File System: 24 hours</li> <li>Configuration: 7 days</li> </ul>"},{"location":"admin-guide/operations/backup/#recovery-priority","title":"Recovery Priority","text":"<ol> <li>PostgreSQL Database - Contains all system state</li> <li>Configuration Files - Required for system operation</li> <li>Hashlists - User-uploaded hash files</li> <li>Wordlists/Rules - Can be re-downloaded if needed</li> <li>Binaries - Can be re-downloaded from version tracking</li> </ol>"},{"location":"admin-guide/operations/backup/#emergency-recovery-checklist","title":"Emergency Recovery Checklist","text":"<ol> <li>Assess Damage</li> <li> Identify failed components</li> <li> Determine data loss extent</li> <li> <p> Document incident timeline</p> </li> <li> <p>Prepare Recovery Environment</p> </li> <li> Provision new hardware/VMs</li> <li> Install Docker and dependencies</li> <li> <p> Restore network configuration</p> </li> <li> <p>Restore Core Services</p> </li> <li> Restore PostgreSQL database</li> <li> Restore configuration files</li> <li> <p> Verify TLS certificates</p> </li> <li> <p>Restore Data</p> </li> <li> Restore file system data</li> <li> Verify hashlist integrity</li> <li> <p> Restore Docker volumes</p> </li> <li> <p>Validation</p> </li> <li> Test user authentication</li> <li> Verify agent connectivity</li> <li> Check job execution</li> <li> <p> Validate data integrity</p> </li> <li> <p>Communication</p> </li> <li> Notify users of recovery status</li> <li> Document lessons learned</li> <li> Update recovery procedures</li> </ol>"},{"location":"admin-guide/operations/backup/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Testing</li> <li>Test restore procedures monthly</li> <li>Perform full disaster recovery drill quarterly</li> <li> <p>Document test results and issues</p> </li> <li> <p>Off-site Storage</p> </li> <li>Keep backups in multiple locations</li> <li>Use cloud storage for critical backups</li> <li> <p>Maintain 3-2-1 backup strategy (3 copies, 2 different media, 1 off-site)</p> </li> <li> <p>Monitoring</p> </li> <li>Monitor backup job success/failure</li> <li>Alert on backup size anomalies</li> <li> <p>Track backup storage usage</p> </li> <li> <p>Security</p> </li> <li>Encrypt backups at rest</li> <li>Restrict backup access</li> <li> <p>Regularly rotate backup credentials</p> </li> <li> <p>Documentation</p> </li> <li>Keep recovery procedures updated</li> <li>Document system dependencies</li> <li>Maintain contact information for key personnel</li> </ol>"},{"location":"admin-guide/operations/backup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/backup/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Backup Fails with \"Permission Denied\" <pre><code># Fix backup directory permissions\nsudo chown -R $(whoami):$(whoami) /backup/krakenhashes\nsudo chmod -R 750 /backup/krakenhashes\n</code></pre></p> </li> <li> <p>Database Restore Fails <pre><code># Check PostgreSQL logs\ndocker logs krakenhashes-postgres\n\n# Verify database exists\ndocker exec krakenhashes-postgres psql -U postgres -l\n\n# Check user permissions\ndocker exec krakenhashes-postgres psql -U postgres -c \"\\du\"\n</code></pre></p> </li> <li> <p>Insufficient Disk Space <pre><code># Check disk usage\ndf -h /backup\n\n# Clean old backups manually\nfind /backup/krakenhashes -type f -mtime +${DAYS} -delete\n</code></pre></p> </li> <li> <p>Slow Backup Performance <pre><code># Use parallel compression\ntar -cf - -C /var/lib/krakenhashes . | pigz &gt; backup.tar.gz\n\n# Adjust PostgreSQL backup parameters\ndocker exec krakenhashes-postgres psql -U postgres -c \"SET maintenance_work_mem = '1GB';\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/backup/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"admin-guide/operations/backup/#backup-monitoring-script","title":"Backup Monitoring Script","text":"<p>Create <code>/usr/local/bin/krakenhashes-backup-monitor.sh</code>:</p> <pre><code>#!/bin/bash\n\n# Check if backups are current and send alerts\n\nBACKUP_DIR=\"/backup/krakenhashes\"\nMAX_AGE_HOURS=25  # Alert if backup is older than 25 hours\nALERT_EMAIL=\"admin@example.com\"\n\ncheck_backup_age() {\n    local backup_type=$1\n    local pattern=$2\n\n    latest=$(find \"${BACKUP_DIR}/${backup_type}\" -name \"${pattern}\" -type f -mtime -1 | sort -r | head -1)\n\n    if [ -z \"$latest\" ]; then\n        echo \"CRITICAL: No recent ${backup_type} backup found\" | \\\n          mail -s \"KrakenHashes Backup Alert\" \"${ALERT_EMAIL}\"\n    fi\n}\n\n# Check each backup type\ncheck_backup_age \"postgres\" \"*.dump.gz\"\ncheck_backup_age \"files\" \"*.tar.gz\"\ncheck_backup_age \"volumes\" \"postgres_data_*.tar.gz\"\n</code></pre> <p>Add to crontab: <pre><code># Monitor backups daily at 9 AM\n0 9 * * * /usr/local/bin/krakenhashes-backup-monitor.sh\n</code></pre></p>"},{"location":"admin-guide/operations/backup/#summary","title":"Summary","text":"<p>This backup strategy ensures:</p> <ul> <li>Comprehensive Coverage: All critical data is backed up</li> <li>Automation: Reduces human error and ensures consistency</li> <li>Verification: Regular testing of backup integrity</li> <li>Quick Recovery: Clear procedures for various failure scenarios</li> <li>Scalability: Procedures scale with system growth</li> </ul> <p>Regular review and testing of these procedures is essential for maintaining a reliable backup and recovery system.</p>"},{"location":"admin-guide/operations/clients/","title":"Client Management","text":"<p>KrakenHashes allows associating hashlists with specific clients or engagements. This helps organize work and enables tailored settings, such as data retention policies.</p>"},{"location":"admin-guide/operations/clients/#overview","title":"Overview","text":"<ul> <li>Purpose: Clients represent distinct entities (e.g., internal teams, external customers, specific penetration testing engagements) for which hashlists are managed.</li> <li>Association: Hashlists can be optionally linked to a client during upload.</li> <li>Administration: Administrators can create, view, update, and delete client records.</li> </ul> <p> Client Management page showing the data table with columns for Name, Description, Contact, Retention period, Creation date, and available Actions</p>"},{"location":"admin-guide/operations/clients/#managing-clients-via-api","title":"Managing Clients via API","text":"<p>Administrators use the following API endpoints to manage clients:</p> <ul> <li> <p><code>GET /api/admin/clients</code></p> <ul> <li>Description: Lists all existing clients.</li> <li>Response: An array of client objects.</li> </ul> </li> <li> <p><code>POST /api/admin/clients</code></p> <ul> <li>Description: Creates a new client.</li> <li>Request Body (Example): <pre><code>{\n  \"name\": \"Project Hydra\",\n  \"description\": \"Q3 Internal Assessment\",\n  \"contact_info\": \"team-lead@example.com\"\n}\n</code></pre></li> <li><code>name</code> is required and must be unique.</li> <li><code>description</code> and <code>contact_info</code> are optional.</li> <li>Response: The newly created client object.</li> </ul> </li> <li> <p><code>GET /api/admin/clients/{id}</code></p> <ul> <li>Description: Retrieves details for a specific client by its UUID.</li> <li>Response: A single client object.</li> </ul> </li> <li> <p><code>PUT /api/admin/clients/{id}</code></p> <ul> <li>Description: Updates an existing client.</li> <li>Request Body: Same format as POST, but fields are optional (only provided fields are updated).</li> <li>Response: The updated client object.</li> </ul> </li> <li> <p><code>DELETE /api/admin/clients/{id}</code></p> <ul> <li>Description: Deletes a client.</li> <li>Important: Deleting a client typically also involves cleaning up associated resources like hashlists or reassigning them. The exact behavior might depend on implementation details (e.g., whether associated hashlists are deleted or disassociated).</li> <li>Response: Typically a 204 No Content on success.</li> </ul> </li> </ul>"},{"location":"admin-guide/operations/clients/#client-specific-data-retention","title":"Client-Specific Data Retention","text":"<p>Administrators can configure data retention policies specific to each client. This overrides the default system-wide retention setting (see Data Retention).</p> <ul> <li>Purpose: Allows different retention periods for data belonging to different clients or engagements.</li> <li>Configuration: Client retention settings are managed alongside other client details, likely via the <code>PUT /api/admin/clients/{id}</code> endpoint or dedicated sub-endpoints (check API definition for specifics).<ul> <li>Expected Fields (Example): <pre><code>{\n  \"name\": \"Project Hydra\",\n  \"description\": \"Q3 Internal Assessment\",\n  // ... other client fields\n  \"retention_months\": 30,       // Months to retain hashlists for THIS client\n  \"retention_override\": true  // Must be true to use retention_days\n}\n</code></pre></li> </ul> </li> <li>Precedence: Client-specific retention policy always takes precedence over the default policy. </li> </ul>"},{"location":"admin-guide/operations/data-retention/","title":"Data Retention","text":"<p>KrakenHashes includes a comprehensive data retention system that automatically and securely purges old hashlists and associated data based on configurable retention policies.</p>"},{"location":"admin-guide/operations/data-retention/#overview","title":"Overview","text":"<p>The retention system ensures compliance with data retention policies by automatically removing expired data from both the database and filesystem. It includes secure deletion mechanisms to prevent data recovery.</p> <p>Potfile Exclusion</p> <p>The retention system does NOT affect the potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>), which contains plaintext passwords from all cracked hashes. The potfile is considered a permanent system resource and must be manually managed. If you need to remove passwords associated with deleted hashlists from the potfile, you must do so manually or implement a separate cleanup process.</p>"},{"location":"admin-guide/operations/data-retention/#retention-policy-configuration","title":"Retention Policy Configuration","text":""},{"location":"admin-guide/operations/data-retention/#default-retention-policy","title":"Default Retention Policy","text":"<ul> <li>Purpose: Sets the default number of months after which hashlists are automatically deleted</li> <li>Scope: Applies to all hashlists unless overridden by client-specific settings</li> <li>Special Values:<ul> <li><code>0</code> = Keep forever (no automatic deletion)</li> <li><code>NULL</code> = Use system default</li> <li>Any positive integer = Number of months to retain data</li> </ul> </li> </ul>"},{"location":"admin-guide/operations/data-retention/#client-specific-retention","title":"Client-Specific Retention","text":"<p>Each client can have their own retention period that overrides the system default: -   Set during client creation or via the client management interface -   Takes precedence over the default retention setting -   Applies to all hashlists associated with that client</p>"},{"location":"admin-guide/operations/data-retention/#automatic-purge-process","title":"Automatic Purge Process","text":""},{"location":"admin-guide/operations/data-retention/#scheduling","title":"Scheduling","text":"<p>The retention purge runs automatically: -   On Startup: 15 seconds after backend initialization -   Daily: At midnight (server time) -   Processes hashlists in batches of 1000 for scalability</p>"},{"location":"admin-guide/operations/data-retention/#what-gets-deleted","title":"What Gets Deleted","text":"<p>When a hashlist expires based on retention policy:</p> <ol> <li>Database Records:</li> <li>Hashlist record from <code>hashlists</code> table</li> <li>All associations in <code>hashlist_hashes</code> table</li> <li>Orphaned hashes (not associated with any other hashlist)</li> <li> <p>Related records via CASCADE deletion:</p> <ul> <li><code>agent_hashlists</code> entries</li> <li><code>job_executions</code> entries</li> </ul> </li> <li> <p>Filesystem:</p> </li> <li>Hashlist file from <code>/var/lib/krakenhashes/hashlists/</code></li> <li>File is securely overwritten with random data before deletion</li> <li> <p>Prevents recovery via filesystem forensics</p> </li> <li> <p>PostgreSQL Maintenance:</p> </li> <li><code>VACUUM ANALYZE</code> runs on affected tables</li> <li>Reclaims storage space</li> <li>Removes dead tuples to prevent WAL recovery</li> </ol>"},{"location":"admin-guide/operations/data-retention/#what-does-not-get-deleted","title":"What Does NOT Get Deleted","text":"<p>Retained Data</p> <p>The following data is NOT removed by the retention system:</p> <ul> <li>Potfile: Contains plaintext passwords for ALL cracked hashes across all hashlists</li> <li>Clients: Client records remain even when all their hashlists are deleted</li> <li>Users: User accounts are never deleted by retention policies</li> <li>Wordlists: Permanent resources not affected by retention</li> <li>Rules: Permanent resources not affected by retention</li> <li>Binaries: System binaries remain unchanged</li> </ul>"},{"location":"admin-guide/operations/data-retention/#security-features","title":"Security Features","text":"<ul> <li>Secure File Deletion: Files are overwritten with random data before removal</li> <li>Transaction Safety: All database operations occur within a single transaction</li> <li>Audit Logging: Complete audit trail of all deletion operations</li> <li>VACUUM Operations: Prevents data recovery from PostgreSQL internals</li> </ul>"},{"location":"admin-guide/operations/data-retention/#agent-side-cleanup","title":"Agent-Side Cleanup","text":"<p>Agents automatically clean up old files to prevent storage accumulation:</p>"},{"location":"admin-guide/operations/data-retention/#retention-policy","title":"Retention Policy","text":"<ul> <li>3-day retention for temporary files</li> <li>Runs every 6 hours automatically</li> <li>Initial cleanup 1 minute after agent startup</li> </ul>"},{"location":"admin-guide/operations/data-retention/#files-cleaned","title":"Files Cleaned","text":"<ul> <li>Hashlists: Removed after 3 days of inactivity</li> <li>Rule Chunks: Temporary rule segments deleted after 3 days</li> <li>Chunk ID Files: Removed when associated chunks are deleted</li> <li>Preserved Files: Base rules, wordlists, and binaries are never auto-deleted</li> </ul>"},{"location":"admin-guide/operations/data-retention/#configuration-via-api","title":"Configuration via API","text":""},{"location":"admin-guide/operations/data-retention/#view-current-settings","title":"View Current Settings","text":"<p><code>GET /api/admin/settings/retention</code></p> <p>Response: <pre><code>{\n  \"default_retention_months\": 36,\n  \"last_purge_run\": \"2025-09-17T10:35:00.391Z\"\n}\n</code></pre></p>"},{"location":"admin-guide/operations/data-retention/#update-retention-settings","title":"Update Retention Settings","text":"<p><code>PUT /api/admin/settings/retention</code></p> <p>Request: <pre><code>{\n  \"default_retention_months\": 24\n}\n</code></pre></p> <p>Requires administrator privileges.</p>"},{"location":"admin-guide/operations/data-retention/#important-considerations","title":"Important Considerations","text":""},{"location":"admin-guide/operations/data-retention/#data-preservation","title":"Data Preservation","text":"<ul> <li>Clients themselves are never deleted by retention policies</li> <li>Hashes shared across multiple hashlists are only deleted when orphaned</li> <li>User accounts and system settings are unaffected</li> <li>Potfile retains ALL plaintext passwords permanently - must be manually managed</li> </ul>"},{"location":"admin-guide/operations/data-retention/#compliance","title":"Compliance","text":"<ul> <li>Retention policies help meet data protection regulations</li> <li>Audit logs provide evidence of proper data disposal</li> <li>Secure deletion prevents unauthorized data recovery</li> <li>NOTE: Potfile retention may conflict with data protection requirements - consider implementing separate potfile cleanup procedures</li> </ul>"},{"location":"admin-guide/operations/data-retention/#performance-impact","title":"Performance Impact","text":"<ul> <li>VACUUM operations may temporarily impact database performance</li> <li>Purge operations are logged but don't block normal operations</li> <li>Large deletions are handled in batches to minimize impact</li> </ul>"},{"location":"admin-guide/operations/data-retention/#potfile-management","title":"Potfile Management","text":"<p>Security Critical</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) contains plaintext passwords from ALL cracked hashes and is NOT managed by the retention system. This has important implications:</p> <ol> <li>Privacy Risk: Passwords from deleted hashlists remain in the potfile</li> <li>Compliance Issue: May violate data protection regulations requiring complete deletion</li> <li>Manual Management Required: You must implement separate procedures to clean the potfile</li> </ol> <p>Recommended Actions: - Implement a separate potfile cleanup script that removes entries for deleted hashlists - Consider rotating or archiving the potfile periodically - Document potfile management procedures for compliance audits - Restrict access to the potfile to authorized personnel only</p>"},{"location":"admin-guide/operations/data-retention/#monitoring","title":"Monitoring","text":"<p>Check retention activity in the backend logs: <pre><code>docker exec krakenhashes-app tail -f /var/log/krakenhashes/backend/backend.log | grep -i purge\n</code></pre></p> <p>View last purge run time: <pre><code>SELECT value FROM client_settings WHERE key = 'last_purge_run';\n</code></pre></p>"},{"location":"admin-guide/operations/job-settings/","title":"Job Execution Settings","text":""},{"location":"admin-guide/operations/job-settings/#overview","title":"Overview","text":"<p>The Job Execution Settings page allows administrators to configure how KrakenHashes executes and distributes password cracking jobs across agents. These settings control chunking behavior, agent coordination, job control, and rule splitting strategies.</p>"},{"location":"admin-guide/operations/job-settings/#accessing-job-execution-settings","title":"Accessing Job Execution Settings","text":"<ol> <li>Navigate to the Admin Panel</li> <li>Click on Settings in the navigation menu</li> <li>Select Job Execution Settings</li> </ol> <p>The settings are organized into four main categories for easier management.</p>"},{"location":"admin-guide/operations/job-settings/#settings-categories","title":"Settings Categories","text":""},{"location":"admin-guide/operations/job-settings/#job-chunking","title":"Job Chunking","text":"<p>Job chunking divides large password cracking tasks into smaller, manageable pieces that can be distributed across multiple agents. This improves resource utilization and allows for better job scheduling.</p> Setting Description Default Range Notes Default Chunk Duration How long each job chunk should run 15 minutes 1+ minutes Shorter chunks provide more flexibility but increase overhead Chunk Fluctuation Percentage Allowed variance for the final chunk 10% 0-100% Prevents creating very small final chunks"},{"location":"admin-guide/operations/job-settings/#best-practices-for-chunking","title":"Best Practices for Chunking","text":"<ul> <li>Short jobs (&lt; 1 hour): Use 5-10 minute chunks for better distribution</li> <li>Long jobs (&gt; 24 hours): Use 30-60 minute chunks to reduce overhead</li> <li>Mixed agent speeds: Shorter chunks help balance workload</li> </ul>"},{"location":"admin-guide/operations/job-settings/#agent-configuration","title":"Agent Configuration","text":"<p>These settings control how agents behave and interact with the backend server.</p> Setting Description Default Range Notes Hashlist Retention How long agents keep hashlists after job completion 7 days 1+ days Reduces re-download for recurring jobs Max Concurrent Jobs per Agent Maximum jobs an agent can run simultaneously 1 1-10 Higher values for powerful multi-GPU systems Progress Reporting Interval How often agents send progress updates 30 seconds 1+ seconds Lower values increase server load Benchmark Cache Duration How long to cache agent performance benchmarks 30 days 1+ days Reduces benchmark frequency Speedtest Timeout Maximum time to wait for speedtest completion 30 seconds 60-600 seconds Increase for slower systems Reconnect Grace Period Time to wait for agents to reconnect after server restart 5 minutes 1-60 minutes Prevents unnecessary task reassignment"},{"location":"admin-guide/operations/job-settings/#reconnect-grace-period-details","title":"Reconnect Grace Period Details","text":"<p>The Reconnect Grace Period is a critical setting for maintaining job continuity during server maintenance or unexpected restarts:</p> <ul> <li>Purpose: Allows agents with running tasks to reconnect and continue their work without losing progress</li> <li>How it works: </li> <li>When the backend restarts, tasks transition to <code>reconnect_pending</code> state</li> <li>Agents cache crack data locally and continue processing</li> <li>Upon reconnection, agents report their current task status</li> <li>If reconnected within the grace period, tasks resume automatically</li> <li>Recommended values:</li> <li>5 minutes (default): Good for most environments</li> <li>10-15 minutes: For environments with slower network recovery</li> <li>1-3 minutes: For highly available setups with quick recovery</li> </ul>"},{"location":"admin-guide/operations/job-settings/#job-control","title":"Job Control","text":"<p>Control job execution behavior and user interface settings.</p> Setting Description Default Range Notes Allow Job Interruption Higher priority jobs can interrupt running jobs Enabled On/Off Ensures critical jobs run immediately Real-time Crack Notifications Send notifications when hashes are cracked Enabled On/Off Can increase server load for large jobs Job Refresh Interval How often the UI refreshes job status 5 seconds 1-60 seconds Lower values increase server load Max Chunk Retry Attempts Number of times to retry failed chunks 3 0-10 Set to 0 to disable retries Jobs Per Page Default pagination size for job lists 25 5-100 Adjust based on UI preferences"},{"location":"admin-guide/operations/job-settings/#job-interruption-behavior","title":"Job Interruption Behavior","text":"<p>When enabled, the system will: 1. Pause lower priority jobs when higher priority jobs arrive 2. Save the state of interrupted jobs 3. Resume interrupted jobs once higher priority jobs complete 4. Maintain crack progress for all interrupted jobs</p>"},{"location":"admin-guide/operations/job-settings/#rule-splitting","title":"Rule Splitting","text":"<p>Rule splitting automatically divides large rule files to improve distribution across agents. This is especially useful for rule files that would otherwise exceed the chunk duration.</p> Setting Description Default Range Notes Enable Rule Splitting Automatically split large rule files Enabled On/Off Improves distribution for large rule sets Rule Split Threshold Split when estimated time exceeds chunk duration by this factor 2.0\u00d7 1.1-10\u00d7 Lower values create more chunks Minimum Rules to Split Don't split files with fewer rules than this 100 10+ Prevents splitting small files Maximum Rule Chunks Maximum chunks to create per rule file 100 2-10000 Limits memory usage Rule Chunk Directory Directory for temporary rule chunks <code>/tmp/rule_chunks</code> Any valid path Must be writable by backend"},{"location":"admin-guide/operations/job-settings/#rule-splitting-algorithm","title":"Rule Splitting Algorithm","text":"<p>The system automatically: 1. Estimates job duration based on hashlist size and rule count 2. Compares estimated duration to chunk duration \u00d7 threshold 3. If exceeding threshold, splits rules into appropriate chunks 4. Distributes chunks across available agents 5. Cleans up temporary chunks after job completion</p>"},{"location":"admin-guide/operations/job-settings/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/operations/job-settings/#network-load","title":"Network Load","text":"<ul> <li>Progress Reporting Interval: Each update creates network traffic</li> <li>Job Refresh Interval: Affects UI responsiveness and server load</li> <li>Calculate: <code>(Number of Agents \u00d7 Active Jobs) / Reporting Interval = Updates per second</code></li> </ul>"},{"location":"admin-guide/operations/job-settings/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>Hashlist Retention: <code>Average Hashlist Size \u00d7 Number of Unique Jobs \u00d7 Retention Days</code></li> <li>Rule Chunks: <code>Original Rule File Size \u00d7 Active Jobs using that rule</code></li> <li>Benchmark Cache: Minimal, typically &lt; 1MB per agent</li> </ul>"},{"location":"admin-guide/operations/job-settings/#optimal-settings-by-environment","title":"Optimal Settings by Environment","text":""},{"location":"admin-guide/operations/job-settings/#small-environment-1-5-agents","title":"Small Environment (1-5 agents)","text":"<ul> <li>Chunk Duration: 10-15 minutes</li> <li>Progress Interval: 30 seconds</li> <li>Max Concurrent Jobs: 1</li> <li>Grace Period: 5 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#medium-environment-5-20-agents","title":"Medium Environment (5-20 agents)","text":"<ul> <li>Chunk Duration: 15-30 minutes</li> <li>Progress Interval: 60 seconds</li> <li>Max Concurrent Jobs: 1-2</li> <li>Grace Period: 10 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#large-environment-20-agents","title":"Large Environment (20+ agents)","text":"<ul> <li>Chunk Duration: 30-60 minutes</li> <li>Progress Interval: 120 seconds</li> <li>Max Concurrent Jobs: 2-3</li> <li>Grace Period: 15 minutes</li> </ul>"},{"location":"admin-guide/operations/job-settings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/job-settings/#common-issues","title":"Common Issues","text":""},{"location":"admin-guide/operations/job-settings/#agents-not-receiving-jobs","title":"Agents Not Receiving Jobs","text":"<ul> <li>Check Max Concurrent Jobs per Agent setting</li> <li>Verify agents are not at capacity</li> <li>Review job priority settings</li> </ul>"},{"location":"admin-guide/operations/job-settings/#poor-job-distribution","title":"Poor Job Distribution","text":"<ul> <li>Reduce Default Chunk Duration for better granularity</li> <li>Enable Rule Splitting for large rule files</li> <li>Adjust Chunk Fluctuation Percentage to avoid tiny chunks</li> </ul>"},{"location":"admin-guide/operations/job-settings/#high-server-load","title":"High Server Load","text":"<ul> <li>Increase Progress Reporting Interval</li> <li>Increase Job Refresh Interval</li> <li>Disable Real-time Crack Notifications for large jobs</li> </ul>"},{"location":"admin-guide/operations/job-settings/#lost-progress-after-server-restart","title":"Lost Progress After Server Restart","text":"<ul> <li>Increase Reconnect Grace Period</li> <li>Ensure agents have stable network connections</li> <li>Check agent logs for reconnection issues</li> </ul>"},{"location":"admin-guide/operations/job-settings/#monitoring-settings-impact","title":"Monitoring Settings Impact","text":"<p>Use the following metrics to evaluate settings effectiveness: - Average chunk completion time vs. configured duration - Number of retry attempts per job - Agent utilization percentage - Task reassignment frequency after restarts</p>"},{"location":"admin-guide/operations/job-settings/#related-documentation","title":"Related Documentation","text":"<ul> <li>Agent Management - Managing and monitoring agents</li> <li>Job Chunking - Detailed chunking strategies</li> <li>Performance Tuning - System optimization</li> <li>Rule Management - Managing rule files</li> </ul>"},{"location":"admin-guide/operations/monitoring/","title":"System Monitoring Guide","text":"<p>This guide covers comprehensive monitoring strategies for KrakenHashes, including system health indicators, performance metrics, logging, and alerting configurations.</p>"},{"location":"admin-guide/operations/monitoring/#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Health Indicators</li> <li>Job Monitoring and Statistics</li> <li>Agent Performance Metrics</li> <li>Database Monitoring</li> <li>Log Analysis and Alerting</li> <li>Performance Baselines</li> <li>Monitoring Dashboards and Tools</li> </ol>"},{"location":"admin-guide/operations/monitoring/#system-health-indicators","title":"System Health Indicators","text":""},{"location":"admin-guide/operations/monitoring/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>The system provides a basic health check endpoint for monitoring service availability:</p> <pre><code># Check system health\ncurl https://localhost:31337/api/health\n\n# Expected response\n200 OK\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#service-status-monitoring","title":"Service Status Monitoring","text":"<p>Monitor the following key services:</p> <ol> <li>Backend API Service</li> <li>Port: 31337 (HTTPS), 1337 (HTTP for CA cert)</li> <li>Health endpoint: <code>/api/health</code></li> <li> <p>Version endpoint: <code>/api/version</code></p> </li> <li> <p>PostgreSQL Database</p> </li> <li>Port: 5432</li> <li>Connection pool status</li> <li> <p>Active connections</p> </li> <li> <p>WebSocket Service</p> </li> <li>Agent connections</li> <li>Heartbeat status</li> <li>Connection count</li> </ol>"},{"location":"admin-guide/operations/monitoring/#docker-container-health","title":"Docker Container Health","text":"<p>Monitor container status using Docker commands:</p> <pre><code># Check container status\ndocker-compose ps\n\n# Monitor resource usage\ndocker stats\n\n# Check container logs\ndocker-compose logs -f backend\ndocker-compose logs -f postgres\ndocker-compose logs -f app\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#job-monitoring-and-statistics","title":"Job Monitoring and Statistics","text":""},{"location":"admin-guide/operations/monitoring/#job-execution-metrics","title":"Job Execution Metrics","text":"<p>The system tracks comprehensive job execution metrics:</p> <ol> <li>Job Status Distribution</li> <li>Pending jobs count</li> <li>Running jobs count</li> <li>Completed jobs count</li> <li>Failed jobs count</li> <li> <p>Cancelled jobs count</p> </li> <li> <p>Job Performance Indicators</p> </li> <li>Average job completion time</li> <li>Job success rate</li> <li>Hash cracking rate</li> <li>Keyspace coverage</li> </ol>"},{"location":"admin-guide/operations/monitoring/#job-monitoring-endpoints","title":"Job Monitoring Endpoints","text":"<pre><code># List all jobs with pagination\nGET /api/jobs?page=1&amp;page_size=20\n\n# Get specific job details\nGET /api/jobs/{job_id}\n\n# Get job statistics\nGET /api/jobs/stats\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#job-progress-tracking","title":"Job Progress Tracking","text":"<p>Monitor job progress through these metrics:</p> <ul> <li>Dispatched Percentage: Portion of keyspace distributed to agents</li> <li>Searched Percentage: Portion of keyspace actually processed</li> <li>Overall Progress: Combined metric considering rule splitting</li> <li>Cracked Count: Number of successfully cracked hashes</li> <li>Total Speed: Combined hash rate across all agents</li> </ul>"},{"location":"admin-guide/operations/monitoring/#enhanced-job-details-view","title":"Enhanced Job Details View","text":"<p>The Job Details page now includes a comprehensive completed tasks history that provides administrators with:</p>"},{"location":"admin-guide/operations/monitoring/#completed-tasks-monitoring","title":"Completed Tasks Monitoring","text":"<ul> <li>Historical Performance Data: View hash rates achieved by each agent for completed tasks</li> <li>Task Completion Patterns: Analyze when and how tasks were completed across the job lifecycle</li> <li>Keyspace Analysis: Identify which keyspace ranges were most productive for password cracking</li> <li>Audit Trail: Maintain a complete record of job execution across distributed agents</li> </ul>"},{"location":"admin-guide/operations/monitoring/#available-metrics","title":"Available Metrics","text":"<p>For each completed task, administrators can review: - Agent assignment and performance metrics - Exact keyspace ranges processed - Completion timestamps for timeline analysis - Number of cracks found per task - Average hash rate achieved during execution</p>"},{"location":"admin-guide/operations/monitoring/#use-cases-for-administrators","title":"Use Cases for Administrators","text":"<ul> <li>Performance Optimization: Compare agent efficiency across tasks to identify hardware or configuration issues</li> <li>Resource Planning: Analyze task distribution patterns to optimize future job configurations</li> <li>Troubleshooting: Identify if specific agents or keyspace ranges consistently underperform</li> <li>Compliance Reporting: Maintain detailed execution records for security audits</li> </ul> <p>Data Retention</p> <p>Completed tasks are retained even after job completion, providing long-term visibility into job execution history. This data persists according to your configured retention policies.</p>"},{"location":"admin-guide/operations/monitoring/#database-queries-for-job-monitoring","title":"Database Queries for Job Monitoring","text":"<pre><code>-- Active jobs by status\nSELECT status, COUNT(*) as count \nFROM job_executions \nGROUP BY status;\n\n-- Jobs with high failure rate\nSELECT je.id, je.name, je.error_message,\n       COUNT(jt.id) as total_tasks,\n       SUM(CASE WHEN jt.status = 'failed' THEN 1 ELSE 0 END) as failed_tasks\nFROM job_executions je\nJOIN job_tasks jt ON je.id = jt.job_execution_id\nWHERE je.status = 'failed'\nGROUP BY je.id, je.name, je.error_message\nHAVING SUM(CASE WHEN jt.status = 'failed' THEN 1 ELSE 0 END) &gt; 0;\n\n-- Job performance over time\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    COUNT(*) as jobs_created,\n    AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration_seconds\nFROM job_executions\nWHERE completed_at IS NOT NULL\nGROUP BY hour\nORDER BY hour DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#agent-performance-metrics","title":"Agent Performance Metrics","text":""},{"location":"admin-guide/operations/monitoring/#agent-metrics-collection","title":"Agent Metrics Collection","text":"<p>The agent collects and reports the following metrics:</p> <ol> <li>System Metrics</li> <li>CPU usage percentage</li> <li>Memory usage percentage</li> <li>GPU utilization</li> <li>GPU temperature</li> <li> <p>GPU memory usage</p> </li> <li> <p>Performance Metrics</p> </li> <li>Hash rate per device</li> <li>Power usage</li> </ol> <p> Device monitoring dashboard showing real-time temperature, utilization, fan speed, and hash rate metrics across multiple agents</p> <p> Alternative view of the device monitoring interface with detailed performance graphs and timeline data    - Fan speed    - Device temperature</p>"},{"location":"admin-guide/operations/monitoring/#agent-monitoring-endpoints","title":"Agent Monitoring Endpoints","text":"<pre><code># List all agents\nGET /api/admin/agents\n\n# Get agent details with devices\nGET /api/admin/agents/{agent_id}\n\n# Get agent performance metrics\nGET /api/admin/agents/{agent_id}/metrics?timeRange=1h&amp;metrics=temperature,utilization,fanspeed,hashrate\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#agent-health-monitoring","title":"Agent Health Monitoring","text":"<p>Monitor agent health through:</p> <ol> <li>Heartbeat Status</li> <li>Last heartbeat timestamp</li> <li>Connection status (active/inactive)</li> <li> <p>Heartbeat interval (30 seconds default)</p> </li> <li> <p>Error Tracking</p> </li> <li>Last error message</li> <li>Error frequency</li> <li>Recovery status</li> </ol>"},{"location":"admin-guide/operations/monitoring/#database-queries-for-agent-monitoring","title":"Database Queries for Agent Monitoring","text":"<pre><code>-- Agents with stale heartbeats\nSELECT id, name, last_heartbeat, status\nFROM agents\nWHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes'\n  AND status = 'active';\n\n-- Agent performance metrics\nSELECT \n    a.name as agent_name,\n    apm.metric_type,\n    AVG(apm.value) as avg_value,\n    MAX(apm.value) as max_value,\n    MIN(apm.value) as min_value\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.timestamp &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.name, apm.metric_type;\n\n-- GPU device utilization\nSELECT \n    a.name as agent_name,\n    apm.device_name,\n    apm.metric_type,\n    AVG(apm.value) as avg_utilization\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.metric_type = 'utilization'\n  AND apm.timestamp &gt; NOW() - INTERVAL '1 hour'\nGROUP BY a.name, apm.device_name, apm.metric_type\nORDER BY avg_utilization DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#database-monitoring","title":"Database Monitoring","text":""},{"location":"admin-guide/operations/monitoring/#connection-pool-monitoring","title":"Connection Pool Monitoring","text":"<p>Monitor database connection health:</p> <pre><code>-- Active connections by state\nSELECT state, COUNT(*) \nFROM pg_stat_activity \nGROUP BY state;\n\n-- Long-running queries\nSELECT \n    pid,\n    now() - pg_stat_activity.query_start AS duration,\n    query,\n    state\nFROM pg_stat_activity\nWHERE (now() - pg_stat_activity.query_start) &gt; interval '5 minutes';\n\n-- Database size growth\nSELECT \n    pg_database.datname,\n    pg_size_pretty(pg_database_size(pg_database.datname)) AS size\nFROM pg_database\nORDER BY pg_database_size(pg_database.datname) DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#table-statistics","title":"Table Statistics","text":"<p>Monitor table growth and performance:</p> <pre><code>-- Table sizes\nSELECT\n    schemaname AS table_schema,\n    tablename AS table_name,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,\n    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS data_size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n-- Index usage\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE schemaname = 'public'\nORDER BY idx_scan DESC;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#performance-metrics-tables","title":"Performance Metrics Tables","text":"<p>The system maintains dedicated tables for performance tracking:</p> <ol> <li>agent_metrics - Real-time agent system metrics</li> <li>agent_performance_metrics - Detailed performance data with aggregation</li> <li>job_performance_metrics - Job execution performance tracking</li> <li>agent_benchmarks - Hashcat benchmark results per agent</li> </ol>"},{"location":"admin-guide/operations/monitoring/#log-analysis-and-alerting","title":"Log Analysis and Alerting","text":""},{"location":"admin-guide/operations/monitoring/#log-configuration","title":"Log Configuration","text":"<p>Configure logging through environment variables:</p> <pre><code># Enable debug logging\nexport DEBUG=true\n\n# Set log level (DEBUG, INFO, WARNING, ERROR)\nexport LOG_LEVEL=INFO\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#log-locations","title":"Log Locations","text":"<p>When running with Docker, logs are stored in:</p> <pre><code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/\n\u251c\u2500\u2500 backend/      # Backend application logs\n\u251c\u2500\u2500 postgres/     # PostgreSQL logs\n\u2514\u2500\u2500 nginx/        # Nginx/frontend logs\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#log-format","title":"Log Format","text":"<p>The system uses structured logging with the following format: <pre><code>[LEVEL] [TIMESTAMP] [FILE:LINE] [FUNCTION] MESSAGE\n</code></pre></p> <p>Example: <pre><code>[INFO] [2025-08-01 15:04:05.000] [/path/to/file.go:42] [FunctionName] Processing job execution\n</code></pre></p>"},{"location":"admin-guide/operations/monitoring/#key-log-patterns-to-monitor","title":"Key Log Patterns to Monitor","text":"<ol> <li> <p>Error Patterns <pre><code># Find all errors across logs\ngrep -i \"error\" /home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/*/*.log\n\n# Find database connection errors\ngrep -i \"database.*error\\|connection.*failed\" logs/backend/*.log\n\n# Find agent disconnections\ngrep -i \"agent.*disconnect\\|websocket.*close\" logs/backend/*.log\n</code></pre></p> </li> <li> <p>Performance Issues <pre><code># Find slow queries\ngrep -i \"slow query\\|query took\" logs/backend/*.log\n\n# Find memory issues\ngrep -i \"out of memory\\|memory.*limit\" logs/*/*.log\n</code></pre></p> </li> <li> <p>Security Events <pre><code># Find authentication failures\ngrep -i \"auth.*fail\\|login.*fail\\|unauthorized\" logs/backend/*.log\n\n# Find suspicious activity\ngrep -i \"invalid.*token\\|forbidden\\|suspicious\" logs/backend/*.log\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/monitoring/#alert-configuration","title":"Alert Configuration","text":"<p>Set up alerts for critical events:</p> <ol> <li>System Health Alerts</li> <li>Service down (health check fails)</li> <li>Database connection pool exhausted</li> <li> <p>High error rate (&gt;5% of requests)</p> </li> <li> <p>Performance Alerts</p> </li> <li>CPU usage &gt; 90% for 5 minutes</li> <li>Memory usage &gt; 85%</li> <li>Database query time &gt; 5 seconds</li> <li> <p>Job queue backlog &gt; 100 jobs</p> </li> <li> <p>Security Alerts</p> </li> <li>Multiple failed login attempts</li> <li>Unauthorized API access attempts</li> <li>Agent registration anomalies</li> </ol>"},{"location":"admin-guide/operations/monitoring/#performance-baselines","title":"Performance Baselines","text":""},{"location":"admin-guide/operations/monitoring/#establishing-baselines","title":"Establishing Baselines","text":"<p>Monitor and document normal operating parameters:</p> <ol> <li>System Resource Baselines</li> <li>Normal CPU usage: 20-40% (idle), 60-80% (active jobs)</li> <li>Memory usage: 2-4GB (base), +1GB per 1M hashes</li> <li> <p>Database connections: 10-20 (normal load)</p> </li> <li> <p>Job Performance Baselines</p> </li> <li>Job creation rate: 10-50 jobs/hour</li> <li>Average job duration: Varies by attack type</li> <li> <p>Hash processing rate: Device-dependent</p> </li> <li> <p>Agent Performance Baselines</p> </li> <li>Heartbeat interval: 30 seconds</li> <li>Benchmark cache duration: 24 hours</li> <li>GPU utilization: 90-100% during jobs</li> </ol>"},{"location":"admin-guide/operations/monitoring/#benchmark-tracking","title":"Benchmark Tracking","text":"<p>The system automatically tracks agent benchmarks:</p> <pre><code>-- View agent benchmarks\nSELECT \n    a.name as agent_name,\n    ab.attack_mode,\n    ab.hash_type,\n    ab.speed,\n    ab.updated_at\nFROM agents a\nJOIN agent_benchmarks ab ON a.id = ab.agent_id\nORDER BY a.name, ab.attack_mode, ab.hash_type;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#performance-degradation-detection","title":"Performance Degradation Detection","text":"<p>Monitor for performance degradation:</p> <pre><code>-- Compare current vs historical performance\nWITH current_metrics AS (\n    SELECT \n        agent_id,\n        AVG(value) as current_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hash_rate'\n      AND timestamp &gt; NOW() - INTERVAL '1 hour'\n    GROUP BY agent_id\n),\nhistorical_metrics AS (\n    SELECT \n        agent_id,\n        AVG(value) as historical_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hash_rate'\n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '1 day'\n    GROUP BY agent_id\n)\nSELECT \n    a.name,\n    cm.current_avg,\n    hm.historical_avg,\n    ((cm.current_avg - hm.historical_avg) / hm.historical_avg * 100) as percent_change\nFROM agents a\nJOIN current_metrics cm ON a.id = cm.agent_id\nJOIN historical_metrics hm ON a.id = hm.agent_id\nWHERE ABS((cm.current_avg - hm.historical_avg) / hm.historical_avg) &gt; 0.1;\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#monitoring-dashboards-and-tools","title":"Monitoring Dashboards and Tools","text":""},{"location":"admin-guide/operations/monitoring/#built-in-monitoring-endpoints","title":"Built-in Monitoring Endpoints","text":"<ol> <li>System Status Dashboard</li> <li>Real-time agent status</li> <li>Active job count</li> <li>System resource usage</li> <li> <p>Recent errors</p> </li> <li> <p>Job Monitoring Dashboard</p> </li> <li>Job queue status</li> <li>Job progress tracking</li> <li>Success/failure rates</li> <li> <p>Performance trends</p> </li> <li> <p>Agent Performance Dashboard</p> </li> <li>Agent availability</li> <li>Device utilization</li> <li>Temperature monitoring</li> <li>Hash rate tracking</li> </ol>"},{"location":"admin-guide/operations/monitoring/#external-monitoring-integration","title":"External Monitoring Integration","text":"<p>The system can be integrated with external monitoring tools:</p> <ol> <li>Prometheus Integration</li> <li>Export metrics via <code>/metrics</code> endpoint (if implemented)</li> <li>Custom metric exporters</li> <li> <p>Alert manager integration</p> </li> <li> <p>Grafana Dashboards</p> </li> <li>PostgreSQL data source</li> <li>Custom dashboard templates</li> <li> <p>Alert visualization</p> </li> <li> <p>Log Aggregation</p> </li> <li>ELK Stack (Elasticsearch, Logstash, Kibana)</li> <li>Fluentd/Fluent Bit</li> <li>Centralized log analysis</li> </ol>"},{"location":"admin-guide/operations/monitoring/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Regular Health Checks</li> <li>Automated health check every 30 seconds</li> <li>Alert on 3 consecutive failures</li> <li> <p>Include dependency checks</p> </li> <li> <p>Capacity Planning</p> </li> <li>Monitor growth trends</li> <li>Plan for peak usage</li> <li> <p>Scale resources proactively</p> </li> <li> <p>Performance Optimization</p> </li> <li>Regular benchmark updates</li> <li>Query optimization based on metrics</li> <li> <p>Resource allocation tuning</p> </li> <li> <p>Security Monitoring</p> </li> <li>Audit log analysis</li> <li>Anomaly detection</li> <li>Access pattern monitoring</li> </ol>"},{"location":"admin-guide/operations/monitoring/#troubleshooting-guide","title":"Troubleshooting Guide","text":"<p>Common issues and monitoring approaches:</p> <ol> <li>High CPU Usage</li> <li>Check active job count</li> <li>Verify agent task distribution</li> <li> <p>Monitor database query performance</p> </li> <li> <p>Memory Leaks</p> </li> <li>Track memory usage over time</li> <li>Identify growing processes</li> <li> <p>Check for unclosed connections</p> </li> <li> <p>Slow Job Processing</p> </li> <li>Verify agent benchmarks</li> <li>Check network latency</li> <li> <p>Monitor file I/O performance</p> </li> <li> <p>Database Performance</p> </li> <li>Analyze slow queries</li> <li>Check index usage</li> <li>Monitor connection pool</li> </ol>"},{"location":"admin-guide/operations/monitoring/#maintenance-and-cleanup","title":"Maintenance and Cleanup","text":""},{"location":"admin-guide/operations/monitoring/#automated-cleanup-services","title":"Automated Cleanup Services","text":"<p>The system includes several cleanup services:</p> <ol> <li>Metrics Cleanup Service</li> <li>Aggregates real-time metrics to daily/weekly</li> <li>Removes old metrics based on retention policy</li> <li> <p>Runs automatically on schedule</p> </li> <li> <p>Agent Cleanup Service</p> </li> <li>Marks stale agents as inactive</li> <li>Cleans up orphaned resources</li> <li> <p>Maintains agent health status</p> </li> <li> <p>Job Cleanup Service</p> </li> <li>Archives completed jobs</li> <li>Removes temporary files</li> <li>Updates job statistics</li> </ol>"},{"location":"admin-guide/operations/monitoring/#manual-maintenance-tasks","title":"Manual Maintenance Tasks","text":"<pre><code># Force cleanup of old metrics\ncurl -X POST https://localhost:31337/api/admin/force-cleanup\n\n# Vacuum database\ndocker exec -it krakenhashes_postgres_1 psql -U postgres -d krakenhashes -c \"VACUUM ANALYZE;\"\n\n# Check database bloat\ndocker exec -it krakenhashes_postgres_1 psql -U postgres -d krakenhashes -c \"\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size\nFROM pg_tables\nWHERE schemaname = 'public'\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\nLIMIT 10;\"\n</code></pre>"},{"location":"admin-guide/operations/monitoring/#conclusion","title":"Conclusion","text":"<p>Effective monitoring is crucial for maintaining a healthy KrakenHashes deployment. Regular monitoring of system health, job performance, and agent metrics ensures optimal operation and early detection of issues. Implement automated alerting for critical metrics and maintain historical data for trend analysis and capacity planning.</p>"},{"location":"admin-guide/operations/potfile/","title":"Potfile Management","text":"<p>The potfile (short for \"pot file\" from hashcat terminology) is an automated feature in KrakenHashes that accumulates successfully cracked passwords into a specialized wordlist. This dynamic wordlist significantly improves cracking efficiency by trying previously successful passwords against new hashes.</p>"},{"location":"admin-guide/operations/potfile/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>How It Works</li> <li>Configuration Settings</li> <li>File Structure and Location</li> <li>Staging and Processing Mechanism</li> <li>Integration with Jobs</li> <li>Monitoring and Troubleshooting</li> <li>Best Practices</li> </ol>"},{"location":"admin-guide/operations/potfile/#overview","title":"Overview","text":""},{"location":"admin-guide/operations/potfile/#purpose","title":"Purpose","text":"<p>The potfile serves as an organizational memory of all passwords that have been successfully cracked. By maintaining this list, KrakenHashes can:</p> <ul> <li>Accelerate future cracking: Common passwords that worked before are likely to work again</li> <li>Identify password reuse: Quickly detect when the same password is used across different accounts</li> <li>Build organizational intelligence: Accumulate knowledge of password patterns specific to your targets</li> <li>Optimize resource usage: Reduce GPU time by trying known passwords first</li> </ul>"},{"location":"admin-guide/operations/potfile/#key-benefits","title":"Key Benefits","text":"<ol> <li>Automatic Management: No manual intervention required - the system handles everything</li> <li>Real-time Updates: Passwords are staged immediately upon cracking</li> <li>Deduplication: Prevents duplicate entries automatically</li> <li>Distributed Access: All agents receive the updated potfile for use in jobs</li> <li>Performance Optimization: Batch processing minimizes system overhead</li> </ol>"},{"location":"admin-guide/operations/potfile/#how-it-works","title":"How It Works","text":"<p>The potfile system operates through a multi-stage automated process:</p>"},{"location":"admin-guide/operations/potfile/#1-password-crack-detection","title":"1. Password Crack Detection","text":"<p>When an agent successfully cracks a password hash: - The cracked password is sent to the backend server - The backend records the crack in the database - The password is checked against existing potfile entries</p>"},{"location":"admin-guide/operations/potfile/#2-staging-process","title":"2. Staging Process","text":"<p>If the password is new (not already in the potfile): - It's added to the <code>potfile_staging</code> table - The entry includes the plaintext password and original hash value - Multiple passwords can accumulate in staging</p>"},{"location":"admin-guide/operations/potfile/#3-batch-processing","title":"3. Batch Processing","text":"<p>A background worker runs periodically (default: every 60 seconds): - Retrieves all unprocessed entries from staging - Loads the current potfile into memory for deduplication - Filters out any passwords already in the potfile - Appends new unique passwords to the potfile - Deletes processed entries from the staging table</p>"},{"location":"admin-guide/operations/potfile/#4-distribution","title":"4. Distribution","text":"<p>Once updated: - The potfile's MD5 hash is recalculated - Agents are notified of the update - The potfile becomes available for use in cracking jobs</p>"},{"location":"admin-guide/operations/potfile/#configuration-settings","title":"Configuration Settings","text":"<p>The potfile feature is controlled through system settings in the database:</p> Setting Default Description <code>potfile_enabled</code> <code>true</code> Master switch to enable/disable the potfile feature <code>potfile_batch_interval</code> <code>60</code> Seconds between batch processing runs <code>potfile_max_batch_size</code> <code>1000</code> Maximum number of entries to process in a single batch <code>potfile_wordlist_id</code> (auto) Database ID of the potfile wordlist entry <code>potfile_preset_job_id</code> (auto) Database ID of the associated preset job"},{"location":"admin-guide/operations/potfile/#modifying-settings","title":"Modifying Settings","text":"<p>Settings can be modified through direct database updates:</p> <pre><code>-- Change batch interval to 30 seconds\nUPDATE system_settings \nSET value = '30' \nWHERE key = 'potfile_batch_interval';\n\n-- Increase max batch size\nUPDATE system_settings \nSET value = '5000' \nWHERE key = 'potfile_max_batch_size';\n\n-- Disable potfile feature\nUPDATE system_settings \nSET value = 'false' \nWHERE key = 'potfile_enabled';\n</code></pre> <p>Note: Changes to settings require a server restart to take effect.</p>"},{"location":"admin-guide/operations/potfile/#file-structure-and-location","title":"File Structure and Location","text":""},{"location":"admin-guide/operations/potfile/#file-location","title":"File Location","text":"<pre><code>&lt;data_dir&gt;/wordlists/custom/potfile.txt\n</code></pre> <p>Typically: <pre><code>/data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre></p>"},{"location":"admin-guide/operations/potfile/#file-format","title":"File Format","text":"<ul> <li>Plain text file: One password per line</li> <li>Initial content: Starts with a single blank line (representing an empty password)</li> <li>Encoding: UTF-8</li> <li>Line endings: Unix-style (LF)</li> </ul>"},{"location":"admin-guide/operations/potfile/#example-content","title":"Example Content","text":"<pre><code>(blank line)\npassword123\nAdmin@1234\nWelcome2024!\nCompanyName123\n</code></pre>"},{"location":"admin-guide/operations/potfile/#automatic-creation","title":"Automatic Creation","text":"<ul> <li>Created automatically on first server startup</li> <li>Initialized with a single blank line</li> <li>Registered as a wordlist in the database</li> <li>Associated with a preset job for easy use</li> </ul> <p>Important: The potfile preset job requires at least one hashcat binary version to be uploaded to the system. On fresh installations: - The potfile wordlist is created immediately - The preset job creation is deferred until a binary is available - A background monitor checks every 5 seconds for binary availability - Once a binary is uploaded, the preset job is automatically created</p>"},{"location":"admin-guide/operations/potfile/#staging-and-processing-mechanism","title":"Staging and Processing Mechanism","text":""},{"location":"admin-guide/operations/potfile/#staging-table-structure","title":"Staging Table Structure","text":"<p>The <code>potfile_staging</code> table temporarily holds passwords before batch processing:</p> Column Type Description <code>id</code> integer Unique identifier <code>password</code> text The plaintext password <code>hash_value</code> text Original hash that was cracked <code>created_at</code> timestamp When the entry was staged <code>processed</code> boolean No longer used (entries are deleted after processing)"},{"location":"admin-guide/operations/potfile/#processing-workflow","title":"Processing Workflow","text":"<pre><code>graph TD\n    A[Password Cracked] --&gt; B{Already in Potfile?}\n    B --&gt;|No| C[Add to Staging Table]\n    B --&gt;|Yes| D[Skip - Duplicate]\n    C --&gt; E[Wait for Batch Timer]\n    E --&gt; F[Background Worker Runs]\n    F --&gt; G[Load Potfile into Memory]\n    G --&gt; H[Filter Duplicates]\n    H --&gt; I[Append to Potfile]\n    I --&gt; J[Delete from Staging]\n    J --&gt; K[Update Metadata]</code></pre>"},{"location":"admin-guide/operations/potfile/#deduplication-logic","title":"Deduplication Logic","text":"<ul> <li>Comparison basis: Plaintext passwords only (not hashes)</li> <li>Case sensitivity: Passwords are case-sensitive</li> <li>Empty passwords: Blank lines are valid passwords</li> <li>Within-batch: Duplicates within the same batch are also filtered</li> </ul>"},{"location":"admin-guide/operations/potfile/#integration-with-jobs","title":"Integration with Jobs","text":""},{"location":"admin-guide/operations/potfile/#automatic-preset-job","title":"Automatic Preset Job","text":"<p>The system automatically creates a preset job for the potfile: - Name: \"Potfile Run\" - Type: Dictionary attack using the potfile as the wordlist - Priority: Can be configured for high-priority execution - Agents: Available to all agents</p> <p>Note: If you don't see the \"Potfile Run\" preset job after installation, ensure you have uploaded at least one hashcat binary through Admin \u2192 Binary Management. The preset job will be created automatically once a binary is available.</p>"},{"location":"admin-guide/operations/potfile/#using-the-potfile-in-jobs","title":"Using the Potfile in Jobs","text":"<ol> <li>Automatic inclusion: The potfile preset job can be included in job workflows</li> <li>Manual selection: Administrators can select the potfile wordlist when creating custom jobs</li> <li>First-pass attack: Often used as the first attack in a workflow due to high success rate</li> </ol>"},{"location":"admin-guide/operations/potfile/#keyspace-calculation","title":"Keyspace Calculation","text":"<ul> <li>The potfile's word count is automatically updated after each batch</li> <li>Keyspace for jobs using the potfile adjusts dynamically</li> <li>Agents receive updated keyspace information</li> </ul>"},{"location":"admin-guide/operations/potfile/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"admin-guide/operations/potfile/#check-potfile-status","title":"Check Potfile Status","text":"<pre><code># View potfile contents\ncat /data/krakenhashes/wordlists/custom/potfile.txt\n\n# Count passwords in potfile\nwc -l /data/krakenhashes/wordlists/custom/potfile.txt\n\n# Check file size\nls -lh /data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre>"},{"location":"admin-guide/operations/potfile/#monitor-staging-table","title":"Monitor Staging Table","text":"<pre><code>-- Count staged entries\nSELECT COUNT(*) FROM potfile_staging;\n\n-- View recent staged passwords\nSELECT password, created_at \nFROM potfile_staging \nORDER BY created_at DESC \nLIMIT 10;\n\n-- Check for processing issues\nSELECT COUNT(*) as stuck_entries \nFROM potfile_staging \nWHERE created_at &lt; NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"admin-guide/operations/potfile/#check-processing-logs","title":"Check Processing Logs","text":"<pre><code># View potfile-related logs\ndocker logs krakenhashes 2&gt;&amp;1 | grep -i potfile\n\n# Check for staging activity\ndocker logs krakenhashes 2&gt;&amp;1 | grep \"staged password\"\n\n# Monitor batch processing\ndocker logs krakenhashes 2&gt;&amp;1 | grep \"Processing.*staged pot-file entries\"\n</code></pre>"},{"location":"admin-guide/operations/potfile/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Issue Cause Solution Passwords not being added Potfile disabled Check <code>potfile_enabled</code> setting Staging table growing Processing stopped Restart server, check logs for errors Duplicate passwords appearing File manually edited Let system manage the file Potfile not updating Batch interval too long Reduce <code>potfile_batch_interval</code> High memory usage Large potfile Consider archiving old entries Potfile preset job missing No binaries uploaded Upload a hashcat binary via Binary Management"},{"location":"admin-guide/operations/potfile/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/operations/potfile/#operational-guidelines","title":"Operational Guidelines","text":"<ol> <li>Let the system manage the potfile</li> <li>Don't manually edit while the server is running</li> <li>Use the staging mechanism for all additions</li> <li> <p>Trust the deduplication logic</p> </li> <li> <p>Monitor staging table size <pre><code>-- Set up an alert if staging exceeds threshold\nSELECT COUNT(*) FROM potfile_staging;\n</code></pre></p> </li> <li> <p>Balance batch processing</p> </li> <li>Shorter intervals: More responsive but higher overhead</li> <li>Longer intervals: More efficient but delayed updates</li> <li> <p>Recommended: 30-60 seconds for most deployments</p> </li> <li> <p>Regular maintenance</p> </li> <li>Monitor potfile size growth</li> <li>Consider rotating very large potfiles (&gt;1GB)</li> <li>Archive historical passwords if needed</li> </ol>"},{"location":"admin-guide/operations/potfile/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory usage: The entire potfile is loaded into memory during processing</li> <li>Disk I/O: Frequent updates can cause disk activity</li> <li>Network transfer: Large potfiles take time to sync to agents</li> <li>Database load: Staging table operations add database activity</li> </ul>"},{"location":"admin-guide/operations/potfile/#security-notes","title":"Security Notes","text":"<p>Important: The potfile contains actual passwords in plaintext</p> <ul> <li>Protect the potfile with appropriate file permissions</li> <li>Ensure backups are encrypted</li> <li>Limit access to the system data directory</li> <li>Consider compliance requirements for password storage</li> <li>Implement audit logging for potfile access</li> </ul> <p>Data Retention Exclusion</p> <p>The potfile is NOT managed by the data retention system. This has critical security and compliance implications:</p> <p>What This Means: - Passwords from deleted hashlists remain in the potfile permanently - The potfile grows indefinitely unless manually managed - Deleting a client or hashlist does NOT remove their passwords from the potfile - This may violate data protection regulations (GDPR, CCPA, etc.) that require complete data deletion</p> <p>Required Manual Management: 1. Implement a cleanup procedure to remove passwords associated with deleted hashlists 2. Document your potfile retention policy separately from the main data retention policy 3. Consider periodic rotation - archive old potfiles and start fresh 4. Audit compliance - ensure your potfile management meets regulatory requirements 5. Track deletions - maintain logs of when and why potfile entries were removed</p> <p>Example Cleanup Approach: <pre><code># Backup current potfile\ncp /data/krakenhashes/wordlists/custom/potfile.txt potfile.backup\n\n# Remove specific patterns (requires careful scripting)\ngrep -v \"pattern_to_remove\" potfile.txt &gt; potfile.tmp\nmv potfile.tmp potfile.txt\n\n# Or rotate periodically\nmv potfile.txt potfile.$(date +%Y%m%d).archive\necho \"\" &gt; potfile.txt  # Start with blank line\n</code></pre></p>"},{"location":"admin-guide/operations/potfile/#backup-and-recovery","title":"Backup and Recovery","text":"<ol> <li>Include in backups: The potfile should be part of regular system backups</li> <li>Database consistency: Back up both the potfile and database together</li> <li>Recovery process: </li> <li>Restore the potfile to its original location</li> <li>Verify the <code>potfile_wordlist_id</code> matches the database</li> <li>Clear any orphaned staging entries</li> <li>Restart the server</li> </ol>"},{"location":"admin-guide/operations/potfile/#integration-tips","title":"Integration Tips","text":"<ul> <li>Workflow optimization: Place potfile attack early in job workflows</li> <li>Custom rules: Combine potfile with rule-based attacks for variations</li> <li>Hybrid attacks: Use potfile + mask attacks for pattern matching</li> <li>Regular updates: Ensure agents sync regularly for latest passwords</li> </ul>"},{"location":"admin-guide/operations/potfile/#advanced-usage","title":"Advanced Usage","text":""},{"location":"admin-guide/operations/potfile/#manual-staging","title":"Manual Staging","text":"<p>If needed, passwords can be manually staged:</p> <pre><code>-- Manually stage a password\nINSERT INTO potfile_staging (password, hash_value, created_at, processed)\nVALUES ('NewPassword123', 'manual_entry', NOW(), false);\n</code></pre>"},{"location":"admin-guide/operations/potfile/#bulk-import","title":"Bulk Import","text":"<p>For importing existing password lists:</p> <pre><code>-- Import from external source (use with caution)\nINSERT INTO potfile_staging (password, hash_value, created_at, processed)\nSELECT DISTINCT password, 'bulk_import', NOW(), false\nFROM external_password_source\nWHERE password NOT IN (\n    SELECT line FROM potfile_lines\n);\n</code></pre>"},{"location":"admin-guide/operations/potfile/#potfile-analysis","title":"Potfile Analysis","text":"<pre><code>-- Most recent additions\nSELECT password, created_at \nFROM potfile_staging \nORDER BY created_at DESC \nLIMIT 20;\n\n-- Processing rate\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    COUNT(*) as passwords_staged\nFROM potfile_staging\nWHERE created_at &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour;\n</code></pre>"},{"location":"admin-guide/operations/potfile/#summary","title":"Summary","text":"<p>The potfile is a powerful feature that automatically builds organizational knowledge of successful passwords. By accumulating cracked passwords and making them available for future jobs, it significantly improves cracking efficiency over time. The automated staging and batch processing system ensures reliable operation with minimal administrative overhead.</p> <p>Key points to remember: - Fully automated operation with configurable parameters - Intelligent deduplication prevents redundant entries - Seamless integration with the job system - Minimal performance impact through batch processing - Critical asset for improving crack rates over time</p> <p>For additional assistance or advanced configuration needs, consult the system logs or contact support.</p>"},{"location":"admin-guide/operations/scheduling/","title":"Agent Scheduling","text":""},{"location":"admin-guide/operations/scheduling/#overview","title":"Overview","text":"<p>The Agent Scheduling feature in KrakenHashes allows administrators to define specific time windows when agents are available for job execution. This feature helps optimize resource usage, manage electricity costs, and ensure agents run during appropriate hours.</p>"},{"location":"admin-guide/operations/scheduling/#key-features","title":"Key Features","text":"<ul> <li>Daily Schedule Configuration: Set different working hours for each day of the week</li> <li>Timezone Support: Schedules are configured in the user's local timezone but stored in UTC</li> <li>Overnight Schedule Support: Schedules can span midnight (e.g., 22:00 - 02:00)</li> <li>Global Enable/Disable: System-wide toggle to enable or disable all scheduling</li> <li>Per-Agent Control: Each agent can have scheduling enabled or disabled independently</li> <li>Schedule Preservation: Schedules are preserved even when disabled</li> </ul>"},{"location":"admin-guide/operations/scheduling/#how-it-works","title":"How It Works","text":""},{"location":"admin-guide/operations/scheduling/#schedule-enforcement","title":"Schedule Enforcement","text":"<p>When scheduling is enabled: 1. The system checks if global scheduling is enabled (admin setting) 2. The system checks if the individual agent has scheduling enabled 3. The system checks if the current UTC time falls within the agent's schedule 4. Only agents that pass all checks are assigned jobs</p>"},{"location":"admin-guide/operations/scheduling/#time-storage-and-display","title":"Time Storage and Display","text":"<ul> <li>Storage: All times are stored in UTC in the database</li> <li>Display: Times are shown in the user's local timezone in the UI</li> <li>Conversion: Automatic conversion happens between local and UTC times</li> </ul>"},{"location":"admin-guide/operations/scheduling/#configuration","title":"Configuration","text":""},{"location":"admin-guide/operations/scheduling/#global-settings","title":"Global Settings","text":"<p>The global scheduling setting can be found in Admin Panel \u2192 System Settings:</p> <pre><code>Enable Agent Scheduling System: [Toggle]\n</code></pre> <p>When disabled: - All agent schedules are ignored - Agents are always available for jobs - Individual agent schedules are preserved but not enforced</p>"},{"location":"admin-guide/operations/scheduling/#per-agent-configuration","title":"Per-Agent Configuration","text":"<p>Individual agent scheduling is configured on the agent details page:</p> <ol> <li>Navigate to Agents \u2192 [Agent Name]</li> <li>Find the Scheduling section</li> <li>Toggle Enable Scheduling to activate scheduling for this agent</li> <li>Click Edit All Schedules to configure daily schedules</li> </ol>"},{"location":"admin-guide/operations/scheduling/#schedule-configuration","title":"Schedule Configuration","text":"<p>When editing schedules:</p> <ol> <li>Add Schedule: Click \"Add Schedule\" for any day to create a time window</li> <li>Set Times: Enter start and end times in 24-hour format (HH:MM)</li> <li>Active Toggle: Enable/disable the schedule for specific days</li> <li>Active (ON): Agent works during the specified hours</li> <li>Active (OFF): Agent does not work at all on this day</li> <li>Copy Schedule: Use the copy icon to apply one day's schedule to all other days</li> <li>Delete Schedule: Remove a schedule for a specific day</li> </ol>"},{"location":"admin-guide/operations/scheduling/#time-input-formats","title":"Time Input Formats","text":"<p>The system accepts various time formats: - <code>9</code> \u2192 <code>09:00:00</code> - <code>17</code> \u2192 <code>17:00:00</code> - <code>9:30</code> \u2192 <code>09:30:00</code> - <code>09:00</code> \u2192 <code>09:00:00</code> - <code>09:00:00</code> \u2192 <code>09:00:00</code></p> <p> Weekly scheduling interface showing daily operating hours configuration with timezone support and per-day activation toggles</p>"},{"location":"admin-guide/operations/scheduling/#examples","title":"Examples","text":""},{"location":"admin-guide/operations/scheduling/#standard-business-hours-9-5-monday-friday","title":"Standard Business Hours (9-5, Monday-Friday)","text":"<pre><code>Monday:    09:00 - 17:00 [Active]\nTuesday:   09:00 - 17:00 [Active]\nWednesday: 09:00 - 17:00 [Active]\nThursday:  09:00 - 17:00 [Active]\nFriday:    09:00 - 17:00 [Active]\nSaturday:  Not scheduled\nSunday:    Not scheduled\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#247-operation-with-weekend-maintenance","title":"24/7 Operation with Weekend Maintenance","text":"<pre><code>Monday:    00:00 - 23:59 [Active]\nTuesday:   00:00 - 23:59 [Active]\nWednesday: 00:00 - 23:59 [Active]\nThursday:  00:00 - 23:59 [Active]\nFriday:    00:00 - 23:59 [Active]\nSaturday:  00:00 - 06:00 [Active]  # Maintenance window 6 AM - Midnight\nSunday:    Not scheduled            # Full day maintenance\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#overnight-processing","title":"Overnight Processing","text":"<pre><code>Monday:    22:00 - 06:00 [Active]  # Runs overnight Mon-Tue\nTuesday:   22:00 - 06:00 [Active]  # Runs overnight Tue-Wed\nWednesday: 22:00 - 06:00 [Active]  # Runs overnight Wed-Thu\nThursday:  22:00 - 06:00 [Active]  # Runs overnight Thu-Fri\nFriday:    22:00 - 06:00 [Active]  # Runs overnight Fri-Sat\nSaturday:  Not scheduled\nSunday:    22:00 - 06:00 [Active]  # Runs overnight Sun-Mon\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#important-behavior-notes","title":"Important Behavior Notes","text":""},{"location":"admin-guide/operations/scheduling/#running-jobs-and-schedule-boundaries","title":"Running Jobs and Schedule Boundaries","text":"<p>The scheduling system only controls when new jobs are assigned, not when running jobs must complete.</p> <p>Key points: - Schedules determine when an agent can receive new jobs - Running jobs will always complete, even if they extend past the scheduled end time - The agent will not accept new jobs outside its schedule, but will finish current work</p>"},{"location":"admin-guide/operations/scheduling/#example-scenario","title":"Example Scenario","text":"<p>If an agent is scheduled to work until 17:00: - At 16:59, the agent receives a job configured for 1-hour chunks - The job will run to completion, potentially until 17:59 or later - No new jobs will be assigned after 17:00 - The agent becomes available for new work at the next scheduled window</p> <p>This design ensures: - No work is lost due to scheduling boundaries - Jobs complete successfully without schedule interruption - Predictable behavior for long-running tasks</p>"},{"location":"admin-guide/operations/scheduling/#job-interruption-and-priority-override","title":"Job Interruption and Priority Override","text":""},{"location":"admin-guide/operations/scheduling/#overview_1","title":"Overview","text":"<p>While the scheduling system doesn't interrupt jobs based on time, KrakenHashes supports priority-based job interruption that works alongside scheduling.</p>"},{"location":"admin-guide/operations/scheduling/#how-job-interruption-works-with-scheduling","title":"How Job Interruption Works with Scheduling","text":"<p>The job interruption system operates independently of agent schedules:</p> <ol> <li>Priority-Based Only: Jobs are interrupted based on priority, not schedule boundaries</li> <li>Schedule Aware: Interrupted jobs can only resume when agents are both:</li> <li>Available (not working on other jobs)</li> <li>Within their scheduled working hours</li> <li>Automatic Management: The system handles all interruption and resumption automatically</li> </ol>"},{"location":"admin-guide/operations/scheduling/#interruption-scenarios","title":"Interruption Scenarios","text":""},{"location":"admin-guide/operations/scheduling/#scenario-1-high-priority-job-during-schedule","title":"Scenario 1: High Priority Job During Schedule","text":"<pre><code>Agent Schedule: 09:00 - 17:00\nCurrent Time: 14:00\nRunning Job: Priority 50 (started at 13:00)\nNew Job: Priority 95 with high priority override\n\nResult: Low priority job interrupted, high priority job takes over\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#scenario-2-interrupted-job-resumes-next-schedule-window","title":"Scenario 2: Interrupted Job Resumes Next Schedule Window","text":"<pre><code>Agent Schedule: 09:00 - 17:00\nJob Interrupted: 16:45 (Priority 50)\nSchedule Ends: 17:00\nNext Day: 09:00 - Job automatically resumes when agent is scheduled again\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#configuration-for-job-interruption","title":"Configuration for Job Interruption","text":""},{"location":"admin-guide/operations/scheduling/#system-wide-setting","title":"System-Wide Setting","text":"<p>Navigate to Admin Panel \u2192 System Settings: <pre><code>Job Interruption Enabled: [Toggle]\n</code></pre></p> <p>When enabled: - Higher priority jobs with override can interrupt lower priority running jobs - Interruption only occurs when no agents are available - Interrupted jobs automatically queue for resumption</p>"},{"location":"admin-guide/operations/scheduling/#per-job-configuration","title":"Per-Job Configuration","text":"<p>In preset job settings: <pre><code>Allow High Priority Override: [Toggle]\nPriority: [0-100]\n</code></pre></p>"},{"location":"admin-guide/operations/scheduling/#best-practices-for-interruption-with-scheduling","title":"Best Practices for Interruption with Scheduling","text":"<ol> <li>Consider Schedule Windows: High-priority jobs should account for agent availability</li> <li>Set Appropriate Chunk Sizes: Smaller chunks (5-10 minutes) allow more responsive interruption</li> <li>Monitor Interruption Patterns: Track if certain schedule windows see excessive interruptions</li> <li>Plan Critical Jobs: Schedule critical jobs during peak agent availability hours</li> </ol>"},{"location":"admin-guide/operations/scheduling/#interaction-between-features","title":"Interaction Between Features","text":"Feature Scheduling Job Interruption Trigger Time-based Priority-based Job Stopping Never stops running jobs Can stop lower priority jobs Resumption N/A - jobs complete Automatic when agents available Configuration Per-agent schedules Per-job override flag System Toggle Global scheduling enable Global interruption enable"},{"location":"admin-guide/operations/scheduling/#common-questions","title":"Common Questions","text":"<p>Q: Can a scheduled agent be interrupted? A: Yes, if a higher priority job with override is waiting and no other agents are available.</p> <p>Q: What happens if an interrupted job's agent goes off-schedule? A: The job remains pending and resumes when any scheduled agent becomes available.</p> <p>Q: Do interrupted jobs lose progress? A: No, all progress is saved and jobs resume from their last checkpoint.</p> <p>Q: Can scheduling prevent interruptions? A: No, but having more agents scheduled reduces the need for interruptions.</p>"},{"location":"admin-guide/operations/scheduling/#schedule-priority","title":"Schedule Priority","text":"<p>The scheduling system follows this priority order:</p> <ol> <li>Global Setting OFF: All schedules ignored, all agents always available</li> <li>Global Setting ON + Agent Scheduling OFF: Agent always available</li> <li>Global Setting ON + Agent Scheduling ON: Agent follows configured schedule</li> </ol>"},{"location":"admin-guide/operations/scheduling/#technical-details","title":"Technical Details","text":""},{"location":"admin-guide/operations/scheduling/#database-schema","title":"Database Schema","text":"<p>Schedules are stored in the <code>agent_schedules</code> table:</p> <pre><code>CREATE TABLE agent_schedules (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER NOT NULL REFERENCES agents(id),\n    day_of_week INTEGER NOT NULL,  -- 0-6 (Sunday-Saturday)\n    start_time TIME NOT NULL,       -- UTC time\n    end_time TIME NOT NULL,         -- UTC time\n    timezone VARCHAR(50) NOT NULL,  -- Original timezone for reference\n    is_active BOOLEAN NOT NULL,\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>GET /api/agents/{id}/schedules</code> - Get agent schedules</li> <li><code>POST /api/agents/{id}/schedules</code> - Update single schedule</li> <li><code>POST /api/agents/{id}/schedules/bulk</code> - Bulk update schedules</li> <li><code>DELETE /api/agents/{id}/schedules/{day}</code> - Delete schedule for a day</li> <li><code>PUT /api/agents/{id}/scheduling-enabled</code> - Toggle scheduling for agent</li> </ul>"},{"location":"admin-guide/operations/scheduling/#job-assignment-integration","title":"Job Assignment Integration","text":"<p>The job assignment service (<code>GetAvailableAgents</code>) checks scheduling:</p> <pre><code>if agent.SchedulingEnabled {\n    schedulingSetting, err := s.systemSettingsRepo.GetSetting(ctx, \"agent_scheduling_enabled\")\n    if err == nil &amp;&amp; schedulingSetting.Value != nil &amp;&amp; *schedulingSetting.Value == \"true\" {\n        isScheduled, err := s.scheduleRepo.IsAgentScheduledNow(ctx, agent.ID)\n        if err != nil || !isScheduled {\n            continue // Skip this agent\n        }\n    }\n}\n</code></pre>"},{"location":"admin-guide/operations/scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Test Schedules: Always test schedules with non-critical jobs first</li> <li>Timezone Awareness: Be mindful of timezone differences when setting schedules</li> <li>Overlap Planning: Ensure adequate agent coverage during peak hours</li> <li>Maintenance Windows: Schedule maintenance during off-hours</li> <li>Documentation: Document your scheduling strategy for team members</li> </ol>"},{"location":"admin-guide/operations/scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/operations/scheduling/#agent-not-getting-jobs-despite-being-scheduled","title":"Agent Not Getting Jobs Despite Being Scheduled","text":"<ol> <li>Check global scheduling is enabled</li> <li>Verify agent scheduling is enabled</li> <li>Confirm current time falls within schedule</li> <li>Check agent is otherwise eligible (enabled, online, etc.)</li> </ol>"},{"location":"admin-guide/operations/scheduling/#schedule-shows-wrong-times","title":"Schedule Shows Wrong Times","text":"<ol> <li>Verify your browser timezone is correct</li> <li>Check the timezone display in the UI</li> <li>Remember all times are stored in UTC</li> </ol>"},{"location":"admin-guide/operations/scheduling/#overnight-schedules-not-working","title":"Overnight Schedules Not Working","text":"<ol> <li>Ensure end time is properly set for next day</li> <li>Verify the schedule spans midnight correctly</li> <li>Check both days involved in the overnight schedule</li> </ol>"},{"location":"admin-guide/operations/scheduling/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements for the scheduling system:</p> <ul> <li>Holiday calendar integration</li> <li>Schedule templates for common patterns</li> <li>Bulk schedule management across multiple agents</li> <li>Schedule conflict detection and warnings</li> <li>Historical schedule effectiveness reporting</li> </ul>"},{"location":"admin-guide/operations/users/","title":"User Management Guide","text":"<p>This guide covers user administration in KrakenHashes, including user roles, authentication, multi-factor authentication (MFA), and security policies.</p>"},{"location":"admin-guide/operations/users/#table-of-contents","title":"Table of Contents","text":"<ol> <li>User Roles and Permissions</li> <li>Creating and Managing Users</li> <li>Password Policies and Requirements</li> <li>Multi-Factor Authentication Management</li> <li>Session Management</li> <li>User Deactivation and Deletion</li> <li>Audit Logging and User Activity</li> </ol>"},{"location":"admin-guide/operations/users/#user-roles-and-permissions","title":"User Roles and Permissions","text":"<p>KrakenHashes implements a role-based access control (RBAC) system with the following roles:</p>"},{"location":"admin-guide/operations/users/#user-roles","title":"User Roles","text":"<ol> <li>User (default role)</li> <li>View and manage their own profile</li> <li>Submit password cracking jobs</li> <li>View their own job results</li> <li>Manage their own MFA settings</li> <li> <p>Access general system features</p> </li> <li> <p>Admin</p> </li> <li>All user permissions plus:</li> <li>View and manage all users</li> <li>Reset user passwords</li> <li>Enable/disable user accounts</li> <li>Modify system settings</li> <li>View all jobs and results</li> <li>Manage agents and vouchers</li> <li> <p>Access admin dashboard</p> </li> <li> <p>System (special role)</p> </li> <li>Reserved for system operations</li> <li>Cannot be assigned to regular users</li> <li>Cannot be modified or disabled</li> </ol>"},{"location":"admin-guide/operations/users/#permission-matrix","title":"Permission Matrix","text":"Action User Admin System View own profile \u2713 \u2713 \u2713 Edit own profile \u2713 \u2713 \u2713 View all users \u2717 \u2713 \u2713 Manage users \u2717 \u2713 \u2717 System settings \u2717 \u2713 \u2717 View all jobs \u2717 \u2713 \u2713"},{"location":"admin-guide/operations/users/#creating-and-managing-users","title":"Creating and Managing Users","text":""},{"location":"admin-guide/operations/users/#user-creation","title":"User Creation","text":"<p>Currently, users are created through the registration process. Admin creation of users is not directly implemented but can be achieved through the following methods:</p> <ol> <li>Self-Registration (if enabled)</li> <li>Users register themselves through the web interface</li> <li> <p>Email verification may be required</p> </li> <li> <p>Admin-Initiated Registration</p> </li> <li>Admin provides registration link to new users</li> <li>User completes registration process</li> </ol>"},{"location":"admin-guide/operations/users/#user-management-operations","title":"User Management Operations","text":""},{"location":"admin-guide/operations/users/#listing-users","title":"Listing Users","text":"<p>Admins can view all users through the admin dashboard: - Navigate to Admin \u2192 Users - Filter by role (admin, user) - View user details including:   - Username and email   - Role   - Account status (enabled/disabled)   - MFA status   - Last login time</p> <p> User Management page displaying the user accounts table with columns for Username, Email, Role, Status, MFA status, and Last Login information</p>"},{"location":"admin-guide/operations/users/#updating-user-information","title":"Updating User Information","text":"<p>Admins can update: - Username - Email address - User role (user \u2194 admin)</p> <p>Note: System users cannot be modified.</p>"},{"location":"admin-guide/operations/users/#resetting-user-passwords","title":"Resetting User Passwords","text":"<p>Admins can reset user passwords in two ways:</p> <ol> <li>Temporary Password</li> <li>System generates a secure temporary password</li> <li>Password must be changed on next login</li> <li> <p>Share password securely with user</p> </li> <li> <p>Custom Password</p> </li> <li>Admin sets a specific password</li> <li>Must meet password policy requirements</li> <li>User should change on next login</li> </ol>"},{"location":"admin-guide/operations/users/#password-policies-and-requirements","title":"Password Policies and Requirements","text":""},{"location":"admin-guide/operations/users/#default-password-requirements","title":"Default Password Requirements","text":"<p>The system enforces configurable password policies:</p> <ul> <li>Minimum Length: 8 characters (configurable)</li> <li>Character Requirements (configurable):</li> <li>Uppercase letters</li> <li>Lowercase letters</li> <li>Numbers</li> <li>Special characters</li> </ul>"},{"location":"admin-guide/operations/users/#password-policy-configuration","title":"Password Policy Configuration","text":"<p>Admins can configure password policies via: 1. Navigate to Admin \u2192 Settings \u2192 Authentication 2. Adjust password requirements:    <pre><code>{\n  \"minPasswordLength\": 8,\n  \"requireUppercase\": true,\n  \"requireLowercase\": true,\n  \"requireNumbers\": true,\n  \"requireSpecialChars\": true\n}\n</code></pre></p>"},{"location":"admin-guide/operations/users/#password-security-features","title":"Password Security Features","text":"<ul> <li>Passwords are hashed using bcrypt with default cost</li> <li>Password history tracking</li> <li>Last password change timestamp</li> <li>Forced password change capability</li> </ul>"},{"location":"admin-guide/operations/users/#multi-factor-authentication-management","title":"Multi-Factor Authentication Management","text":""},{"location":"admin-guide/operations/users/#mfa-overview","title":"MFA Overview","text":"<p>KrakenHashes supports multiple MFA methods:</p> <ol> <li>Email-based MFA (default)</li> <li>6-digit codes sent via email</li> <li>Configurable code validity period</li> <li> <p>Built-in cooldown between requests</p> </li> <li> <p>Authenticator App (TOTP)</p> </li> <li>Compatible with Google Authenticator, Authy, etc.</li> <li>SHA-512 algorithm</li> <li>30-second time window</li> <li> <p>6-digit codes</p> </li> <li> <p>Backup Codes</p> </li> <li>8 single-use recovery codes (configurable)</li> <li>Generated when MFA is enabled</li> <li>Can be regenerated by user</li> </ol>"},{"location":"admin-guide/operations/users/#global-mfa-settings","title":"Global MFA Settings","text":"<p>Admins can configure MFA requirements:</p> <ol> <li>Mandatory MFA</li> <li>Require all users to enable MFA</li> <li>Configurable allowed methods</li> <li> <p>Email gateway must be configured first</p> </li> <li> <p>MFA Configuration Options <pre><code>{\n  \"requireMfa\": false,\n  \"allowedMfaMethods\": [\"email\", \"authenticator\"],\n  \"emailCodeValidity\": 5,        // minutes\n  \"backupCodesCount\": 8,\n  \"mfaCodeCooldownMinutes\": 1,\n  \"mfaCodeExpiryMinutes\": 5,\n  \"mfaMaxAttempts\": 3\n}\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/operations/users/#user-mfa-management","title":"User MFA Management","text":""},{"location":"admin-guide/operations/users/#enabling-mfa-for-a-user","title":"Enabling MFA for a User","text":"<p>Users can enable MFA through their profile: 1. Navigate to Profile \u2192 Security 2. Choose MFA method 3. Complete verification process 4. Save backup codes</p> <p> User Profile Settings interface displaying account information, password change functionality, and MFA controls that reflect the authentication policies configured by administrators</p>"},{"location":"admin-guide/operations/users/#admin-mfa-operations","title":"Admin MFA Operations","text":"<p>Admins can: - View user MFA status - Disable MFA for a user (if not globally required) - Reset MFA settings in case of lockout</p>"},{"location":"admin-guide/operations/users/#mfa-authentication-flow","title":"MFA Authentication Flow","text":"<ol> <li>User enters username/password</li> <li>System validates credentials</li> <li>If MFA enabled, prompt for code</li> <li>User provides code via preferred method</li> <li>System validates code with retry limits</li> <li>Grant access upon successful verification</li> </ol>"},{"location":"admin-guide/operations/users/#session-management","title":"Session Management","text":""},{"location":"admin-guide/operations/users/#session-security-features","title":"Session Security Features","text":"<ol> <li>JWT-based Authentication</li> <li>Configurable token expiry (default: 7 days)</li> <li>Secure HTTP-only cookies</li> <li> <p>Refresh token support</p> </li> <li> <p>Account Security</p> </li> <li>Failed login attempt tracking</li> <li>Automatic account lockout after threshold</li> <li>Configurable lockout duration</li> <li>Admin unlock capability</li> </ol>"},{"location":"admin-guide/operations/users/#session-configuration","title":"Session Configuration","text":"<pre><code>{\n  \"maxFailedAttempts\": 5,\n  \"lockoutDurationMinutes\": 30,\n  \"jwtExpiryMinutes\": 10080,  // 7 days\n  \"sessionTimeout\": 60         // minutes of inactivity\n}\n</code></pre>"},{"location":"admin-guide/operations/users/#managing-user-sessions","title":"Managing User Sessions","text":""},{"location":"admin-guide/operations/users/#account-lockout","title":"Account Lockout","text":"<p>When a user exceeds failed login attempts: 1. Account is automatically locked 2. User sees lockout message with duration 3. Admin can manually unlock via Admin \u2192 Users \u2192 Unlock</p>"},{"location":"admin-guide/operations/users/#session-monitoring","title":"Session Monitoring","text":"<ul> <li>Last login timestamp tracked</li> <li>Failed login attempts counted</li> <li>Last failed attempt timestamp</li> <li>Account lock status and duration</li> </ul>"},{"location":"admin-guide/operations/users/#user-deactivation-and-deletion","title":"User Deactivation and Deletion","text":""},{"location":"admin-guide/operations/users/#account-deactivation","title":"Account Deactivation","text":"<p>Admins can disable user accounts:</p> <ol> <li>Disable Account</li> <li>User cannot log in</li> <li>Requires disable reason</li> <li>Tracks who disabled and when</li> <li>Account data preserved</li> <li> <p>Can be re-enabled later</p> </li> <li> <p>Enable Account</p> </li> <li>Restores account access</li> <li>Clears disable reason</li> <li>User can log in again</li> </ol>"},{"location":"admin-guide/operations/users/#important-considerations","title":"Important Considerations","text":"<ul> <li>System users cannot be disabled</li> <li>Disabled users' data remains intact</li> <li>Active sessions are not immediately terminated</li> <li>User deletion is not implemented (data retention)</li> </ul>"},{"location":"admin-guide/operations/users/#deactivation-process","title":"Deactivation Process","text":"<ol> <li>Navigate to Admin \u2192 Users</li> <li>Select user to disable</li> <li>Click \"Disable Account\"</li> <li>Provide reason for audit trail</li> <li>Confirm action</li> </ol> <p>To re-enable: 1. Find disabled user 2. Click \"Enable Account\" 3. User can now log in</p>"},{"location":"admin-guide/operations/users/#audit-logging-and-user-activity","title":"Audit Logging and User Activity","text":""},{"location":"admin-guide/operations/users/#user-activity-tracking","title":"User Activity Tracking","text":"<p>The system tracks the following user activities:</p> <ol> <li>Authentication Events</li> <li>Login attempts (successful/failed)</li> <li>MFA verification attempts</li> <li>Password changes</li> <li> <p>Account lockouts</p> </li> <li> <p>Account Modifications</p> </li> <li>Profile updates</li> <li>MFA changes</li> <li>Password resets</li> <li> <p>Role changes</p> </li> <li> <p>Administrative Actions</p> </li> <li>User account modifications</li> <li>Password resets by admin</li> <li>Account enable/disable</li> <li>MFA resets</li> </ol>"},{"location":"admin-guide/operations/users/#audit-information-captured","title":"Audit Information Captured","text":"<p>For each auditable event: - Timestamp - User ID performing action - Target user ID (if applicable) - Action type - Additional context (e.g., disable reason) - IP address (where applicable)</p>"},{"location":"admin-guide/operations/users/#accessing-audit-logs","title":"Accessing Audit Logs","text":"<p>While a dedicated audit log viewer is not yet implemented, audit information is stored in the database:</p> <ul> <li>Login tracking via <code>last_login</code> and <code>failed_login_attempts</code></li> <li>Account changes via <code>disabled_by</code>, <code>disabled_at</code>, <code>disabled_reason</code></li> <li>Password changes via <code>last_password_change</code></li> </ul>"},{"location":"admin-guide/operations/users/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Regular Reviews</li> <li>Review user list for inactive accounts</li> <li>Check for users with excessive privileges</li> <li> <p>Monitor failed login patterns</p> </li> <li> <p>MFA Enforcement</p> </li> <li>Consider mandatory MFA for admin users</li> <li>Regular review of MFA methods in use</li> <li> <p>Ensure email gateway configured for email MFA</p> </li> <li> <p>Password Policies</p> </li> <li>Enforce strong password requirements</li> <li>Consider regular password rotation for admins</li> <li> <p>Monitor for compromised credentials</p> </li> <li> <p>Account Hygiene</p> </li> <li>Disable accounts promptly when users leave</li> <li>Regular review of admin role assignments</li> <li>Document reasons for account actions</li> </ol>"},{"location":"admin-guide/operations/users/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"admin-guide/operations/users/#mfa-issues","title":"MFA Issues","text":"<p>Problem: User locked out of MFA - Solution: Admin can disable MFA for user, user re-enables</p> <p>Problem: Email codes not received - Solution: Check email gateway configuration, verify email address</p> <p>Problem: Authenticator app out of sync - Solution: Verify system time, consider increasing TOTP skew tolerance</p>"},{"location":"admin-guide/operations/users/#account-access-issues","title":"Account Access Issues","text":"<p>Problem: User account locked - Solution: Admin unlocks account or wait for lockout duration</p> <p>Problem: Password reset not working - Solution: Verify password meets policy requirements</p> <p>Problem: Cannot modify system user - Solution: System users are protected and cannot be modified</p>"},{"location":"admin-guide/operations/users/#configuration-issues","title":"Configuration Issues","text":"<p>Problem: Cannot enable global MFA - Solution: Configure email gateway first in Email Settings</p> <p>Problem: Users bypassing MFA - Solution: Check global MFA requirement is enabled</p>"},{"location":"admin-guide/operations/users/#api-endpoints-reference","title":"API Endpoints Reference","text":"<p>For programmatic access, the following admin endpoints are available:</p> <ul> <li><code>GET /api/admin/users</code> - List all users</li> <li><code>GET /api/admin/users/{id}</code> - Get user details</li> <li><code>PUT /api/admin/users/{id}</code> - Update user</li> <li><code>POST /api/admin/users/{id}/disable</code> - Disable account</li> <li><code>POST /api/admin/users/{id}/enable</code> - Enable account</li> <li><code>POST /api/admin/users/{id}/reset-password</code> - Reset password</li> <li><code>POST /api/admin/users/{id}/disable-mfa</code> - Disable MFA</li> <li><code>POST /api/admin/users/{id}/unlock</code> - Unlock account</li> </ul> <p>All endpoints require admin authentication via JWT token.</p>"},{"location":"admin-guide/resource-management/binaries/","title":"Binary Management","text":"<p>KrakenHashes provides a comprehensive binary management system for hashcat and other password cracking tools. This document explains how to manage binaries, track versions, and ensure secure distribution to agents.</p>"},{"location":"admin-guide/resource-management/binaries/#overview","title":"Overview","text":"<p>The binary management system allows administrators to: - Upload and manage multiple versions of hashcat binaries - Track different binary types and compression formats - Verify binary integrity with MD5 checksums - Automatically extract and prepare binaries for server-side execution - Distribute binaries securely to agents - Maintain an audit trail of all binary operations</p>"},{"location":"admin-guide/resource-management/binaries/#understanding-binary-management","title":"Understanding Binary Management","text":""},{"location":"admin-guide/resource-management/binaries/#architecture","title":"Architecture","text":"<p>The binary management system consists of several components:</p> <ol> <li>Binary Storage: Files are stored in <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Local Extraction: Server-side binaries are extracted to <code>&lt;data_dir&gt;/binaries/local/&lt;version_id&gt;/</code></li> <li>Version Tracking: Database tracks all binary versions with metadata</li> <li>Distribution: Agents download binaries via secure API endpoints</li> <li>Verification: Automatic integrity checking with MD5 hashes</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#binary-types","title":"Binary Types","text":"<p>Currently supported binary types: - <code>hashcat</code> - Hashcat password cracking tool - <code>john</code> - John the Ripper (future support)</p>"},{"location":"admin-guide/resource-management/binaries/#compression-types","title":"Compression Types","text":"<p>Supported compression formats: - <code>7z</code> - 7-Zip archive format - <code>zip</code> - ZIP archive format - <code>tar.gz</code> - Gzip-compressed TAR archive - <code>tar.xz</code> - XZ-compressed TAR archive</p>"},{"location":"admin-guide/resource-management/binaries/#uploading-new-binaries","title":"Uploading New Binaries","text":""},{"location":"admin-guide/resource-management/binaries/#via-admin-api","title":"Via Admin API","text":"<p>To add a new binary version, use the admin API endpoint:</p> <pre><code>POST /api/admin/binary\nAuthorization: Bearer &lt;admin_token&gt;\nContent-Type: application/json\n\n{\n  \"binary_type\": \"hashcat\",\n  \"compression_type\": \"7z\",\n  \"source_url\": \"https://github.com/hashcat/hashcat/releases/download/v6.2.6/hashcat-6.2.6.7z\",\n  \"file_name\": \"hashcat-6.2.6.7z\"\n}\n</code></pre> <p>The system will: 1. Download the binary from the specified URL 2. Calculate and store the MD5 hash 3. Verify the download integrity 4. Extract the binary for server-side use 5. Mark the version as active and verified</p>"},{"location":"admin-guide/resource-management/binaries/#upload-process","title":"Upload Process","text":"<p>When a binary is uploaded:</p> <ol> <li>Download Phase: The system downloads the binary with retry logic (up to 3 attempts)</li> <li>Verification Phase: MD5 hash is calculated and stored</li> <li>Storage Phase: Binary is saved to <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Extraction Phase: Archive is extracted to <code>&lt;data_dir&gt;/binaries/local/&lt;version_id&gt;/</code></li> <li>Status Update: Version status is set to <code>verified</code></li> </ol>"},{"location":"admin-guide/resource-management/binaries/#version-management-and-tracking","title":"Version Management and Tracking","text":""},{"location":"admin-guide/resource-management/binaries/#database-schema","title":"Database Schema","text":"<p>Binary versions are tracked in the <code>binary_versions</code> table with the following fields:</p> Field Type Description <code>id</code> SERIAL Unique version identifier <code>binary_type</code> ENUM Type of binary (hashcat, john) <code>compression_type</code> ENUM Compression format <code>source_url</code> TEXT Original download URL <code>file_name</code> VARCHAR(255) Stored filename <code>md5_hash</code> VARCHAR(32) MD5 checksum <code>file_size</code> BIGINT File size in bytes <code>created_at</code> TIMESTAMP Creation timestamp <code>created_by</code> UUID User who added the version <code>is_active</code> BOOLEAN Whether version is active <code>last_verified_at</code> TIMESTAMP Last verification time <code>verification_status</code> VARCHAR(50) Status: pending, verified, failed, deleted"},{"location":"admin-guide/resource-management/binaries/#verification-status","title":"Verification Status","text":"<p>Binary versions can have the following statuses: - <code>pending</code> - Initial state, download/verification in progress - <code>verified</code> - Successfully downloaded and verified - <code>failed</code> - Download or verification failed - <code>deleted</code> - Binary has been deleted</p>"},{"location":"admin-guide/resource-management/binaries/#listing-versions","title":"Listing Versions","text":"<p>To list all binary versions:</p> <pre><code>GET /api/admin/binary?type=hashcat&amp;active=true\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>Query parameters: - <code>type</code> - Filter by binary type - <code>active</code> - Filter by active status (true/false) - <code>status</code> - Filter by verification status</p>"},{"location":"admin-guide/resource-management/binaries/#getting-latest-version","title":"Getting Latest Version","text":"<p>Agents can retrieve the latest active version:</p> <pre><code>GET /api/binary/latest?type=hashcat\nX-API-Key: &lt;agent_api_key&gt;\n</code></pre>"},{"location":"admin-guide/resource-management/binaries/#platform-specific-considerations","title":"Platform-Specific Considerations","text":""},{"location":"admin-guide/resource-management/binaries/#linux","title":"Linux","text":"<p>The system automatically handles Linux-specific binary names: - Checks for both <code>hashcat</code> and <code>hashcat.bin</code> - Sets executable permissions (0750) on extracted binaries</p>"},{"location":"admin-guide/resource-management/binaries/#windows","title":"Windows","text":"<ul> <li>Looks for <code>hashcat.exe</code> in extracted archives</li> <li>Handles Windows-specific path separators</li> </ul>"},{"location":"admin-guide/resource-management/binaries/#archive-extraction","title":"Archive Extraction","text":"<p>The extraction process intelligently handles common archive structures: - Single directory archives: Contents are moved to the target directory - Multi-file archives: All files are extracted as-is - Nested structures: Properly flattened during extraction</p>"},{"location":"admin-guide/resource-management/binaries/#binary-synchronization-to-agents","title":"Binary Synchronization to Agents","text":""},{"location":"admin-guide/resource-management/binaries/#agent-download-process","title":"Agent Download Process","text":"<p>Agents download binaries through the following process:</p> <ol> <li>Version Check: Agent queries for the latest active version</li> <li>Download Request: Agent requests binary download by version ID</li> <li>Authentication: API key authentication is required</li> <li>Streaming Download: Binary is streamed to the agent</li> <li>Local Verification: Agent verifies MD5 hash after download</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#api-endpoints-for-agents","title":"API Endpoints for Agents","text":"<pre><code># Get latest version metadata\nGET /api/binary/latest?type=hashcat\nX-API-Key: &lt;agent_api_key&gt;\n\n# Download specific version\nGET /api/binary/download/{version_id}\nX-API-Key: &lt;agent_api_key&gt;\n</code></pre>"},{"location":"admin-guide/resource-management/binaries/#synchronization-protocol","title":"Synchronization Protocol","text":"<p>The agent file sync system (<code>agent/internal/sync/sync.go</code>) handles: - Concurrent downloads with configurable limits - Retry logic for failed downloads - Local caching to avoid re-downloads - Integrity verification with MD5 hashes</p>"},{"location":"admin-guide/resource-management/binaries/#updating-and-replacing-binaries","title":"Updating and Replacing Binaries","text":""},{"location":"admin-guide/resource-management/binaries/#adding-a-new-version","title":"Adding a New Version","text":"<p>To add a new version of hashcat:</p> <ol> <li>Upload the new version via the admin API</li> <li>The system automatically downloads and verifies it</li> <li>Previous versions remain available but can be deactivated</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#deactivating-old-versions","title":"Deactivating Old Versions","text":"<pre><code>DELETE /api/admin/binary/{version_id}\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>This will: - Mark the version as inactive (<code>is_active = false</code>) - Set verification status to <code>deleted</code> - Remove the binary file from disk - Preserve the database record for audit purposes</p>"},{"location":"admin-guide/resource-management/binaries/#version-verification","title":"Version Verification","text":"<p>To manually verify a binary's integrity:</p> <pre><code>POST /api/admin/binary/{version_id}/verify\nAuthorization: Bearer &lt;admin_token&gt;\n</code></pre> <p>This will: - Check if the file exists on disk - Recalculate the MD5 hash - Compare with stored hash - Update verification status and timestamp</p>"},{"location":"admin-guide/resource-management/binaries/#best-practices-and-security","title":"Best Practices and Security","text":""},{"location":"admin-guide/resource-management/binaries/#security-considerations","title":"Security Considerations","text":"<ol> <li>Source URLs: Only download binaries from trusted sources</li> <li>Official hashcat releases: https://github.com/hashcat/hashcat/releases</li> <li> <p>Verify SSL certificates for download sources</p> </li> <li> <p>Hash Verification: Always verify MD5 hashes after download</p> </li> <li>The system automatically calculates and stores hashes</li> <li> <p>Manual verification can be triggered via API</p> </li> <li> <p>Access Control: Binary management requires admin privileges</p> </li> <li>Only administrators can add/remove binaries</li> <li> <p>Agents have read-only access for downloads</p> </li> <li> <p>File Permissions: Extracted binaries have restricted permissions (0750)</p> </li> <li>Only the application user can execute binaries</li> <li>Group members have read access</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#operational-best-practices","title":"Operational Best Practices","text":"<ol> <li>Version Testing: Test new binary versions before deployment</li> <li>Upload to a test environment first</li> <li>Verify extraction and execution work correctly</li> <li> <p>Check compatibility with existing jobs</p> </li> <li> <p>Retention Policy: Maintain a reasonable number of versions</p> </li> <li>Keep at least 2-3 recent versions for rollback</li> <li>Delete very old versions to save storage space</li> <li> <p>Archive important versions externally</p> </li> <li> <p>Monitoring: Regular verification of binary integrity</p> </li> <li>Schedule periodic verification checks</li> <li>Monitor download failures in logs</li> <li> <p>Track agent synchronization success rates</p> </li> <li> <p>Documentation: Document version changes</p> </li> <li>Note any breaking changes between versions</li> <li>Track performance improvements</li> <li>Document known issues with specific versions</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#storage-management","title":"Storage Management","text":"<ol> <li>Disk Space: Monitor available disk space</li> <li>Binary archives can be large (100MB+)</li> <li>Extracted binaries double the storage requirement</li> <li> <p>Plan for growth with multiple versions</p> </li> <li> <p>Cleanup: Regular cleanup of old versions</p> </li> <li>Delete inactive versions after confirming they're not needed</li> <li>Remove failed download attempts</li> <li>Clean up orphaned extraction directories</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/resource-management/binaries/#common-issues","title":"Common Issues","text":"<ol> <li>Download Failures</li> <li>Check network connectivity to source URL</li> <li>Verify SSL/TLS certificates are valid</li> <li>Check firewall rules for outbound HTTPS</li> <li> <p>Review logs for specific error messages</p> </li> <li> <p>Extraction Failures</p> </li> <li>Ensure required tools are installed (7z, unzip, tar)</li> <li>Check disk space for extraction</li> <li>Verify archive isn't corrupted</li> <li> <p>Check file permissions on data directory</p> </li> <li> <p>Verification Failures</p> </li> <li>File may be corrupted during download</li> <li>Source file may have changed</li> <li>Disk errors could cause corruption</li> <li>Try re-downloading the binary</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#log-locations","title":"Log Locations","text":"<p>Binary management logs are written to: - Backend logs: Check for <code>[Binary Manager]</code> entries - Download attempts: Look for HTTP client errors - Extraction logs: Command output is logged - Verification results: Hash comparison details</p>"},{"location":"admin-guide/resource-management/binaries/#manual-recovery","title":"Manual Recovery","text":"<p>If automated processes fail:</p> <ol> <li>Manual Download: Download binary to a temporary location</li> <li>Manual Upload: Place in <code>&lt;data_dir&gt;/binaries/&lt;version_id&gt;/</code></li> <li>Update Database: Set correct hash and file size</li> <li>Manual Extraction: Extract to local directory</li> <li>Verify Permissions: Ensure correct file permissions</li> </ol>"},{"location":"admin-guide/resource-management/binaries/#versionjson-file","title":"Version.json File","text":"<p>The <code>versions.json</code> file in the repository root tracks component versions:</p> <pre><code>{\n    \"backend\": \"0.1.0\",\n    \"frontend\": \"0.1.0\",\n    \"agent\": \"0.1.0\",\n    \"api\": \"0.1.0\",\n    \"database\": \"0.1.0\"\n}\n</code></pre> <p>This file is used for: - Build-time version embedding - API version compatibility checks - Component version tracking - Release management</p> <p>Note: This tracks KrakenHashes component versions, not binary tool versions.</p>"},{"location":"admin-guide/resource-management/binaries/#api-reference","title":"API Reference","text":""},{"location":"admin-guide/resource-management/binaries/#admin-endpoints","title":"Admin Endpoints","text":"Method Endpoint Description POST <code>/api/admin/binary</code> Add new binary version GET <code>/api/admin/binary</code> List all versions GET <code>/api/admin/binary/{id}</code> Get specific version DELETE <code>/api/admin/binary/{id}</code> Delete/deactivate version POST <code>/api/admin/binary/{id}/verify</code> Verify binary integrity"},{"location":"admin-guide/resource-management/binaries/#agent-endpoints","title":"Agent Endpoints","text":"Method Endpoint Description GET <code>/api/binary/latest</code> Get latest active version GET <code>/api/binary/download/{id}</code> Download binary file"},{"location":"admin-guide/resource-management/binaries/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements to the binary management system:</p> <ol> <li>Automatic Updates: Check for new releases periodically</li> <li>Version Channels: Support for stable/beta/nightly channels</li> <li>Platform Detection: Automatic platform-specific binary selection</li> <li>Signature Verification: GPG signature verification for downloads</li> <li>Delta Updates: Differential updates for minor versions</li> <li>Binary Caching: CDN integration for faster agent downloads</li> <li>Performance Metrics: Track binary performance across versions</li> </ol>"},{"location":"admin-guide/resource-management/rules/","title":"Rules Management","text":"<p>KrakenHashes provides comprehensive management of rule files used for password cracking operations. This document explains how the system handles rule files and provides best practices for administrators.</p>"},{"location":"admin-guide/resource-management/rules/#overview","title":"Overview","text":"<p>Rules are transformation patterns applied to wordlists during password cracking operations. They allow you to generate password variations without storing massive wordlists. For example, a single rule can transform \"password\" into \"Password123!\", \"p@ssw0rd\", and many other variations.</p> <p> The Rule Management interface displaying uploaded rule files with status, type, size, and rule count information. The interface provides filtering options and actions for managing Hashcat and John the Ripper rule files.</p>"},{"location":"admin-guide/resource-management/rules/#directory-structure","title":"Directory Structure","text":"<p>The system monitors the rules directory and automatically creates the following subdirectory structure:</p> <pre><code>rules/\n\u251c\u2500\u2500 hashcat/       # Hashcat-compatible rules\n\u251c\u2500\u2500 john/          # John the Ripper rules\n\u2514\u2500\u2500 custom/        # User-created or modified rules\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#rule-file-formats-and-best-practices","title":"Rule File Formats and Best Practices","text":""},{"location":"admin-guide/resource-management/rules/#hashcat-rules","title":"Hashcat Rules","text":"<p>Hashcat rules use a simple syntax where each character represents an operation:</p> <pre><code># Example hashcat rules\n:                    # Try the original word\nl                    # Convert to lowercase\nu                    # Convert to uppercase\nc                    # Capitalize first letter, lowercase rest\n$1                   # Append '1' to the word\n^a                   # Prepend 'a' to the word\nsa@                  # Replace all 'a' with '@'\n$1$2$3               # Append '123'\nc$!                  # Capitalize and append '!'\n</code></pre> <p>Best Practices for Hashcat Rules: - Use comments (lines starting with #) to document complex rules - Group related rules together - Test rules with <code>--stdout</code> before using in production - Keep rule files focused on specific transformation types</p>"},{"location":"admin-guide/resource-management/rules/#john-the-ripper-rules","title":"John the Ripper Rules","text":"<p>John rules have a similar but slightly different syntax:</p> <pre><code># Example John the Ripper rules\n:                    # Try the original word\n[lL]                 # Convert to lowercase\n[uU]                 # Convert to uppercase\n[cC]                 # Capitalize\n$[0-9]               # Append a digit\n^[aA]                # Prepend 'a' or 'A'\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#custom-rules","title":"Custom Rules","text":"<p>Custom rules should follow either Hashcat or John syntax depending on your cracking engine.</p>"},{"location":"admin-guide/resource-management/rules/#rule-type-detection","title":"Rule Type Detection","text":"<p>The system automatically assigns a rule type based on the file path: - If the path contains \"john\" (case-insensitive), it's classified as John the Ripper - Otherwise, it's classified as Hashcat - Administrators can change the type after import if needed</p>"},{"location":"admin-guide/resource-management/rules/#rule-splitting-for-large-files","title":"Rule Splitting for Large Files","text":"<p>KrakenHashes includes an intelligent rule splitting system for distributing work across multiple agents:</p>"},{"location":"admin-guide/resource-management/rules/#automatic-rule-splitting","title":"Automatic Rule Splitting","text":"<p>When rule splitting is enabled, the system can automatically split large rule files:</p> <p>Configuration Settings: - <code>rule_split_enabled</code>: Enable/disable automatic rule splitting - <code>rule_split_threshold</code>: Threshold ratio for triggering splits (default: 0.8) - <code>rule_split_min_rules</code>: Minimum number of rules before splitting (default: 10,000) - <code>rule_split_max_chunks</code>: Maximum number of chunks to create (default: 100)</p>"},{"location":"admin-guide/resource-management/rules/#how-rule-splitting-works","title":"How Rule Splitting Works","text":"<ol> <li>Detection: When a job uses a rule file with more rules than the threshold, splitting is triggered</li> <li>Chunking: The rule file is divided into equal chunks based on available agents</li> <li>Distribution: Each agent receives a chunk of rules to process</li> <li>Cleanup: Temporary chunk files are removed after job completion</li> </ol>"},{"location":"admin-guide/resource-management/rules/#manual-rule-splitting","title":"Manual Rule Splitting","text":"<p>For optimal performance, you can pre-split large rule files:</p> <pre><code># Split a rule file into 10 parts\nsplit -n 10 large_rules.rule rules_part_\n\n# Split by number of lines (1000 rules per file)\nsplit -l 1000 large_rules.rule rules_chunk_\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#performance-considerations","title":"Performance Considerations","text":""},{"location":"admin-guide/resource-management/rules/#rule-complexity","title":"Rule Complexity","text":"<p>Different rule types have varying performance impacts:</p> <ol> <li>Simple Rules (minimal impact):</li> <li>Case changes (l, u, c)</li> <li> <p>Single character operations ($x, ^x)</p> </li> <li> <p>Moderate Rules (noticeable impact):</p> </li> <li>Multiple substitutions (sa@sb$sc()</li> <li> <p>Positional operations</p> </li> <li> <p>Complex Rules (significant impact):</p> </li> <li>Multiple operations per rule</li> <li>Conditional rules</li> <li>Memory operations</li> </ol>"},{"location":"admin-guide/resource-management/rules/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Order Rules by Frequency: Place most likely successful rules first</li> <li>Avoid Redundancy: Remove duplicate or overlapping rules</li> <li>Benchmark First: Test rule performance with small wordlists</li> <li>Use Rule Splitting: For rules &gt;10,000 lines, enable splitting</li> <li>Monitor Memory: Complex rules can increase memory usage</li> </ol>"},{"location":"admin-guide/resource-management/rules/#common-rule-sets-and-their-uses","title":"Common Rule Sets and Their Uses","text":""},{"location":"admin-guide/resource-management/rules/#basic-password-variations","title":"Basic Password Variations","text":"<pre><code># basic_variations.rule\n:                    # Original\nc                    # Capitalize\nu                    # Uppercase\nl                    # Lowercase\nc$1                  # Capitalize + append 1\nc$!                  # Capitalize + append !\n$2023                # Append year\n$2024                # Append year\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#leetspeak-transformations","title":"Leetspeak Transformations","text":"<pre><code># leetspeak.rule\nsa@                  # a -&gt; @\nse3                  # e -&gt; 3\nsi1                  # i -&gt; 1\nso0                  # o -&gt; 0\nss$                  # s -&gt; $\nsa@se3              # Multiple substitutions\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#corporate-passwords","title":"Corporate Passwords","text":"<pre><code># corporate.rule\nc$1$2$3              # Capitalize + 123\nc$!$@$#              # Capitalize + special chars\n$@company            # Append company name\n^Company             # Prepend company name\nc$2023               # Capitalize + year\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#keyboard-patterns","title":"Keyboard Patterns","text":"<pre><code># keyboard_patterns.rule\n$!@#                 # Common keyboard pattern\n$123                 # Sequential numbers\n$qwe                 # Keyboard row\n$!qaz                # Vertical keyboard pattern\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#creating-custom-rules","title":"Creating Custom Rules","text":""},{"location":"admin-guide/resource-management/rules/#rule-development-workflow","title":"Rule Development Workflow","text":"<ol> <li>Analyze Target Patterns: Study password patterns from previous cracks</li> <li>Write Initial Rules: Create rules based on observed patterns</li> <li>Test with Hashcat: Use <code>--stdout</code> to verify transformations</li> <li>Refine and Optimize: Remove ineffective rules, add variations</li> <li>Document: Add comments explaining rule purpose</li> </ol>"},{"location":"admin-guide/resource-management/rules/#example-creating-domain-specific-rules","title":"Example: Creating Domain-Specific Rules","text":"<pre><code># finance_sector.rule\n# Common patterns in financial sector\n$2023                # Current year\n$Q1                  # Quarter notation\n$USD                 # Currency\n^FIN                 # Department prefix\nsa@s$$               # Common substitutions\nc$123                # Compliance requirement\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#testing-custom-rules","title":"Testing Custom Rules","text":"<pre><code># Test rules with sample wordlist\nhashcat --stdout wordlist.txt -r custom.rule | head -20\n\n# Count generated candidates\nhashcat --stdout wordlist.txt -r custom.rule | wc -l\n\n# Verify specific transformations\necho \"password\" | hashcat --stdout -r custom.rule\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#file-management","title":"File Management","text":""},{"location":"admin-guide/resource-management/rules/#uploading-rules","title":"Uploading Rules","text":"<p>When uploading rule files:</p> <ol> <li>Choose appropriate rule type (Hashcat/John/Custom)</li> <li>Add descriptive tags for organization</li> <li>Include documentation in description field</li> <li>For large files (&gt;10MB), upload is processed asynchronously</li> </ol>"},{"location":"admin-guide/resource-management/rules/#automatic-processing","title":"Automatic Processing","text":"<p>The system automatically: - Calculates MD5 hash for integrity - Counts rules (excluding comments and empty lines) - Verifies file accessibility - Tags auto-imported files</p>"},{"location":"admin-guide/resource-management/rules/#duplicate-handling","title":"Duplicate Handling","text":"<ul> <li>Same filename + same content = Skip upload</li> <li>Same filename + different content = Update existing</li> <li>Different filename + same content = Create new entry</li> </ul>"},{"location":"admin-guide/resource-management/rules/#best-practices","title":"Best Practices","text":""},{"location":"admin-guide/resource-management/rules/#organization","title":"Organization","text":"<ol> <li>Categorize by Purpose: Use subdirectories for different rule types</li> <li>Version Control: Include version numbers in filenames</li> <li>Documentation: Include README files explaining rule sets</li> </ol>"},{"location":"admin-guide/resource-management/rules/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Good naming examples\nbasic_english_v2.rule\ncorporate_2024Q1.rule\nweb_app_patterns.rule\nfinance_sector_specific.rule\n\n# Avoid\nrules1.txt\nnew.rule\ntest.rule\n</code></pre>"},{"location":"admin-guide/resource-management/rules/#maintenance","title":"Maintenance","text":"<ol> <li>Regular Reviews: Audit rule effectiveness quarterly</li> <li>Update Patterns: Add new patterns as they emerge</li> <li>Remove Obsolete: Delete rules for outdated patterns</li> <li>Benchmark Performance: Test rule speed on new hardware</li> </ol>"},{"location":"admin-guide/resource-management/rules/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"admin-guide/resource-management/rules/#import-status","title":"Import Status","text":"<p>Check rule import status through: - Admin dashboard for overview - Server logs for detailed processing info - Database <code>verification_status</code> field</p>"},{"location":"admin-guide/resource-management/rules/#common-issues","title":"Common Issues","text":"<ol> <li>Large File Processing: Files &gt;1GB may take time to verify</li> <li>Rule Syntax Errors: Invalid rules are skipped during counting</li> <li>File Access: Ensure proper permissions on rule directories</li> </ol>"},{"location":"admin-guide/resource-management/rules/#performance-metrics","title":"Performance Metrics","text":"<p>Monitor rule performance: - Rules/second processing rate - Memory usage during rule application - Success rate (cracks per rule application)</p>"},{"location":"admin-guide/resource-management/rules/#security-considerations","title":"Security Considerations","text":"<ol> <li>Access Control: Limit rule file access to authorized users</li> <li>Validation: System validates rule syntax during import</li> <li>Audit Trail: All rule modifications are logged</li> <li>Sensitive Patterns: Avoid hardcoding sensitive data in rules</li> </ol>"},{"location":"admin-guide/resource-management/rules/#integration-with-jobs","title":"Integration with Jobs","text":"<p>Rules are selected during job creation: 1. Browse available verified rules 2. Select appropriate rule for attack type 3. System handles rule distribution to agents 4. Progress tracking shows rules processed</p> <p>For optimal performance, match rule complexity to available computational resources and expected password patterns.</p>"},{"location":"admin-guide/resource-management/storage/","title":"Storage Architecture","text":""},{"location":"admin-guide/resource-management/storage/#overview","title":"Overview","text":"<p>KrakenHashes implements a centralized file storage system with intelligent deduplication, hash verification, and performance optimizations. This guide covers the storage architecture, capacity planning, and maintenance procedures.</p>"},{"location":"admin-guide/resource-management/storage/#storage-directory-structure","title":"Storage Directory Structure","text":"<p>The system organizes files into a hierarchical structure under the configured data directory (default: <code>/var/lib/krakenhashes</code> in Docker, <code>~/.krakenhashes-data</code> locally):</p> <pre><code>/var/lib/krakenhashes/           # Root data directory (KH_DATA_DIR)\n\u251c\u2500\u2500 binaries/                    # Hashcat/John binaries\n\u2502   \u251c\u2500\u2500 hashcat_7.6.0_linux64.tar.gz\n\u2502   \u2514\u2500\u2500 john_1.9.0_linux64.tar.gz\n\u251c\u2500\u2500 wordlists/                   # Wordlist files by category\n\u2502   \u251c\u2500\u2500 general/                 # Common wordlists\n\u2502   \u251c\u2500\u2500 specialized/             # Domain-specific lists\n\u2502   \u251c\u2500\u2500 targeted/                # Custom targeted lists\n\u2502   \u2514\u2500\u2500 custom/                  # User-uploaded lists\n\u251c\u2500\u2500 rules/                       # Rule files by type\n\u2502   \u251c\u2500\u2500 hashcat/                 # Hashcat-compatible rules\n\u2502   \u251c\u2500\u2500 john/                    # John-compatible rules\n\u2502   \u2514\u2500\u2500 custom/                  # Custom rule sets\n\u251c\u2500\u2500 hashlists/                   # Processed hashlist files\n\u2502   \u251c\u2500\u2500 1.hash                   # Uncracked hashes for job ID 1\n\u2502   \u2514\u2500\u2500 2.hash                   # Uncracked hashes for job ID 2\n\u251c\u2500\u2500 hashlist_uploads/            # Temporary upload storage\n\u2502   \u2514\u2500\u2500 &lt;user-id&gt;/              # User-specific upload directories\n\u2514\u2500\u2500 local/                       # Extracted binaries (server-side)\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#directory-permissions","title":"Directory Permissions","text":"<p>All directories are created with mode <code>0750</code> (rwxr-x---) to ensure: - Owner has full access - Group has read and execute access - Others have no access</p>"},{"location":"admin-guide/resource-management/storage/#file-deduplication-and-hash-verification","title":"File Deduplication and Hash Verification","text":""},{"location":"admin-guide/resource-management/storage/#md5-based-deduplication","title":"MD5-Based Deduplication","text":"<p>KrakenHashes uses MD5 hashes for file deduplication across all resource types:</p> <ol> <li>Upload Processing</li> <li>Calculate MD5 hash of uploaded file</li> <li>Check database for existing file with same hash</li> <li>If exists, reference existing file instead of storing duplicate</li> <li> <p>If new, store file and record hash in database</p> </li> <li> <p>Verification States</p> </li> <li><code>pending</code> - File uploaded but not yet verified</li> <li><code>verified</code> - File hash matches database record</li> <li><code>failed</code> - Hash mismatch or file corrupted</li> <li> <p><code>deleted</code> - File removed from storage</p> </li> <li> <p>Database Schema <pre><code>-- Example: Wordlists table\nCREATE TABLE wordlists (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    file_name VARCHAR(255) NOT NULL,\n    md5_hash VARCHAR(32) NOT NULL,\n    file_size BIGINT NOT NULL,\n    verification_status VARCHAR(20) DEFAULT 'pending',\n    UNIQUE(md5_hash)  -- Ensures deduplication\n);\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#file-synchronization","title":"File Synchronization","text":"<p>The agent file sync system ensures consistency across distributed agents:</p> <ol> <li>Sync Protocol</li> <li>Agent reports current files with MD5 hashes</li> <li>Server compares against master file list</li> <li>Server sends list of files to download</li> <li> <p>Agent downloads only missing/changed files</p> </li> <li> <p>Hash Verification</p> </li> <li>Files are verified after download</li> <li>Failed verifications trigger re-download</li> <li>Corrupted files are automatically replaced</li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-requirements-and-capacity-planning","title":"Storage Requirements and Capacity Planning","text":""},{"location":"admin-guide/resource-management/storage/#estimating-storage-needs","title":"Estimating Storage Needs","text":"<p>Calculate storage requirements based on:</p> <ol> <li>Wordlists</li> <li>Common wordlists: 10-50 GB</li> <li>Specialized lists: 50-200 GB</li> <li> <p>Large collections: 500+ GB</p> </li> <li> <p>Rules</p> </li> <li>Basic rule sets: 1-10 MB</li> <li> <p>Comprehensive sets: 100-500 MB</p> </li> <li> <p>Hashlists</p> </li> <li>Original uploads: Variable</li> <li>Processed files: ~32 bytes per hash</li> <li> <p>Example: 1M hashes \u2248 32 MB</p> </li> <li> <p>Binaries</p> </li> <li>Hashcat package: ~100 MB</li> <li>John package: ~50 MB</li> <li>Multiple versions: Plan for 3-5 versions</li> </ol>"},{"location":"admin-guide/resource-management/storage/#recommended-minimums","title":"Recommended Minimums","text":"Deployment Size Storage Rationale Development 50 GB Basic wordlists and testing Small Team 200 GB Standard wordlists + custom data Enterprise 1 TB+ Comprehensive wordlists + history"},{"location":"admin-guide/resource-management/storage/#growth-considerations","title":"Growth Considerations","text":"<ul> <li>Hashlist accumulation: ~10-20% monthly growth typical</li> <li>Wordlist expansion: New lists added periodically</li> <li>Binary versions: Keep 3-5 recent versions</li> <li>Backup overhead: 2x storage for full backups</li> </ul>"},{"location":"admin-guide/resource-management/storage/#backup-considerations","title":"Backup Considerations","text":""},{"location":"admin-guide/resource-management/storage/#what-to-backup","title":"What to Backup","text":"<ol> <li>Critical Data</li> <li>PostgreSQL database (contains all metadata)</li> <li>Custom wordlists and rules</li> <li> <p>Configuration files (<code>/etc/krakenhashes</code>)</p> </li> <li> <p>Recoverable Data</p> </li> <li>Standard wordlists (can be re-downloaded)</li> <li>Binaries (can be re-downloaded)</li> <li>Processed hashlists (can be regenerated)</li> </ol>"},{"location":"admin-guide/resource-management/storage/#backup-strategy","title":"Backup Strategy","text":"<pre><code>#!/bin/bash\n# Example backup script\n\n# Backup database\npg_dump -h postgres -U krakenhashes krakenhashes &gt; backup/db_$(date +%Y%m%d).sql\n\n# Backup custom data\nrsync -av /var/lib/krakenhashes/wordlists/custom/ backup/wordlists/\nrsync -av /var/lib/krakenhashes/rules/custom/ backup/rules/\nrsync -av /etc/krakenhashes/ backup/config/\n\n# Backup file metadata\ndocker-compose exec backend \\\n  psql -c \"COPY (SELECT * FROM wordlists) TO STDOUT CSV\" &gt; backup/wordlists_meta.csv\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#restore-procedures","title":"Restore Procedures","text":"<ol> <li> <p>Database Restore <pre><code>psql -h postgres -U krakenhashes krakenhashes &lt; backup/db_20240115.sql\n</code></pre></p> </li> <li> <p>File Restore <pre><code>rsync -av backup/wordlists/ /var/lib/krakenhashes/wordlists/custom/\nrsync -av backup/rules/ /var/lib/krakenhashes/rules/custom/\n</code></pre></p> </li> <li> <p>Verify Integrity</p> </li> <li>Run file verification for all restored files</li> <li>Check MD5 hashes against database records</li> </ol>"},{"location":"admin-guide/resource-management/storage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"admin-guide/resource-management/storage/#file-system-considerations","title":"File System Considerations","text":"<ol> <li>File System Choice</li> <li>ext4: Good general performance</li> <li>XFS: Better for large files</li> <li> <p>ZFS: Built-in deduplication and compression</p> </li> <li> <p>Mount Options <pre><code># Example /etc/fstab entry with optimizations\n/dev/sdb1 /var/lib/krakenhashes ext4 defaults,noatime,nodiratime 0 2\n</code></pre></p> </li> <li> <p>Storage Layout</p> </li> <li>Use separate volumes for different data types</li> <li>Consider SSD for hashlists (frequent reads)</li> <li>HDDs acceptable for wordlists (sequential reads)</li> </ol>"},{"location":"admin-guide/resource-management/storage/#caching-strategy","title":"Caching Strategy","text":"<ol> <li>Application-Level Caching</li> <li>Recently used wordlists kept in memory</li> <li>Hash type definitions cached</li> <li> <p>File metadata cached for 15 minutes</p> </li> <li> <p>File System Caching</p> </li> <li>Linux page cache handles frequently accessed files</li> <li>Monitor with <code>free -h</code> and adjust <code>vm.vfs_cache_pressure</code></li> </ol>"},{"location":"admin-guide/resource-management/storage/#io-optimization","title":"I/O Optimization","text":"<pre><code># Tune kernel parameters for better I/O\necho 'vm.dirty_ratio = 5' &gt;&gt; /etc/sysctl.conf\necho 'vm.dirty_background_ratio = 2' &gt;&gt; /etc/sysctl.conf\necho 'vm.vfs_cache_pressure = 50' &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#docker-volume-management","title":"Docker Volume Management","text":""},{"location":"admin-guide/resource-management/storage/#volume-configuration","title":"Volume Configuration","text":"<p>Docker Compose creates named volumes for persistent storage:</p> <pre><code>volumes:\n  krakenhashes_data:        # Main data directory\n    name: krakenhashes_app_data\n  postgres_data:            # Database storage\n    name: krakenhashes_postgres_data\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#volume-operations","title":"Volume Operations","text":"<ol> <li> <p>Inspect Volumes <pre><code>docker volume inspect krakenhashes_app_data\ndocker volume ls\n</code></pre></p> </li> <li> <p>Backup Volumes <pre><code># Backup data volume\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd)/backup:/backup \\\n  alpine tar czf /backup/data_backup.tar.gz -C /data .\n</code></pre></p> </li> <li> <p>Restore Volumes <pre><code># Restore data volume\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd)/backup:/backup \\\n  alpine tar xzf /backup/data_backup.tar.gz -C /data\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-driver-optimization","title":"Storage Driver Optimization","text":"<p>For production deployments:</p> <pre><code>{\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ]\n}\n</code></pre>"},{"location":"admin-guide/resource-management/storage/#file-cleanup-and-maintenance","title":"File Cleanup and Maintenance","text":""},{"location":"admin-guide/resource-management/storage/#automated-cleanup","title":"Automated Cleanup","text":"<p>The system includes automated cleanup for:</p> <ol> <li>Temporary Upload Files</li> <li>Deleted after successful processing</li> <li> <p>Orphaned files cleaned after 24 hours</p> </li> <li> <p>Old Hashlist Files</p> </li> <li>Configurable retention period</li> <li>Default: Keep for job lifetime + 30 days</li> </ol>"},{"location":"admin-guide/resource-management/storage/#manual-cleanup-procedures","title":"Manual Cleanup Procedures","text":"<ol> <li> <p>Remove Orphaned Files <pre><code># Find files not referenced in database\ndocker-compose exec backend bash\ncd /var/lib/krakenhashes\n\n# Check for orphaned wordlists\nfind wordlists -type f -name \"*.txt\" | while read f; do\n  hash=$(md5sum \"$f\" | cut -d' ' -f1)\n  # Query database for hash\ndone\n</code></pre></p> </li> <li> <p>Clean Old Hashlists <pre><code>-- Remove hashlists older than 90 days with no active jobs\nDELETE FROM hashlists \nWHERE updated_at &lt; NOW() - INTERVAL '90 days'\nAND id NOT IN (\n  SELECT DISTINCT hashlist_id \n  FROM job_executions \n  WHERE status IN ('pending', 'running')\n);\n</code></pre></p> </li> <li> <p>Vacuum Database <pre><code>docker-compose exec postgres \\\n  psql -U krakenhashes -c \"VACUUM ANALYZE;\"\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#storage-monitoring","title":"Storage Monitoring","text":"<ol> <li> <p>Disk Usage Monitoring <pre><code># Monitor storage usage\ndf -h /var/lib/krakenhashes\ndu -sh /var/lib/krakenhashes/*\n\n# Set up alerts\necho '0 * * * * root df -h | grep krakenhashes | \\\n  awk '\\''$5+0 &gt; 80 {print \"Storage warning: \" $0}'\\''' \\\n  &gt;&gt; /etc/crontab\n</code></pre></p> </li> <li> <p>File Count Monitoring <pre><code>-- Monitor file counts\nSELECT \n  'wordlists' as type, COUNT(*) as count,\n  SUM(file_size)/1024/1024/1024 as size_gb\nFROM wordlists\nWHERE verification_status = 'verified'\nUNION ALL\nSELECT 'rules', COUNT(*), SUM(file_size)/1024/1024/1024\nFROM rules\nWHERE verification_status = 'verified';\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Maintenance Schedule</li> <li>Weekly: Check disk usage and clean temp files</li> <li>Monthly: Verify file integrity and clean old hashlists</li> <li> <p>Quarterly: Full backup and storage audit</p> </li> <li> <p>Monitoring Alerts</p> </li> <li>Set up alerts for &gt;80% disk usage</li> <li>Monitor file verification failures</li> <li> <p>Track deduplication efficiency</p> </li> <li> <p>Documentation</p> </li> <li>Document custom wordlist sources</li> <li>Maintain changelog for rule modifications</li> <li>Record storage growth trends</li> </ol>"},{"location":"admin-guide/resource-management/storage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/resource-management/storage/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Disk Space Exhaustion <pre><code># Emergency cleanup\nfind /var/lib/krakenhashes/hashlist_uploads -mtime +1 -delete\ndocker system prune -f\n</code></pre></p> </li> <li> <p>File Verification Failures <pre><code>-- Find failed verifications\nSELECT * FROM wordlists \nWHERE verification_status = 'failed';\n\n-- Reset for re-verification\nUPDATE wordlists \nSET verification_status = 'pending' \nWHERE verification_status = 'failed';\n</code></pre></p> </li> <li> <p>Permission Issues <pre><code># Fix permissions\nchown -R 1000:1000 /var/lib/krakenhashes\nchmod -R 750 /var/lib/krakenhashes\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/resource-management/storage/#debug-commands","title":"Debug Commands","text":"<pre><code># Check file system integrity\nfsck -n /dev/sdb1\n\n# Monitor I/O performance\niostat -x 1\n\n# Check open files\nlsof | grep krakenhashes\n\n# Verify Docker volumes\ndocker volume inspect krakenhashes_app_data\n</code></pre>"},{"location":"admin-guide/resource-management/wordlists/","title":"Wordlists and Rules Management","text":"<p>KrakenHashes provides automatic monitoring and management of wordlists and rules used for password cracking operations. This document explains how the system handles these files and provides best practices for administrators.</p>"},{"location":"admin-guide/resource-management/wordlists/#directory-structure","title":"Directory Structure","text":"<p>The system monitors two specific directories for files:</p> <ul> <li>Wordlists Directory: <code>&lt;data_dir&gt;/wordlists/</code></li> <li>Rules Directory: <code>&lt;data_dir&gt;/rules/</code></li> </ul> <p>The system automatically creates the following subdirectory structure:</p> <p>Wordlists: <pre><code>wordlists/\n\u251c\u2500\u2500 general/       # Common wordlists for general use\n\u251c\u2500\u2500 specialized/   # Domain-specific wordlists\n\u251c\u2500\u2500 targeted/      # Target-specific wordlists\n\u2514\u2500\u2500 custom/        # User-created or modified wordlists\n</code></pre></p> <p>Rules: <pre><code>rules/\n\u251c\u2500\u2500 hashcat/       # Hashcat-compatible rules\n\u251c\u2500\u2500 john/          # John the Ripper rules\n\u2514\u2500\u2500 custom/        # User-created or modified rules\n</code></pre></p>"},{"location":"admin-guide/resource-management/wordlists/#file-formats","title":"File Formats","text":""},{"location":"admin-guide/resource-management/wordlists/#wordlists","title":"Wordlists","text":"<p>The system supports the following wordlist formats: - Plaintext: <code>.txt</code>, <code>.lst</code>, <code>.dict</code> files - Compressed: <code>.gz</code>, <code>.zip</code> files</p>"},{"location":"admin-guide/resource-management/wordlists/#rules","title":"Rules","text":"<p>The system supports the following rule formats: - Hashcat: Standard hashcat rule files - John: John the Ripper rule files</p>"},{"location":"admin-guide/resource-management/wordlists/#auto-monitoring-process","title":"Auto-Monitoring Process","text":"<p>When files are added to these directories, the system automatically:</p> <ol> <li>Detects new or modified files</li> <li>Calculates MD5 hashes for integrity verification</li> <li>Imports metadata into the database</li> <li>Counts words/rules in the files</li> <li>Makes them available for use in password cracking jobs</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#file-upload-handling","title":"File Upload Handling","text":"<p>When uploading files through the web interface:</p> <ol> <li>The system preserves the original filename (with sanitization for security)</li> <li>Files are automatically placed in the appropriate subdirectory based on their type</li> <li>Duplicate detection is performed based on filename:</li> <li>If a file with the same name exists and has the same MD5 hash, the upload is skipped</li> <li>If a file with the same name exists but has a different MD5 hash, the file is updated</li> <li>The system automatically calculates the MD5 hash and counts words/rules</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#duplicate-detection","title":"Duplicate Detection","text":"<p>The system handles duplicate files intelligently:</p> <ul> <li>Same filename, same content: The system will recognize the file as already existing and return the existing entry</li> <li>Same filename, different content: The system will update the existing file with the new content</li> <li>Different filename, same content: The system will store both files separately</li> </ul> <p>This approach ensures that: - Files are not unnecessarily duplicated - Updates to existing files are properly tracked - Users can maintain multiple versions of similar files with different names</p>"},{"location":"admin-guide/resource-management/wordlists/#auto-monitoring-details","title":"Auto-Monitoring Details","text":""},{"location":"admin-guide/resource-management/wordlists/#system-user","title":"System User","text":"<p>All auto-imported wordlists and rules are created in the database using a special system user (UUID: <code>00000000-0000-0000-0000-000000000000</code>). This user:</p> <ul> <li>Cannot be used for frontend login</li> <li>Is used exclusively for system-generated actions</li> <li>Helps track which files were auto-imported vs. manually added</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#monitoring-interval","title":"Monitoring Interval","text":"<ul> <li>The system checks for new files every 5 minutes</li> <li>Initial scan happens immediately when the server starts</li> <li>There's a small delay (2 seconds) after database migrations complete before monitoring starts to prevent race conditions</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#file-detection-process","title":"File Detection Process","text":"<ol> <li>The system detects new or modified files in the monitored directories</li> <li>Files are checked to ensure they're not still being transferred</li> <li>MD5 hash is calculated for each file</li> <li>If a file with the same name exists but has a different hash, it's updated</li> <li>If a file with the same name and hash exists, it's skipped</li> <li>New files are added to the database with \"pending\" verification status</li> <li>File contents are counted (words or rules)</li> <li>Status is updated to \"verified\" once counting is complete</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#file-transfer-considerations","title":"File Transfer Considerations","text":"<p>When transferring files to the monitored directories, be aware of the following:</p> <ul> <li>File Stability Check: The system waits for files to be stable (not actively changing) before processing them</li> <li>SCP/SFTP Transfer Time: For large files transferred via SCP or SFTP, allow sufficient time for the transfer to complete before the file will be processed</li> <li>The system waits 30 seconds after the last modification before considering a file stable</li> <li>For files larger than 100MB, the system also checks if the file size has changed</li> </ul> <p>Note: When using SCP to transfer large wordlists, the file may not be detected for import until 30 seconds after the transfer completes.</p>"},{"location":"admin-guide/resource-management/wordlists/#file-size-and-format-restrictions","title":"File Size and Format Restrictions","text":""},{"location":"admin-guide/resource-management/wordlists/#size-restrictions","title":"Size Restrictions","text":"<ul> <li>No file size limits: The system can handle wordlists and rules of any size</li> <li>For very large files (&gt;1GB), be aware that:</li> <li>Initial hash calculation may take longer</li> <li>Word/rule counting operations run in the background</li> <li>The file will be available with a \"pending\" status until counting completes</li> </ul>"},{"location":"admin-guide/resource-management/wordlists/#automatic-tagging","title":"Automatic Tagging","text":"<p>Auto-imported files are automatically tagged for easy identification:</p> <ul> <li>All auto-imported files receive the tag <code>auto-imported</code></li> <li>Updated files additionally receive the tag <code>updated</code></li> </ul> <p>The system does not automatically tag files based on subdirectories or other criteria.</p>"},{"location":"admin-guide/resource-management/wordlists/#rule-type-detection","title":"Rule Type Detection","text":"<p>The system automatically assigns a default rule type based on the file path:</p> <ul> <li>If the path contains \"john\" (case-insensitive), the rule is classified as a John the Ripper rule</li> <li>Otherwise, it's classified as a Hashcat rule</li> </ul> <p>Administrators can edit the rule type after import if needed.</p>"},{"location":"admin-guide/resource-management/wordlists/#wordlist-type-classification","title":"Wordlist Type Classification","text":"<p>Wordlists are classified based on their subdirectory:</p> <ul> <li>General: Common wordlists suitable for most cracking jobs</li> <li>Specialized: Wordlists focused on specific patterns or domains</li> <li>Targeted: Wordlists tailored for specific targets</li> <li>Custom: User-created or modified wordlists</li> </ul> <p>When uploading through the web interface, you can specify the wordlist type, which determines the subdirectory where the file will be stored.</p>"},{"location":"admin-guide/resource-management/wordlists/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive filenames: Filenames are used as the default name in the database</li> <li>Pre-verify large files: For very large wordlists, consider pre-calculating word counts to display immediately</li> <li>Monitor the logs: Check server logs for any import errors or issues</li> <li>Avoid frequent updates: Updating large files frequently can cause unnecessary processing overhead</li> <li>Organize by type: Use the appropriate wordlist type when uploading to keep files organized</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#monitoring-import-status","title":"Monitoring Import Status","text":"<p>Administrators can check the status of file imports through:</p> <ol> <li>The admin dashboard in the web interface</li> <li>Server logs, which show detailed information about file processing</li> <li>The database, where each wordlist and rule has a <code>verification_status</code> field</li> </ol>"},{"location":"admin-guide/resource-management/wordlists/#manual-management","title":"Manual Management","text":"<p>While auto-importing is convenient, you can also manually:</p> <ol> <li>Add wordlists and rules through the web interface</li> <li>Update metadata for auto-imported files</li> <li>Delete files that are no longer needed</li> </ol> <p>Important: Deleting a file from the database does not remove it from the filesystem. Similarly, removing a file from the filesystem will not automatically remove it from the database.</p>"},{"location":"admin-guide/resource-management/wordlists/#wordlist-types","title":"Wordlist Types","text":"<p>Wordlists are categorized into the following types: - General: Common wordlists for general use - Specialized: Domain-specific wordlists - Targeted: Target-specific wordlists - Custom: User-created or modified wordlists </p>"},{"location":"admin-guide/system-setup/authentication/","title":"Authentication Settings Administration","text":""},{"location":"admin-guide/system-setup/authentication/#overview","title":"Overview","text":"<p>KrakenHashes provides robust authentication settings to ensure system security. This document covers the configuration of password policies, account security settings, and multi-factor authentication (MFA) options.</p>"},{"location":"admin-guide/system-setup/authentication/#password-policy","title":"Password Policy","text":"<p>The password policy settings define the requirements for user passwords across the system.</p>"},{"location":"admin-guide/system-setup/authentication/#configuration-options","title":"Configuration Options","text":"<ol> <li>Minimum Password Length</li> <li>Default: 15 characters</li> <li>Must be a positive integer</li> <li>Recommended: 15+ characters minimum</li> <li> <p>Enforced during password creation and changes</p> </li> <li> <p>Character Requirements</p> </li> <li>Require Uppercase Letters: When enabled, passwords must contain at least one uppercase letter (A-Z)</li> <li>Require Lowercase Letters: When enabled, passwords must contain at least one lowercase letter (a-z)</li> <li>Require Numbers: When enabled, passwords must contain at least one number (0-9)</li> <li>Require Special Characters: When enabled, passwords must contain at least one special character (!@#$%^&amp;*(),.?\":{}|&lt;&gt;)</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices","title":"Best Practices","text":"<ul> <li>Enable all character requirements for maximum security</li> <li>Balance security with usability when setting minimum length</li> <li>Consider industry standards (NIST, OWASP) when configuring</li> <li>Document password requirements clearly for users</li> </ul>"},{"location":"admin-guide/system-setup/authentication/#account-security","title":"Account Security","text":"<p>Account security settings manage login attempts, session duration, and security notifications.</p>"},{"location":"admin-guide/system-setup/authentication/#configuration-options_1","title":"Configuration Options","text":"<ol> <li>Maximum Failed Login Attempts</li> <li>Default: 5 attempts</li> <li>Defines how many failed login attempts are allowed before account lockout</li> <li>Must be a positive integer</li> <li> <p>Recommended range: 3-5 attempts</p> </li> <li> <p>Account Lockout Duration</p> </li> <li>Default: 60 minutes</li> <li>Duration in minutes before a locked account is automatically unlocked</li> <li>Must be a positive integer</li> <li> <p>Affects accounts locked due to exceeded login attempts</p> </li> <li> <p>JWT Token Expiry</p> </li> <li>Default: 60 minutes</li> <li>Duration in minutes before an authentication token expires</li> <li>Forces users to re-authenticate after expiration</li> <li> <p>Balances security with user convenience</p> </li> <li> <p>Notification Aggregation Interval</p> </li> <li>Default: 60 minutes</li> <li>How often to aggregate and send security notifications</li> <li>Prevents notification fatigue while maintaining awareness</li> <li>Groups similar security events within the interval</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices_1","title":"Best Practices","text":"<ul> <li>Adjust lockout duration based on threat model</li> <li>Consider user experience when setting token expiry</li> <li>Monitor failed login attempts for attack patterns</li> <li>Review security notifications regularly</li> </ul>"},{"location":"admin-guide/system-setup/authentication/#multi-factor-authentication-mfa-settings","title":"Multi-Factor Authentication (MFA) Settings","text":"<p>MFA provides an additional layer of security beyond passwords.</p>"},{"location":"admin-guide/system-setup/authentication/#general-settings","title":"General Settings","text":"<ol> <li>Require MFA for All Users</li> <li>Toggle to enforce MFA across all user accounts</li> <li>To enable an email provider must be configured as email is the default MFA</li> <li>Affects new and existing users</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#allowed-mfa-methods","title":"Allowed MFA Methods","text":"<p>The system supports multiple MFA methods:</p> <ol> <li>Email Authentication</li> <li>Sends verification codes to user's registered email</li> <li>Requires configured email provider</li> <li> <p>Good balance of security and convenience</p> </li> <li> <p>Authenticator Apps</p> </li> <li>Compatible with standard TOTP authenticator apps</li> <li>More secure than email-based authentication</li> <li>Works offline once configured</li> <li> <p>Examples: Bitwarden, Google Authenticator, Authy, Microsoft Authenticator</p> </li> <li> <p>Passkey (Future Feature)</p> </li> <li>Currently disabled</li> <li>Will support FIDO2/WebAuthn standard</li> <li>Provides highest security level</li> <li>Requires compatible hardware/devices</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#code-settings","title":"Code Settings","text":"<ol> <li>Email Code Validity</li> <li>Default: 5 minutes</li> <li>How long email-based MFA codes remain valid</li> <li>Must be at least 1 minute</li> <li> <p>Balance security with delivery delays</p> </li> <li> <p>Code Cooldown Period</p> </li> <li>Default: 1 minute</li> <li>Minimum time between code requests</li> <li>Prevents code request spam</li> <li> <p>Must be at least 1 minute</p> </li> <li> <p>Code Expiry Time</p> </li> <li>Default: 5 minutes</li> <li>How long codes remain valid after generation</li> <li>Applies to all MFA methods</li> <li> <p>Should account for potential delays</p> </li> <li> <p>Maximum Code Attempts</p> </li> <li>Default: 3 attempts</li> <li>Maximum invalid code entries before invalidation</li> <li>Requires new code generation after exceeded</li> <li> <p>Prevents brute force attacks</p> </li> <li> <p>Number of Backup Codes</p> </li> <li>Default: 8 codes</li> <li>One-time use backup codes for account recovery</li> <li>Must be at least 1 code</li> <li>Recommended: 8-10 codes</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#best-practices_2","title":"Best Practices","text":"<ol> <li>MFA Implementation</li> <li>Consider enforcing MFA for all users</li> <li>Enable multiple MFA methods for flexibility</li> <li>Educate users about backup codes importance</li> <li> <p>Regular review of MFA settings</p> </li> <li> <p>Code Security</p> </li> <li>Keep validity periods short (5-15 minutes)</li> <li>Implement reasonable cooldown periods</li> <li>Limit invalid attempts</li> <li> <p>Generate sufficient backup codes</p> </li> <li> <p>User Experience</p> </li> <li>Clear communication about MFA requirements</li> <li>Document recovery procedures</li> <li>Train support staff on MFA issues</li> <li> <p>Regular testing of MFA workflows</p> </li> <li> <p>Monitoring and Maintenance</p> </li> <li>Regular review of MFA logs</li> <li>Monitor failed MFA attempts</li> <li>Update settings based on security needs</li> <li>Keep documentation current</li> </ol>"},{"location":"admin-guide/system-setup/authentication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/authentication/#common-issues","title":"Common Issues","text":"<ol> <li>Users Unable to Enable MFA</li> <li>Verify email provider configuration</li> <li>Check user permissions</li> <li>Confirm supported authenticator app</li> <li> <p>Review error messages</p> </li> <li> <p>Locked Accounts</p> </li> <li>Verify lockout duration settings</li> <li>Check failed attempt count</li> <li>Review security logs</li> <li> <p>Consider administrative unlock</p> </li> <li> <p>MFA Code Issues</p> </li> <li>Verify code validity period</li> <li>Check cooldown period</li> <li>Confirm correct email delivery</li> <li> <p>Review time synchronization</p> </li> <li> <p>Password Policy Problems</p> </li> <li>Review current policy settings</li> <li>Check character requirement conflicts</li> <li>Verify minimum length appropriateness</li> <li>Consider user feedback </li> </ol>"},{"location":"admin-guide/system-setup/configuration/","title":"KrakenHashes System Configuration Guide","text":"<p>This guide provides comprehensive documentation for all configuration options available in the KrakenHashes system, including environment variables, configuration files, and settings for all components.</p>"},{"location":"admin-guide/system-setup/configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Environment Variables</li> <li>Core System Settings</li> <li>Database Configuration</li> <li>Backend Configuration</li> <li>Frontend Configuration</li> <li>Agent Configuration</li> <li>TLS/SSL Configuration</li> <li>Debug and Logging</li> <li>Docker-Specific Settings</li> <li>Configuration Files</li> <li>Common Configuration Scenarios</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#overview","title":"Overview","text":"<p>KrakenHashes uses environment variables for all runtime configuration. Configuration can be provided through:</p> <ol> <li>Environment Variables: Direct system environment variables</li> <li><code>.env</code> Files: Local environment files (development)</li> <li>Docker Compose: Environment variables in docker-compose.yml</li> <li>Kubernetes ConfigMaps: For production deployments</li> </ol>"},{"location":"admin-guide/system-setup/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>The system follows this precedence order (highest to lowest): 1. Runtime environment variables 2. Docker Compose environment settings 3. <code>.env</code> file values 4. Default values in code</p>"},{"location":"admin-guide/system-setup/configuration/#configuration-types","title":"Configuration Types","text":"<p>KrakenHashes uses two types of configuration:</p>"},{"location":"admin-guide/system-setup/configuration/#1-environment-variables-system-configuration","title":"1. Environment Variables (System Configuration)","text":"<p>These settings control core system behavior and are set at deployment time. They configure infrastructure elements like database connections, ports, and file paths.</p>"},{"location":"admin-guide/system-setup/configuration/#2-admin-panel-settings-runtime-configuration","title":"2. Admin Panel Settings (Runtime Configuration)","text":"<p>These settings can be changed through the web interface without restarting services. They control operational behavior like job execution, chunking, and agent coordination.</p> <p>Configuration Best Practice</p> <p>Use environment variables for infrastructure settings that rarely change. Use admin panel settings for operational parameters that need frequent adjustment.</p>"},{"location":"admin-guide/system-setup/configuration/#admin-panel-settings","title":"Admin Panel Settings","text":"<p>Several configuration options are available through the Admin Panel UI rather than environment variables. These settings can be changed at runtime without restarting services.</p>"},{"location":"admin-guide/system-setup/configuration/#available-admin-panel-settings","title":"Available Admin Panel Settings","text":"<ul> <li>Job Execution Settings: Control job chunking, agent behavior, and task distribution</li> <li>Default chunk duration</li> <li>Reconnect grace period</li> <li>Progress reporting intervals</li> <li> <p>Rule splitting configuration</p> </li> <li> <p>Data Retention Settings: Configure automatic data cleanup</p> </li> <li>Hashlist retention periods</li> <li>Job history retention</li> <li> <p>Metrics retention</p> </li> <li> <p>Agent Scheduling: Define when agents are available</p> </li> <li>Daily schedules per agent</li> <li>Global scheduling enable/disable</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#accessing-admin-panel-settings","title":"Accessing Admin Panel Settings","text":"<ol> <li>Log in as an administrator</li> <li>Navigate to the Admin Panel</li> <li>Click Settings in the navigation menu</li> <li>Select the appropriate settings category</li> </ol> <p>Changes to admin panel settings take effect immediately without requiring service restarts.</p>"},{"location":"admin-guide/system-setup/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"admin-guide/system-setup/configuration/#core-system-settings","title":"Core System Settings","text":"Variable Description Default Example <code>PUID</code> User ID for file permissions <code>1000</code> <code>1001</code> <code>PGID</code> Group ID for file permissions <code>1000</code> <code>1001</code> <code>TZ</code> Timezone <code>UTC</code> <code>America/New_York</code> <code>KH_IN_DOCKER</code> Whether running in Docker <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#database-configuration","title":"Database Configuration","text":"Variable Description Default Example <code>DB_HOST</code> PostgreSQL host <code>localhost</code> <code>postgres</code> <code>DB_PORT</code> PostgreSQL port <code>5432</code> <code>5432</code> <code>DB_NAME</code> Database name <code>krakenhashes</code> <code>krakenhashes_prod</code> <code>DB_USER</code> Database username <code>krakenhashes</code> <code>khuser</code> <code>DB_PASSWORD</code> Database password <code>krakenhashes</code> <code>secure_password</code> <code>DB_CONNECTION_STRING</code> Full connection string (alternative) - <code>postgres://user:pass@host:port/db?sslmode=disable</code>"},{"location":"admin-guide/system-setup/configuration/#backend-configuration","title":"Backend Configuration","text":""},{"location":"admin-guide/system-setup/configuration/#server-settings","title":"Server Settings","text":"Variable Description Default Example <code>KH_HOST</code> Backend host binding <code>localhost</code> (or <code>0.0.0.0</code> in Docker) <code>0.0.0.0</code> <code>KH_HTTPS_PORT</code> HTTPS API port <code>31337</code> <code>8443</code> <code>KH_HTTP_PORT</code> HTTP port (CA certificate) <code>1337</code> <code>8080</code> <code>KH_CONFIG_DIR</code> Configuration directory <code>~/.krakenhashes</code> <code>/etc/krakenhashes</code> <code>KH_DATA_DIR</code> Data storage directory <code>~/.krakenhashes-data</code> <code>/var/lib/krakenhashes</code> <code>KH_CERTS_DIR</code> Certificate directory <code>{KH_CONFIG_DIR}/certs</code> <code>/etc/krakenhashes/certs</code>"},{"location":"admin-guide/system-setup/configuration/#file-handling","title":"File Handling","text":"Variable Description Default Example <code>KH_HASHLIST_BATCH_SIZE</code> Max hashes per DB batch <code>1000</code> <code>5000</code> <code>KH_MAX_UPLOAD_SIZE_MB</code> Max file upload size (MB) <code>32</code> <code>100</code> <code>KH_HASH_UPLOAD_DIR</code> Hash upload directory <code>{KH_DATA_DIR}/hashlist_uploads</code> <code>/var/lib/krakenhashes/uploads</code>"},{"location":"admin-guide/system-setup/configuration/#jwt-authentication","title":"JWT Authentication","text":"Variable Description Default Example <code>JWT_SECRET</code> JWT signing secret - <code>your-secret-key</code> <code>JWT_EXPIRATION</code> Token expiration time <code>24h</code> <code>7d</code> <code>DEFAULT_ADMIN_ID</code> Default admin user ID - <code>uuid-here</code>"},{"location":"admin-guide/system-setup/configuration/#websocket-configuration","title":"WebSocket Configuration","text":"Variable Description Default Example <code>KH_WRITE_WAIT</code> Time allowed to write messages <code>4s</code> <code>10s</code> <code>KH_PONG_WAIT</code> Time to wait for pong response <code>10s</code> <code>30s</code> <code>KH_PING_PERIOD</code> How often to send pings <code>6s</code> <code>15s</code>"},{"location":"admin-guide/system-setup/configuration/#frontend-configuration","title":"Frontend Configuration","text":"Variable Description Default Example <code>REACT_APP_API_URL</code> HTTPS API endpoint <code>https://localhost:31337</code> <code>https://api.example.com</code> <code>REACT_APP_HTTP_API_URL</code> HTTP API endpoint <code>http://localhost:1337</code> <code>http://api.example.com:8080</code> <code>REACT_APP_WS_URL</code> WebSocket endpoint <code>wss://localhost:31337</code> <code>wss://api.example.com</code> <code>FRONTEND_PORT</code> Frontend HTTPS port <code>443</code> <code>3000</code> <code>PORT</code> Development server port <code>3000</code> <code>3001</code> <code>NODE_ENV</code> Node environment <code>development</code> <code>production</code> <code>HTTPS</code> Enable HTTPS in dev server <code>true</code> <code>false</code> <code>SSL_CRT_FILE</code> Dev server SSL certificate - <code>../certs/server.crt</code> <code>SSL_KEY_FILE</code> Dev server SSL key - <code>../certs/server.key</code>"},{"location":"admin-guide/system-setup/configuration/#agent-configuration","title":"Agent Configuration","text":"Variable Description Default Example <code>KH_CONFIG_DIR</code> Agent config directory <code>{executable_dir}/config</code> <code>/opt/krakenhashes/config</code> <code>KH_DATA_DIR</code> Agent data directory <code>{executable_dir}/data</code> <code>/opt/krakenhashes/data</code> <code>HASHCAT_EXTRA_PARAMS</code> Extra hashcat parameters - <code>-O -w 3</code>"},{"location":"admin-guide/system-setup/configuration/#tlsssl-configuration","title":"TLS/SSL Configuration","text":""},{"location":"admin-guide/system-setup/configuration/#general-tls-settings","title":"General TLS Settings","text":"Variable Description Default Example <code>KH_TLS_MODE</code> TLS provider mode <code>self-signed</code> <code>certbot</code>, <code>provided</code> <code>KH_CERT_KEY_SIZE</code> RSA key size (bits) <code>4096</code> <code>2048</code> <code>KH_CERT_VALIDITY_DAYS</code> Server cert validity (days) <code>365</code> <code>730</code> <code>KH_CA_VALIDITY_DAYS</code> CA cert validity (days) <code>3650</code> <code>7300</code>"},{"location":"admin-guide/system-setup/configuration/#certificate-details","title":"Certificate Details","text":"Variable Description Default Example <code>KH_CA_COUNTRY</code> CA country code <code>US</code> <code>UK</code> <code>KH_CA_ORGANIZATION</code> CA organization <code>KrakenHashes</code> <code>YourOrg</code> <code>KH_CA_ORGANIZATIONAL_UNIT</code> CA organizational unit <code>KrakenHashes CA</code> <code>IT Department</code> <code>KH_CA_COMMON_NAME</code> CA common name <code>KrakenHashes Root CA</code> <code>YourOrg Root CA</code> <code>KH_ADDITIONAL_DNS_NAMES</code> Additional DNS names (comma-separated) - <code>localhost,app.local,*.example.com</code> <code>KH_ADDITIONAL_IP_ADDRESSES</code> Additional IP addresses (comma-separated) - <code>192.168.1.100,10.0.0.5</code>"},{"location":"admin-guide/system-setup/configuration/#provided-certificate-mode","title":"Provided Certificate Mode","text":"Variable Description Default Example <code>KH_CERT_FILE</code> Path to certificate file <code>{KH_CERTS_DIR}/server.crt</code> <code>/etc/ssl/server.crt</code> <code>KH_KEY_FILE</code> Path to private key file <code>{KH_CERTS_DIR}/server.key</code> <code>/etc/ssl/server.key</code> <code>KH_CA_FILE</code> Path to CA certificate <code>{KH_CERTS_DIR}/ca.crt</code> <code>/etc/ssl/ca.crt</code>"},{"location":"admin-guide/system-setup/configuration/#certbot-mode-lets-encrypt","title":"Certbot Mode (Let's Encrypt)","text":"Variable Description Default Example <code>KH_CERTBOT_DOMAIN</code> Domain for certificate - <code>kraken.example.com</code> <code>KH_CERTBOT_EMAIL</code> Email for notifications - <code>admin@example.com</code> <code>KH_CERTBOT_STAGING</code> Use staging server <code>false</code> <code>true</code> <code>KH_CERTBOT_AUTO_RENEW</code> Enable auto-renewal <code>true</code> <code>false</code> <code>KH_CERTBOT_RENEW_HOOK</code> Post-renewal hook script - <code>/opt/scripts/reload.sh</code> <code>CLOUDFLARE_API_TOKEN</code> Cloudflare API token for DNS-01 - <code>your-api-token</code>"},{"location":"admin-guide/system-setup/configuration/#debug-and-logging","title":"Debug and Logging","text":""},{"location":"admin-guide/system-setup/configuration/#general-debug-settings","title":"General Debug Settings","text":"Variable Description Default Example <code>DEBUG</code> Enable debug mode <code>false</code> <code>true</code> <code>LOG_LEVEL</code> Logging level <code>INFO</code> <code>DEBUG</code>, <code>WARNING</code>, <code>ERROR</code>"},{"location":"admin-guide/system-setup/configuration/#component-specific-debug-flags","title":"Component-Specific Debug Flags","text":"Variable Description Default Example <code>DEBUG_SQL</code> Enable SQL query logging <code>false</code> <code>true</code> <code>DEBUG_HTTP</code> Enable HTTP request/response logging <code>false</code> <code>true</code> <code>DEBUG_WEBSOCKET</code> Enable WebSocket message logging <code>false</code> <code>true</code> <code>DEBUG_AUTH</code> Enable authentication debugging <code>false</code> <code>true</code> <code>DEBUG_JOBS</code> Enable job processing debugging <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#frontend-debug-settings","title":"Frontend Debug Settings","text":"Variable Description Default Example <code>REACT_APP_DEBUG</code> Enable frontend debugging <code>false</code> <code>true</code> <code>REACT_APP_DEBUG_REDUX</code> Enable Redux debugging <code>false</code> <code>true</code>"},{"location":"admin-guide/system-setup/configuration/#log-directories","title":"Log Directories","text":"Variable Description Default Example <code>LOG_DIR</code> Base log directory <code>/var/log/krakenhashes</code> <code>/logs</code> <code>BACKEND_LOG_DIR</code> Backend logs <code>${LOG_DIR}/backend</code> <code>/logs/backend</code> <code>FRONTEND_LOG_DIR</code> Frontend logs <code>${LOG_DIR}/frontend</code> <code>/logs/frontend</code> <code>NGINX_LOG_DIR</code> Nginx logs <code>${LOG_DIR}/nginx</code> <code>/logs/nginx</code> <code>POSTGRES_LOG_DIR</code> PostgreSQL logs <code>${LOG_DIR}/postgres</code> <code>/logs/postgres</code>"},{"location":"admin-guide/system-setup/configuration/#docker-specific-settings","title":"Docker-Specific Settings","text":""},{"location":"admin-guide/system-setup/configuration/#volume-mounts","title":"Volume Mounts","text":"Variable Description Default Example <code>KH_CONFIG_DIR_HOST</code> Host config directory <code>/etc/krakenhashes</code> <code>./config</code> <code>KH_DATA_DIR_HOST</code> Host data directory <code>/var/lib/krakenhashes</code> <code>./data</code>"},{"location":"admin-guide/system-setup/configuration/#nginx-configuration","title":"Nginx Configuration","text":"Variable Description Default Example <code>NGINX_ACCESS_LOG_LEVEL</code> Nginx access log level <code>info</code> <code>debug</code> <code>NGINX_ERROR_LOG_LEVEL</code> Nginx error log level <code>warn</code> <code>error</code> <code>NGINX_CLIENT_MAX_BODY_SIZE</code> Max request body size <code>50M</code> <code>100M</code>"},{"location":"admin-guide/system-setup/configuration/#cors-configuration","title":"CORS Configuration","text":"Variable Description Default Example <code>CORS_ALLOWED_ORIGIN</code> Allowed CORS origins <code>https://localhost:443</code> <code>https://app.example.com</code>"},{"location":"admin-guide/system-setup/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"admin-guide/system-setup/configuration/#directory-structure","title":"Directory Structure","text":"<p>The system creates and uses the following directory structure:</p> <pre><code>${KH_CONFIG_DIR}/\n\u251c\u2500\u2500 certs/              # TLS certificates\n\u2502   \u251c\u2500\u2500 ca.crt         # CA certificate\n\u2502   \u251c\u2500\u2500 ca.key         # CA private key\n\u2502   \u251c\u2500\u2500 server.crt     # Server certificate\n\u2502   \u2514\u2500\u2500 server.key     # Server private key\n\u2514\u2500\u2500 config/            # Application configuration\n\n${KH_DATA_DIR}/\n\u251c\u2500\u2500 binaries/          # Hashcat and other tools\n\u251c\u2500\u2500 wordlists/         # Wordlist files\n\u2502   \u251c\u2500\u2500 general/       # General purpose wordlists\n\u2502   \u251c\u2500\u2500 specialized/   # Domain-specific wordlists\n\u2502   \u251c\u2500\u2500 targeted/      # Target-specific wordlists\n\u2502   \u2514\u2500\u2500 custom/        # User-uploaded wordlists\n\u251c\u2500\u2500 rules/             # Rule files\n\u2502   \u251c\u2500\u2500 hashcat/       # Hashcat rule files\n\u2502   \u251c\u2500\u2500 john/          # John the Ripper rules\n\u2502   \u2514\u2500\u2500 custom/        # Custom rule files\n\u251c\u2500\u2500 hashlists/         # Hash files\n\u2514\u2500\u2500 hashlist_uploads/  # Temporary upload directory\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#configuration-file-locations","title":"Configuration File Locations","text":"<ul> <li>Backend: No configuration files - all settings via environment variables</li> <li>Frontend: <code>.env</code> file in frontend directory (development only)</li> <li>Agent: Configuration stored in <code>${KH_CONFIG_DIR}/agent.json</code> (auto-generated)</li> <li>Docker: <code>.env</code> file in project root for docker-compose</li> </ul>"},{"location":"admin-guide/system-setup/configuration/#common-configuration-scenarios","title":"Common Configuration Scenarios","text":""},{"location":"admin-guide/system-setup/configuration/#development-setup","title":"Development Setup","text":"<pre><code># .env file for development\nDEBUG=true\nLOG_LEVEL=DEBUG\nKH_TLS_MODE=self-signed\nJWT_SECRET=dev-secret-key\nDB_HOST=localhost\nDB_PASSWORD=dev-password\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#production-with-lets-encrypt","title":"Production with Let's Encrypt","text":"<pre><code># .env file for production\nDEBUG=false\nLOG_LEVEL=INFO\nKH_TLS_MODE=certbot\nKH_CERTBOT_DOMAIN=kraken.example.com\nKH_CERTBOT_EMAIL=admin@example.com\nCLOUDFLARE_API_TOKEN=your-token\nJWT_SECRET=secure-random-secret\nDB_PASSWORD=strong-password\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#high-security-environment","title":"High-Security Environment","text":"<pre><code># .env file for high security\nKH_TLS_MODE=provided\nKH_CERT_FILE=/etc/ssl/certs/server.crt\nKH_KEY_FILE=/etc/ssl/private/server.key\nKH_CA_FILE=/etc/ssl/certs/ca-bundle.crt\nKH_CERT_KEY_SIZE=4096\nJWT_SECRET=very-long-secure-secret\nDEBUG=false\nDEBUG_SQL=false\nDEBUG_HTTP=false\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#agent-configuration_1","title":"Agent Configuration","text":"<pre><code># Agent environment\nKH_CONFIG_DIR=/opt/krakenhashes/config\nKH_DATA_DIR=/opt/krakenhashes/data\nHASHCAT_EXTRA_PARAMS=-O -w 3\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#docker-production-deployment","title":"Docker Production Deployment","text":"<pre><code># Production docker-compose override\nPUID=1000\nPGID=1000\nKH_CONFIG_DIR_HOST=/opt/krakenhashes/config\nKH_DATA_DIR_HOST=/data/krakenhashes\nLOG_DIR=/var/log/krakenhashes\nNGINX_CLIENT_MAX_BODY_SIZE=100M\n</code></pre>"},{"location":"admin-guide/system-setup/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Security:</li> <li>Always use strong, unique values for <code>JWT_SECRET</code> in production</li> <li>Never commit <code>.env</code> files with secrets to version control</li> <li> <p>Use environment-specific configurations</p> </li> <li> <p>File Permissions:</p> </li> <li>Set <code>PUID</code> and <code>PGID</code> to match your host user for proper permissions</li> <li> <p>Ensure certificate files have restricted permissions (600 or 640)</p> </li> <li> <p>TLS/SSL:</p> </li> <li>Use Let's Encrypt (<code>certbot</code> mode) for production</li> <li>Self-signed certificates only for development</li> <li> <p>Always include all required DNS names and IP addresses</p> </li> <li> <p>Performance:</p> </li> <li>Adjust <code>KH_HASHLIST_BATCH_SIZE</code> based on available memory</li> <li>Configure <code>NGINX_CLIENT_MAX_BODY_SIZE</code> for expected file sizes</li> <li> <p>Set appropriate WebSocket timeouts for your network</p> </li> <li> <p>Logging:</p> </li> <li>Use <code>INFO</code> level for production</li> <li>Enable component-specific debugging only when needed</li> <li> <p>Regularly rotate log files</p> </li> <li> <p>Database:</p> </li> <li>Use strong passwords in production</li> <li>Consider using SSL for database connections in production</li> <li>Regular backups of the PostgreSQL data volume</li> </ol>"},{"location":"admin-guide/system-setup/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/configuration/#common-issues","title":"Common Issues","text":"<ol> <li>Certificate Errors:</li> <li>Check <code>KH_ADDITIONAL_DNS_NAMES</code> includes all hostnames</li> <li>Verify certificate paths are correct</li> <li> <p>Ensure proper file permissions</p> </li> <li> <p>Database Connection:</p> </li> <li>Verify database credentials</li> <li>Check network connectivity between containers</li> <li> <p>Ensure PostgreSQL is healthy before backend starts</p> </li> <li> <p>File Upload Issues:</p> </li> <li>Check <code>KH_MAX_UPLOAD_SIZE_MB</code> setting</li> <li>Verify <code>NGINX_CLIENT_MAX_BODY_SIZE</code> is sufficient</li> <li> <p>Ensure data directories have proper permissions</p> </li> <li> <p>WebSocket Disconnections:</p> </li> <li>Adjust timeout values (<code>KH_PONG_WAIT</code>, <code>KH_PING_PERIOD</code>)</li> <li>Check for proxy/firewall interference</li> <li>Verify WebSocket URL configuration</li> </ol>"},{"location":"admin-guide/system-setup/email/","title":"Email Settings Administration","text":""},{"location":"admin-guide/system-setup/email/#overview","title":"Overview","text":"<p>KrakenHashes supports email functionality through multiple providers, currently SendGrid and Mailgun. This document covers the configuration and management of email settings through the admin interface.</p>"},{"location":"admin-guide/system-setup/email/#provider-configuration","title":"Provider Configuration","text":"<p> Admin Settings Email Configuration page showing Mailgun provider setup with API Key, Domain, From Name, From Email, and Monthly Limit fields</p>"},{"location":"admin-guide/system-setup/email/#sendgrid","title":"SendGrid","text":"<p>To configure SendGrid as your email provider:</p> <ol> <li>Select \"SendGrid\" from the Provider dropdown</li> <li>Configure the following fields:</li> <li>API Key: Your SendGrid API key with email sending permissions</li> <li>From Email: The verified sender email address</li> <li>From Name: Display name for the sender (defaults to \"KrakenHashes\")</li> <li>Monthly Limit: (Optional) Set a monthly email sending limit</li> </ol>"},{"location":"admin-guide/system-setup/email/#mailgun","title":"Mailgun","text":"<p>To configure Mailgun as your email provider:</p> <ol> <li>Select \"Mailgun\" from the Provider dropdown</li> <li>Configure the following fields:</li> <li>API Key: Your Mailgun API key</li> <li>Domain: Your verified Mailgun domain</li> <li>From Email: The verified sender email address</li> <li>From Name: Display name for the sender (defaults to \"KrakenHashes\")</li> <li>Monthly Limit: (Optional) Set a monthly email sending limit</li> </ol>"},{"location":"admin-guide/system-setup/email/#monthly-limit","title":"Monthly Limit","text":"<p>The monthly limit field is optional: - Leave empty for unlimited emails - Set a numeric value to limit monthly email sending - Helps prevent unexpected costs from email service providers</p>"},{"location":"admin-guide/system-setup/email/#testing-and-saving-configuration","title":"Testing and Saving Configuration","text":""},{"location":"admin-guide/system-setup/email/#configuration-options","title":"Configuration Options","text":"<p>When saving email provider settings, you have three options:</p> <ol> <li>Cancel: Discard changes and return to previous settings</li> <li>Save Configuration: Save settings without testing</li> <li>Test and Save: Test the configuration before saving</li> </ol>"},{"location":"admin-guide/system-setup/email/#testing-process","title":"Testing Process","text":"<p>When using \"Test and Save\":</p> <ol> <li>Enter a test email address</li> <li>System sends a test email to verify configuration</li> <li>If successful:</li> <li>Configuration is saved</li> <li>Confirmation message displayed</li> <li>If failed:</li> <li>Error message displayed</li> <li>Configuration not saved</li> <li>Troubleshooting information provided</li> </ol>"},{"location":"admin-guide/system-setup/email/#email-templates","title":"Email Templates","text":"<p>Email templates are managed separately from provider configuration.</p>"},{"location":"admin-guide/system-setup/email/#best-practices","title":"Best Practices","text":"<ol> <li>Provider Selection</li> <li>Choose based on your volume needs</li> <li>Consider provider-specific features</li> <li> <p>Review pricing structures</p> </li> <li> <p>Configuration Testing</p> </li> <li>Always test configuration before deployment</li> <li>Verify emails are received</li> <li> <p>Check spam folder during testing</p> </li> <li> <p>Monthly Limits</p> </li> <li>Set based on expected usage</li> <li>Include buffer for unexpected spikes</li> <li> <p>Monitor usage through provider dashboards</p> </li> <li> <p>Security Considerations</p> </li> <li>Store API keys securely</li> <li>Use dedicated sending domains</li> <li>Regularly rotate API keys</li> <li>Monitor for unusual activity</li> </ol>"},{"location":"admin-guide/system-setup/email/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/email/#common-issues","title":"Common Issues","text":"<ol> <li>Emails Not Sending</li> <li>Verify API key permissions</li> <li>Check monthly limit hasn't been reached</li> <li>Confirm sender email is verified</li> <li> <p>Review provider dashboard for blocks</p> </li> <li> <p>Test Emails Failing</p> </li> <li>Verify API key is correct</li> <li>Check domain configuration (Mailgun)</li> <li>Ensure test email address is valid</li> <li> <p>Review error messages in admin interface</p> </li> <li> <p>Template Issues</p> </li> <li>Verify template syntax</li> <li>Check variable names match expected format</li> <li>Preview templates before saving</li> <li>Test with various data scenarios </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/","title":"SSL/TLS Setup Guide for KrakenHashes","text":"<p>This guide explains how to set up and trust the self-signed SSL certificates used by KrakenHashes on various platforms.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#overview","title":"Overview","text":"<p>KrakenHashes uses self-signed certificates by default to secure communications between components. When you first access the web interface, your browser will warn you about the untrusted certificate. This is expected behavior for self-signed certificates.</p> <p>To avoid these warnings, you need to install the KrakenHashes Certificate Authority (CA) certificate as a trusted root certificate on your system.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#downloading-the-ca-certificate","title":"Downloading the CA Certificate","text":"<p>The CA certificate can be downloaded from: - HTTP endpoint: <code>http://your-server:1337/ca.crt</code> - Via the web interface: When you see the certificate warning, there's usually an option to download the CA certificate</p> <p>Save the file as <code>krakenhashes-ca.crt</code> for the following instructions.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#installation-instructions-by-platform","title":"Installation Instructions by Platform","text":""},{"location":"admin-guide/system-setup/ssl-tls/#linux-systems","title":"Linux Systems","text":""},{"location":"admin-guide/system-setup/ssl-tls/#debianubuntu","title":"Debian/Ubuntu","text":"<ol> <li> <p>Copy the CA certificate to the system certificate directory:    <pre><code>sudo cp krakenhashes-ca.crt /usr/local/share/ca-certificates/\n</code></pre></p> </li> <li> <p>Update the certificate store:    <pre><code>sudo update-ca-certificates\n</code></pre></p> </li> <li> <p>You should see output indicating the certificate was added:    <pre><code>Updating certificates in /etc/ssl/certs...\n1 added, 0 removed; done.\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#fedorarhelcentosrocky-linux","title":"Fedora/RHEL/CentOS/Rocky Linux","text":"<ol> <li> <p>Copy the CA certificate to the trust anchors directory:    <pre><code>sudo cp krakenhashes-ca.crt /etc/pki/ca-trust/source/anchors/\n</code></pre></p> </li> <li> <p>Update the trust store:    <pre><code>sudo update-ca-trust\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#arch-linux","title":"Arch Linux","text":"<p>Using trust anchor command (Recommended): <pre><code># This is the official Arch way - it handles everything automatically\nsudo trust anchor --store krakenhashes-ca.crt\n</code></pre></p> <p>Note: The <code>trust anchor --store</code> command automatically: - Places the certificate in <code>/etc/ca-certificates/trust-source/anchors/</code> - Updates the trust database with <code>update-ca-trust</code> - Handles p11-kit integration properly</p>"},{"location":"admin-guide/system-setup/ssl-tls/#additional-step-for-chromechromium-on-linux","title":"Additional Step for Chrome/Chromium on Linux","text":"<p>Chrome and Chromium browsers maintain their own certificate store on Linux. After installing the system certificate, you also need to:</p> <ol> <li> <p>Install the certificate in the NSS database:    <pre><code>certutil -d sql:$HOME/.pki/nssdb -A -t \"CP,CP,\" -n \"KrakenHashes Root CA\" -i krakenhashes-ca.crt\n</code></pre></p> </li> <li> <p>Restart Chrome/Chromium for the changes to take effect.</p> </li> </ol> <p>To verify the certificate was installed: <pre><code>certutil -d sql:$HOME/.pki/nssdb -L\n</code></pre></p> <p>To remove the certificate later if needed: <pre><code>certutil -d sql:$HOME/.pki/nssdb -D -n \"KrakenHashes Root CA\"\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#windows","title":"Windows","text":""},{"location":"admin-guide/system-setup/ssl-tls/#using-certificate-manager-gui","title":"Using Certificate Manager (GUI)","text":"<ol> <li>Press <code>Win + R</code>, type <code>certmgr.msc</code>, and press Enter</li> <li>Navigate to \"Trusted Root Certification Authorities\" &gt; \"Certificates\"</li> <li>Right-click on \"Certificates\" and select \"All Tasks\" &gt; \"Import...\"</li> <li>Follow the Certificate Import Wizard:</li> <li>Click \"Next\"</li> <li>Browse and select the <code>krakenhashes-ca.crt</code> file</li> <li>Click \"Next\"</li> <li>Ensure \"Place all certificates in the following store\" is selected with \"Trusted Root Certification Authorities\"</li> <li>Click \"Next\" and then \"Finish\"</li> <li>When prompted with a security warning, click \"Yes\" to install the certificate</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#using-powershell-administrator","title":"Using PowerShell (Administrator)","text":"<pre><code># Import the certificate to the Trusted Root store\nImport-Certificate -FilePath \"C:\\path\\to\\krakenhashes-ca.crt\" -CertStoreLocation Cert:\\LocalMachine\\Root\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#using-command-prompt-administrator","title":"Using Command Prompt (Administrator)","text":"<pre><code>certutil -addstore \"Root\" \"C:\\path\\to\\krakenhashes-ca.crt\"\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#macos","title":"macOS","text":""},{"location":"admin-guide/system-setup/ssl-tls/#using-keychain-access-gui","title":"Using Keychain Access (GUI)","text":"<ol> <li>Double-click the <code>krakenhashes-ca.crt</code> file</li> <li>Keychain Access will open</li> <li>Select \"System\" keychain (you may need to authenticate)</li> <li>The certificate will be added but marked as untrusted</li> <li>Double-click on the \"KrakenHashes Root CA\" certificate</li> <li>Expand the \"Trust\" section</li> <li>Change \"When using this certificate\" to \"Always Trust\"</li> <li>Close the window and authenticate to save changes</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#using-command-line","title":"Using Command Line","text":"<pre><code># Add certificate to system keychain\nsudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain krakenhashes-ca.crt\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#verification","title":"Verification","text":"<p>After installing the certificate, you can verify it's working:</p> <ol> <li>Browser Test: Navigate to <code>https://your-server:31337</code> - you should no longer see certificate warnings</li> <li>Command Line Test:     <pre><code>curl https://your-server:31337/health\n# Should work without certificate errors\n</code></pre></li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-details","title":"Certificate Details","text":"<p>The self-signed certificates generated by KrakenHashes include:</p> <ul> <li>CA Certificate: Valid for 10 years (3650 days)</li> <li>Server Certificate: Valid for 1 year (365 days)</li> <li>Key Size: 4096-bit RSA (configurable)</li> <li>Extensions: </li> <li>SubjectKeyIdentifier and AuthorityKeyIdentifier for proper chain validation</li> <li>BasicConstraints properly set (CA:TRUE for CA, CA:FALSE for server)</li> <li>Appropriate KeyUsage and ExtendedKeyUsage flags</li> </ul>"},{"location":"admin-guide/system-setup/ssl-tls/#environment-variables","title":"Environment Variables","text":"<p>You can customize certificate generation with these environment variables:</p> <ul> <li><code>KH_ADDITIONAL_DNS_NAMES</code>: Comma-separated additional DNS names (e.g., \"krakenhashes.local,kraken.internal\")</li> <li><code>KH_ADDITIONAL_IP_ADDRESSES</code>: Comma-separated additional IP addresses (e.g., \"192.168.1.100,10.0.0.50\")</li> <li><code>KH_CERT_KEY_SIZE</code>: RSA key size (2048 or 4096, default: 4096)</li> <li><code>KH_CERT_VALIDITY_DAYS</code>: Server certificate validity in days (default: 365)</li> <li><code>KH_CA_VALIDITY_DAYS</code>: CA certificate validity in days (default: 3650)</li> </ul>"},{"location":"admin-guide/system-setup/ssl-tls/#security-considerations","title":"Security Considerations","text":"<ol> <li>Self-signed certificates are suitable for:</li> <li>Development environments</li> <li>Internal networks</li> <li>Testing and staging</li> <li> <p>Environments where you control all clients</p> </li> <li> <p>For production use, consider:</p> </li> <li>Using certificates from a trusted CA (Let's Encrypt, etc.)</li> <li>Implementing proper certificate rotation</li> <li> <p>Using the <code>certbot</code> mode if you have a public domain</p> </li> <li> <p>Certificate Storage:</p> </li> <li>Private keys are stored with 0600 permissions</li> <li>Certificates are stored in the configured certs directory</li> <li>Default location: <code>~/.krakenhashes/certs/</code> or <code>/etc/krakenhashes/certs/</code> in Docker</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#regenerating-certificates","title":"Regenerating Certificates","text":"<p>If you need to regenerate certificates (e.g., to add new SANs):</p> <ol> <li>Stop the KrakenHashes backend</li> <li>Delete the existing certificates:    <pre><code>rm -rf ~/.krakenhashes/certs/*\n# Or your configured certs directory\n</code></pre></li> <li>Start the backend - new certificates will be generated automatically</li> <li>Reinstall the new CA certificate on client systems</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-modes","title":"Certificate Modes","text":"<p>KrakenHashes supports three TLS modes:</p> <ol> <li>self-signed (default): Automatically generates and manages certificates</li> <li>provided: Use your own certificates (set paths via environment variables)</li> <li>certbot: Integration with Let's Encrypt for public domains</li> </ol> <p>To change modes, set the <code>KH_TLS_MODE</code> environment variable.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#certbot-mode-lets-encrypt","title":"Certbot Mode (Let's Encrypt)","text":"<p>This mode allows you to use Let's Encrypt certificates via Certbot with Cloudflare DNS-01 challenge. This is ideal for internal applications that aren't publicly accessible.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#prerequisites","title":"Prerequisites","text":"<ol> <li>Domain Name: You need to own a domain (e.g., <code>zerkersec.io</code>)</li> <li>Cloudflare Account: Your domain must be managed by Cloudflare</li> <li>API Token: Create a Cloudflare API token with proper permissions</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-1-create-cloudflare-api-token","title":"Step 1: Create Cloudflare API Token","text":"<ol> <li>Log in to your Cloudflare dashboard</li> <li>Go to My Profile \u2192 API Tokens</li> <li>Click Create Token</li> <li>Use the Custom token template with these permissions:</li> <li>Zone \u2192 DNS \u2192 Edit</li> <li>Zone \u2192 Zone \u2192 Read (optional but recommended)</li> <li>Under Zone Resources, select:</li> <li>Include \u2192 Specific zone \u2192 Your domain (e.g., <code>zerkersec.io</code>)</li> <li>Click Continue to summary and Create Token</li> <li>Save the token securely - you won't be able to see it again!</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-2-dns-configuration","title":"Step 2: DNS Configuration","text":"<p>Create an A record for your subdomain:</p> <ol> <li>In Cloudflare dashboard, go to your domain</li> <li>Navigate to DNS \u2192 Records</li> <li>Add a new A record:</li> <li>Type: A</li> <li>Name: <code>kraken</code> (or your chosen subdomain)</li> <li>IPv4 address: Your internal IP (e.g., <code>10.0.0.100</code>)</li> <li>Proxy status: DNS only (gray cloud)</li> <li>TTL: Auto</li> </ol> <p>Note: The IP address doesn't need to be publicly accessible. Certbot only needs to verify domain ownership via DNS TXT records.</p>"},{"location":"admin-guide/system-setup/ssl-tls/#step-3-configure-krakenhashes","title":"Step 3: Configure KrakenHashes","text":"<p>Edit your <code>.env</code> file:</p> <pre><code># Change TLS mode to certbot\nKH_TLS_MODE=certbot\n\n# Certbot Configuration\nKH_CERTBOT_DOMAIN=kraken.zerkersec.io    # Your full domain\nKH_CERTBOT_EMAIL=admin@zerkersec.io      # Your email for Let's Encrypt\nKH_CERTBOT_STAGING=true                  # Start with staging for testing!\nKH_CERTBOT_AUTO_RENEW=true               # Enable automatic renewal\n\n# Cloudflare Configuration\nCLOUDFLARE_API_TOKEN=your-token-here     # The token from Step 1\n\n# Update frontend URLs to use your domain\nREACT_APP_API_URL=https://kraken.zerkersec.io:31337\nREACT_APP_WS_URL=wss://kraken.zerkersec.io:31337\n\n# Update CORS to allow your domain\nCORS_ALLOWED_ORIGIN=https://kraken.zerkersec.io\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#step-4-initial-setup-with-staging","title":"Step 4: Initial Setup with Staging","text":"<p>Important: Always test with Let's Encrypt staging environment first to avoid rate limits!</p> <ol> <li> <p>Stop existing containers:    <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>Start with certbot mode:    <pre><code>docker-compose up -d\n</code></pre></p> </li> <li> <p>Monitor the logs:    <pre><code>docker-compose logs -f krakenhashes\n</code></pre></p> </li> <li> <p>Look for messages indicating successful certificate generation:    <pre><code>Obtaining certificates for domain: kraken.zerkersec.io\nSuccessfully obtained certificates\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-5-verify-staging-certificates","title":"Step 5: Verify Staging Certificates","text":"<ol> <li> <p>Check certificate files:    <pre><code>ls -la kh-backend/config/certs/live/kraken.zerkersec.io/\n</code></pre></p> </li> <li> <p>You should see:</p> </li> <li><code>fullchain.pem</code> - Certificate chain</li> <li><code>privkey.pem</code> - Private key</li> <li><code>chain.pem</code> - Intermediate certificates</li> <li> <p><code>cert.pem</code> - Domain certificate</p> </li> <li> <p>Test the application (you'll get a browser warning for staging certificates)</p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#step-6-switch-to-production","title":"Step 6: Switch to Production","text":"<p>Once staging certificates work:</p> <ol> <li> <p>Update <code>.env</code>:    <pre><code>KH_CERTBOT_STAGING=false\n</code></pre></p> </li> <li> <p>Remove staging certificates:    <pre><code>rm -rf kh-backend/config/certs/live/*\nrm -rf kh-backend/config/certs/archive/*\nrm -rf kh-backend/config/certs/renewal/*\n</code></pre></p> </li> <li> <p>Restart to get production certificates:    <pre><code>docker-compose down\ndocker-compose up -d\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-renewal","title":"Certificate Renewal","text":"<p>Certificates are automatically renewed:</p> <ul> <li>Renewal checks run twice daily (3 AM and 3 PM)</li> <li>Certificates renew when less than 30 days remain</li> <li>Services reload automatically after renewal</li> </ul> <p>Manual Renewal: <pre><code>docker exec krakenhashes /usr/local/bin/certbot-renew.sh\n</code></pre></p> <p>Monitor Renewal: <pre><code>docker exec krakenhashes tail -f /var/log/krakenhashes/certbot-renew.log\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#certbot-troubleshooting","title":"Certbot Troubleshooting","text":""},{"location":"admin-guide/system-setup/ssl-tls/#common-issues","title":"Common Issues","text":"<ol> <li>\"CLOUDFLARE_API_TOKEN environment variable is required\"</li> <li>Ensure the token is set in your <code>.env</code> file</li> <li> <p>Token must have DNS:Edit permissions</p> </li> <li> <p>\"Failed to obtain certificates\"</p> </li> <li>Check Cloudflare API token permissions</li> <li>Verify domain ownership</li> <li> <p>Check certbot logs: <code>docker exec krakenhashes cat /etc/krakenhashes/certs/logs/letsencrypt.log</code></p> </li> <li> <p>Browser still shows certificate warnings</p> </li> <li>Ensure you switched from staging to production</li> <li>Clear browser cache</li> <li> <p>Verify certificates: <code>docker exec krakenhashes openssl x509 -in /etc/krakenhashes/certs/live/kraken.zerkersec.io/cert.pem -text -noout</code></p> </li> <li> <p>Rate Limits</p> </li> <li>Let's Encrypt has rate limits (50 certificates per domain per week)</li> <li>Always test with staging first</li> <li>See: https://letsencrypt.org/docs/rate-limits/</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-information","title":"Certificate Information","text":"<p>View certificate details: <pre><code># Inside container\ndocker exec krakenhashes certbot certificates --config-dir /etc/krakenhashes/certs\n\n# Certificate expiry\ndocker exec krakenhashes openssl x509 -enddate -noout -in /etc/krakenhashes/certs/live/kraken.zerkersec.io/cert.pem\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#additional-domains","title":"Additional Domains","text":"<p>To add more domains/subdomains:</p> <ol> <li> <p>Add them to <code>KH_ADDITIONAL_DNS_NAMES</code> in <code>.env</code>:    <pre><code>KH_ADDITIONAL_DNS_NAMES=kraken.zerkersec.io,api.zerkersec.io,*.kraken.zerkersec.io\n</code></pre></p> </li> <li> <p>Ensure all domains are in Cloudflare and accessible by your API token</p> </li> <li> <p>Restart the container to obtain certificates for all domains</p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#migration-from-self-signed","title":"Migration from Self-Signed","text":"<p>If migrating from self-signed certificates:</p> <ol> <li>Back up existing certificates (optional)</li> <li>Update <code>.env</code> as shown above</li> <li>Restart containers</li> <li>Update any clients/browsers that have the old CA certificate cached</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admin-guide/system-setup/ssl-tls/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"admin-guide/system-setup/ssl-tls/#browser-still-shows-certificate-warnings","title":"Browser Still Shows Certificate Warnings","text":"<ol> <li>Certificate not properly installed:</li> <li>Verify the certificate is in the correct store (Trusted Root, not Personal or Intermediate)</li> <li>Check that you installed the CA certificate, not the server certificate</li> <li> <p>Some browsers cache certificate decisions - try clearing browser data or using incognito mode</p> </li> <li> <p>Browser-specific issues:</p> </li> <li>Chrome/Edge: May require restart after certificate installation</li> <li>Firefox: Has its own certificate store - you may need to import via Firefox settings</li> <li> <p>Safari: Requires explicit trust settings in Keychain Access</p> </li> <li> <p>Certificate regenerated:</p> </li> <li>If the backend regenerated certificates, you need to reinstall the new CA certificate</li> <li>Remove the old certificate first to avoid conflicts</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#err_ssl_key_usage_incompatible-chromeedge","title":"ERR_SSL_KEY_USAGE_INCOMPATIBLE (Chrome/Edge)","text":"<p>This error indicates the certificate doesn't have the correct key usage flags. The updated certificate generation should prevent this, but if you encounter it:</p> <ol> <li>Ensure you're running the latest version with the certificate improvements</li> <li>Delete existing certificates and let them regenerate</li> <li>As a temporary workaround on Windows:    <pre><code># Create registry key to disable the check (use with caution)\nNew-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Microsoft\\Edge\" -Name \"RSAKeyUsageForLocalAnchorsEnabled\" -Value 0 -PropertyType DWORD\n# For Chrome:\nNew-ItemProperty -Path \"HKLM:\\SOFTWARE\\Policies\\Google\\Chrome\" -Name \"RSAKeyUsageForLocalAnchorsEnabled\" -Value 0 -PropertyType DWORD\n</code></pre></li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-not-trusted-on-linux","title":"Certificate Not Trusted on Linux","text":"<ol> <li> <p>Check certificate format:    <pre><code># Verify it's a valid PEM certificate\nopenssl x509 -in krakenhashes-ca.crt -text -noout\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code># Check if certificate is in the trust store\ntrust list | grep -i krakenhashes\n\n# For Debian/Ubuntu, check the symlink was created\nls -la /etc/ssl/certs/ | grep krakenhashes\n</code></pre></p> </li> <li> <p>SELinux issues (RHEL/Fedora):    <pre><code># Check for SELinux denials\nsudo ausearch -m avc -ts recent\n\n# If needed, restore context\nsudo restorecon -v /etc/pki/ca-trust/source/anchors/krakenhashes-ca.crt\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#certificate-chain-issues","title":"Certificate Chain Issues","text":"<ol> <li> <p>Verify the full chain is being sent:    <pre><code># Check what certificates the server is presenting\nopenssl s_client -connect your-server:31337 -showcerts\n</code></pre></p> </li> <li> <p>Validate certificate chain:    <pre><code># Download server certificate\necho | openssl s_client -connect your-server:31337 -servername your-server 2&gt;/dev/null | \\\n  openssl x509 &gt; server.crt\n\n# Verify against CA\nopenssl verify -CAfile krakenhashes-ca.crt server.crt\n</code></pre></p> </li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#connection-refused-or-timeout","title":"Connection Refused or Timeout","text":"<ol> <li> <p>Check the service is running:    <pre><code># Check if ports are listening\nnetstat -tlnp | grep -E \"1337|31337\"\n# or\nss -tlnp | grep -E \"1337|31337\"\n</code></pre></p> </li> <li> <p>Firewall rules:    <pre><code># Check firewall status\nsudo iptables -L -n | grep -E \"1337|31337\"\n# or for firewalld\nsudo firewall-cmd --list-all\n</code></pre></p> </li> <li> <p>Docker networking (if using Docker):</p> </li> <li>Ensure ports are properly mapped in docker-compose.yml</li> <li>Check Docker network connectivity</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#diagnostic-commands","title":"Diagnostic Commands","text":""},{"location":"admin-guide/system-setup/ssl-tls/#view-certificate-details","title":"View Certificate Details","text":"<pre><code># View CA certificate details\nopenssl x509 -in krakenhashes-ca.crt -text -noout\n\n# Check certificate dates\nopenssl x509 -in krakenhashes-ca.crt -noout -dates\n\n# Check certificate subject and issuer\nopenssl x509 -in krakenhashes-ca.crt -noout -subject -issuer\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#test-tls-connection","title":"Test TLS Connection","text":"<pre><code># Test with curl (verbose)\ncurl -v https://your-server:31337/health\n\n# Test with openssl\nopenssl s_client -connect your-server:31337 -CAfile krakenhashes-ca.crt\n\n# Test cipher suites\nnmap --script ssl-enum-ciphers -p 31337 your-server\n</code></pre>"},{"location":"admin-guide/system-setup/ssl-tls/#browser-certificate-debugging","title":"Browser Certificate Debugging","text":"<ol> <li>Chrome: Navigate to <code>chrome://settings/certificates</code> to manage certificates</li> <li>Firefox: Navigate to <code>about:preferences#privacy</code> &gt; View Certificates</li> <li>Edge: Navigate to <code>edge://settings/privacy</code> &gt; Manage certificates</li> </ol>"},{"location":"admin-guide/system-setup/ssl-tls/#removing-old-certificates","title":"Removing Old Certificates","text":"<p>If you need to remove old certificates before installing new ones:</p> <p>Arch Linux: <pre><code># Find and remove the certificate\nsudo rm -f /etc/ca-certificates/trust-source/anchors/krakenhashes-ca.crt\nsudo update-ca-trust\n\n# Or use trust anchor to remove by name\nsudo trust anchor --remove \"KrakenHashes Root CA\"\n\n# Remove from Chrome/Chromium\ncertutil -d sql:$HOME/.pki/nssdb -D -n \"KrakenHashes Root CA\"\n</code></pre></p> <p>Debian/Ubuntu: <pre><code>sudo rm /usr/local/share/ca-certificates/krakenhashes-ca.crt\nsudo update-ca-certificates --fresh\n</code></pre></p> <p>Fedora/RHEL: <pre><code>sudo rm /etc/pki/ca-trust/source/anchors/krakenhashes-ca.crt\nsudo update-ca-trust\n</code></pre></p>"},{"location":"admin-guide/system-setup/ssl-tls/#getting-help","title":"Getting Help","text":"<p>If you're still experiencing issues:</p> <ol> <li> <p>Check the backend logs for certificate generation errors:    <pre><code>grep -i \"certificate\\|tls\\|ssl\" /path/to/backend.log\n</code></pre></p> </li> <li> <p>Ensure your environment variables are set correctly</p> </li> <li>Try regenerating certificates with a clean state</li> <li>Report issues at: https://github.com/ZerkerEOD/krakenhashes/issues</li> </ol>"},{"location":"agent-guide/","title":"Agent Guide","text":""},{"location":"agent-guide/#overview","title":"Overview","text":"<p>Agents are the computational workhorses of KrakenHashes, responsible for executing password cracking jobs using hashcat. This guide covers all aspects of agent deployment, configuration, and management.</p>"},{"location":"agent-guide/#quick-start","title":"Quick Start","text":"<ol> <li>Download the agent binary from the Releases page</li> <li>Generate a claim code in the Admin UI</li> <li>Register the agent:    <pre><code>./krakenhashes-agent -claim YOUR_CLAIM_CODE -host IP:31337\n</code></pre></li> <li>Start the agent:    <pre><code>./krakenhashes-agent\n</code></pre></li> </ol>"},{"location":"agent-guide/#guide-contents","title":"Guide Contents","text":""},{"location":"agent-guide/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Installation - Installing and setting up agents</li> <li>Configuration - Agent configuration options</li> <li>Systemd Service Setup - Running agents as systemd services</li> <li>File Synchronization - How agents sync files with the backend</li> </ul>"},{"location":"agent-guide/#operations","title":"Operations","text":"<ul> <li>Scheduling - Configure working hours and availability</li> <li>Device Management - GPU and device configuration</li> <li>Monitoring - Performance metrics and health checks</li> </ul>"},{"location":"agent-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Common Issues - Solutions to frequent problems</li> <li>Logs and Debugging - Finding and understanding agent logs</li> </ul>"},{"location":"agent-guide/#key-concepts","title":"Key Concepts","text":""},{"location":"agent-guide/#agent-registration","title":"Agent Registration","text":"<p>Agents use a claim code system for secure registration: - One-time codes: Single use, automatically deactivated - Continuous codes: Can register multiple agents - API keys: Generated during registration for ongoing authentication</p>"},{"location":"agent-guide/#device-support","title":"Device Support","text":"<p>KrakenHashes agents support multiple device types: - NVIDIA GPUs (CUDA) - AMD GPUs (OpenCL) - Intel GPUs (OpenCL) - CPU-based cracking (fallback)</p>"},{"location":"agent-guide/#file-management","title":"File Management","text":"<p>Agents automatically manage required files: - Wordlists are downloaded on-demand - Rules are cached locally - Hashcat binaries are auto-updated - All files are verified using checksums</p>"},{"location":"agent-guide/#security","title":"Security","text":"<p>Agent security features: - TLS encrypted communication - API key authentication - No inbound connections required - Certificate validation</p>"},{"location":"agent-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Install your first agent</li> <li>Configure agent scheduling</li> <li>Learn about file synchronization</li> </ul>"},{"location":"agent-guide/configuration/","title":"Agent Configuration","text":""},{"location":"agent-guide/configuration/#overview","title":"Overview","text":"<p>This guide covers all configuration options available for KrakenHashes agents.</p>"},{"location":"agent-guide/configuration/#configuration-file","title":"Configuration File","text":"<p>The agent uses a <code>.env</code> configuration file that is automatically created on first run. The file is located in the agent's working directory and contains all necessary configuration.</p>"},{"location":"agent-guide/configuration/#location","title":"Location","text":"<ul> <li>Default: <code>./.env</code> (in the current working directory)</li> <li>Custom: Specified via command-line flags on first run</li> </ul>"},{"location":"agent-guide/configuration/#configuration-management","title":"Configuration Management","text":"<p>The <code>.env</code> file is automatically managed by the agent: - Created on first run with values from command-line flags - Updated with any missing configuration keys on subsequent runs - Preserves existing values when new options are added</p>"},{"location":"agent-guide/configuration/#manual-editing","title":"Manual Editing","text":"<p>You can manually edit the <code>.env</code> file to adjust configuration:</p> <pre><code># Stop the agent (choose based on your setup)\n\n# For manual run: Ctrl+C in the terminal or kill the process\n# For user service:\nsystemctl --user stop krakenhashes-agent\n# For system service:\nsudo systemctl stop krakenhashes-agent\n\n# Edit the configuration\nnano .env\n\n# Restart the agent\n\n# For manual run:\n./krakenhashes-agent\n# For user service:\nsystemctl --user start krakenhashes-agent\n# For system service:\nsudo systemctl start krakenhashes-agent\n</code></pre> <p>Note: If you haven't set up systemd yet, see the Systemd Service Setup guide for automatic startup and easier management.</p>"},{"location":"agent-guide/configuration/#environment-variables-env-file","title":"Environment Variables (.env File)","text":"<p>The agent uses a <code>.env</code> file for configuration, which is automatically created during first run. The file is loaded at startup and values are NOT taken from system environment variables to avoid conflicts when running on the same host as the backend.</p>"},{"location":"agent-guide/configuration/#complete-env-file-example","title":"Complete .env File Example","text":"<p>This is the actual <code>.env</code> file generated by the agent on first run:</p> <pre><code># KrakenHashes Agent Configuration\n# Generated on: 2025-09-05T12:05:32+01:00\n\n# Server Configuration\nKH_HOST=your-server.example.com  # Backend server hostname\nKH_PORT=31337                    # Backend server port\nUSE_TLS=true                     # Use TLS for secure communication (wss:// and https://)\nLISTEN_INTERFACE=                # Network interface to bind to (leave empty for all)\nHEARTBEAT_INTERVAL=5             # Heartbeat interval in seconds\n\n# Agent Configuration\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE  # Claim code for first-time registration (auto-commented after success)\n\n# Directory Configuration\nKH_CONFIG_DIR=./config  # Configuration directory for certificates and credentials\nKH_DATA_DIR=./data      # Data directory for binaries, wordlists, rules, and hashlists\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s   # Timeout for writing messages to WebSocket\nKH_PONG_WAIT=60s    # Timeout for receiving pong from server\nKH_PING_PERIOD=54s  # Interval for sending ping to server (must be less than pong wait)\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Maximum number of concurrent file downloads\nKH_DOWNLOAD_TIMEOUT=1h         # Timeout for large file downloads\n\n# Hashcat Configuration\nHASHCAT_EXTRA_PARAMS=  # Extra parameters to pass to hashcat (e.g., \"-O -w 3\" for optimized kernels and high workload)\n\n# Logging Configuration\nDEBUG=false            # Enable debug logging\nLOG_LEVEL=INFO        # Log level (DEBUG, INFO, WARNING, ERROR)\n</code></pre>"},{"location":"agent-guide/configuration/#important-hashcat-parameter-precedence","title":"Important: Hashcat Parameter Precedence","text":"<p>HASHCAT_EXTRA_PARAMS Behavior: - Parameters configured in the frontend/backend (per-agent settings) take precedence over the agent's .env file - The agent's .env <code>HASHCAT_EXTRA_PARAMS</code> is only used as a fallback when the backend doesn't send any parameters - Best Practice: Configure agent parameters via the frontend UI for centralized management - Only use .env parameters for local overrides that should NOT be managed by the backend</p> <p>Parameter Priority (highest to lowest): 1. Backend/Frontend per-agent settings (stored in database) 2. Agent .env file <code>HASHCAT_EXTRA_PARAMS</code> (fallback only)</p>"},{"location":"agent-guide/configuration/#manual-env-file-creation","title":"Manual .env File Creation","text":"<p>You can manually create a <code>.env</code> file for agent registration instead of using command-line flags:</p> <ol> <li>Create a <code>.env</code> file in the agent's working directory</li> <li>Copy the example above and fill in your values:</li> <li>Set <code>KH_HOST</code> to your backend server hostname</li> <li>Set <code>KH_PORT</code> to your backend server port (usually 31337)</li> <li>Set <code>KH_CLAIM_CODE</code> to your claim code (get from Admin UI)</li> <li>Adjust directory paths as needed</li> <li>Run the agent without any flags: <code>./krakenhashes-agent</code></li> <li>After successful registration, the claim code will be automatically commented out</li> </ol> <p>Note: The agent reads from the <code>.env</code> file, not from system environment variables. This prevents conflicts when running the agent and backend on the same host.</p>"},{"location":"agent-guide/configuration/#command-line-options","title":"Command Line Options","text":"<pre><code>krakenhashes-agent [flags]\n\nFlags:\n  -host string           Backend server host (e.g., localhost:31337)\n  -tls                   Use TLS for secure communication (default: true)\n  -interface string      Network interface to listen on (optional)\n  -heartbeat int         Heartbeat interval in seconds (default: 5)\n  -claim string          Agent claim code (required only for first-time registration)\n  -debug                 Enable debug logging (default: false)\n  -hashcat-params string Extra parameters to pass to hashcat (e.g., '-O -w 3')\n  -config-dir string     Configuration directory for certificates and credentials\n  -data-dir string       Data directory for binaries, wordlists, rules, and hashlists\n  -help                  Show help\n</code></pre>"},{"location":"agent-guide/configuration/#example-usage","title":"Example Usage","text":"<pre><code># First-time registration from agent directory\ncd ~/krakenhashes-agent\n./krakenhashes-agent \\\n  -host your-server:31337 \\\n  -claim YOUR_CLAIM_CODE \\\n  -debug\n\n# Subsequent runs (uses .env file created during first run)\ncd ~/krakenhashes-agent\n./krakenhashes-agent\n\n# Override specific settings\n./krakenhashes-agent -debug -hashcat-params \"-O -w 4\"\n</code></pre>"},{"location":"agent-guide/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are applied in this order (later overrides earlier): 1. Default values 2. <code>.env</code> file values (created/updated on first run) 3. Command line flags</p> <p>Important: The agent does NOT read from system environment variables to avoid conflicts when running on the same host as the backend. All configuration is handled through the <code>.env</code> file and command-line flags.</p>"},{"location":"agent-guide/configuration/#device-configuration","title":"Device Configuration","text":""},{"location":"agent-guide/configuration/#enablingdisabling-devices","title":"Enabling/Disabling Devices","text":"<p>You can control which devices the agent uses:</p> <pre><code>devices:\n  # Disable specific device IDs\n  disabled_devices:\n    - 0  # Disable first GPU\n\n  # Or only enable specific devices\n  enabled_devices:\n    - 1\n    - 2\n</code></pre>"},{"location":"agent-guide/configuration/#device-specific-settings","title":"Device-Specific Settings","text":"<pre><code>devices:\n  # Per-device temperature limits\n  device_temps:\n    0: 80  # Device 0 max temp\n    1: 85  # Device 1 max temp\n\n  # Per-device workload\n  device_workloads:\n    0: 2  # Lower workload for device 0\n    1: 4  # Higher workload for device 1\n</code></pre>"},{"location":"agent-guide/configuration/#security-configuration","title":"Security Configuration","text":""},{"location":"agent-guide/configuration/#tlsssl-settings","title":"TLS/SSL Settings","text":"<pre><code>tls:\n  # Skip certificate verification (not recommended)\n  insecure_skip_verify: false\n\n  # Custom CA certificate\n  ca_cert_file: /etc/krakenhashes/ca.crt\n\n  # Client certificates (if required)\n  client_cert_file: /etc/krakenhashes/client.crt\n  client_key_file: /etc/krakenhashes/client.key\n</code></pre>"},{"location":"agent-guide/configuration/#api-key-security","title":"API Key Security","text":"<ul> <li>API keys are stored encrypted in the config file</li> <li>Keys are never logged or displayed after registration</li> <li>Regenerate keys if compromised</li> </ul>"},{"location":"agent-guide/configuration/#performance-tuning","title":"Performance Tuning","text":""},{"location":"agent-guide/configuration/#memory-management","title":"Memory Management","text":"<pre><code>performance:\n  # Hashcat memory settings\n  hashcat_memory_limit: 4096  # MB per device\n\n  # System memory reservation\n  system_memory_reserve: 2048  # MB to leave free\n\n  # File cache settings\n  max_cache_size: 10240  # MB for wordlists/rules\n</code></pre>"},{"location":"agent-guide/configuration/#gpu-optimization","title":"GPU Optimization","text":"<pre><code>performance:\n  # GPU utilization target\n  gpu_utilization_target: 90  # Percent\n\n  # Kernel tuning\n  kernel_accel: 0  # 0=auto, or specific value\n  kernel_loops: 0  # 0=auto, or specific value\n\n  # Power management\n  gpu_power_tune: 0  # Percent adjustment (-50 to +50)\n</code></pre>"},{"location":"agent-guide/configuration/#monitoring-configuration","title":"Monitoring Configuration","text":"<pre><code>monitoring:\n  # Metrics collection\n  collect_metrics: true\n  metrics_interval: 30  # seconds\n\n  # Hardware monitoring\n  monitor_temps: true\n  monitor_fan_speed: true\n  monitor_power: true\n  monitor_memory: true\n\n  # Alerts\n  alerts:\n    high_temp_threshold: 85\n    low_hashrate_threshold: 1000000  # H/s\n    error_rate_threshold: 0.05  # 5%\n</code></pre>"},{"location":"agent-guide/configuration/#scheduling-configuration","title":"Scheduling Configuration","text":"<p>See Agent Scheduling for detailed scheduling configuration.</p>"},{"location":"agent-guide/configuration/#troubleshooting-configuration-issues","title":"Troubleshooting Configuration Issues","text":""},{"location":"agent-guide/configuration/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging to see configuration loading: <pre><code>cd ~/krakenhashes-agent\n./krakenhashes-agent -debug\n</code></pre></p> <p>This will show: - Configuration file loading - Connection attempts - Certificate validation - File synchronization</p>"},{"location":"agent-guide/configuration/#common-issues","title":"Common Issues","text":"<ol> <li>Permission Denied: Ensure agent user can read config file</li> <li>Invalid YAML: Use a YAML validator</li> <li>Missing Required Fields: Check server URL and data directory</li> <li>Environment Variable Conflicts: Check for conflicting env vars</li> </ol>"},{"location":"agent-guide/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use Configuration Management: Store configs in Git/Ansible</li> <li>Secure API Keys: Use appropriate file permissions (600)</li> <li>Monitor Logs: Set up log rotation and monitoring</li> <li>Test Changes: Validate config before restarting agent</li> <li>Document Custom Settings: Keep notes on non-default values</li> </ol>"},{"location":"agent-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Set up agent scheduling</li> <li>Configure file synchronization</li> <li>Monitor agent performance</li> </ul>"},{"location":"agent-guide/debugging/","title":"Agent Debugging Guide","text":"<p>This guide covers debugging the KrakenHashes agent, including enabling debug mode, interpreting logs, and using development tools to diagnose issues.</p>"},{"location":"agent-guide/debugging/#quick-start-enable-debug-mode","title":"Quick Start - Enable Debug Mode","text":"<p>The fastest way to enable debug logging:</p> <pre><code># Method 1: Command line flag\n./krakenhashes-agent --debug --host backend.example.com:31337\n\n# Method 2: Environment variable\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n./krakenhashes-agent --host backend.example.com:31337\n\n# Method 3: Edit .env file\necho \"DEBUG=true\" &gt;&gt; .env\necho \"LOG_LEVEL=DEBUG\" &gt;&gt; .env\n./krakenhashes-agent --host backend.example.com:31337\n</code></pre>"},{"location":"agent-guide/debugging/#debug-configuration-options","title":"Debug Configuration Options","text":""},{"location":"agent-guide/debugging/#command-line-flags","title":"Command Line Flags","text":"<p>The agent supports several debugging-related command line flags:</p> <pre><code>./krakenhashes-agent --help\n\n  --debug               Enable debug logging (default: false)\n  --host string         Backend server host (e.g., localhost:31337)\n  --tls                 Use TLS for secure communication (default: true)\n  --interface string    Network interface to listen on (optional)\n  --heartbeat int       Heartbeat interval in seconds (default: 5)\n  --config-dir string   Configuration directory for certificates and credentials\n  --data-dir string     Data directory for binaries, wordlists, rules, and hashlists\n  --hashcat-params string  Extra parameters to pass to hashcat (e.g., '-O -w 3')\n</code></pre>"},{"location":"agent-guide/debugging/#environment-variables","title":"Environment Variables","text":"<p>Configure debugging through environment variables in <code>.env</code> file:</p> <pre><code># Logging Configuration\nDEBUG=true                    # Enable debug logging\nLOG_LEVEL=DEBUG              # Set minimum log level (DEBUG, INFO, WARNING, ERROR)\n\n# Server Configuration\nKH_HOST=backend.example.com  # Backend hostname\nKH_PORT=31337               # Backend port\nUSE_TLS=true                # Use secure connections\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s           # WebSocket write timeout\nKH_PONG_WAIT=60s            # Server pong timeout\nKH_PING_PERIOD=54s          # Ping interval\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Concurrent download limit\nKH_DOWNLOAD_TIMEOUT=1h         # Download timeout\nKH_MAX_DOWNLOAD_RETRIES=3      # Download retry attempts\n\n# Development Configuration\nHEARTBEAT_INTERVAL=5        # Heartbeat frequency (seconds)\n</code></pre>"},{"location":"agent-guide/debugging/#log-levels","title":"Log Levels","text":"<p>The agent supports four log levels in order of severity:</p> <ol> <li>DEBUG - Detailed diagnostic information</li> <li>INFO - General operational messages</li> <li>WARNING - Potential issues that don't stop operation</li> <li>ERROR - Serious problems that may cause failures</li> </ol> <p>Set the minimum level with <code>LOG_LEVEL</code> environment variable:</p> <pre><code># Show all messages\nLOG_LEVEL=DEBUG\n\n# Show info, warnings, and errors\nLOG_LEVEL=INFO\n\n# Show only warnings and errors\nLOG_LEVEL=WARNING\n\n# Show only errors\nLOG_LEVEL=ERROR\n</code></pre>"},{"location":"agent-guide/debugging/#debug-output-interpretation","title":"Debug Output Interpretation","text":""},{"location":"agent-guide/debugging/#log-message-format","title":"Log Message Format","text":"<p>Debug messages follow this format: <pre><code>[LEVEL] [TIMESTAMP] [FILE:LINE] [FUNCTION] MESSAGE\n</code></pre></p> <p>Example: <pre><code>[DEBUG] [2025-01-10 15:04:05.123] [/path/to/file.go:42] [package.Function] Connecting to backend server\n</code></pre></p>"},{"location":"agent-guide/debugging/#common-debug-messages","title":"Common Debug Messages","text":""},{"location":"agent-guide/debugging/#startup-and-configuration","title":"Startup and Configuration","text":"<pre><code>[INFO] Debug logging initialized - Debug enabled: true\n[INFO] Current working directory: /path/to/agent\n[INFO] Loading agent configuration...\n[INFO] Using config directory: /path/to/config\n[INFO] Using data directory: /path/to/data\n</code></pre>"},{"location":"agent-guide/debugging/#websocket-connection","title":"WebSocket Connection","text":"<pre><code>[DEBUG] Starting WebSocket connection process\n[INFO] Connection attempt 1 of 3\n[DEBUG] WebSocket connected to wss://backend.example.com:31337/ws/agent\n[INFO] Connection attempt 1 successful\n</code></pre>"},{"location":"agent-guide/debugging/#hardware-detection","title":"Hardware Detection","text":"<pre><code>[INFO] Detecting compute devices at startup...\n[DEBUG] Found GPU: NVIDIA RTX 4090 (Device ID: 0)\n[DEBUG] Found GPU: NVIDIA RTX 4080 (Device ID: 1)\n[INFO] Successfully detected and sent device information to server\n</code></pre>"},{"location":"agent-guide/debugging/#file-synchronization","title":"File Synchronization","text":"<pre><code>[INFO] Initializing file sync with max downloads: 3, timeout: 1h0m0s, max retries: 3\n[DEBUG] Scanning wordlists directory: /path/to/data/wordlists\n[INFO] File sync: Found 15 local files, backend has 23 files\n[DEBUG] Downloading missing file: rockyou.txt (14344391 bytes)\n</code></pre>"},{"location":"agent-guide/debugging/#job-execution","title":"Job Execution","text":"<pre><code>[INFO] Received job assignment for task: task_abc123\n[DEBUG] Starting hashcat with command: hashcat -m 1000 -a 0 hashes.txt wordlist.txt\n[DEBUG] Hashcat process started with PID: 12345\n[INFO] Job progress: Task task_abc123, Keyspace 1000000, Hash rate 2500000 H/s\n</code></pre>"},{"location":"agent-guide/debugging/#component-specific-debugging","title":"Component-Specific Debugging","text":""},{"location":"agent-guide/debugging/#websocket-connection-debugging","title":"WebSocket Connection Debugging","text":"<p>Enable verbose WebSocket debugging:</p> <pre><code># Add to .env file\nDEBUG=true\nLOG_LEVEL=DEBUG\n\n# Monitor WebSocket messages\ntail -f agent.log | grep -E \"(WebSocket|WSMessage|connection)\"\n</code></pre> <p>Common WebSocket issues and debugging:</p> <pre><code># Connection timeout\n[ERROR] Failed to create connection on attempt 1: dial tcp: i/o timeout\n\n# Certificate issues\n[ERROR] Failed to load CA certificate: certificate signed by unknown authority\n\n# Authentication failures\n[ERROR] WebSocket handshake failed: HTTP 401 Unauthorized\n</code></pre>"},{"location":"agent-guide/debugging/#file-synchronization-debugging","title":"File Synchronization Debugging","text":"<p>Monitor file sync operations:</p> <pre><code># Filter sync-related logs\ntail -f agent.log | grep -E \"(sync|download|FileInfo)\"\n\n# Debug specific file types\ntail -f agent.log | grep -E \"(wordlist|rule|binary)\"\n</code></pre> <p>File sync debug messages: <pre><code>[DEBUG] Calculating MD5 hash for file: /path/to/wordlist.txt\n[INFO] File sync: Downloading wordlist: rockyou.txt (14MB)\n[WARNING] Download retry 2/3 for file: large_wordlist.txt\n[ERROR] Failed to download file after 3 attempts: connection timeout\n</code></pre></p>"},{"location":"agent-guide/debugging/#job-execution-debugging","title":"Job Execution Debugging","text":"<p>Monitor hashcat job execution:</p> <pre><code># Job-specific logs\ntail -f agent.log | grep -E \"(job|task|hashcat|progress)\"\n\n# Real-time hashcat output\ntail -f agent.log | grep \"hashcat_output\"\n</code></pre> <p>Job debug messages: <pre><code>[INFO] Starting hashcat executor with extra params: -O -w 3\n[DEBUG] Hashcat working directory: /tmp/krakenhashes/task_abc123\n[DEBUG] Hashcat stdout: Session..........: hashcat\n[DEBUG] Hashcat stdout: Status...........: Running\n[INFO] Job completed successfully, found 15 cracked hashes\n</code></pre></p>"},{"location":"agent-guide/debugging/#hardware-detection-debugging","title":"Hardware Detection Debugging","text":"<p>Monitor GPU and hardware detection:</p> <pre><code># Hardware detection logs\ntail -f agent.log | grep -E \"(hardware|GPU|device|monitor)\"\n\n# Device capabilities\ntail -f agent.log | grep -E \"(OpenCL|CUDA|compute)\"\n</code></pre> <p>Hardware debug messages: <pre><code>[DEBUG] Detecting NVIDIA GPUs using nvidia-ml-py\n[INFO] Found NVIDIA GPU: GeForce RTX 4090 (12GB VRAM)\n[DEBUG] GPU compute capability: 8.9\n[WARNING] GPU temperature high: 85\u00b0C\n</code></pre></p>"},{"location":"agent-guide/debugging/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"agent-guide/debugging/#building-debug-builds","title":"Building Debug Builds","text":"<p>Create debug builds with additional debugging information:</p> <pre><code># Build with debug symbols (no optimization)\ncd agent\nmake clean\n\n# Set debug build flags\nexport GOFLAGS=\"-gcflags=-N -gcflags=-l\"\nmake build\n\n# Or build with race detection\ngo build -race -o debug-agent ./cmd/agent\n</code></pre>"},{"location":"agent-guide/debugging/#using-go-debugger-delve","title":"Using Go Debugger (Delve)","text":"<p>Install and use Delve for interactive debugging:</p> <pre><code># Install delve\ngo install github.com/go-delve/delve/cmd/dlv@latest\n\n# Build and debug\ncd agent\ngo build -gcflags=\"all=-N -l\" -o debug-agent ./cmd/agent\n\n# Start debugging session\ndlv exec ./debug-agent -- --debug --host localhost:31337\n\n# Common delve commands:\n# (dlv) break main.main\n# (dlv) continue\n# (dlv) print cfg\n# (dlv) step\n# (dlv) next\n</code></pre>"},{"location":"agent-guide/debugging/#using-pprof-for-performance-profiling","title":"Using pprof for Performance Profiling","text":"<p>Add profiling endpoints for performance analysis:</p> <pre><code>// Add to main.go for profiling (development only)\nimport _ \"net/http/pprof\"\n\n// Start profiling server (development builds only)\ngo func() {\n    log.Println(http.ListenAndServe(\"localhost:6060\", nil))\n}()\n</code></pre> <p>Then profile the running agent:</p> <pre><code># CPU profiling\ngo tool pprof http://localhost:6060/debug/pprof/profile\n\n# Memory profiling\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# Goroutine profiling\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n</code></pre>"},{"location":"agent-guide/debugging/#memory-and-goroutine-analysis","title":"Memory and Goroutine Analysis","text":"<p>Monitor resource usage during development:</p> <pre><code># Monitor memory usage\nwhile true; do\n    ps -p $(pgrep krakenhashes-agent) -o pid,rss,vsz,pcpu,pmem,cmd\n    sleep 5\ndone\n\n# Monitor goroutines (with pprof endpoint)\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n</code></pre>"},{"location":"agent-guide/debugging/#common-debugging-scenarios","title":"Common Debugging Scenarios","text":""},{"location":"agent-guide/debugging/#1-agent-wont-connect-to-backend","title":"1. Agent Won't Connect to Backend","text":"<p>Symptoms: - Connection timeout errors - Authentication failures - Certificate errors</p> <p>Debugging steps: <pre><code># Enable debug logging\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n\n# Test network connectivity\ntelnet backend.example.com 31337\ncurl -k https://backend.example.com:31337/health\n\n# Check certificate issues\nopenssl s_client -connect backend.example.com:31337\n\n# Verify API key and agent ID\ncat config/agent_credentials.json\ncat config/api_key.json\n</code></pre></p>"},{"location":"agent-guide/debugging/#2-file-sync-issues","title":"2. File Sync Issues","text":"<p>Symptoms: - Files not downloading - Constant re-downloading - MD5 hash mismatches</p> <p>Debugging steps: <pre><code># Check file permissions\nls -la data/wordlists/\nls -la data/rules/\n\n# Verify network connectivity for downloads\ncurl -I https://backend.example.com:31337/api/files/download/wordlist/1\n\n# Check available disk space\ndf -h data/\n\n# Manual MD5 verification\nmd5sum data/wordlists/rockyou.txt\n</code></pre></p>"},{"location":"agent-guide/debugging/#3-job-execution-problems","title":"3. Job Execution Problems","text":"<p>Symptoms: - Jobs not starting - Hashcat errors - No progress updates</p> <p>Debugging steps: <pre><code># Check hashcat installation\nwhich hashcat\nhashcat --version\n\n# Test hashcat manually\nhashcat -m 1000 -a 3 --stdout ?d?d?d?d | head -10\n\n# Check GPU availability\nhashcat -I\n\n# Monitor system resources\ntop -p $(pgrep hashcat)\nnvidia-smi  # For NVIDIA GPUs\n</code></pre></p>"},{"location":"agent-guide/debugging/#4-high-memory-usage","title":"4. High Memory Usage","text":"<p>Symptoms: - Agent consuming excessive RAM - System becoming slow - Out of memory errors</p> <p>Debugging steps: <pre><code># Enable memory profiling\nexport DEBUG=true\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# Check for memory leaks\n# Monitor over time with:\nwatch \"ps -p $(pgrep krakenhashes-agent) -o pid,rss,vsz\"\n\n# Reduce concurrent operations\n# In .env file:\nKH_MAX_CONCURRENT_DOWNLOADS=1\nHEARTBEAT_INTERVAL=10\n</code></pre></p>"},{"location":"agent-guide/debugging/#debugging-tools-and-utilities","title":"Debugging Tools and Utilities","text":""},{"location":"agent-guide/debugging/#log-analysis-scripts","title":"Log Analysis Scripts","text":"<p>Create helper scripts for log analysis:</p> <pre><code>#!/bin/bash\n# debug-helper.sh\n\n# Show only error messages\nshow_errors() {\n    grep \"\\[ERROR\\]\" agent.log | tail -20\n}\n\n# Show WebSocket connection events\nshow_websocket() {\n    grep -E \"(WebSocket|connection|disconnect)\" agent.log | tail -20\n}\n\n# Show file sync activity\nshow_sync() {\n    grep -E \"(sync|download|upload)\" agent.log | tail -20\n}\n\n# Show job execution\nshow_jobs() {\n    grep -E \"(job|task|hashcat)\" agent.log | tail -20\n}\n\n# Usage: ./debug-helper.sh show_errors\n$1\n</code></pre>"},{"location":"agent-guide/debugging/#real-time-monitoring","title":"Real-time Monitoring","text":"<p>Monitor agent activity in real-time:</p> <pre><code># Multi-pane monitoring with tmux\ntmux new-session -d -s agent-debug\n\n# Pane 1: Agent output\ntmux send-keys -t agent-debug \"tail -f agent.log\" Enter\n\n# Pane 2: System resources\ntmux split-window -v -t agent-debug\ntmux send-keys -t agent-debug \"htop\" Enter\n\n# Pane 3: Network connections\ntmux split-window -h -t agent-debug\ntmux send-keys -t agent-debug \"watch 'netstat -an | grep :31337'\" Enter\n\n# Attach to session\ntmux attach-session -t agent-debug\n</code></pre>"},{"location":"agent-guide/debugging/#configuration-validation","title":"Configuration Validation","text":"<p>Validate agent configuration:</p> <pre><code>#!/bin/bash\n# validate-config.sh\n\necho \"=== Agent Configuration Validation ===\"\n\n# Check required directories\necho \"Checking directories...\"\n[ -d \"config\" ] &amp;&amp; echo \"\u2705 config/\" || echo \"\u274c config/ missing\"\n[ -d \"data\" ] &amp;&amp; echo \"\u2705 data/\" || echo \"\u274c data/ missing\"\n\n# Check .env file\necho \"Checking .env configuration...\"\nif [ -f \".env\" ]; then\n    echo \"\u2705 .env file exists\"\n    grep -q \"KH_HOST=\" .env &amp;&amp; echo \"\u2705 KH_HOST set\" || echo \"\u274c KH_HOST missing\"\n    grep -q \"DEBUG=\" .env &amp;&amp; echo \"\u2705 DEBUG set\" || echo \"\u274c DEBUG missing\"\nelse\n    echo \"\u274c .env file missing\"\nfi\n\n# Check certificates\necho \"Checking certificates...\"\n[ -f \"config/agent.crt\" ] &amp;&amp; echo \"\u2705 Agent certificate\" || echo \"\u274c Agent certificate missing\"\n[ -f \"config/ca.crt\" ] &amp;&amp; echo \"\u2705 CA certificate\" || echo \"\u274c CA certificate missing\"\n\n# Check API key\n[ -f \"config/api_key.json\" ] &amp;&amp; echo \"\u2705 API key\" || echo \"\u274c API key missing\"\n\necho \"=== Validation Complete ===\"\n</code></pre>"},{"location":"agent-guide/debugging/#automated-testing-and-debugging","title":"Automated Testing and Debugging","text":""},{"location":"agent-guide/debugging/#unit-tests-with-debug-output","title":"Unit Tests with Debug Output","text":"<p>Run tests with verbose output:</p> <pre><code>cd agent\n\n# Run all tests with verbose output\ngo test -v ./...\n\n# Run specific package tests\ngo test -v ./internal/config\ngo test -v ./internal/agent\ngo test -v ./pkg/debug\n\n# Run tests with race detection\ngo test -race -v ./...\n\n# Generate test coverage\ngo test -coverprofile=coverage.out ./...\ngo tool cover -html=coverage.out -o coverage.html\n</code></pre>"},{"location":"agent-guide/debugging/#integration-testing","title":"Integration Testing","text":"<p>Test agent integration with a local backend:</p> <pre><code># Start local backend for testing\ncd ../backend\ndocker-compose -f docker-compose.dev-local.yml up -d\n\n# Test agent connection\ncd ../agent\n./krakenhashes-agent --debug --host localhost:31337\n</code></pre>"},{"location":"agent-guide/debugging/#performance-profiling","title":"Performance Profiling","text":""},{"location":"agent-guide/debugging/#cpu-profiling","title":"CPU Profiling","text":"<p>Profile CPU usage during job execution:</p> <pre><code># Start agent with profiling\ngo tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile?seconds=30\n\n# During heavy computation (hashcat jobs)\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=60\n</code></pre>"},{"location":"agent-guide/debugging/#memory-profiling","title":"Memory Profiling","text":"<p>Identify memory usage patterns:</p> <pre><code># Heap profiling\ngo tool pprof -http=:8080 http://localhost:6060/debug/pprof/heap\n\n# Allocation profiling\ngo tool pprof http://localhost:6060/debug/pprof/allocs\n</code></pre>"},{"location":"agent-guide/debugging/#goroutine-analysis","title":"Goroutine Analysis","text":"<p>Monitor concurrent operations:</p> <pre><code># Goroutine dump\ncurl http://localhost:6060/debug/pprof/goroutine?debug=1\n\n# Interactive analysis\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n</code></pre>"},{"location":"agent-guide/debugging/#logging-best-practices","title":"Logging Best Practices","text":""},{"location":"agent-guide/debugging/#custom-debug-messages","title":"Custom Debug Messages","text":"<p>Add debug messages to your code:</p> <pre><code>import \"github.com/ZerkerEOD/krakenhashes/agent/pkg/debug\"\n\n// Different log levels\ndebug.Debug(\"Detailed diagnostic: variable=%v\", someVar)\ndebug.Info(\"Operation started: %s\", operation)\ndebug.Warning(\"Potential issue detected: %s\", issue)\ndebug.Error(\"Critical error: %v\", err)\n</code></pre>"},{"location":"agent-guide/debugging/#structured-logging","title":"Structured Logging","text":"<p>Organize debug output by component:</p> <pre><code>// Component-specific logging\ndebug.Info(\"[CONNECTION] WebSocket connected to %s\", url)\ndebug.Info(\"[SYNC] Downloaded file: %s (%d bytes)\", filename, size)\ndebug.Info(\"[JOB] Task started: %s\", taskID)\ndebug.Info(\"[HARDWARE] GPU detected: %s\", gpuName)\n</code></pre>"},{"location":"agent-guide/debugging/#contributing-and-bug-reports","title":"Contributing and Bug Reports","text":""},{"location":"agent-guide/debugging/#preparing-debug-information","title":"Preparing Debug Information","text":"<p>When reporting bugs, include:</p> <ol> <li> <p>Agent version and build info: <pre><code>./krakenhashes-agent --version\n</code></pre></p> </li> <li> <p>Complete configuration: <pre><code># Sanitized .env file (remove sensitive data)\ncat .env | sed 's/\\(API_KEY\\|PASSWORD\\)=.*/\\1=***REDACTED***/'\n</code></pre></p> </li> <li> <p>Debug logs: <pre><code># Last 100 lines with debug enabled\nDEBUG=true LOG_LEVEL=DEBUG ./krakenhashes-agent --host backend.example.com &gt; debug.log 2&gt;&amp;1\ntail -100 debug.log\n</code></pre></p> </li> <li> <p>System information: <pre><code>uname -a\nlscpu | grep -E \"(Architecture|CPU|Thread)\"\nnvidia-smi  # If using NVIDIA GPUs\n</code></pre></p> </li> <li> <p>Network connectivity: <pre><code>curl -I https://backend.example.com:31337/health\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/debugging/#debug-build-for-development","title":"Debug Build for Development","text":"<p>Create debug builds for development:</p> <pre><code># Clean build with debug symbols\ncd agent\nmake clean\n\n# Build with debug flags\ngo build -gcflags=\"all=-N -l\" -ldflags=\"-X main.BuildMode=debug\" -o debug-agent ./cmd/agent\n\n# Run with additional debugging\n./debug-agent --debug --host backend.example.com:31337\n</code></pre> <p>This debug build includes: - No compiler optimizations - Full symbol information - Additional runtime checks - Enhanced logging</p>"},{"location":"agent-guide/debugging/#troubleshooting-quick-reference","title":"Troubleshooting Quick Reference","text":"Issue Debug Steps Key Files Won't start Check <code>.env</code>, verify paths <code>.env</code>, <code>config/</code> Connection fails Test network, check certs <code>config/ca.crt</code>, <code>config/agent.crt</code> Auth errors Verify API key and agent ID <code>config/api_key.json</code>, <code>config/agent_credentials.json</code> Files not syncing Check permissions, disk space <code>data/wordlists/</code>, <code>data/rules/</code> Jobs not running Test hashcat, check GPU System hashcat, <code>nvidia-smi</code> High memory Enable profiling, reduce concurrency <code>.env</code> (set lower limits) Slow performance CPU/memory profiling pprof endpoints"},{"location":"agent-guide/debugging/#advanced-debugging-techniques","title":"Advanced Debugging Techniques","text":""},{"location":"agent-guide/debugging/#custom-debug-builds","title":"Custom Debug Builds","text":"<p>Build with custom debug flags:</p> <pre><code># Build with additional debugging\ngo build -tags debug -gcflags=\"all=-N -l\" ./cmd/agent\n\n# Build with memory debugging\ngo build -gcflags=\"all=-m\" ./cmd/agent\n\n# Build with race detection (development only)\ngo build -race ./cmd/agent\n</code></pre>"},{"location":"agent-guide/debugging/#remote-debugging","title":"Remote Debugging","text":"<p>Debug agent running on remote systems:</p> <pre><code># On remote system\ndlv exec ./krakenhashes-agent --listen=:2345 --headless=true --api-version=2 -- --debug\n\n# From local system\ndlv connect remote-host:2345\n</code></pre>"},{"location":"agent-guide/debugging/#container-debugging","title":"Container Debugging","text":"<p>Debug agent running in containers:</p> <pre><code># Build debug container\ndocker build -f Dockerfile.debug -t agent-debug .\n\n# Run with debug enabled\ndocker run -e DEBUG=true -e LOG_LEVEL=DEBUG agent-debug\n\n# Attach debugger to container\ndocker exec -it &lt;container_id&gt; dlv attach &lt;pid&gt;\n</code></pre> <p>This comprehensive debugging guide should help developers and advanced users effectively debug agent issues, profile performance, and contribute to the project development.</p>"},{"location":"agent-guide/device-management/","title":"Device Management","text":"<p>The KrakenHashes agent provides comprehensive device detection, management, and optimization capabilities for password cracking workloads. This guide covers everything you need to know about configuring and optimizing hardware resources.</p>"},{"location":"agent-guide/device-management/#overview","title":"Overview","text":"<p>The agent uses hashcat's built-in device detection capabilities to identify and manage compute devices. This approach ensures compatibility with hashcat's device handling and provides accurate performance characteristics for each device.</p>"},{"location":"agent-guide/device-management/#key-features","title":"Key Features","text":"<ul> <li>Automatic device detection using hashcat's <code>-I</code> flag</li> <li>Multi-GPU support with intelligent device allocation</li> <li>Cross-platform compatibility (Windows, Linux, macOS)</li> <li>Backend optimization with priority-based device selection</li> <li>Real-time monitoring during job execution</li> <li>Alias filtering to prevent duplicate device entries</li> </ul>"},{"location":"agent-guide/device-management/#supported-hardware","title":"Supported Hardware","text":""},{"location":"agent-guide/device-management/#nvidia-gpus","title":"NVIDIA GPUs","text":"<p>Supported backends: - CUDA (Primary) - OpenCL (Fallback)</p> <p>Requirements: - NVIDIA driver 450.80.02 or newer - CUDA toolkit 11.0 or newer (for CUDA backend) - OpenCL 1.2 or newer (for OpenCL backend)</p> <p>Installation: <pre><code># Ubuntu/Debian\nsudo apt-get install nvidia-driver nvidia-cuda-toolkit\n\n# Verify installation\nnvidia-smi\nnvidia-settings --version\n</code></pre></p> <p>Optimal configuration: - Use CUDA backend when available (higher performance) - Enable GPU boost for maximum clock speeds - Ensure adequate power supply (750W+ for high-end cards) - Monitor temperatures (keep below 83\u00b0C for optimal performance)</p>"},{"location":"agent-guide/device-management/#amd-gpus","title":"AMD GPUs","text":"<p>Supported backends: - HIP (Primary for modern cards) - OpenCL (Universal)</p> <p>Requirements: - AMD Radeon Software 22.7.1 or newer - ROCm 5.2 or newer (for HIP backend) - OpenCL 2.0 or newer</p> <p>Installation: <pre><code># Ubuntu/Debian - ROCm installation\nwget https://repo.radeon.com/amdgpu-install/5.4/ubuntu/jammy/amdgpu-install_5.4.50400-1_all.deb\nsudo dpkg -i amdgpu-install_5.4.50400-1_all.deb\nsudo amdgpu-install --usecase=rocm\n\n# Verify installation\nrocm-smi\nclinfo | grep AMD\n</code></pre></p> <p>Optimal configuration: - Use HIP backend for RX 6000/7000 series and newer - Use OpenCL for older cards (RX 500/Vega series) - Enable GPU memory overclocking for hash-heavy algorithms - Monitor junction temperatures (keep below 110\u00b0C)</p>"},{"location":"agent-guide/device-management/#intel-gpus","title":"Intel GPUs","text":"<p>Supported backends: - OpenCL - Level Zero (Arc series)</p> <p>Requirements: - Intel Graphics Driver 30.0.101.1404 or newer - Intel GPU tools for monitoring - OpenCL runtime</p> <p>Installation: <pre><code># Ubuntu/Debian\nsudo apt-get install intel-gpu-tools intel-opencl-icd\n\n# Verify installation\nintel_gpu_top\nclinfo | grep Intel\n</code></pre></p> <p>Notes: - Intel Arc GPUs provide competitive hash rates for certain algorithms - Integrated graphics can be used for light workloads - Limited hashcat optimization compared to NVIDIA/AMD</p>"},{"location":"agent-guide/device-management/#cpu-processing","title":"CPU Processing","text":"<p>Supported: - All x86-64 processors - ARM processors (limited algorithm support)</p> <p>Requirements: - Modern multi-core processor - Sufficient system memory (8GB+ recommended)</p> <p>Optimal configuration: - Enable all CPU cores for maximum throughput - Ensure adequate cooling for sustained workloads - Consider CPU-only for specific algorithms (bcrypt, scrypt)</p>"},{"location":"agent-guide/device-management/#device-detection","title":"Device Detection","text":""},{"location":"agent-guide/device-management/#automatic-detection","title":"Automatic Detection","text":"<p>The agent automatically detects devices on startup using hashcat's device enumeration:</p> <pre><code>./agent -debug  # Enable debug output to see detection process\n</code></pre> <p>Detection process: 1. Locates latest hashcat binary in data directory 2. Executes <code>hashcat -I</code> command 3. Parses device information and capabilities 4. Filters aliases and invalid devices 5. Stores device configuration for job allocation</p>"},{"location":"agent-guide/device-management/#device-properties","title":"Device Properties","text":"<p>Each detected device includes:</p> <pre><code>{\n  \"device_id\": 1,\n  \"device_name\": \"NVIDIA GeForce RTX 4090\",\n  \"device_type\": \"GPU\",\n  \"enabled\": true,\n  \"processors\": 128,\n  \"clock\": 2520,\n  \"memory_total\": 24576,\n  \"memory_free\": 23552,\n  \"pci_address\": \"01:00.0\",\n  \"backend\": \"CUDA\",\n  \"is_alias\": false\n}\n</code></pre>"},{"location":"agent-guide/device-management/#backend-priority","title":"Backend Priority","text":"<p>When multiple backends are available for the same device, the agent uses this priority order:</p> <ol> <li>HIP (AMD optimized)</li> <li>CUDA (NVIDIA optimized)</li> <li>OpenCL (Universal fallback)</li> </ol> <p>This ensures optimal performance by selecting the most efficient backend for each device.</p>"},{"location":"agent-guide/device-management/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":""},{"location":"agent-guide/device-management/#automatic-load-balancing","title":"Automatic Load Balancing","text":"<p>The agent automatically distributes workload across available GPUs:</p> <ul> <li>Equal distribution for identical GPU models</li> <li>Proportional allocation based on compute capability</li> <li>Dynamic adjustment based on real-time performance</li> </ul>"},{"location":"agent-guide/device-management/#manual-device-selection","title":"Manual Device Selection","text":"<p>You can manually enable/disable specific devices:</p> <pre><code># Through the web interface:\n# Agent Details \u2192 Device Management \u2192 Toggle device status\n\n# Or via API:\ncurl -X PUT http://localhost:8080/api/agents/{agent_id}/devices/{device_id} \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"enabled\": false}'\n</code></pre>"},{"location":"agent-guide/device-management/#optimal-multi-gpu-setups","title":"Optimal Multi-GPU Setups","text":"<p>Recommended configurations:</p> <ol> <li> <p>Identical GPUs: Best performance and load balancing    <pre><code>4x RTX 4090 \u2192 ~400 GH/s MD5\n8x RTX 3080 \u2192 ~640 GH/s MD5\n</code></pre></p> </li> <li> <p>Mixed GPUs: Group similar performance tiers    <pre><code>2x RTX 4090 + 2x RTX 4080 \u2192 Separate job allocation\n</code></pre></p> </li> <li> <p>CPU + GPU hybrid: Use CPU for specific algorithms    <pre><code>GPU: Fast hashes (MD5, SHA1, NTLM)\nCPU: Slow hashes (bcrypt, scrypt, Argon2)\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/device-management/#performance-optimization","title":"Performance Optimization","text":""},{"location":"agent-guide/device-management/#gpu-optimization","title":"GPU Optimization","text":"<p>Memory optimization: <pre><code># Enable optimized kernels (uses more VRAM but faster)\n-O\n\n# Set workload tuning\n-w 1  # Low power usage\n-w 2  # Default\n-w 3  # High performance (recommended)\n-w 4  # Insane performance (may cause instability)\n</code></pre></p> <p>Hashcat performance flags: <pre><code># Example optimization for RTX 4090\n--gpu-loops 1024 --gpu-accel 128 --gpu-threads 1024\n</code></pre></p>"},{"location":"agent-guide/device-management/#thermal-management","title":"Thermal Management","text":"<p>Temperature monitoring: - NVIDIA: Use <code>nvidia-smi</code> or built-in monitoring - AMD: Use <code>rocm-smi</code> or <code>amdgpu-pro</code> - Intel: Use <code>intel_gpu_top</code></p> <p>Thermal limits: - NVIDIA: 83\u00b0C (optimal), 91\u00b0C (maximum) - AMD: 110\u00b0C (junction), 95\u00b0C (edge) - Intel: 100\u00b0C (throttling)</p> <p>Cooling recommendations: - Ensure adequate case ventilation - Monitor ambient temperature (keep below 25\u00b0C) - Consider undervolting for 24/7 operations - Use custom fan curves for sustained workloads</p>"},{"location":"agent-guide/device-management/#power-management","title":"Power Management","text":"<p>Power considerations: <pre><code># Check GPU power limits\nnvidia-smi -q -d POWER     # NVIDIA\nrocm-smi --showpower       # AMD\n</code></pre></p> <p>Power optimization: - Ensure adequate PSU capacity (add 20% headroom) - Enable power limit increases where possible - Monitor power consumption during long jobs - Consider efficiency curves for different algorithms</p>"},{"location":"agent-guide/device-management/#device-allocation-strategies","title":"Device Allocation Strategies","text":""},{"location":"agent-guide/device-management/#job-based-allocation","title":"Job-Based Allocation","text":"<p>Strategy selection: 1. Round-robin: Distribute tasks evenly across devices 2. Performance-based: Allocate based on device capability 3. Memory-based: Consider VRAM requirements 4. Thermal-aware: Avoid overheated devices</p>"},{"location":"agent-guide/device-management/#workload-distribution","title":"Workload Distribution","text":"<p>Hash type considerations: - Fast hashes (MD5, SHA1): Use all available GPUs - Medium hashes (SHA256, SHA512): Balance GPU count vs. memory - Slow hashes (bcrypt, scrypt): May benefit from CPU processing - Memory-hard (Argon2): Requires high VRAM devices</p> <p>Example allocations: <pre><code># Large wordlist + fast hash = all devices\nhashcat -m 0 -a 0 hashes.txt wordlist.txt -d 1,2,3,4\n\n# Complex rules + medium hash = subset of devices\nhashcat -m 1000 -a 0 hashes.txt wordlist.txt -r rules.txt -d 1,2\n\n# Brute force + slow hash = single high-end device\nhashcat -m 3200 -a 3 hashes.txt ?a?a?a?a?a?a -d 1\n</code></pre></p>"},{"location":"agent-guide/device-management/#benchmarking-and-capabilities","title":"Benchmarking and Capabilities","text":""},{"location":"agent-guide/device-management/#hashcat-benchmarking","title":"Hashcat Benchmarking","text":"<p>Run benchmarks: <pre><code># Full benchmark suite\nhashcat -b\n\n# Specific algorithm benchmark\nhashcat -b -m 1000  # NTLM benchmark\n\n# Device-specific benchmark\nhashcat -b -d 1     # Benchmark only device 1\n</code></pre></p> <p>Interpreting results: <pre><code>Speed.#1.........:   123.4 GH/s (95.2ms) @ Accel:512 Loops:1024 Thr:64 Vec:1\n</code></pre> - <code>123.4 GH/s</code>: Hash rate (Giga-hashes per second) - <code>95.2ms</code>: Kernel execution time - <code>Accel:512</code>: Acceleration factor - <code>Loops:1024</code>: Iteration loops - <code>Thr:64</code>: Thread count - <code>Vec:1</code>: Vector width</p>"},{"location":"agent-guide/device-management/#performance-baselines","title":"Performance Baselines","text":"<p>Expected performance (RTX 4090): - MD5: ~100 GH/s - SHA1: ~35 GH/s - NTLM: ~180 GH/s - SHA256: ~15 GH/s - bcrypt: ~150 KH/s</p> <p>Expected performance (RX 7900 XTX): - MD5: ~75 GH/s - SHA1: ~25 GH/s - NTLM: ~130 GH/s - SHA256: ~12 GH/s - bcrypt: ~120 KH/s</p>"},{"location":"agent-guide/device-management/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"agent-guide/device-management/#minimum-requirements","title":"Minimum Requirements","text":"<p>System specifications: - CPU: 4 cores, 2.5GHz - RAM: 8GB - Storage: 100GB available space - GPU: Any OpenCL 1.2 compatible device - Network: 100 Mbps for file synchronization</p>"},{"location":"agent-guide/device-management/#recommended-requirements","title":"Recommended Requirements","text":"<p>High-performance setup: - CPU: 16+ cores, 3.0GHz+ - RAM: 32GB+ (64GB for large hashlists) - Storage: 1TB+ NVMe SSD - GPU: RTX 4080/4090 or RX 7900 XT/XTX - Network: 1 Gbps for large file transfers - PSU: 1000W+ 80+ Gold rated</p>"},{"location":"agent-guide/device-management/#enterprise-requirements","title":"Enterprise Requirements","text":"<p>Large-scale deployment: - CPU: 32+ cores server processor - RAM: 128GB+ ECC memory - Storage: 10TB+ enterprise SSD array - GPU: Multiple high-end cards with NVLink/Infinity Cache - Network: 10 Gbps with redundancy - Power: Redundant 1600W+ PSUs - Cooling: Dedicated server room cooling</p>"},{"location":"agent-guide/device-management/#driver-requirements","title":"Driver Requirements","text":""},{"location":"agent-guide/device-management/#nvidia-drivers","title":"NVIDIA Drivers","text":"<p>Recommended versions: - Production: Latest stable driver (535.x+) - Development: Latest beta driver for new features - Enterprise: Long-term support versions (470.x LTS)</p> <p>Installation verification: <pre><code>nvidia-smi\nnvcc --version  # CUDA compiler\nnvidia-settings --version\n</code></pre></p>"},{"location":"agent-guide/device-management/#amd-drivers","title":"AMD Drivers","text":"<p>Recommended versions: - ROCm: 5.4+ for HIP support - AMDGPU-PRO: 23.20+ for OpenCL - Mesa: 23.0+ for open-source stack</p> <p>Installation verification: <pre><code>rocm-smi\nrocminfo | grep \"Agent\"\nclinfo | grep AMD\n</code></pre></p>"},{"location":"agent-guide/device-management/#intel-drivers","title":"Intel Drivers","text":"<p>Recommended versions: - Graphics Driver: 30.0.101.1404+ - OpenCL Runtime: 22.43+ - Level Zero: 1.8+ (Arc series)</p> <p>Installation verification: <pre><code>intel_gpu_top\nclinfo | grep Intel\nvainfo | grep \"Driver version\"\n</code></pre></p>"},{"location":"agent-guide/device-management/#troubleshooting-device-issues","title":"Troubleshooting Device Issues","text":""},{"location":"agent-guide/device-management/#common-issues","title":"Common Issues","text":"<p>Device not detected: 1. Verify driver installation 2. Check hardware compatibility 3. Ensure proper PCI Express connection 4. Verify power supply adequacy 5. Test with hashcat directly: <code>hashcat -I</code></p> <p>Poor performance: 1. Check thermal throttling 2. Verify power limits 3. Update drivers 4. Check for conflicting processes 5. Validate hashcat parameters</p> <p>System instability: 1. Reduce workload tuning (<code>-w 2</code> instead of <code>-w 3</code>) 2. Lower GPU clocks and memory speeds 3. Improve cooling and power delivery 4. Check for hardware defects 5. Verify system memory integrity</p>"},{"location":"agent-guide/device-management/#debug-commands","title":"Debug Commands","text":"<p>Hardware diagnostics: <pre><code># System information\nlspci | grep -i vga\nlshw -c display\n\n# GPU status\nnvidia-smi -l 1        # NVIDIA monitoring\nrocm-smi -l            # AMD monitoring\nintel_gpu_top          # Intel monitoring\n\n# Temperature monitoring\nsensors                # System sensors\nnvidia-smi dmon        # NVIDIA detailed monitoring\n</code></pre></p> <p>Hashcat diagnostics: <pre><code># Device information\nhashcat -I\n\n# Test device functionality\nhashcat -t\n\n# Benchmark specific device\nhashcat -b -d 1\n\n# Debug mode\nhashcat --debug-mode=1 -m 1000 hash.txt wordlist.txt\n</code></pre></p>"},{"location":"agent-guide/device-management/#performance-troubleshooting","title":"Performance Troubleshooting","text":"<p>If hash rates are lower than expected:</p> <ol> <li> <p>Check thermal throttling: <pre><code>nvidia-smi dmon -s pucvmet -c 60  # Monitor for 60 seconds\n</code></pre></p> </li> <li> <p>Verify power limits: <pre><code>nvidia-smi -q -d POWER\n</code></pre></p> </li> <li> <p>Test with different parameters: <pre><code># Conservative settings\nhashcat -w 2 -O -m 1000 hash.txt wordlist.txt\n\n# Aggressive settings (may cause instability)\nhashcat -w 4 -O -m 1000 hash.txt wordlist.txt\n</code></pre></p> </li> <li> <p>Check for competing processes: <pre><code>ps aux | grep -E \"(hashcat|john|nvidia|rocm)\"\nnvidia-smi pmon  # Process monitoring\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/device-management/#best-practices","title":"Best Practices","text":""},{"location":"agent-guide/device-management/#hardware-selection","title":"Hardware Selection","text":"<ol> <li>Match workload to hardware:</li> <li>Fast hashes: High core count GPUs (RTX 4090, RX 7900 XTX)</li> <li>Slow hashes: High memory bandwidth (RTX 3090, RX 6900 XT)</li> <li> <p>Mixed workloads: Balanced systems with CPU + GPU</p> </li> <li> <p>Consider total cost of ownership:</p> </li> <li>Power consumption over lifetime</li> <li>Cooling requirements and costs</li> <li>Maintenance and replacement cycles</li> <li>Performance per dollar ratios</li> </ol>"},{"location":"agent-guide/device-management/#configuration-management","title":"Configuration Management","text":"<ol> <li>Document hardware configurations:</li> <li>GPU models, VRAM, clock speeds</li> <li>Driver versions and update schedules</li> <li>Optimal hashcat parameters for each device</li> <li> <p>Thermal and power limit settings</p> </li> <li> <p>Monitor performance trends:</p> </li> <li>Track hash rates over time</li> <li>Monitor for degradation or throttling</li> <li>Log hardware errors and failures</li> <li> <p>Schedule preventive maintenance</p> </li> <li> <p>Implement redundancy:</p> </li> <li>Deploy multiple agents for high availability</li> <li>Use mixed hardware to avoid single points of failure</li> <li>Maintain spare hardware for critical operations</li> <li>Implement proper backup and recovery procedures</li> </ol>"},{"location":"agent-guide/device-management/#security-considerations","title":"Security Considerations","text":"<ol> <li>Physical security:</li> <li>Secure hardware from unauthorized access</li> <li>Monitor for tampering or theft</li> <li>Implement proper access controls</li> <li> <p>Use hardware-based attestation where possible</p> </li> <li> <p>Driver security:</p> </li> <li>Keep drivers updated for security patches</li> <li>Verify driver signatures and authenticity</li> <li>Monitor for driver-level exploits</li> <li> <p>Use enterprise driver branches when available</p> </li> <li> <p>Performance isolation:</p> </li> <li>Isolate cracking workloads from other processes</li> <li>Use dedicated hardware for sensitive operations</li> <li>Monitor for unauthorized resource usage</li> <li>Implement resource quotas and limits</li> </ol> <p>By following this comprehensive guide, you'll be able to effectively configure, optimize, and manage hardware resources for maximum password cracking performance while maintaining system stability and security.</p>"},{"location":"agent-guide/file-sync/","title":"Agent File Synchronization","text":"<p>This document explains how KrakenHashes agents synchronize files with the backend server.</p>"},{"location":"agent-guide/file-sync/#overview","title":"Overview","text":"<p>KrakenHashes agents need access to the same wordlists and rules as the backend server to perform password cracking operations. The system implements a WebSocket-based file synchronization mechanism to ensure agents have the necessary files.</p>"},{"location":"agent-guide/file-sync/#synchronization-process","title":"Synchronization Process","text":"<p>The file synchronization process follows these steps:</p> <ol> <li>When an agent connects to the backend server via WebSocket, the server initiates a file synchronization request</li> <li>The agent scans its local directories and reports all files with their MD5 hashes</li> <li>The server compares the agent's files with its database and identifies missing or outdated files</li> <li>The server sends a synchronization command with a list of files the agent should download</li> <li>The agent downloads each file in parallel from the backend server</li> </ol>"},{"location":"agent-guide/file-sync/#file-types","title":"File Types","text":"<p>The system synchronizes the following types of files:</p> <ul> <li>Wordlists: Password dictionaries used for cracking</li> <li>Rules: Hashcat and John the Ripper rule files for password mutations</li> <li>Binaries: Tool binaries (future implementation)</li> </ul>"},{"location":"agent-guide/file-sync/#directory-structure","title":"Directory Structure","text":"<p>Agents store synchronized files in a data directory structure:</p> <pre><code>&lt;data_dir&gt;/\n\u251c\u2500\u2500 wordlists/\n\u2502   \u251c\u2500\u2500 general/\n\u2502   \u251c\u2500\u2500 specialized/\n\u2502   \u251c\u2500\u2500 targeted/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 rules/\n\u2502   \u251c\u2500\u2500 hashcat/\n\u2502   \u251c\u2500\u2500 john/\n\u2502   \u2514\u2500\u2500 custom/\n\u251c\u2500\u2500 binaries/\n\u2514\u2500\u2500 hashlists/\n</code></pre> <p>The base data directory location is determined by:</p> <ol> <li>The <code>KH_DATA_DIR</code> environment variable, if set</li> <li>Otherwise, a <code>data</code> directory relative to the agent executable</li> </ol>"},{"location":"agent-guide/file-sync/#websocket-messages","title":"WebSocket Messages","text":"<p>The file synchronization uses the following WebSocket message types:</p>"},{"location":"agent-guide/file-sync/#file-sync-request","title":"File Sync Request","text":"<p>Sent from server to agent to request a list of files:</p> <pre><code>{\n  \"type\": \"file_sync_request\",\n  \"payload\": {\n    \"file_types\": [\"wordlist\", \"rule\", \"binary\"]\n  },\n  \"timestamp\": \"2023-07-01T12:00:00Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-sync-response","title":"File Sync Response","text":"<p>Sent from agent to server with the list of files:</p> <pre><code>{\n  \"type\": \"file_sync_response\",\n  \"payload\": {\n    \"agent_id\": 123,\n    \"files\": [\n      {\n        \"name\": \"rockyou.txt\",\n        \"file_type\": \"wordlist\",\n        \"hash\": \"7bfc9d4df2b5ce4e29ca14d40f7aef1b\",\n        \"size\": 139921507\n      },\n      {\n        \"name\": \"best64.rule\",\n        \"file_type\": \"rule\",\n        \"hash\": \"1e5f4a7e3cc31bd12a0f7a42c6ebab29\",\n        \"size\": 1234\n      }\n    ]\n  },\n  \"timestamp\": \"2023-07-01T12:00:05Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-sync-command","title":"File Sync Command","text":"<p>Sent from server to agent with files to download:</p> <pre><code>{\n  \"type\": \"file_sync_command\",\n  \"payload\": {\n    \"files\": [\n      {\n        \"name\": \"darkweb2017.txt\",\n        \"file_type\": \"wordlist\",\n        \"hash\": \"8b1a9953c4611296a827abf8c47804d7\",\n        \"size\": 8553126\n      }\n    ]\n  },\n  \"timestamp\": \"2023-07-01T12:00:10Z\"\n}\n</code></pre>"},{"location":"agent-guide/file-sync/#file-download-process","title":"File Download Process","text":"<p>When an agent receives a file sync command:</p> <ol> <li>It processes each file in the command asynchronously</li> <li>For each file, it creates the appropriate directory structure if needed</li> <li>It downloads the file from the backend server's file API endpoint</li> <li>It verifies the downloaded file's MD5 hash matches the expected hash</li> <li>If verification fails, it retries the download (up to 3 times)</li> </ol>"},{"location":"agent-guide/file-sync/#synchronization-timing","title":"Synchronization Timing","text":"<p>File synchronization occurs at the following times:</p> <ol> <li>When an agent first connects to the backend server</li> <li>Periodically (every 6 hours by default)</li> <li>When the backend server explicitly requests synchronization (e.g., after new files are added)</li> </ol>"},{"location":"agent-guide/file-sync/#error-handling","title":"Error Handling","text":"<p>The system implements several error handling mechanisms:</p> <ul> <li>Download timeouts (1 hour per file)</li> <li>Retry logic for failed downloads (3 attempts with exponential backoff)</li> <li>Partial file cleanup if a download is interrupted</li> <li>Verification of file integrity via MD5 hash</li> </ul>"},{"location":"agent-guide/file-sync/#security-considerations","title":"Security Considerations","text":"<p>All file transfers occur over secure HTTPS connections with:</p> <ul> <li>TLS encryption for all communications</li> <li>Agent authentication required for file downloads</li> <li>File integrity verification via MD5 hash</li> </ul>"},{"location":"agent-guide/file-sync/#monitoring","title":"Monitoring","text":"<p>Administrators can monitor file synchronization through:</p> <ol> <li>Agent logs, which show detailed information about file downloads</li> <li>Backend server logs, which show synchronization requests and commands</li> <li>The admin dashboard, which displays synchronization status for each agent</li> </ol>"},{"location":"agent-guide/file-sync/#best-practices","title":"Best Practices","text":"<ol> <li>Ensure adequate storage: Agents need sufficient disk space for wordlists and rules</li> <li>Monitor bandwidth usage: Large file transfers may impact network performance</li> <li>Stagger agent registrations: To prevent overwhelming the server with simultaneous downloads</li> <li>Pre-populate common files: For faster agent setup, pre-copy large wordlists to agent machines </li> </ol>"},{"location":"agent-guide/installation/","title":"Agent Installation","text":""},{"location":"agent-guide/installation/#overview","title":"Overview","text":"<p>This guide covers installing and setting up KrakenHashes agents on various platforms.</p>"},{"location":"agent-guide/installation/#system-requirements","title":"System Requirements","text":""},{"location":"agent-guide/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>4GB RAM</li> <li>10GB free disk space (You need enough disk space to cover all wordlists)</li> <li>Linux (Ubuntu 20.04+, Debian 11+, RHEL 8+, or similar)</li> <li>Network connectivity to backend server</li> </ul>"},{"location":"agent-guide/installation/#gpu-requirements-optional-but-recommended","title":"GPU Requirements (Optional but Recommended)","text":"<ul> <li>NVIDIA: CUDA 11.0+ compatible GPU with 4GB+ VRAM</li> <li>AMD: ROCm compatible GPU or OpenCL support</li> <li>Intel: OpenCL compatible GPU</li> </ul>"},{"location":"agent-guide/installation/#installation","title":"Installation","text":"<p>The agent is distributed as a standalone binary that runs from your home directory. No root access is required for basic installation.</p>"},{"location":"agent-guide/installation/#step-1-create-agent-directory","title":"Step 1: Create Agent Directory","text":"<pre><code># Create a directory for the agent in your home folder\nmkdir ~/krakenhashes-agent\ncd ~/krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/installation/#step-2-download-the-agent-binary","title":"Step 2: Download the Agent Binary","text":"<p>Download the appropriate binary for your platform from the GitHub Releases page or directly from your KrakenHashes server:</p>"},{"location":"agent-guide/installation/#option-1-download-from-github-releases","title":"Option 1: Download from GitHub Releases","text":"<pre><code># For Linux AMD64 (most common)\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-linux-amd64 -O krakenhashes-agent\n\n# For Linux ARM64\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-linux-arm64 -O krakenhashes-agent\n\n# For macOS AMD64\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-darwin-amd64 -O krakenhashes-agent\n\n# For macOS ARM64 (Apple Silicon)\nwget https://github.com/ZerkerEOD/krakenhashes/releases/latest/download/krakenhashes-agent-darwin-arm64 -O krakenhashes-agent\n\n# Make the binary executable\nchmod +x krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/installation/#option-2-download-from-krakenhashes-server","title":"Option 2: Download from KrakenHashes Server","text":"<p>The KrakenHashes web UI provides convenient download options with ready-to-use commands. Navigate to the Agent Downloads page in your installation to access:</p> <ul> <li>Direct Download Buttons: Click to download the agent binary for your platform</li> <li>Copy URL: Get the direct download URL</li> <li>curl Command: One-click copy of curl command with SSL bypass flags for self-signed certificates</li> <li>wget Command: One-click copy of wget command with SSL bypass flags for self-signed certificates</li> </ul> <p>Example commands (replace <code>your-server</code> with your actual server URL):</p> <pre><code># Using curl (Linux AMD64)\ncurl -k -o krakenhashes-agent https://your-server:31337/api/public/agent/download/linux/amd64\nchmod +x krakenhashes-agent\n\n# Using wget (Linux AMD64)\nwget --no-check-certificate -O krakenhashes-agent https://your-server:31337/api/public/agent/download/linux/amd64\nchmod +x krakenhashes-agent\n</code></pre> <p>Note: The <code>-k</code> (curl) and <code>--no-check-certificate</code> (wget) flags bypass SSL certificate validation, which is necessary when using self-signed certificates. All downloads save as <code>krakenhashes-agent</code> regardless of platform.</p>"},{"location":"agent-guide/installation/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<pre><code># Verify the binary is executable\nls -la ~/krakenhashes-agent/krakenhashes-agent\n# Should show executable permissions (x)\n\n# Check available options\n./krakenhashes-agent -help\n\n# Test connectivity (without registering)\n./krakenhashes-agent -host your-server:31337 -debug\n# This will show connection attempts even without a claim code\n</code></pre>"},{"location":"agent-guide/installation/#optional-set-up-as-a-service","title":"Optional: Set up as a Service","text":"<p>For automatic startup and easier management, see the Systemd Service Setup guide. This allows the agent to run in the background and start automatically on boot.</p>"},{"location":"agent-guide/installation/#initial-configuration","title":"Initial Configuration","text":"<p>The agent supports two configuration methods:</p>"},{"location":"agent-guide/installation/#method-1-automatic-configuration-recommended","title":"Method 1: Automatic Configuration (Recommended)","text":"<p>The agent automatically creates a <code>.env</code> configuration file on first run. From the agent directory:</p> <pre><code>cd ~/krakenhashes-agent\n\n# Run with your server details and claim code\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE\n\n# The agent will create:\n# - .env configuration file\n# - config/ directory for certificates\n# - data/ directory for files\n</code></pre>"},{"location":"agent-guide/installation/#method-2-manual-env-file-creation","title":"Method 2: Manual .env File Creation","text":"<p>You can manually create a <code>.env</code> file before running the agent:</p> <ol> <li>Create a <code>.env</code> file in <code>~/krakenhashes-agent/.env</code>:</li> </ol> <pre><code>cd ~/krakenhashes-agent\nnano .env  # or your preferred editor\n</code></pre> <ol> <li>Add the following configuration:</li> </ol> <pre><code># KrakenHashes Agent Configuration\n\n# Server Configuration\nKH_HOST=your-server.example.com  # Backend server hostname\nKH_PORT=31337                    # Backend server port\nUSE_TLS=true                     # Use TLS for secure communication\nLISTEN_INTERFACE=                # Network interface to bind to\nHEARTBEAT_INTERVAL=5             # Heartbeat interval in seconds\n\n# Agent Configuration\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE  # Your claim code from Admin UI\n\n# Directory Configuration\nKH_CONFIG_DIR=./config  # Configuration directory\nKH_DATA_DIR=./data      # Data directory\n\n# WebSocket Timing Configuration\nKH_WRITE_WAIT=10s   # Timeout for writing messages\nKH_PONG_WAIT=60s    # Timeout for receiving pong\nKH_PING_PERIOD=54s  # Ping interval\n\n# File Transfer Configuration\nKH_MAX_CONCURRENT_DOWNLOADS=3  # Max concurrent downloads\nKH_DOWNLOAD_TIMEOUT=1h         # Download timeout\n\n# Hashcat Configuration\nHASHCAT_EXTRA_PARAMS=  # Extra hashcat parameters (see note below)\n\n# Logging Configuration\nDEBUG=false            # Enable debug logging\nLOG_LEVEL=INFO        # Log level\n</code></pre> <ol> <li>Replace the placeholder values with your actual configuration</li> <li>Run the agent: <code>cd ~/krakenhashes-agent &amp;&amp; ./krakenhashes-agent</code></li> </ol> <p>Important Note on HASHCAT_EXTRA_PARAMS: - Parameters configured via the frontend (per-agent settings) take precedence - The .env file parameters are only used as a fallback - Best practice: Configure parameters via the frontend UI for centralized management</p>"},{"location":"agent-guide/installation/#post-configuration","title":"Post-Configuration","text":"<p>After the first run, the agent will use the <code>.env</code> file for all configuration. You can edit this file manually if needed:</p> <pre><code># View/edit the generated configuration\ncat .env\nnano .env  # or your preferred editor\n\n# Note: After successful registration, the KH_CLAIM_CODE will be automatically commented out\n</code></pre>"},{"location":"agent-guide/installation/#agent-registration","title":"Agent Registration","text":""},{"location":"agent-guide/installation/#step-1-generate-a-claim-code","title":"Step 1: Generate a Claim Code","text":"<p>In the KrakenHashes Admin UI: 1. Navigate to Agents \u2192 Manage Vouchers 2. Click \"Create Voucher\" 3. Choose voucher type (one-time or continuous) 4. Copy the generated code</p>"},{"location":"agent-guide/installation/#step-2-register-the-agent","title":"Step 2: Register the Agent","text":"<p>From your agent directory, run the agent with your claim code:</p> <pre><code>cd ~/krakenhashes-agent\n\n# Register and run the agent\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE\n\n# With debug output (helpful for troubleshooting)\n./krakenhashes-agent -host your-server:31337 -claim YOUR_CLAIM_CODE -debug\n</code></pre> <p>The agent will: - Connect to the backend server - Register using the claim code - Generate certificates and API keys - Create a <code>.env</code> file with your configuration - Automatically comment out the claim code after successful registration</p>"},{"location":"agent-guide/installation/#step-3-running-the-agent","title":"Step 3: Running the Agent","text":"<p>After registration, simply run:</p> <pre><code>cd ~/krakenhashes-agent\n./krakenhashes-agent\n</code></pre> <p>The agent will use the <code>.env</code> file created during registration. You don't need to specify the claim code again.</p> <p>For automatic startup, see the Systemd Service Setup guide.</p>"},{"location":"agent-guide/installation/#gpu-driver-installation","title":"GPU Driver Installation","text":"<p>The following is only to help but not a full list or updated each time. Please review what drivers you need for your distribution.</p>"},{"location":"agent-guide/installation/#nvidia-gpus","title":"NVIDIA GPUs","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install -y nvidia-driver-525 nvidia-cuda-toolkit\n\n# RHEL/CentOS/Rocky\nsudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\nsudo dnf install -y nvidia-driver cuda\n</code></pre>"},{"location":"agent-guide/installation/#amd-gpus","title":"AMD GPUs","text":"<pre><code># Install ROCm\nwget -q -O - https://repo.radeon.com/rocm/rocm.gpg.key | sudo apt-key add -\necho 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/debian/ ubuntu main' | sudo tee /etc/apt/sources.list.d/rocm.list\nsudo apt update\nsudo apt install rocm-dev\n</code></pre>"},{"location":"agent-guide/installation/#verification","title":"Verification","text":"<ol> <li>Check agent status:</li> </ol> <p>For manual run, check if the process is running:    <pre><code>ps aux | grep krakenhashes-agent\n</code></pre></p> <p>For systemd service:    <pre><code># User service\nsystemctl --user status krakenhashes-agent\n\n# System service  \nsudo systemctl status krakenhashes-agent\n</code></pre></p> <ol> <li>View logs:</li> </ol> <p>For manual run, check the terminal output or log files in the agent directory.</p> <p>For systemd service:    <pre><code># User service\njournalctl --user -u krakenhashes-agent -f\n\n# System service\nsudo journalctl -u krakenhashes-agent -f\n</code></pre></p> <ol> <li>Verify in Web UI:</li> <li>Navigate to Agents section</li> <li>Confirm agent appears as \"Online\"</li> <li>Check detected devices</li> </ol>"},{"location":"agent-guide/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure the agent</li> <li>Set up scheduling</li> <li>Learn about file synchronization</li> </ul>"},{"location":"agent-guide/monitoring/","title":"Agent Monitoring Guide","text":"<p>This comprehensive guide covers monitoring distributed agents in KrakenHashes, including real-time metrics, health checks, performance monitoring, and troubleshooting strategies for administrators managing agent fleets.</p>"},{"location":"agent-guide/monitoring/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Real-time Agent Status</li> <li>Heartbeat and Connection Monitoring</li> <li>Metrics Collection and Monitoring</li> <li>Device Performance Monitoring</li> <li>Agent Health Indicators</li> <li>Multi-Agent Fleet Monitoring</li> <li>Status Indicators and States</li> <li>Performance Analysis and Trends</li> <li>Troubleshooting with Monitoring Data</li> <li>Best Practices</li> </ol>"},{"location":"agent-guide/monitoring/#overview","title":"Overview","text":"<p>KrakenHashes provides comprehensive monitoring capabilities for distributed agents, enabling administrators to track agent health, performance, and operational status across their entire fleet. The monitoring system includes real-time metrics collection, WebSocket-based status reporting, and detailed performance analytics.</p>"},{"location":"agent-guide/monitoring/#key-monitoring-features","title":"Key Monitoring Features","text":"<ul> <li>Real-time Agent Status: Live connection status and heartbeat monitoring</li> <li>Device Metrics: GPU temperature, utilization, fan speed, and hash rate tracking</li> <li>WebSocket Communication: Persistent connections with automatic reconnection</li> <li>Performance Analytics: Historical data and trend analysis</li> <li>Multi-Agent Dashboard: Fleet-wide visibility and management</li> <li>Automated Health Checks: Connection validation and failure detection</li> </ul>"},{"location":"agent-guide/monitoring/#real-time-agent-status","title":"Real-time Agent Status","text":""},{"location":"agent-guide/monitoring/#agent-status-overview","title":"Agent Status Overview","text":"<p>The system continuously monitors agent status through multiple channels:</p> <pre><code>Active Agents: Online and ready for work\n\u251c\u2500\u2500 Connected: WebSocket connection established\n\u251c\u2500\u2500 Idle: Available for task assignment\n\u251c\u2500\u2500 Busy: Currently executing tasks\n\u2514\u2500\u2500 Reconnecting: Temporary disconnection with recovery in progress\n\nInactive Agents: Not currently operational\n\u251c\u2500\u2500 Offline: No recent heartbeat or connection\n\u251c\u2500\u2500 Disabled: Administratively disabled\n\u251c\u2500\u2500 Error: Failed connection or system error\n\u2514\u2500\u2500 Pending: Newly registered, awaiting first connection\n</code></pre>"},{"location":"agent-guide/monitoring/#agent-connection-states","title":"Agent Connection States","text":"<p>The monitoring system tracks detailed connection states:</p> State Description Color Indicator Action Required <code>active</code> Connected and operational \ud83d\udfe2 Green None <code>inactive</code> Disconnected or offline \ud83d\udd34 Red Check agent service <code>pending</code> Registration in progress \ud83d\udfe1 Yellow Wait for completion <code>disabled</code> Administratively disabled \u26ab Gray Manual re-enable <code>error</code> System or hardware error \ud83d\udd34 Red Investigate error"},{"location":"agent-guide/monitoring/#agent-dashboard-view","title":"Agent Dashboard View","text":"<p>Access the agent monitoring dashboard at: - Frontend: <code>https://your-server:31337/agents</code> - Agent Details: <code>https://your-server:31337/agents/{agent_id}</code></p> <p>The dashboard provides: - Real-time connection status - Last activity timestamps - Device configuration and status - Performance metrics and charts - Task assignment history</p>"},{"location":"agent-guide/monitoring/#heartbeat-and-connection-monitoring","title":"Heartbeat and Connection Monitoring","text":""},{"location":"agent-guide/monitoring/#websocket-heartbeat-system","title":"WebSocket Heartbeat System","text":"<p>KrakenHashes uses a robust WebSocket-based heartbeat system for monitoring agent connectivity:</p> <pre><code>Backend \u2190\u2192 Agent WebSocket Connection\n\u251c\u2500\u2500 Ping/Pong Messages: Every 54 seconds (configurable)\n\u251c\u2500\u2500 Agent Status Updates: Every 60 seconds\n\u251c\u2500\u2500 Heartbeat Timeout: 60 seconds maximum\n\u2514\u2500\u2500 Automatic Reconnection: Exponential backoff (1s to 30s)\n</code></pre>"},{"location":"agent-guide/monitoring/#connection-timing-configuration","title":"Connection Timing Configuration","text":"<p>The system uses configurable timing parameters:</p> <pre><code># Backend WebSocket Settings (environment variables)\nKH_WRITE_WAIT=10s      # Write operation timeout\nKH_PONG_WAIT=60s       # Pong response timeout  \nKH_PING_PERIOD=54s     # Ping interval\n\n# Agent automatically fetches these from backend\n# No manual configuration required\n</code></pre>"},{"location":"agent-guide/monitoring/#heartbeat-monitoring-queries","title":"Heartbeat Monitoring Queries","text":"<p>Monitor agent heartbeat status using database queries:</p> <pre><code>-- Agents with recent heartbeats (last 5 minutes)\nSELECT \n    id,\n    name,\n    status,\n    last_heartbeat,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) AS seconds_since_heartbeat\nFROM agents \nWHERE last_heartbeat &gt; NOW() - INTERVAL '5 minutes'\nORDER BY last_heartbeat DESC;\n\n-- Agents with stale heartbeats (potential issues)\nSELECT \n    id,\n    name, \n    status,\n    last_heartbeat,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) AS seconds_since_heartbeat\nFROM agents\nWHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes'\n  AND status = 'active'\nORDER BY last_heartbeat ASC;\n\n-- Connection status distribution\nSELECT status, COUNT(*) as count\nFROM agents\nGROUP BY status\nORDER BY count DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#metrics-collection-and-monitoring","title":"Metrics Collection and Monitoring","text":""},{"location":"agent-guide/monitoring/#system-metrics-collection","title":"System Metrics Collection","text":"<p>Agents automatically collect and report system metrics:</p>"},{"location":"agent-guide/monitoring/#cpu-and-memory-metrics","title":"CPU and Memory Metrics","text":"<ul> <li>CPU Usage: Overall processor utilization percentage</li> <li>Memory Usage: System memory utilization percentage</li> <li>Collection Interval: 5 seconds (configurable)</li> <li>Data Retention: Based on monitoring settings</li> </ul>"},{"location":"agent-guide/monitoring/#agent-metrics-structure","title":"Agent Metrics Structure","text":"<pre><code>type MetricsData struct {\n    AgentID     int                `json:\"agent_id\"`\n    CollectedAt time.Time          `json:\"collected_at\"`\n    CPUs        []CPUMetrics       `json:\"cpus\"`\n    GPUs        []GPUMetrics       `json:\"gpus\"`\n    Memory      MemoryMetrics      `json:\"memory\"`\n    Disk        []DiskMetrics      `json:\"disk\"`\n    Network     []NetworkMetrics   `json:\"network\"`\n    Process     []ProcessMetrics   `json:\"process\"`\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#gpu-metrics-integration","title":"GPU Metrics Integration","text":"<p>GPU metrics are obtained from hashcat's JSON status output during job execution:</p> <ul> <li>GPU Utilization: Device usage percentage</li> <li>GPU Temperature: Operating temperature in Celsius</li> <li>GPU Memory: Memory utilization</li> <li>Power Usage: Power consumption in watts</li> <li>Hash Rate: Real-time hashing performance</li> </ul>"},{"location":"agent-guide/monitoring/#metrics-storage-and-retention","title":"Metrics Storage and Retention","text":"<p>The system implements a cascading retention policy:</p> <pre><code>Real-time Metrics (Fine-grained)\n\u251c\u2500\u2500 Retention: 7 days (configurable)  \n\u251c\u2500\u2500 Resolution: 5-second intervals\n\u2514\u2500\u2500 Use: Recent activity monitoring\n\nDaily Aggregates (Medium-term)\n\u251c\u2500\u2500 Retention: 30 days (configurable)\n\u251c\u2500\u2500 Resolution: Daily summaries\n\u2514\u2500\u2500 Use: Performance analysis\n\nWeekly Aggregates (Long-term)  \n\u251c\u2500\u2500 Retention: 365 days (configurable)\n\u251c\u2500\u2500 Resolution: Weekly summaries\n\u2514\u2500\u2500 Use: Historical trends\n</code></pre>"},{"location":"agent-guide/monitoring/#monitoring-settings-configuration","title":"Monitoring Settings Configuration","text":"<p>Configure metrics retention through the admin interface:</p> <pre><code># Access monitoring settings\nhttps://your-server:31337/admin/monitoring\n\n# Available settings:\n- Real-time Data Retention: 7 days\n- Daily Aggregates Retention: 30 days  \n- Weekly Aggregates Retention: 365 days\n- Enable Aggregation: true\n- Aggregation Interval: daily\n</code></pre>"},{"location":"agent-guide/monitoring/#device-performance-monitoring","title":"Device Performance Monitoring","text":""},{"location":"agent-guide/monitoring/#real-time-device-metrics","title":"Real-time Device Metrics","text":"<p>The Agent Details page provides comprehensive device monitoring with live charts:</p>"},{"location":"agent-guide/monitoring/#available-device-charts","title":"Available Device Charts","text":"<ol> <li>Temperature Monitoring</li> <li>Real-time GPU temperature tracking</li> <li>Temperature threshold alerts</li> <li>Historical temperature trends</li> <li> <p>Multi-device comparison</p> </li> <li> <p>Utilization Tracking</p> </li> <li>GPU utilization percentage</li> <li>Device workload distribution</li> <li>Efficiency analysis</li> <li> <p>Performance optimization insights</p> </li> <li> <p>Fan Speed Monitoring</p> </li> <li>Cooling system performance</li> <li>Fan curve analysis</li> <li>Thermal management tracking</li> <li> <p>Hardware health indicators</p> </li> <li> <p>Hash Rate Performance</p> </li> <li>Real-time hashing performance</li> <li>Per-device contribution</li> <li>Cumulative hash rate</li> <li>Performance benchmarking</li> </ol>"},{"location":"agent-guide/monitoring/#chart-configuration-options","title":"Chart Configuration Options","text":"<pre><code>// Time Range Options\nconst timeRanges = [\n    '10m',  // 10 minutes\n    '20m',  // 20 minutes  \n    '1h',   // 1 hour\n    '5h',   // 5 hours\n    '24h'   // 24 hours\n];\n\n// Metric Types\nconst metricTypes = [\n    'temperature',  // GPU temperature in \u00b0C\n    'utilization',  // GPU utilization %\n    'fanspeed',     // Fan speed %\n    'hashrate'      // Hash rate (varies by algorithm)\n];\n</code></pre>"},{"location":"agent-guide/monitoring/#device-metrics-api-endpoints","title":"Device Metrics API Endpoints","text":"<pre><code># Get device metrics for agent\nGET /api/agents/{agent_id}/metrics?timeRange=1h&amp;metrics=temperature,utilization,fanspeed,hashrate\n\n# Response format\n{\n    \"devices\": [\n        {\n            \"deviceId\": 0,\n            \"deviceName\": \"NVIDIA RTX 4090\",\n            \"metrics\": {\n                \"temperature\": [\n                    {\"timestamp\": 1640995200000, \"value\": 65.0},\n                    {\"timestamp\": 1640995205000, \"value\": 67.2}\n                ],\n                \"utilization\": [\n                    {\"timestamp\": 1640995200000, \"value\": 98.5},\n                    {\"timestamp\": 1640995205000, \"value\": 99.1}\n                ]\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#device-enabledisable-monitoring","title":"Device Enable/Disable Monitoring","text":"<p>Monitor device status changes through the interface:</p> <pre><code># Update device status\nPUT /api/agents/{agent_id}/devices/{device_id}\n{\n    \"enabled\": true\n}\n\n# Monitor device state changes in logs\ngrep -i \"device.*update.*enabled\" logs/backend/*.log\n</code></pre>"},{"location":"agent-guide/monitoring/#agent-health-indicators","title":"Agent Health Indicators","text":""},{"location":"agent-guide/monitoring/#connection-health-metrics","title":"Connection Health Metrics","text":"<p>Monitor agent connection health through multiple indicators:</p>"},{"location":"agent-guide/monitoring/#connection-status-indicators","title":"Connection Status Indicators","text":"<ul> <li>WebSocket State: Connected/Disconnected</li> <li>Last Heartbeat: Timestamp of last communication</li> <li>Response Time: WebSocket ping-pong latency</li> <li>Reconnection Attempts: Failed connection retry count</li> </ul>"},{"location":"agent-guide/monitoring/#system-health-indicators","title":"System Health Indicators","text":"<ul> <li>CPU Load: System processor utilization</li> <li>Memory Usage: Available system memory</li> <li>Disk Space: Storage availability</li> <li>Network Latency: Communication delays</li> </ul>"},{"location":"agent-guide/monitoring/#hardware-health-indicators","title":"Hardware Health Indicators","text":"<ul> <li>GPU Temperature: Thermal status and limits</li> <li>GPU Utilization: Device workload efficiency</li> <li>Power Consumption: Electrical usage monitoring</li> <li>Fan Performance: Cooling system operation</li> </ul>"},{"location":"agent-guide/monitoring/#agent-status-reporting","title":"Agent Status Reporting","text":"<p>Agents automatically report detailed status information:</p> <pre><code>{\n    \"status\": \"active\",\n    \"version\": \"v0.15.7\",  \n    \"updated_at\": \"2025-09-11T10:30:00Z\",\n    \"environment\": {\n        \"os\": \"linux\",\n        \"arch\": \"amd64\",\n        \"hostname\": \"worker-01\"\n    },\n    \"os_info\": {\n        \"platform\": \"linux\",\n        \"hostname\": \"worker-01\", \n        \"os_name\": \"Ubuntu\",\n        \"os_version\": \"22.04.3 LTS\",\n        \"kernel_version\": \"Linux version 6.5.0\",\n        \"go_version\": \"go1.21.0\"\n    }\n}\n</code></pre>"},{"location":"agent-guide/monitoring/#health-check-queries","title":"Health Check Queries","text":"<pre><code>-- Agent health summary\nSELECT \n    a.name,\n    a.status,\n    a.last_heartbeat,\n    a.version,\n    CASE \n        WHEN a.last_heartbeat &gt; NOW() - INTERVAL '2 minutes' THEN 'Healthy'\n        WHEN a.last_heartbeat &gt; NOW() - INTERVAL '5 minutes' THEN 'Warning'\n        ELSE 'Critical'\n    END as health_status,\n    COUNT(ad.id) as device_count,\n    SUM(CASE WHEN ad.enabled THEN 1 ELSE 0 END) as enabled_devices\nFROM agents a\nLEFT JOIN agent_devices ad ON a.id = ad.agent_id\nGROUP BY a.id, a.name, a.status, a.last_heartbeat, a.version\nORDER BY a.last_heartbeat DESC;\n\n-- Agents with hardware issues\nSELECT \n    a.name,\n    ad.device_name,\n    apm.metric_type,\n    apm.value,\n    apm.timestamp\nFROM agents a\nJOIN agent_devices ad ON a.id = ad.agent_id\nJOIN agent_performance_metrics apm ON ad.agent_id = apm.agent_id\nWHERE (apm.metric_type = 'temperature' AND apm.value &gt; 85)\n   OR (apm.metric_type = 'utilization' AND apm.value &lt; 50)\nORDER BY apm.timestamp DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#multi-agent-fleet-monitoring","title":"Multi-Agent Fleet Monitoring","text":""},{"location":"agent-guide/monitoring/#fleet-overview-dashboard","title":"Fleet Overview Dashboard","text":"<p>Monitor your entire agent fleet from the main agents page:</p> <pre><code># Access fleet monitoring\nhttps://your-server:31337/agents\n\n# Key fleet metrics:\n- Total Agents: Active + Inactive count\n- Agent Distribution: By status and location  \n- Hardware Summary: Total GPU count and types\n- Performance Metrics: Combined hash rates\n- Health Status: Overall fleet health\n</code></pre>"},{"location":"agent-guide/monitoring/#fleet-status-categories","title":"Fleet Status Categories","text":""},{"location":"agent-guide/monitoring/#active-agents","title":"Active Agents","text":"<ul> <li>Online and Ready: Available for job assignment</li> <li>Busy: Currently executing tasks</li> <li>Idle: Connected but not actively working</li> </ul>"},{"location":"agent-guide/monitoring/#inactive-agents","title":"Inactive Agents","text":"<ul> <li>Offline: No recent heartbeat</li> <li>Disabled: Administratively disabled</li> <li>Error State: Hardware or connection issues</li> </ul>"},{"location":"agent-guide/monitoring/#pending-agents","title":"Pending Agents","text":"<ul> <li>Registering: New agent setup in progress</li> <li>Authenticating: Certificate and API key validation</li> </ul>"},{"location":"agent-guide/monitoring/#fleet-wide-monitoring-queries","title":"Fleet-wide Monitoring Queries","text":"<pre><code>-- Fleet status summary\nSELECT \n    status,\n    COUNT(*) as agent_count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\nFROM agents\nGROUP BY status\nORDER BY agent_count DESC;\n\n-- Fleet hardware summary\nSELECT \n    ad.device_type,\n    COUNT(DISTINCT a.id) as agents_with_device,\n    COUNT(ad.id) as total_devices,\n    SUM(CASE WHEN ad.enabled THEN 1 ELSE 0 END) as enabled_devices\nFROM agents a\nJOIN agent_devices ad ON a.id = ad.agent_id\nGROUP BY ad.device_type\nORDER BY total_devices DESC;\n\n-- Fleet performance overview\nSELECT \n    COUNT(DISTINCT a.id) as total_agents,\n    COUNT(DISTINCT CASE WHEN a.status = 'active' THEN a.id END) as active_agents,\n    AVG(CASE WHEN apm.metric_type = 'utilization' THEN apm.value END) as avg_gpu_utilization,\n    AVG(CASE WHEN apm.metric_type = 'temperature' THEN apm.value END) as avg_gpu_temperature\nFROM agents a\nLEFT JOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.timestamp &gt; NOW() - INTERVAL '1 hour';\n</code></pre>"},{"location":"agent-guide/monitoring/#geographic-and-organizational-fleet-monitoring","title":"Geographic and Organizational Fleet Monitoring","text":"<pre><code>-- Agents by network location (using IP metadata)\nSELECT \n    SUBSTRING(metadata-&gt;&gt;'ipAddress', 1, \n              POSITION('.' IN metadata-&gt;&gt;'ipAddress' || '.')) as network_prefix,\n    COUNT(*) as agent_count,\n    COUNT(CASE WHEN status = 'active' THEN 1 END) as active_count\nFROM agents\nWHERE metadata ? 'ipAddress'\nGROUP BY network_prefix\nORDER BY agent_count DESC;\n\n-- Agents by owner (team/user assignments)\nSELECT \n    COALESCE(u.username, 'Unassigned') as owner,\n    COUNT(a.id) as agent_count,\n    COUNT(CASE WHEN a.status = 'active' THEN 1 END) as active_count\nFROM agents a\nLEFT JOIN users u ON a.owner_id = u.id  \nGROUP BY u.username\nORDER BY agent_count DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#status-indicators-and-states","title":"Status Indicators and States","text":""},{"location":"agent-guide/monitoring/#agent-status-state-machine","title":"Agent Status State Machine","text":"<pre><code>[pending] \u2192 [active] \u2192 [inactive]\n    \u2193         \u2193           \u2193\n[error]   [busy/idle]  [disabled]\n    \u2193         \u2193           \u2193\n[active]  [active]   [active]\n</code></pre>"},{"location":"agent-guide/monitoring/#detailed-status-descriptions","title":"Detailed Status Descriptions","text":"Status Description Typical Causes Recovery Actions <code>pending</code> New agent registration First-time setup Wait for completion <code>active</code> Fully operational Normal state None required <code>busy</code> Executing tasks Job assignment Monitor progress <code>idle</code> Connected, available Between jobs None required <code>inactive</code> Disconnected Network/service issues Restart agent service <code>disabled</code> Manually disabled Admin action Re-enable through UI <code>error</code> System/hardware error Hardware failure, config error Check logs, fix issues"},{"location":"agent-guide/monitoring/#status-transition-triggers","title":"Status Transition Triggers","text":""},{"location":"agent-guide/monitoring/#automatic-transitions","title":"Automatic Transitions","text":"<ul> <li>pending \u2192 active: Successful device detection and registration</li> <li>active \u2192 inactive: Heartbeat timeout (&gt;5 minutes)</li> <li>active \u2192 error: Device detection failure or system error</li> <li>error \u2192 active: Successful reconnection after error resolution</li> </ul>"},{"location":"agent-guide/monitoring/#manual-transitions","title":"Manual Transitions","text":"<ul> <li>Any \u2192 disabled: Administrative disable action</li> <li>disabled \u2192 active: Administrative enable action</li> <li>Any \u2192 active: Force status change through API</li> </ul>"},{"location":"agent-guide/monitoring/#visual-status-indicators","title":"Visual Status Indicators","text":"<p>The web interface uses consistent visual indicators:</p> <pre><code>/* Status indicator colors */\n.status-active     { color: #4caf50; }  /* Green */\n.status-inactive   { color: #f44336; }  /* Red */\n.status-pending    { color: #ff9800; }  /* Orange */\n.status-disabled   { color: #9e9e9e; }  /* Gray */\n.status-error      { color: #f44336; }  /* Red */\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-analysis-and-trends","title":"Performance Analysis and Trends","text":""},{"location":"agent-guide/monitoring/#historical-performance-tracking","title":"Historical Performance Tracking","text":"<p>The system maintains comprehensive historical performance data for trend analysis:</p>"},{"location":"agent-guide/monitoring/#performance-metrics-database-schema","title":"Performance Metrics Database Schema","text":"<pre><code>-- Agent performance metrics table structure\nCREATE TABLE agent_performance_metrics (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER REFERENCES agents(id),\n    device_name VARCHAR(255),\n    metric_type VARCHAR(50),  -- 'temperature', 'utilization', 'fanspeed', 'hashrate'\n    value NUMERIC(10,2),\n    timestamp TIMESTAMP DEFAULT NOW()\n);\n\n-- Benchmark results tracking\nCREATE TABLE agent_benchmarks (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER REFERENCES agents(id),\n    attack_mode INTEGER,\n    hash_type INTEGER, \n    speed BIGINT,\n    device_speeds JSONB,\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-analysis-queries","title":"Performance Analysis Queries","text":"<pre><code>-- Agent performance trends over time\nSELECT \n    DATE_TRUNC('hour', timestamp) as hour,\n    AVG(CASE WHEN metric_type = 'utilization' THEN value END) as avg_utilization,\n    AVG(CASE WHEN metric_type = 'temperature' THEN value END) as avg_temperature,\n    AVG(CASE WHEN metric_type = 'hashrate' THEN value END) as avg_hashrate\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND timestamp &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour;\n\n-- Top performing agents by hash rate\nSELECT \n    a.name,\n    AVG(apm.value) as avg_hashrate,\n    MAX(apm.value) as peak_hashrate,\n    COUNT(apm.id) as measurement_count\nFROM agents a\nJOIN agent_performance_metrics apm ON a.id = apm.agent_id\nWHERE apm.metric_type = 'hashrate'\n  AND apm.timestamp &gt; NOW() - INTERVAL '1 week'\nGROUP BY a.id, a.name\nORDER BY avg_hashrate DESC\nLIMIT 10;\n\n-- Performance degradation detection\nWITH recent_performance AS (\n    SELECT \n        agent_id,\n        AVG(value) as recent_avg\n    FROM agent_performance_metrics\n    WHERE metric_type = 'hashrate'\n      AND timestamp &gt; NOW() - INTERVAL '24 hours'\n    GROUP BY agent_id\n),\nbaseline_performance AS (\n    SELECT \n        agent_id,\n        AVG(value) as baseline_avg\n    FROM agent_performance_metrics  \n    WHERE metric_type = 'hashrate'\n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '2 days'\n    GROUP BY agent_id\n)\nSELECT \n    a.name,\n    r.recent_avg,\n    b.baseline_avg,\n    ROUND(((r.recent_avg - b.baseline_avg) / b.baseline_avg * 100), 2) as percent_change\nFROM agents a\nJOIN recent_performance r ON a.id = r.agent_id\nJOIN baseline_performance b ON a.id = b.agent_id\nWHERE ABS((r.recent_avg - b.baseline_avg) / b.baseline_avg) &gt; 0.15  -- &gt;15% change\nORDER BY percent_change;\n</code></pre>"},{"location":"agent-guide/monitoring/#benchmark-performance-tracking","title":"Benchmark Performance Tracking","text":"<p>Monitor agent benchmark performance over time:</p> <pre><code>-- Benchmark history for agent\nSELECT \n    attack_mode,\n    hash_type,\n    speed,\n    created_at,\n    LAG(speed) OVER (PARTITION BY attack_mode, hash_type ORDER BY created_at) as previous_speed,\n    speed - LAG(speed) OVER (PARTITION BY attack_mode, hash_type ORDER BY created_at) as speed_change\nFROM agent_benchmarks\nWHERE agent_id = $1\nORDER BY created_at DESC;\n\n-- Fleet benchmark comparison\nSELECT \n    a.name,\n    ab.attack_mode,\n    ab.hash_type,\n    ab.speed,\n    RANK() OVER (PARTITION BY ab.attack_mode, ab.hash_type ORDER BY ab.speed DESC) as rank\nFROM agents a\nJOIN agent_benchmarks ab ON a.id = ab.agent_id\nWHERE ab.updated_at &gt; NOW() - INTERVAL '1 week'\nORDER BY ab.attack_mode, ab.hash_type, ab.speed DESC;\n</code></pre>"},{"location":"agent-guide/monitoring/#troubleshooting-with-monitoring-data","title":"Troubleshooting with Monitoring Data","text":""},{"location":"agent-guide/monitoring/#common-issues-and-diagnostic-approaches","title":"Common Issues and Diagnostic Approaches","text":""},{"location":"agent-guide/monitoring/#1-agent-connection-issues","title":"1. Agent Connection Issues","text":"<p>Symptoms: - Agent status shows \"inactive\" or \"error\" - Missing from active agents list - Stale heartbeat timestamps</p> <p>Diagnostic Steps: <pre><code>-- Check agent connection history\nSELECT \n    id,\n    name,\n    status,\n    last_heartbeat,\n    last_error,\n    EXTRACT(EPOCH FROM (NOW() - last_heartbeat)) as seconds_offline\nFROM agents\nWHERE name = 'problem-agent'\nOR id = 123;\n\n-- Check for recent WebSocket errors\n</code></pre></p> <p>Log Analysis: <pre><code># Check agent logs for connection issues\ngrep -i \"connection\\|websocket\\|heartbeat\" /path/to/agent.log\n\n# Check backend logs for agent-related errors\ngrep -i \"agent.*error\\|websocket.*close\\|connection.*failed\" logs/backend/*.log\n\n# Look for certificate or authentication issues\ngrep -i \"certificate\\|auth.*fail\\|tls.*error\" logs/backend/*.log\n</code></pre></p>"},{"location":"agent-guide/monitoring/#2-performance-degradation","title":"2. Performance Degradation","text":"<p>Symptoms: - Decreased hash rates - Higher GPU temperatures - Reduced utilization</p> <p>Diagnostic Queries: <pre><code>-- Performance comparison (current vs historical)\nWITH current_perf AS (\n    SELECT AVG(value) as current_hashrate\n    FROM agent_performance_metrics\n    WHERE agent_id = $1 \n      AND metric_type = 'hashrate'\n      AND timestamp &gt; NOW() - INTERVAL '1 hour'\n),\nhistorical_perf AS (\n    SELECT AVG(value) as historical_hashrate\n    FROM agent_performance_metrics\n    WHERE agent_id = $1\n      AND metric_type = 'hashrate' \n      AND timestamp BETWEEN NOW() - INTERVAL '1 week' AND NOW() - INTERVAL '1 day'\n)\nSELECT \n    c.current_hashrate,\n    h.historical_hashrate,\n    ((c.current_hashrate - h.historical_hashrate) / h.historical_hashrate * 100) as percent_change\nFROM current_perf c, historical_perf h;\n\n-- Temperature analysis\nSELECT \n    DATE_TRUNC('hour', timestamp) as hour,\n    AVG(value) as avg_temp,\n    MAX(value) as max_temp,\n    COUNT(*) as measurements\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND metric_type = 'temperature'\n  AND timestamp &gt; NOW() - INTERVAL '24 hours'\nGROUP BY hour\nORDER BY hour DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#3-hardware-health-issues","title":"3. Hardware Health Issues","text":"<p>Symptoms: - High GPU temperatures (&gt;85\u00b0C) - Fan speed abnormalities - Utilization inconsistencies</p> <p>Monitoring Approach: <pre><code>-- Hardware health check\nSELECT \n    device_name,\n    metric_type,\n    value,\n    timestamp,\n    CASE \n        WHEN metric_type = 'temperature' AND value &gt; 85 THEN 'CRITICAL'\n        WHEN metric_type = 'temperature' AND value &gt; 75 THEN 'WARNING'\n        WHEN metric_type = 'utilization' AND value &lt; 80 THEN 'LOW_UTIL'\n        WHEN metric_type = 'fanspeed' AND value &gt; 90 THEN 'HIGH_FAN'\n        ELSE 'NORMAL'\n    END as status\nFROM agent_performance_metrics\nWHERE agent_id = $1\n  AND timestamp &gt; NOW() - INTERVAL '1 hour'\n  AND (\n    (metric_type = 'temperature' AND value &gt; 75)\n    OR (metric_type = 'utilization' AND value &lt; 80)\n    OR (metric_type = 'fanspeed' AND value &gt; 90)\n  )\nORDER BY timestamp DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#4-task-assignment-issues","title":"4. Task Assignment Issues","text":"<p>Symptoms: - Agents remain idle during jobs - Uneven task distribution - Tasks stuck in \"reconnect_pending\"</p> <p>Diagnostic Queries: <pre><code>-- Check agent busy status and current tasks\nSELECT \n    a.name,\n    a.status,\n    a.metadata-&gt;&gt;'busy_status' as busy_status,\n    a.metadata-&gt;&gt;'current_task_id' as current_task,\n    jt.status as task_status,\n    je.name as job_name\nFROM agents a\nLEFT JOIN job_tasks jt ON jt.id = a.metadata-&gt;&gt;'current_task_id'  \nLEFT JOIN job_executions je ON jt.job_execution_id = je.id\nWHERE a.id = $1;\n\n-- Check for reconnect_pending tasks\nSELECT \n    jt.id,\n    jt.status,\n    jt.agent_id,\n    a.name,\n    jt.keyspace_start,\n    jt.keyspace_end,\n    jt.updated_at\nFROM job_tasks jt\nJOIN agents a ON jt.agent_id = a.id\nWHERE jt.status = 'reconnect_pending'\nORDER BY jt.updated_at DESC;\n</code></pre></p>"},{"location":"agent-guide/monitoring/#log-analysis-techniques","title":"Log Analysis Techniques","text":""},{"location":"agent-guide/monitoring/#structured-log-search","title":"Structured Log Search","text":"<pre><code># Find connection events for specific agent\ngrep -i \"agent.*123\\|Agent 123\" logs/backend/*.log | grep -i \"connect\"\n\n# Track WebSocket message flow\ngrep -i \"websocket\\|message.*type\" logs/backend/*.log | tail -50\n\n# Monitor heartbeat activity\ngrep -i \"heartbeat\\|ping\\|pong\" logs/backend/*.log | tail -20\n\n# Check for error patterns\ngrep -i \"error\\|fail\\|timeout\" logs/backend/*.log | grep -i \"agent\" | tail -10\n</code></pre>"},{"location":"agent-guide/monitoring/#performance-issue-detection","title":"Performance Issue Detection","text":"<pre><code># Find performance-related messages\ngrep -i \"performance\\|slow\\|timeout\\|benchmark\" logs/backend/*.log\n\n# Check for resource issues\ngrep -i \"memory\\|cpu\\|disk\\|resource\" logs/backend/*.log\n\n# Monitor cleanup operations\ngrep -i \"cleanup\\|maintenance\\|retention\" logs/backend/*.log\n</code></pre>"},{"location":"agent-guide/monitoring/#automated-issue-detection","title":"Automated Issue Detection","text":"<p>Set up monitoring alerts for common issues:</p> <pre><code># Script: monitor_agents.sh\n#!/bin/bash\n\n# Check for agents with stale heartbeats\nSTALE_AGENTS=$(psql -t -c \"SELECT COUNT(*) FROM agents WHERE last_heartbeat &lt; NOW() - INTERVAL '5 minutes' AND status = 'active';\")\n\nif [ \"$STALE_AGENTS\" -gt 0 ]; then\n    echo \"ALERT: $STALE_AGENTS agents have stale heartbeats\"\n    # Send notification\nfi\n\n# Check for high GPU temperatures\nHOT_GPUS=$(psql -t -c \"SELECT COUNT(*) FROM agent_performance_metrics WHERE metric_type = 'temperature' AND value &gt; 85 AND timestamp &gt; NOW() - INTERVAL '5 minutes';\")\n\nif [ \"$HOT_GPUS\" -gt 0 ]; then\n    echo \"ALERT: $HOT_GPUS GPUs running hot (&gt;85\u00b0C)\"\n    # Send notification\nfi\n\n# Check for agents with no enabled devices\nNO_DEVICE_AGENTS=$(psql -t -c \"SELECT COUNT(DISTINCT a.id) FROM agents a LEFT JOIN agent_devices ad ON a.id = ad.agent_id WHERE a.status = 'active' AND NOT EXISTS (SELECT 1 FROM agent_devices ad2 WHERE ad2.agent_id = a.id AND ad2.enabled = true);\")\n\nif [ \"$NO_DEVICE_AGENTS\" -gt 0 ]; then\n    echo \"ALERT: $NO_DEVICE_AGENTS active agents have no enabled devices\"\n    # Send notification  \nfi\n</code></pre>"},{"location":"agent-guide/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"agent-guide/monitoring/#monitoring-strategy","title":"Monitoring Strategy","text":""},{"location":"agent-guide/monitoring/#1-proactive-monitoring","title":"1. Proactive Monitoring","text":"<ul> <li>Set up automated alerts for critical metrics (heartbeat failures, high temperatures, low utilization)</li> <li>Establish performance baselines for each agent to detect degradation</li> <li>Monitor trends rather than just current values</li> <li>Use multiple monitoring approaches (real-time dashboard + historical analysis)</li> </ul>"},{"location":"agent-guide/monitoring/#2-alert-thresholds","title":"2. Alert Thresholds","text":"<pre><code># Recommended alert thresholds:\nAgent Heartbeat: &gt; 5 minutes offline\nGPU Temperature: &gt; 85\u00b0C sustained  \nGPU Utilization: &lt; 50% during jobs\nConnection Failures: &gt; 3 consecutive failures\nPerformance Degradation: &gt; 20% decrease from baseline\n</code></pre>"},{"location":"agent-guide/monitoring/#3-regular-health-checks","title":"3. Regular Health Checks","text":"<ul> <li>Daily: Review agent status dashboard, check for offline agents</li> <li>Weekly: Analyze performance trends, identify degradation patterns  </li> <li>Monthly: Review hardware utilization, plan capacity changes</li> <li>Quarterly: Update performance baselines, optimize configurations</li> </ul>"},{"location":"agent-guide/monitoring/#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"agent-guide/monitoring/#1-fleet-management","title":"1. Fleet Management","text":"<ul> <li>Group agents by location, hardware type, or team assignment</li> <li>Use naming conventions that reflect agent purpose and location</li> <li>Document agent configurations including hardware specs and special parameters</li> <li>Maintain agent inventory with ownership and responsibility assignments</li> </ul>"},{"location":"agent-guide/monitoring/#2-performance-optimization","title":"2. Performance Optimization","text":"<ul> <li>Monitor benchmark results and update when hardware changes</li> <li>Balance task distribution across agents based on capability</li> <li>Track device utilization to identify underused resources</li> <li>Optimize extra parameters for each agent's hardware configuration</li> </ul>"},{"location":"agent-guide/monitoring/#3-maintenance-scheduling","title":"3. Maintenance Scheduling","text":"<ul> <li>Plan maintenance windows during low-activity periods</li> <li>Coordinate updates to minimize impact on running jobs</li> <li>Test configuration changes on non-critical agents first</li> <li>Document maintenance activities and their impact on performance</li> </ul>"},{"location":"agent-guide/monitoring/#monitoring-data-retention","title":"Monitoring Data Retention","text":""},{"location":"agent-guide/monitoring/#1-storage-management","title":"1. Storage Management","text":"<pre><code>-- Configure retention policies based on needs:\nReal-time metrics: 7-14 days (high-frequency data)\nDaily aggregates: 30-90 days (performance analysis)  \nWeekly aggregates: 1-2 years (long-term trends)\nBenchmark results: Indefinite (configuration reference)\n</code></pre>"},{"location":"agent-guide/monitoring/#2-data-cleanup-automation","title":"2. Data Cleanup Automation","text":"<ul> <li>Enable automatic aggregation to reduce storage requirements</li> <li>Monitor database growth and adjust retention as needed</li> <li>Archive historical data for compliance or analysis needs</li> <li>Use monitoring settings UI to adjust retention policies</li> </ul>"},{"location":"agent-guide/monitoring/#security-and-access-control","title":"Security and Access Control","text":""},{"location":"agent-guide/monitoring/#1-monitoring-access","title":"1. Monitoring Access","text":"<ul> <li>Restrict monitoring access to appropriate administrators</li> <li>Use role-based permissions for different monitoring functions</li> <li>Audit monitoring activities and configuration changes</li> <li>Secure monitoring endpoints with proper authentication</li> </ul>"},{"location":"agent-guide/monitoring/#2-agent-communication-security","title":"2. Agent Communication Security","text":"<ul> <li>Monitor certificate status and renewal schedules</li> <li>Track authentication failures and suspicious activity</li> <li>Use secure WebSocket connections (WSS) in production</li> <li>Regularly rotate API keys and monitor key usage</li> </ul>"},{"location":"agent-guide/monitoring/#disaster-recovery-and-continuity","title":"Disaster Recovery and Continuity","text":""},{"location":"agent-guide/monitoring/#1-monitoring-system-availability","title":"1. Monitoring System Availability","text":"<ul> <li>Implement monitoring redundancy to avoid single points of failure</li> <li>Backup monitoring configurations and historical data</li> <li>Test monitoring system recovery procedures</li> <li>Document escalation procedures for monitoring system failures</li> </ul>"},{"location":"agent-guide/monitoring/#2-agent-recovery-procedures","title":"2. Agent Recovery Procedures","text":"<ul> <li>Automate agent reconnection with exponential backoff</li> <li>Implement graceful degradation when agents are offline</li> <li>Buffer critical messages during disconnections for recovery</li> <li>Track task recovery and automatic redistribution</li> </ul> <p>This comprehensive monitoring guide enables administrators to effectively manage distributed KrakenHashes agent fleets, ensuring optimal performance, rapid issue detection, and reliable operation across diverse hardware configurations and network environments.</p>"},{"location":"agent-guide/scheduling/","title":"Agent Scheduling","text":""},{"location":"agent-guide/scheduling/#overview","title":"Overview","text":"<p>The Agent Scheduling feature in KrakenHashes allows administrators to define specific time windows when agents are available for job execution. This feature helps optimize resource usage, manage electricity costs, and ensure agents run during appropriate hours.</p>"},{"location":"agent-guide/scheduling/#key-features","title":"Key Features","text":"<ul> <li>Daily Schedule Configuration: Set different working hours for each day of the week</li> <li>Timezone Support: Schedules are configured in the user's local timezone but stored in UTC</li> <li>Overnight Schedule Support: Schedules can span midnight (e.g., 22:00 - 02:00)</li> <li>Global Enable/Disable: System-wide toggle to enable or disable all scheduling</li> <li>Per-Agent Control: Each agent can have scheduling enabled or disabled independently</li> <li>Schedule Preservation: Schedules are preserved even when disabled</li> </ul>"},{"location":"agent-guide/scheduling/#how-it-works","title":"How It Works","text":""},{"location":"agent-guide/scheduling/#schedule-enforcement","title":"Schedule Enforcement","text":"<p>When scheduling is enabled: 1. The system checks if global scheduling is enabled (admin setting) 2. The system checks if the individual agent has scheduling enabled 3. The system checks if the current UTC time falls within the agent's schedule 4. Only agents that pass all checks are assigned jobs</p>"},{"location":"agent-guide/scheduling/#time-storage-and-display","title":"Time Storage and Display","text":"<ul> <li>Storage: All times are stored in UTC in the database</li> <li>Display: Times are shown in the user's local timezone in the UI</li> <li>Conversion: Automatic conversion happens between local and UTC times</li> </ul>"},{"location":"agent-guide/scheduling/#configuration","title":"Configuration","text":""},{"location":"agent-guide/scheduling/#global-settings","title":"Global Settings","text":"<p>The global scheduling setting can be found in Admin Panel \u2192 System Settings:</p> <pre><code>Enable Agent Scheduling System: [Toggle]\n</code></pre> <p>When disabled: - All agent schedules are ignored - Agents are always available for jobs - Individual agent schedules are preserved but not enforced</p>"},{"location":"agent-guide/scheduling/#per-agent-configuration","title":"Per-Agent Configuration","text":"<p>Individual agent scheduling is configured on the agent details page:</p> <ol> <li>Navigate to Agents \u2192 [Agent Name]</li> <li>Find the Scheduling section</li> <li>Toggle Enable Scheduling to activate scheduling for this agent</li> <li>Click Edit All Schedules to configure daily schedules</li> </ol>"},{"location":"agent-guide/scheduling/#schedule-configuration","title":"Schedule Configuration","text":"<p>When editing schedules:</p> <ol> <li>Add Schedule: Click \"Add Schedule\" for any day to create a time window</li> <li>Set Times: Enter start and end times in 24-hour format (HH:MM)</li> <li>Active Toggle: Enable/disable the schedule for specific days</li> <li>Active (ON): Agent works during the specified hours</li> <li>Active (OFF): Agent does not work at all on this day</li> <li>Copy Schedule: Use the copy icon to apply one day's schedule to all other days</li> <li>Delete Schedule: Remove a schedule for a specific day</li> </ol>"},{"location":"agent-guide/scheduling/#time-input-formats","title":"Time Input Formats","text":"<p>The system accepts various time formats: - <code>9</code> \u2192 <code>09:00:00</code> - <code>17</code> \u2192 <code>17:00:00</code> - <code>9:30</code> \u2192 <code>09:30:00</code> - <code>09:00</code> \u2192 <code>09:00:00</code> - <code>09:00:00</code> \u2192 <code>09:00:00</code></p>"},{"location":"agent-guide/scheduling/#examples","title":"Examples","text":""},{"location":"agent-guide/scheduling/#standard-business-hours-9-5-monday-friday","title":"Standard Business Hours (9-5, Monday-Friday)","text":"<pre><code>Monday:    09:00 - 17:00 [Active]\nTuesday:   09:00 - 17:00 [Active]\nWednesday: 09:00 - 17:00 [Active]\nThursday:  09:00 - 17:00 [Active]\nFriday:    09:00 - 17:00 [Active]\nSaturday:  Not scheduled\nSunday:    Not scheduled\n</code></pre>"},{"location":"agent-guide/scheduling/#247-operation-with-weekend-maintenance","title":"24/7 Operation with Weekend Maintenance","text":"<pre><code>Monday:    00:00 - 23:59 [Active]\nTuesday:   00:00 - 23:59 [Active]\nWednesday: 00:00 - 23:59 [Active]\nThursday:  00:00 - 23:59 [Active]\nFriday:    00:00 - 23:59 [Active]\nSaturday:  00:00 - 06:00 [Active]  # Maintenance window 6 AM - Midnight\nSunday:    Not scheduled            # Full day maintenance\n</code></pre>"},{"location":"agent-guide/scheduling/#overnight-processing","title":"Overnight Processing","text":"<pre><code>Monday:    22:00 - 06:00 [Active]  # Runs overnight Mon-Tue\nTuesday:   22:00 - 06:00 [Active]  # Runs overnight Tue-Wed\nWednesday: 22:00 - 06:00 [Active]  # Runs overnight Wed-Thu\nThursday:  22:00 - 06:00 [Active]  # Runs overnight Thu-Fri\nFriday:    22:00 - 06:00 [Active]  # Runs overnight Fri-Sat\nSaturday:  Not scheduled\nSunday:    22:00 - 06:00 [Active]  # Runs overnight Sun-Mon\n</code></pre>"},{"location":"agent-guide/scheduling/#important-behavior-notes","title":"Important Behavior Notes","text":""},{"location":"agent-guide/scheduling/#running-jobs-are-not-interrupted","title":"Running Jobs Are Not Interrupted","text":"<p>The scheduling system only controls when new jobs are assigned, not when running jobs must complete.</p> <p>Key points: - Schedules determine when an agent can receive new jobs - Running jobs will always complete, even if they extend past the scheduled end time - The agent will not accept new jobs outside its schedule, but will finish current work</p>"},{"location":"agent-guide/scheduling/#example-scenario","title":"Example Scenario","text":"<p>If an agent is scheduled to work until 17:00: - At 16:59, the agent receives a job configured for 1-hour chunks - The job will run to completion, potentially until 17:59 or later - No new jobs will be assigned after 17:00 - The agent becomes available for new work at the next scheduled window</p> <p>This design ensures: - No work is lost due to scheduling boundaries - Jobs complete successfully without interruption - Predictable behavior for long-running tasks</p>"},{"location":"agent-guide/scheduling/#schedule-priority","title":"Schedule Priority","text":"<p>The scheduling system follows this priority order:</p> <ol> <li>Global Setting OFF: All schedules ignored, all agents always available</li> <li>Global Setting ON + Agent Scheduling OFF: Agent always available</li> <li>Global Setting ON + Agent Scheduling ON: Agent follows configured schedule</li> </ol>"},{"location":"agent-guide/scheduling/#technical-details","title":"Technical Details","text":""},{"location":"agent-guide/scheduling/#database-schema","title":"Database Schema","text":"<p>Schedules are stored in the <code>agent_schedules</code> table:</p> <pre><code>CREATE TABLE agent_schedules (\n    id SERIAL PRIMARY KEY,\n    agent_id INTEGER NOT NULL REFERENCES agents(id),\n    day_of_week INTEGER NOT NULL,  -- 0-6 (Sunday-Saturday)\n    start_time TIME NOT NULL,       -- UTC time\n    end_time TIME NOT NULL,         -- UTC time\n    timezone VARCHAR(50) NOT NULL,  -- Original timezone for reference\n    is_active BOOLEAN NOT NULL,\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"agent-guide/scheduling/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>GET /api/agents/{id}/schedules</code> - Get agent schedules</li> <li><code>POST /api/agents/{id}/schedules</code> - Update single schedule</li> <li><code>POST /api/agents/{id}/schedules/bulk</code> - Bulk update schedules</li> <li><code>DELETE /api/agents/{id}/schedules/{day}</code> - Delete schedule for a day</li> <li><code>PUT /api/agents/{id}/scheduling-enabled</code> - Toggle scheduling for agent</li> </ul>"},{"location":"agent-guide/scheduling/#job-assignment-integration","title":"Job Assignment Integration","text":"<p>The job assignment service (<code>GetAvailableAgents</code>) checks scheduling:</p> <pre><code>if agent.SchedulingEnabled {\n    schedulingSetting, err := s.systemSettingsRepo.GetSetting(ctx, \"agent_scheduling_enabled\")\n    if err == nil &amp;&amp; schedulingSetting.Value != nil &amp;&amp; *schedulingSetting.Value == \"true\" {\n        isScheduled, err := s.scheduleRepo.IsAgentScheduledNow(ctx, agent.ID)\n        if err != nil || !isScheduled {\n            continue // Skip this agent\n        }\n    }\n}\n</code></pre>"},{"location":"agent-guide/scheduling/#best-practices","title":"Best Practices","text":"<ol> <li>Test Schedules: Always test schedules with non-critical jobs first</li> <li>Timezone Awareness: Be mindful of timezone differences when setting schedules</li> <li>Overlap Planning: Ensure adequate agent coverage during peak hours</li> <li>Maintenance Windows: Schedule maintenance during off-hours</li> <li>Documentation: Document your scheduling strategy for team members</li> </ol>"},{"location":"agent-guide/scheduling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agent-guide/scheduling/#agent-not-getting-jobs-despite-being-scheduled","title":"Agent Not Getting Jobs Despite Being Scheduled","text":"<ol> <li>Check global scheduling is enabled</li> <li>Verify agent scheduling is enabled</li> <li>Confirm current time falls within schedule</li> <li>Check agent is otherwise eligible (enabled, online, etc.)</li> </ol>"},{"location":"agent-guide/scheduling/#schedule-shows-wrong-times","title":"Schedule Shows Wrong Times","text":"<ol> <li>Verify your browser timezone is correct</li> <li>Check the timezone display in the UI</li> <li>Remember all times are stored in UTC</li> </ol>"},{"location":"agent-guide/scheduling/#overnight-schedules-not-working","title":"Overnight Schedules Not Working","text":"<ol> <li>Ensure end time is properly set for next day</li> <li>Verify the schedule spans midnight correctly</li> <li>Check both days involved in the overnight schedule</li> </ol>"},{"location":"agent-guide/scheduling/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements for the scheduling system:</p> <ul> <li>Holiday calendar integration</li> <li>Schedule templates for common patterns</li> <li>Bulk schedule management across multiple agents</li> <li>Schedule conflict detection and warnings</li> <li>Historical schedule effectiveness reporting</li> </ul>"},{"location":"agent-guide/systemd-setup/","title":"Systemd Service Setup","text":""},{"location":"agent-guide/systemd-setup/#overview","title":"Overview","text":"<p>I have yet to test systemd. Please open any issues you may have so that I can address them as needed.</p> <p>This guide explains how to set up the KrakenHashes agent as a systemd service for automatic startup and management. There are two approaches: user services (recommended for personal use) and system services (for production servers).</p>"},{"location":"agent-guide/systemd-setup/#quick-decision-guide","title":"Quick Decision Guide","text":"<ul> <li>User Service: If you're running the agent on your personal machine or don't have root access</li> <li>System Service: If you're setting up on a production server with multiple users or need the agent to start before login</li> </ul>"},{"location":"agent-guide/systemd-setup/#user-service-setup-no-root-required","title":"User Service Setup (No Root Required)","text":"<p>User systemd services run under your user account and don't require sudo privileges. This is the recommended approach for most users.</p>"},{"location":"agent-guide/systemd-setup/#1-create-the-service-directory","title":"1. Create the Service Directory","text":"<pre><code>mkdir -p ~/.config/systemd/user/\n</code></pre>"},{"location":"agent-guide/systemd-setup/#2-create-the-service-file","title":"2. Create the Service File","text":"<p>Create <code>~/.config/systemd/user/krakenhashes-agent.service</code>:</p> <pre><code>[Unit]\nDescription=KrakenHashes Agent\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\n# Working directory where the agent and .env file are located\nWorkingDirectory=%h/krakenhashes-agent\n# Path to the agent executable\nExecStart=%h/krakenhashes-agent/krakenhashes-agent\nRestart=on-failure\nRestartSec=10\n# Optional: Set resource limits\n# MemoryLimit=4G\n# CPUQuota=80%\n\n# Environment variables (optional)\n# Environment=\"DEBUG=true\"\n# Or use an environment file:\n# EnvironmentFile=%h/krakenhashes-agent/.env.systemd\n\n[Install]\nWantedBy=default.target\n</code></pre> <p>Note: <code>%h</code> is automatically replaced with your home directory (e.g., <code>/home/username</code>)</p>"},{"location":"agent-guide/systemd-setup/#3-enable-and-start-the-service","title":"3. Enable and Start the Service","text":"<pre><code># Reload systemd to recognize the new service\nsystemctl --user daemon-reload\n\n# Enable the service to start on boot\nsystemctl --user enable krakenhashes-agent\n\n# Start the service now\nsystemctl --user start krakenhashes-agent\n\n# Check status\nsystemctl --user status krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#4-enable-lingering-optional","title":"4. Enable Lingering (Optional)","text":"<p>To start the service at boot (before you log in):</p> <pre><code>sudo loginctl enable-linger $USER\n</code></pre>"},{"location":"agent-guide/systemd-setup/#managing-user-services","title":"Managing User Services","text":"<pre><code># View logs\njournalctl --user -u krakenhashes-agent -f\n\n# Stop the service\nsystemctl --user stop krakenhashes-agent\n\n# Restart the service\nsystemctl --user restart krakenhashes-agent\n\n# Disable auto-start\nsystemctl --user disable krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#system-service-setup-advanced-requires-root","title":"System Service Setup (Advanced - Requires Root)","text":"<p>System services run at the system level and require root/sudo access. This approach is typically used for production servers where the agent needs to run before any user logs in.</p> <p>Note: Most users should use the User Service setup above. Only use system services if you specifically need the agent to run at boot before login.</p>"},{"location":"agent-guide/systemd-setup/#1-create-a-dedicated-user-optional-but-recommended","title":"1. Create a Dedicated User (Optional but Recommended)","text":"<pre><code>sudo useradd -r -s /bin/false -d /var/lib/krakenhashes -m krakenhashes\n</code></pre>"},{"location":"agent-guide/systemd-setup/#2-install-the-agent","title":"2. Install the Agent","text":"<pre><code># Create directory structure\nsudo mkdir -p /opt/krakenhashes-agent\nsudo chown krakenhashes:krakenhashes /opt/krakenhashes-agent\n\n# Copy agent binary from your download location\nsudo cp ~/krakenhashes-agent/krakenhashes-agent /opt/krakenhashes-agent/\nsudo chown krakenhashes:krakenhashes /opt/krakenhashes-agent/krakenhashes-agent\nsudo chmod +x /opt/krakenhashes-agent/krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#3-create-configuration","title":"3. Create Configuration","text":"<p>Create <code>/opt/krakenhashes-agent/.env</code> with your configuration:</p> <pre><code>sudo -u krakenhashes tee /opt/krakenhashes-agent/.env &gt; /dev/null &lt;&lt;EOF\n# Agent configuration\nKH_HOST=your-server.example.com\nKH_PORT=31337\nUSE_TLS=true\nKH_CLAIM_CODE=YOUR-CLAIM-CODE-HERE\nKH_CONFIG_DIR=/opt/krakenhashes-agent/config\nKH_DATA_DIR=/opt/krakenhashes-agent/data\n# Add other configuration as needed\nEOF\n</code></pre>"},{"location":"agent-guide/systemd-setup/#4-create-system-service-file","title":"4. Create System Service File","text":"<p>Create <code>/etc/systemd/system/krakenhashes-agent.service</code>:</p> <pre><code>[Unit]\nDescription=KrakenHashes Agent\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=krakenhashes\nGroup=krakenhashes\nWorkingDirectory=/opt/krakenhashes-agent\nExecStart=/opt/krakenhashes-agent/krakenhashes-agent\nRestart=always\nRestartSec=10\n\n# Security hardening (optional)\nNoNewPrivileges=true\nPrivateTmp=true\nProtectSystem=strict\nProtectHome=true\nReadWritePaths=/opt/krakenhashes-agent\n\n# Resource limits (optional)\n# MemoryLimit=4G\n# CPUQuota=80%\n\n# Environment (optional - .env file is preferred)\n# Environment=\"KH_DATA_DIR=/opt/krakenhashes-agent/data\"\n# Environment=\"KH_CONFIG_DIR=/opt/krakenhashes-agent/config\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"agent-guide/systemd-setup/#5-enable-and-start-the-service","title":"5. Enable and Start the Service","text":"<pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable service to start on boot\nsudo systemctl enable krakenhashes-agent\n\n# Start the service\nsudo systemctl start krakenhashes-agent\n\n# Check status\nsudo systemctl status krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#managing-system-services","title":"Managing System Services","text":"<pre><code># View logs\nsudo journalctl -u krakenhashes-agent -f\n\n# Stop the service\nsudo systemctl stop krakenhashes-agent\n\n# Restart the service\nsudo systemctl restart krakenhashes-agent\n\n# Disable auto-start\nsudo systemctl disable krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/systemd-setup/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"agent-guide/systemd-setup/#using-environment-files","title":"Using Environment Files","text":"<p>Instead of hardcoding environment variables in the service file, you can use a separate environment file:</p> <ol> <li> <p>Create an environment file (note: different from .env): <pre><code># For user service: ~/.config/krakenhashes-agent.env\n# For system service: /etc/krakenhashes-agent.env\n\nDEBUG=false\nLOG_LEVEL=INFO\n# Don't include sensitive data here as it may be world-readable\n</code></pre></p> </li> <li> <p>Reference it in the service file: <pre><code>[Service]\nEnvironmentFile=/path/to/environment/file\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/systemd-setup/#resource-limits","title":"Resource Limits","text":"<p>Control agent resource usage:</p> <pre><code>[Service]\n# Limit memory usage\nMemoryLimit=4G\nMemoryAccounting=true\n\n# Limit CPU usage (percentage)\nCPUQuota=80%\nCPUAccounting=true\n\n# Limit number of tasks/threads\nTasksMax=100\n</code></pre>"},{"location":"agent-guide/systemd-setup/#automatic-restart-configuration","title":"Automatic Restart Configuration","text":"<pre><code>[Service]\n# Restart on failure\nRestart=on-failure\nRestartSec=10\n\n# Or always restart (even on clean exit)\nRestart=always\nRestartSec=10\n\n# Limit restart attempts\nStartLimitInterval=600\nStartLimitBurst=5\n</code></pre>"},{"location":"agent-guide/systemd-setup/#gpu-access-for-system-services","title":"GPU Access for System Services","text":"<p>If running as a system service with GPU access:</p> <pre><code>[Service]\n# Add the service user to the video/render groups\nSupplementaryGroups=video render\n\n# Or for NVIDIA GPUs specifically\nSupplementaryGroups=video\n# May need to adjust device permissions\nDeviceAllow=/dev/nvidia* rw\nDeviceAllow=/dev/nvidiactl rw\nDeviceAllow=/dev/nvidia-uvm rw\n</code></pre>"},{"location":"agent-guide/systemd-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agent-guide/systemd-setup/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Service fails to start: Check logs with <code>journalctl --user -u krakenhashes-agent</code> (user) or <code>sudo journalctl -u krakenhashes-agent</code> (system)</p> </li> <li> <p>Permission denied errors: </p> </li> <li>User service: Ensure the agent binary is executable and in your home directory</li> <li> <p>System service: Check file ownership and permissions</p> </li> <li> <p>Agent can't find .env file:</p> </li> <li>Ensure WorkingDirectory is set correctly in the service file</li> <li> <p>Check that the .env file exists in that directory</p> </li> <li> <p>GPU not detected:</p> </li> <li>User service: Should work if you can access GPU normally</li> <li>System service: May need SupplementaryGroups configuration</li> </ol>"},{"location":"agent-guide/systemd-setup/#viewing-logs","title":"Viewing Logs","text":"<pre><code># User service - last 100 lines\njournalctl --user -u krakenhashes-agent -n 100\n\n# System service - last 100 lines  \nsudo journalctl -u krakenhashes-agent -n 100\n\n# Follow logs in real-time\njournalctl --user -u krakenhashes-agent -f  # User\nsudo journalctl -u krakenhashes-agent -f     # System\n\n# Logs from last boot\njournalctl --user -u krakenhashes-agent -b  # User\nsudo journalctl -u krakenhashes-agent -b     # System\n\n# Export logs to file\njournalctl --user -u krakenhashes-agent &gt; agent.log  # User\nsudo journalctl -u krakenhashes-agent &gt; agent.log     # System\n</code></pre>"},{"location":"agent-guide/systemd-setup/#service-status-commands","title":"Service Status Commands","text":"<pre><code># Check if service is active\nsystemctl --user is-active krakenhashes-agent   # User\nsudo systemctl is-active krakenhashes-agent      # System\n\n# Check if service is enabled\nsystemctl --user is-enabled krakenhashes-agent  # User  \nsudo systemctl is-enabled krakenhashes-agent     # System\n\n# Show service details\nsystemctl --user show krakenhashes-agent        # User\nsudo systemctl show krakenhashes-agent           # System\n</code></pre>"},{"location":"agent-guide/systemd-setup/#migration-between-service-types","title":"Migration Between Service Types","text":""},{"location":"agent-guide/systemd-setup/#from-manual-to-user-service","title":"From Manual to User Service","text":"<ol> <li>Stop the manual agent process</li> <li>Copy your existing <code>.env</code> file to the agent directory</li> <li>Follow the user service setup steps above</li> <li>Start the user service</li> </ol>"},{"location":"agent-guide/systemd-setup/#from-user-service-to-system-service","title":"From User Service to System Service","text":"<ol> <li>Stop the user service: <code>systemctl --user stop krakenhashes-agent</code></li> <li>Disable the user service: <code>systemctl --user disable krakenhashes-agent</code></li> <li>Copy your agent files to system location</li> <li>Follow the system service setup steps above</li> <li>Start the system service</li> </ol>"},{"location":"agent-guide/systemd-setup/#best-practices","title":"Best Practices","text":"<ol> <li>Use user services when possible - they're simpler and don't require root</li> <li>Keep the .env file in the same directory as specified in WorkingDirectory</li> <li>Set resource limits to prevent the agent from consuming too many resources</li> <li>Monitor logs regularly to catch issues early</li> <li>Use enable-linger for user services that should run without login</li> <li>Document your configuration including any custom paths or settings</li> </ol>"},{"location":"agent-guide/systemd-setup/#next-steps","title":"Next Steps","text":"<ul> <li>Configure the agent</li> <li>Set up scheduling</li> <li>Monitor agent performance</li> </ul>"},{"location":"agent-guide/troubleshooting/","title":"Agent Troubleshooting Guide","text":"<p>This guide helps diagnose and resolve common issues with KrakenHashes agents. Use this reference when agents fail to connect, register, sync files, detect hardware, or execute jobs.</p>"},{"location":"agent-guide/troubleshooting/#quick-diagnostic-commands","title":"Quick Diagnostic Commands","text":"<p>Before diving into specific issues, run these commands to gather diagnostic information:</p> <pre><code># Check agent status\nsystemctl status krakenhashes-agent\n\n# View recent agent logs\njournalctl -u krakenhashes-agent -f --since \"5 minutes ago\"\n\n# Check agent configuration\n/path/to/krakenhashes-agent --version\ncat ~/.krakenhashes/agent/.env\n\n# Test connectivity to backend\ncurl -k https://your-backend:31337/api/health\n\n# Verify certificate files\nls -la ~/.krakenhashes/agent/config/\nopenssl x509 -in ~/.krakenhashes/agent/config/client.crt -text -noout\n</code></pre>"},{"location":"agent-guide/troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"agent-guide/troubleshooting/#agent-cannot-connect-to-backend","title":"Agent Cannot Connect to Backend","text":"<p>Symptoms: - Agent logs show \"failed to connect to WebSocket server\" - Repeated connection retry attempts - Certificate verification errors</p> <p>Common Causes:</p> <ol> <li> <p>Incorrect Backend URL Configuration <pre><code># Check agent configuration\ngrep -E \"KH_HOST|KH_PORT\" ~/.krakenhashes/agent/.env\n\n# Test backend accessibility\nping your-backend-host\ntelnet your-backend-host 31337\n</code></pre></p> </li> <li> <p>Certificate Issues <pre><code># Check certificate files exist\nls -la ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Verify certificate validity\nopenssl x509 -in ~/.krakenhashes/agent/config/client.crt -text -noout | grep -E \"Valid|Subject|Issuer\"\n</code></pre></p> </li> <li> <p>Network Firewall Blocking <pre><code># Test HTTPS connectivity\ncurl -k https://your-backend:31337/api/health\n\n# Test WebSocket connectivity (if nc available)\nnc -zv your-backend 31337\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li> <p>Update Backend URL <pre><code># Edit agent configuration\nnano ~/.krakenhashes/agent/.env\n\n# Set correct values\nKH_HOST=your-backend-hostname\nKH_PORT=31337\nUSE_TLS=true\n\n# Restart agent\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> <li> <p>Renew Certificates <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Remove old certificates\nrm ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Start agent (will automatically renew certificates)\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> <li> <p>Fix Network/Firewall <pre><code># Check firewall rules\nsudo ufw status\nsudo iptables -L\n\n# Open required ports\nsudo ufw allow out 31337\nsudo ufw allow out 443\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#connection-drops-frequently","title":"Connection Drops Frequently","text":"<p>Symptoms: - Agent connects but disconnects after short periods - WebSocket ping/pong timeouts - Frequent reconnection attempts</p> <p>Causes and Solutions:</p> <ol> <li> <p>Network Instability <pre><code># Monitor network quality\nping -c 10 your-backend-host\n\n# Check for packet loss\nmtr your-backend-host\n</code></pre></p> </li> <li> <p>Backend Overload <pre><code># Check backend logs for resource issues\ndocker-compose -f docker-compose.dev-local.yml logs backend | grep -i \"error\\|timeout\\|overload\"\n</code></pre></p> </li> <li> <p>Aggressive Firewall/NAT <pre><code># Adjust WebSocket keepalive settings in agent config\necho \"KH_PING_PERIOD=30s\" &gt;&gt; ~/.krakenhashes/agent/.env\necho \"KH_PONG_WAIT=60s\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#registration-and-authentication-issues","title":"Registration and Authentication Issues","text":""},{"location":"agent-guide/troubleshooting/#agent-registration-fails","title":"Agent Registration Fails","text":"<p>Symptoms: - \"Registration failed\" errors - \"Invalid claim code\" messages - \"Registration request failed\" in logs</p> <p>Common Causes:</p> <ol> <li>Invalid or Expired Claim Code</li> <li>Check admin panel for active vouchers</li> <li> <p>Generate new voucher if expired</p> </li> <li> <p>Certificate Download Issues <pre><code># Test CA certificate download\ncurl -k https://your-backend:31337/ca.crt -o /tmp/ca.crt\nopenssl x509 -in /tmp/ca.crt -text -noout\n</code></pre></p> </li> <li> <p>Clock Synchronization Issues <pre><code># Check system time\ntimedatectl status\n\n# Sync time if needed\nsudo ntpdate -s time.nist.gov\n# or\nsudo chrony sources -v\n</code></pre></p> </li> </ol> <p>Solutions:</p> <ol> <li>Get Valid Claim Code</li> <li>Access backend admin panel</li> <li>Go to Agent Management \u2192 Generate Voucher</li> <li> <p>Use the new claim code immediately</p> </li> <li> <p>Manual Registration <pre><code># Stop agent service\nsystemctl stop krakenhashes-agent\n\n# Register manually\n/path/to/krakenhashes-agent --register --claim-code YOUR_CLAIM_CODE --host your-backend:31337\n\n# Start service\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#authentication-errors-after-registration","title":"Authentication Errors After Registration","text":"<p>Symptoms: - \"Failed to load API key\" errors - \"Authentication failed\" messages - Agent connected but backend rejects requests</p> <p>Diagnostic Steps: <pre><code># Check credentials files\nls -la ~/.krakenhashes/agent/config/\ncat ~/.krakenhashes/agent/config/agent.key\n\n# Verify API key format (should be UUID)\ngrep -E '^[0-9a-f-]{36}:[0-9]+$' ~/.krakenhashes/agent/config/agent.key\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Regenerate Credentials <pre><code># Remove existing credentials\nrm ~/.krakenhashes/agent/config/agent.key\nrm ~/.krakenhashes/agent/config/*.crt ~/.krakenhashes/agent/config/*.key\n\n# Re-register\nsystemctl stop krakenhashes-agent\n/path/to/krakenhashes-agent --register --claim-code NEW_CLAIM_CODE --host your-backend:31337\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> <li> <p>Fix Permissions <pre><code># Set correct ownership and permissions\nchown -R $(whoami):$(whoami) ~/.krakenhashes/agent/\nchmod 700 ~/.krakenhashes/agent/config/\nchmod 600 ~/.krakenhashes/agent/config/agent.key\nchmod 600 ~/.krakenhashes/agent/config/client.key\nchmod 644 ~/.krakenhashes/agent/config/*.crt\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#hardware-detection-issues","title":"Hardware Detection Issues","text":""},{"location":"agent-guide/troubleshooting/#no-devices-detected","title":"No Devices Detected","text":"<p>Symptoms: - Agent shows \"0 devices detected\" - Missing GPU information in admin panel - Hashcat fails to find OpenCL/CUDA devices</p> <p>Diagnostic Steps: <pre><code># Check if hashcat binary exists\nls -la ~/.krakenhashes/agent/data/binaries/\n\n# Manually test hashcat device detection\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1 | xargs -I {} {} -I\n\n# Check for GPU drivers\nnvidia-smi           # NVIDIA\nrocm-smi             # AMD\nintel_gpu_top        # Intel\nlspci | grep -i vga  # General\n</code></pre></p> <p>Common Solutions:</p> <ol> <li> <p>Install GPU Drivers <pre><code># NVIDIA\nsudo apt update\nsudo apt install nvidia-driver-470  # or latest\n\n# AMD\nsudo apt install rocm-opencl-runtime\n\n# Intel\nsudo apt install intel-opencl-icd\n</code></pre></p> </li> <li> <p>Install OpenCL Runtime <pre><code># Install generic OpenCL\nsudo apt install ocl-icd-opencl-dev opencl-headers\n\n# Verify OpenCL installation\nclinfo  # if available\n</code></pre></p> </li> <li> <p>Fix Hashcat Binary Issues <pre><code># Check hashcat binary permissions\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs ls -la\n\n# Make executable if needed\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs chmod +x\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#partial-device-detection","title":"Partial Device Detection","text":"<p>Symptoms: - Some GPUs detected, others missing - Device count mismatch - Specific GPU types not showing</p> <p>Solutions:</p> <ol> <li> <p>Mixed GPU Environment <pre><code># Ensure all necessary drivers installed\nnvidia-smi &amp;&amp; rocm-smi &amp;&amp; intel_gpu_top --list\n\n# Check for driver conflicts\ndmesg | grep -i \"gpu\\|nvidia\\|amd\\|intel\" | tail -20\n</code></pre></p> </li> <li> <p>PCIe/Power Issues <pre><code># Check PCIe slot detection\nlspci | grep -i vga\nsudo lshw -c display\n\n# Check power management\ncat /sys/class/drm/card*/device/power_state\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#file-synchronization-problems","title":"File Synchronization Problems","text":""},{"location":"agent-guide/troubleshooting/#files-not-downloading","title":"Files Not Downloading","text":"<p>Symptoms: - Wordlists/rules not available for jobs - \"File not found\" errors during job execution - Sync requests timing out</p> <p>Diagnostic Steps: <pre><code># Check data directories\nls -la ~/.krakenhashes/agent/data/\nls ~/.krakenhashes/agent/data/wordlists/\nls ~/.krakenhashes/agent/data/rules/\nls ~/.krakenhashes/agent/data/binaries/\n\n# Test file download manually\ncurl -k -H \"X-API-Key: YOUR_API_KEY\" -H \"X-Agent-ID: YOUR_AGENT_ID\" \\\n     https://your-backend:31337/api/agent/files/wordlists/rockyou.txt \\\n     -o /tmp/test_download.txt\n</code></pre></p> <p>Common Solutions:</p> <ol> <li> <p>Fix Authentication <pre><code># Verify API key is valid\ngrep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1\n\n# Test API authentication\nAPI_KEY=$(grep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1)\nAGENT_ID=$(grep -o '[^:]*$' ~/.krakenhashes/agent/config/agent.key)\ncurl -k -H \"X-API-Key: $API_KEY\" -H \"X-Agent-ID: $AGENT_ID\" \\\n     https://your-backend:31337/api/agent/info\n</code></pre></p> </li> <li> <p>Fix Directory Permissions <pre><code># Ensure agent can write to data directories\nchown -R $(whoami):$(whoami) ~/.krakenhashes/agent/data/\nchmod -R 755 ~/.krakenhashes/agent/data/\n</code></pre></p> </li> <li> <p>Clear Corrupted Downloads <pre><code># Remove partial/corrupted files\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -delete\nfind ~/.krakenhashes/agent/data/ -size 0 -delete\n\n# Force re-sync\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#binary-extraction-failures","title":"Binary Extraction Failures","text":"<p>Symptoms: - Downloaded .7z files not extracted - Hashcat binary not executable - \"No such file or directory\" when running hashcat</p> <p>Solutions:</p> <ol> <li> <p>Install 7-Zip Support <pre><code>sudo apt install p7zip-full\n\n# Test extraction manually\ncd ~/.krakenhashes/agent/data/binaries/\nfind . -name \"*.7z\" | head -1 | xargs 7z t  # Test archive\n</code></pre></p> </li> <li> <p>Fix Extraction Permissions <pre><code># Ensure extraction destination is writable\nchmod 755 ~/.krakenhashes/agent/data/binaries/\n\n# Re-extract manually if needed\ncd ~/.krakenhashes/agent/data/binaries/\nfind . -name \"*.7z\" -exec 7z x {} \\;\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#job-execution-failures","title":"Job Execution Failures","text":""},{"location":"agent-guide/troubleshooting/#jobs-not-starting","title":"Jobs Not Starting","text":"<p>Symptoms: - Tasks assigned but never start - Agent shows as idle despite task assignment - \"No enabled devices\" errors</p> <p>Diagnostic Steps: <pre><code># Check agent task status\njournalctl -u krakenhashes-agent | grep -i \"task\\|job\" | tail -10\n\n# Verify enabled devices in backend\n# (Check admin panel Agent Details page)\n\n# Test hashcat manually\nHASHCAT=$(find ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1)\n$HASHCAT --help\n</code></pre></p> <p>Solutions:</p> <ol> <li>Enable Devices</li> <li>Go to backend Admin Panel</li> <li>Navigate to Agent Management</li> <li> <p>Select agent and enable required devices</p> </li> <li> <p>Fix Hashcat Path <pre><code># Ensure hashcat binary is executable\nfind ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f | xargs chmod +x\n\n# Create symlink if needed\nHASHCAT=$(find ~/.krakenhashes/agent/data/binaries -name \"hashcat*\" -type f -executable | head -1)\nsudo ln -sf \"$HASHCAT\" /usr/local/bin/hashcat\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#jobs-crash-or-stop-unexpectedly","title":"Jobs Crash or Stop Unexpectedly","text":"<p>Symptoms: - Jobs start but terminate quickly - \"Process killed\" messages - Hashcat segmentation faults</p> <p>Diagnostic Steps: <pre><code># Check system resources\nfree -h\ndf -h ~/.krakenhashes/agent/data/\nps aux | grep hashcat\n\n# Check for OOM kills\ndmesg | grep -i \"killed process\\|out of memory\" | tail -5\njournalctl -f | grep -i \"oom\\|memory\"\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Resource Issues <pre><code># Check memory usage\nfree -h\n\n# Clear cache if needed\nsudo sync\necho 3 | sudo tee /proc/sys/vm/drop_caches\n\n# Check disk space\ndf -h ~/.krakenhashes/agent/data/\n\n# Clean old files if needed\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -mtime +7 -delete\n</code></pre></p> </li> <li> <p>Driver/Hardware Issues <pre><code># Check GPU status\nnvidia-smi  # Check temperature, power, utilization\n\n# Test memory stability\nnvidia-smi --query-gpu=memory.used,memory.free,temperature.gpu --format=csv -lms 1000\n\n# Check for hardware errors\ndmesg | grep -i \"error\\|fault\" | tail -10\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#job-progress-not-reporting","title":"Job Progress Not Reporting","text":"<p>Symptoms: - Jobs running but no progress updates - Backend shows tasks as \"running\" indefinitely - No crack notifications</p> <p>Solutions:</p> <ol> <li> <p>Check WebSocket Connection <pre><code># Verify agent is connected\njournalctl -u krakenhashes-agent | grep -i \"websocket\\|connected\" | tail -5\n\n# Look for progress send errors\njournalctl -u krakenhashes-agent | grep -i \"progress\\|send.*fail\" | tail -10\n</code></pre></p> </li> <li> <p>Restart Agent Connection <pre><code># Restart agent service\nsystemctl restart krakenhashes-agent\n\n# Monitor connection establishment\njournalctl -u krakenhashes-agent -f | grep -i \"connect\\|progress\"\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#performance-problems","title":"Performance Problems","text":""},{"location":"agent-guide/troubleshooting/#slow-hash-rates","title":"Slow Hash Rates","text":"<p>Symptoms: - Lower than expected H/s rates - GPU underutilization - Benchmark speeds don't match job speeds</p> <p>Solutions:</p> <ol> <li> <p>GPU Optimization <pre><code># Check GPU power limits\nnvidia-smi -q -d POWER\n\n# Increase power limit (if supported)\nsudo nvidia-smi -pl 300  # 300W example\n\n# Set performance mode\nsudo nvidia-smi -pm 1\n</code></pre></p> </li> <li> <p>Cooling and Throttling <pre><code># Monitor temperatures\nwatch nvidia-smi\n\n# Check thermal throttling\nnvidia-smi --query-gpu=temperature.gpu,clocks_throttle_reasons.gpu_idle,clocks_throttle_reasons.applications_clocks_setting --format=csv -lms 1000\n</code></pre></p> </li> <li> <p>Hashcat Parameters <pre><code># Add optimization flags in agent config\necho \"HASHCAT_EXTRA_PARAMS=-O -w 4\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#high-system-load","title":"High System Load","text":"<p>Symptoms: - System becomes unresponsive - Other applications slow down - CPU usage constantly high</p> <p>Solutions:</p> <ol> <li> <p>Limit Resource Usage <pre><code># Limit hashcat workload\necho \"HASHCAT_EXTRA_PARAMS=-w 2\" &gt;&gt; ~/.krakenhashes/agent/.env\n\n# Set CPU affinity (example: use only cores 0-3)\nsystemctl edit krakenhashes-agent\n# Add:\n# [Service]\n# CPUAffinity=0-3\n</code></pre></p> </li> <li> <p>System Tuning <pre><code># Increase file descriptor limits\necho \"* soft nofile 65536\" | sudo tee -a /etc/security/limits.conf\necho \"* hard nofile 65536\" | sudo tee -a /etc/security/limits.conf\n\n# Optimize memory management\necho 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf\nsudo sysctl -p\n</code></pre></p> </li> </ol>"},{"location":"agent-guide/troubleshooting/#error-message-reference","title":"Error Message Reference","text":""},{"location":"agent-guide/troubleshooting/#common-error-patterns","title":"Common Error Patterns","text":"Error Message Cause Solution <code>failed to connect to WebSocket server</code> Network/TLS issues Check connectivity, renew certificates <code>failed to load API key</code> Missing/corrupt credentials Re-register agent <code>registration failed</code> Invalid claim code Generate new voucher <code>failed to detect devices</code> Missing drivers/OpenCL Install GPU drivers <code>no enabled devices</code> Devices disabled in backend Enable devices in admin panel <code>file sync timeout</code> Network/authentication issues Check API credentials <code>hashcat not found</code> Missing/corrupt binary Re-download binaries <code>certificate verify failed</code> Expired/invalid certificates Renew certificates <code>connection refused</code> Backend not accessible Check backend status <code>permission denied</code> File/directory permissions Fix ownership/permissions"},{"location":"agent-guide/troubleshooting/#debug-logging","title":"Debug Logging","text":"<p>Enable detailed logging for troubleshooting:</p> <pre><code># Enable debug logging\necho \"DEBUG=true\" &gt;&gt; ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n\n# View detailed logs\njournalctl -u krakenhashes-agent -f\n\n# Disable debug logging after troubleshooting\nsed -i '/DEBUG=true/d' ~/.krakenhashes/agent/.env\nsystemctl restart krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#recovery-procedures","title":"Recovery Procedures","text":""},{"location":"agent-guide/troubleshooting/#complete-agent-reset","title":"Complete Agent Reset","text":"<p>When all else fails, completely reset the agent:</p> <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Backup current configuration\ncp -r ~/.krakenhashes/agent ~/.krakenhashes/agent.backup.$(date +%Y%m%d)\n\n# Remove all agent data\nrm -rf ~/.krakenhashes/agent/\n\n# Re-register with new claim code\n/path/to/krakenhashes-agent --register --claim-code NEW_CLAIM_CODE --host your-backend:31337\n\n# Start agent\nsystemctl start krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#emergency-job-cleanup","title":"Emergency Job Cleanup","text":"<p>Force cleanup of stuck hashcat processes:</p> <pre><code># Kill all hashcat processes\npkill -f hashcat\n\n# Clean temporary files\nfind ~/.krakenhashes/agent/data/ -name \"*.tmp\" -delete\nfind ~/.krakenhashes/agent/data/ -name \"*.restore\" -delete\n\n# Restart agent to reset job state\nsystemctl restart krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#certificate-recovery","title":"Certificate Recovery","text":"<p>Recover from certificate issues:</p> <pre><code># Stop agent\nsystemctl stop krakenhashes-agent\n\n# Download CA certificate manually\ncurl -k https://your-backend:31337/ca.crt -o ~/.krakenhashes/agent/config/ca.crt\n\n# Use API key to renew client certificates\nAPI_KEY=$(grep -o '^[^:]*' ~/.krakenhashes/agent/config/agent.key | head -1)\nAGENT_ID=$(grep -o '[^:]*$' ~/.krakenhashes/agent/config/agent.key)\ncurl -k -X POST -H \"X-API-Key: $API_KEY\" -H \"X-Agent-ID: $AGENT_ID\" \\\n     https://your-backend:31337/api/agent/renew-certificates\n\n# Start agent\nsystemctl start krakenhashes-agent\n</code></pre>"},{"location":"agent-guide/troubleshooting/#when-to-restart-vs-reinstall","title":"When to Restart vs Reinstall","text":""},{"location":"agent-guide/troubleshooting/#restart-agent-service","title":"Restart Agent Service","text":"<ul> <li>Connection drops</li> <li>Configuration changes</li> <li>Minor authentication issues</li> <li>After enabling/disabling devices</li> </ul>"},{"location":"agent-guide/troubleshooting/#restart-system","title":"Restart System","text":"<ul> <li>GPU driver updates</li> <li>System resource exhaustion</li> <li>Hardware changes</li> <li>Kernel updates</li> </ul>"},{"location":"agent-guide/troubleshooting/#reinstall-agent","title":"Reinstall Agent","text":"<ul> <li>Corrupt binary files</li> <li>Persistent authentication failures after certificate renewal</li> <li>File system permission issues that can't be resolved</li> <li>Agent binary corruption</li> </ul>"},{"location":"agent-guide/troubleshooting/#complete-reset-last-resort","title":"Complete Reset (Last Resort)","text":"<ul> <li>Multiple interconnected issues</li> <li>System contamination from previous installations</li> <li>Unknown configuration corruption</li> <li>When restart and reinstall don't resolve issues</li> </ul> <p>Use the diagnostic commands at the beginning of this guide to determine the appropriate recovery level.</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/","title":"Implementation Summary: Benchmark-Based Job Assignment","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#overview","title":"Overview","text":"<p>Updated the backend's job scheduling service to wait for agent benchmarks before assigning work. This ensures accurate chunk calculations based on real-world performance metrics.</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#key-changes","title":"Key Changes","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#1-jobschedulingservice-backendinternalservicesjob_scheduling_servicego","title":"1. JobSchedulingService (<code>backend/internal/services/job_scheduling_service.go</code>)","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#updated-interface","title":"Updated Interface","text":"<pre><code>type JobWebSocketIntegration interface {\n    SendJobAssignment(ctx context.Context, task *models.JobTask, jobExecution *models.JobExecution) error\n    RequestAgentBenchmark(ctx context.Context, agentID int, jobExecution *models.JobExecution) error\n}\n</code></pre>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#modified-assignworktoagent-function","title":"Modified <code>assignWorkToAgent</code> Function","text":"<ul> <li>Added benchmark validation before calculating chunks</li> <li>Retrieves hashlist to get hash type for benchmark lookup</li> <li>Checks if agent has a valid benchmark for the attack mode/hash type combination</li> <li>Validates benchmark freshness using configurable cache duration</li> <li>If no valid benchmark exists:</li> <li>Sends benchmark request via WebSocket</li> <li>Returns without assigning work (deferred assignment)</li> <li>Agent remains available for next scheduling cycle</li> <li>If valid benchmark exists:</li> <li>Proceeds with normal chunk calculation and assignment</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#2-jobwebsocketintegration-backendinternalintegrationjob_websocket_integrationgo","title":"2. JobWebSocketIntegration (<code>backend/internal/integration/job_websocket_integration.go</code>)","text":""},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#new-method-requestagentbenchmark","title":"New Method: <code>RequestAgentBenchmark</code>","text":"<ul> <li>Implements the new interface method</li> <li>Retrieves full job configuration:</li> <li>Preset job details (binary version, wordlists, rules, mask)</li> <li>Hashlist details (hash type)</li> <li>Agent information</li> <li>Builds enhanced benchmark request with:</li> <li>Actual wordlist paths from the job</li> <li>Rule files if applicable</li> <li>Binary path for the specific version</li> <li>Hash type and attack mode</li> <li>Test duration (30 seconds for accuracy)</li> <li>Sends comprehensive benchmark request to agent</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#3-websocket-types-already-existed-enhanced-usage","title":"3. WebSocket Types (already existed, enhanced usage)","text":"<p>The <code>BenchmarkRequestPayload</code> now includes: - <code>HashlistID</code> and <code>HashlistPath</code> for real-world testing - <code>WordlistPaths</code> array for dictionary attacks - <code>RulePaths</code> array for rule-based attacks - <code>Mask</code> for brute force patterns - <code>TestDuration</code> for benchmark duration</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#workflow","title":"Workflow","text":"<ol> <li>Agent requests work \u2192 Scheduler finds pending job</li> <li>Benchmark check \u2192 System verifies agent has valid benchmark</li> <li>If no benchmark:</li> <li>Request benchmark with full job config</li> <li>Agent performs real-world speed test</li> <li>Reports results back</li> <li>Next scheduling cycle assigns work</li> <li>If valid benchmark exists:</li> <li>Calculate chunk based on known performance</li> <li>Assign work immediately</li> </ol>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#benefits","title":"Benefits","text":"<ul> <li>Accurate Performance: Benchmarks use actual job parameters</li> <li>Optimal Chunks: Prevents over/under-sized work assignments</li> <li>Reduced Failures: Avoids assigning impossible workloads</li> <li>Better Utilization: Maximizes agent efficiency</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#configuration","title":"Configuration","text":"<ul> <li><code>benchmark_cache_duration_hours</code>: Benchmark validity period (default: 168 hours)</li> <li><code>default_chunk_duration</code>: Target chunk duration in seconds (default: 1200)</li> <li><code>chunk_fluctuation_percentage</code>: Final chunk size tolerance (default: 20%)</li> </ul>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#testing","title":"Testing","text":"<p>Created unit tests demonstrating: - Benchmark request flow when no benchmark exists - Job assignment flow with valid benchmark - Mock WebSocket integration for testing</p>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#files-modified","title":"Files Modified","text":"<ol> <li><code>/backend/internal/services/job_scheduling_service.go</code></li> <li><code>/backend/internal/integration/job_websocket_integration.go</code></li> <li><code>/backend/internal/services/job_scheduling_benchmark_test.go</code> (new)</li> <li><code>/backend/BENCHMARK_WORKFLOW.md</code> (new documentation)</li> </ol>"},{"location":"archive/development-notes/IMPLEMENTATION_SUMMARY/#future-considerations","title":"Future Considerations","text":"<ul> <li>Benchmark history tracking</li> <li>Performance anomaly detection</li> <li>Multi-GPU per-device benchmarking</li> <li>Predictive performance modeling</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/","title":"Efficient Paginated Polling Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#overview","title":"Overview","text":"<p>This document describes the implementation of an efficient paginated polling system for the Jobs page, replacing the SSE (Server-Sent Events) approach with a more scalable solution.</p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#key-features","title":"Key Features","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#1-paginated-data-fetching","title":"1. Paginated Data Fetching","text":"<ul> <li>Only fetches data for the current page (e.g., 25 jobs per page)</li> <li>Reduces network traffic and server load</li> <li>Faster response times for large job lists</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#2-advanced-filtering","title":"2. Advanced Filtering","text":"<ul> <li>Status Filter: Filter by pending, running, completed, failed, or interrupted</li> <li>Priority Filter: Filter by priority levels (1-5)</li> <li>Search: Search in job names and hashlist names</li> <li>All filters work with pagination</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#3-smart-polling","title":"3. Smart Polling","text":"<ul> <li>Polls every 5 seconds by default</li> <li>Only fetches current page with active filters</li> <li>Can be toggled on/off by users</li> <li>Cancels in-flight requests when parameters change</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#4-status-counts","title":"4. Status Counts","text":"<ul> <li>Shows badge counts for each status</li> <li>Updates with each poll</li> <li>Helps users understand job distribution</li> </ul>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#backend-implementation","title":"Backend Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#repository-layer-job_execution_repository_extensiongo","title":"Repository Layer (<code>job_execution_repository_extension.go</code>)","text":"<pre><code>// New filter structure\ntype JobFilter struct {\n    Status   *string\n    Priority *int\n    Search   *string\n}\n\n// Key methods added:\n- ListWithFilters()      // Paginated list with filters\n- GetFilteredCount()     // Count matching filter criteria\n- GetStatusCounts()      // Get counts by status\n</code></pre>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#handler-layer-user_jobsgo","title":"Handler Layer (<code>user_jobs.go</code>)","text":"<p>The <code>/api/jobs</code> endpoint now supports: - <code>page</code> - Page number (default: 1) - <code>page_size</code> - Items per page (default: 25, max: 200) - <code>status</code> - Filter by job status - <code>priority</code> - Filter by priority (1-5) - <code>search</code> - Search in job/hashlist names</p> <p>Response includes: <pre><code>{\n  \"jobs\": [...],\n  \"pagination\": {\n    \"page\": 1,\n    \"page_size\": 25,\n    \"total\": 150,\n    \"total_pages\": 6\n  },\n  \"status_counts\": {\n    \"pending\": 10,\n    \"running\": 5,\n    \"completed\": 120,\n    \"failed\": 10,\n    \"interrupted\": 5\n  }\n}\n</code></pre></p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#frontend-implementation","title":"Frontend Implementation","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#jobs-page-frontendsrcpagesjobsindextsx","title":"Jobs Page (<code>/frontend/src/pages/Jobs/index.tsx</code>)","text":"<p>Key features: 1. State Management    - Separate state for pagination, filters, and data    - Maintains user's position during polls</p> <ol> <li>Efficient Polling</li> <li>Uses <code>setInterval</code> with 5-second intervals</li> <li>Cancels previous requests using <code>AbortController</code></li> <li> <p>Only shows loading on initial load or manual refresh</p> </li> <li> <p>User Controls</p> </li> <li>Toggle auto-refresh on/off</li> <li>Manual refresh button</li> <li>Page size selector</li> <li>Status filter buttons with counts</li> <li>Priority dropdown filter</li> <li> <p>Search field</p> </li> <li> <p>Performance Optimizations</p> </li> <li>Debounced search input</li> <li>Memoized query building</li> <li>Proper cleanup on unmount</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#benefits-over-sse","title":"Benefits Over SSE","text":"<ol> <li>Scalability</li> <li>No persistent connections</li> <li>Reduced server memory usage</li> <li> <p>Works better with load balancers</p> </li> <li> <p>Efficiency</p> </li> <li>Only fetches visible data</li> <li>Reduces bandwidth usage</li> <li> <p>Faster initial page loads</p> </li> <li> <p>User Experience</p> </li> <li>Maintains user's current view</li> <li>No sudden jumps or resets</li> <li> <p>Clear feedback on data freshness</p> </li> <li> <p>Reliability</p> </li> <li>No connection drops</li> <li>Works with all proxies</li> <li>Simpler error handling</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#usage-examples","title":"Usage Examples","text":""},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#basic-usage","title":"Basic Usage","text":"<ol> <li>Navigate to Jobs page</li> <li>Jobs auto-refresh every 5 seconds</li> <li>Use pagination to navigate large lists</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#filtering","title":"Filtering","text":"<ol> <li>Click status buttons to filter by status</li> <li>Select priority from dropdown</li> <li>Type in search box to search jobs</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#performance-tuning","title":"Performance Tuning","text":"<ol> <li>Increase page size for fewer requests</li> <li>Disable auto-refresh when not needed</li> <li>Use filters to reduce data volume</li> </ol>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#migration-notes","title":"Migration Notes","text":"<p>To migrate from SSE to polling: 1. Update backend handlers to support filtering 2. Replace SSE hooks with polling logic 3. Add filter UI components 4. Test with large datasets</p>"},{"location":"archive/development-notes/POLLING_IMPLEMENTATION/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Configurable Poll Interval: Allow users to set custom intervals</li> <li>Smart Polling: Slow down polling when no changes detected</li> <li>Batch Operations: Select multiple jobs for bulk actions</li> <li>Export Functionality: Export filtered job lists</li> <li>Advanced Filters: Date ranges, agent filters, etc.</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/","title":"Work Directory and Job Error Status Fixes","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#summary-of-changes","title":"Summary of Changes","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#1-removed-work-directory-references","title":"1. Removed Work Directory References","text":"<ul> <li>Agent HashcatExecutor: Removed all references to <code>workDirectory</code> since we're capturing output from stdout</li> <li>Agent JobManager: Updated to not pass work directory to HashcatExecutor</li> <li>Hashcat Command: Removed <code>cmd.Dir = e.workDirectory</code> to prevent \"chdir: no such file or directory\" error</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#2-fixed-hashcat-binary-permissions","title":"2. Fixed Hashcat Binary Permissions","text":"<ul> <li>File Sync: Enhanced binary extraction to set executable permissions (0755) for hashcat binaries</li> <li>Binary Detection: Improved detection of executable files during extraction (hashcat, hashcat.exe, hashcat.bin)</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#3-implemented-job-error-status-updates","title":"3. Implemented Job Error Status Updates","text":"<ul> <li>Agent JobProgress: Added <code>Status</code> and <code>ErrorMessage</code> fields to track task status</li> <li>Agent HashcatExecutor: Updated error handling to include error message in progress updates</li> <li>Backend JobProgress Model: Added matching <code>Status</code> and <code>ErrorMessage</code> fields</li> <li>Backend JobWebSocketIntegration: Added handling for failed status to update both task and job execution</li> <li>Backend JobExecutionRepository: Added <code>UpdateErrorMessage</code> method to store error messages</li> </ul>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#key-changes-by-file","title":"Key Changes by File","text":""},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#agent-changes","title":"Agent Changes","text":"<ol> <li><code>agent/internal/jobs/hashcat_executor.go</code></li> <li>Removed <code>workDirectory</code> field from struct</li> <li>Updated <code>NewHashcatExecutor</code> to not take work directory parameter</li> <li>Added status and error fields to JobProgress struct</li> <li> <p>Updated error handling to send proper error status</p> </li> <li> <p><code>agent/internal/jobs/jobs.go</code></p> </li> <li>Updated <code>NewJobManager</code> to not create work directory</li> <li> <p>Pass only data directory to HashcatExecutor</p> </li> <li> <p><code>agent/internal/sync/sync.go</code></p> </li> <li>Enhanced binary extraction to set executable permissions (0755)</li> <li>Improved detection of hashcat executables</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#backend-changes","title":"Backend Changes","text":"<ol> <li><code>backend/internal/models/jobs.go</code></li> <li> <p>Added <code>Status</code> and <code>ErrorMessage</code> fields to JobProgress struct</p> </li> <li> <p><code>backend/internal/integration/job_websocket_integration.go</code></p> </li> <li>Added handling for failed status in <code>HandleJobProgress</code></li> <li>Updates task status to failed with error message</li> <li> <p>Updates job execution status to failed</p> </li> <li> <p><code>backend/internal/repository/job_execution_repository.go</code></p> </li> <li>Added <code>UpdateErrorMessage</code> method to store error messages</li> </ol>"},{"location":"archive/development-notes/WORK_DIRECTORY_FIX_SUMMARY/#result","title":"Result","text":"<ul> <li>Agent no longer tries to use non-existent work directory</li> <li>Hashcat binaries have proper execute permissions after extraction</li> <li>Failed jobs now properly show error status instead of remaining as pending</li> <li>Error messages are captured and displayed in the UI</li> </ul>"},{"location":"archive/development-notes/docker-initialization/","title":"Docker Initialization Guide","text":""},{"location":"archive/development-notes/docker-initialization/#overview","title":"Overview","text":"<p>This guide explains how to initialize and run KrakenHashes using Docker. KrakenHashes runs as a single container that includes the frontend, backend, and PostgreSQL database.</p>"},{"location":"archive/development-notes/docker-initialization/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10.0 or higher</li> <li>Docker Compose v2.0.0 or higher</li> <li>At least 4GB of free disk space</li> </ul>"},{"location":"archive/development-notes/docker-initialization/#environment-configuration","title":"Environment Configuration","text":"<p>The application uses a single <code>.env</code> file for all configuration. Copy the <code>.env.example</code> file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"archive/development-notes/docker-initialization/#important-environment-variables","title":"Important Environment Variables","text":""},{"location":"archive/development-notes/docker-initialization/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>DEBUG</code>: Set to 'true' or '1' to enable debug output</li> <li><code>LOG_LEVEL</code>: Controls message verbosity (DEBUG, INFO, WARNING, ERROR)</li> <li><code>DEBUG_SQL</code>: Enable SQL query logging</li> <li><code>DEBUG_HTTP</code>: Enable HTTP request/response logging</li> <li><code>DEBUG_WEBSOCKET</code>: Enable WebSocket message logging</li> <li><code>DEBUG_AUTH</code>: Enable authentication debugging</li> <li><code>DEBUG_JOBS</code>: Enable job processing debugging</li> </ul>"},{"location":"archive/development-notes/docker-initialization/#log-file-locations","title":"Log File Locations","text":"<p>All logs are written to both stdout/stderr and files within the container: - <code>/var/log/krakenhashes/</code>: Base log directory mounted from host   - <code>backend/</code>: Backend service logs   - <code>frontend/</code>: Frontend service logs   - <code>nginx/</code>: Nginx access and error logs   - <code>postgres/</code>: PostgreSQL logs</p>"},{"location":"archive/development-notes/docker-initialization/#starting-the-service","title":"Starting the Service","text":"<ol> <li> <p>Build and start the service:    <pre><code>docker-compose up --build\n</code></pre></p> </li> <li> <p>Verify the service is running:    <pre><code>docker-compose ps\n</code></pre></p> </li> </ol>"},{"location":"archive/development-notes/docker-initialization/#debugging","title":"Debugging","text":""},{"location":"archive/development-notes/docker-initialization/#viewing-logs","title":"Viewing Logs","text":"<ol> <li> <p>Real-time container logs:    <pre><code>docker-compose logs -f\n</code></pre></p> </li> <li> <p>Access log files directly from host machine:    <pre><code>ls /var/log/krakenhashes/\n</code></pre></p> </li> </ol>"},{"location":"archive/development-notes/docker-initialization/#common-issues","title":"Common Issues","text":"<ol> <li>Database Connection Issues</li> <li>Check PostgreSQL logs in <code>/var/log/krakenhashes/postgres/</code></li> <li>Verify database credentials in .env</li> <li> <p>Ensure database port is not in use</p> </li> <li> <p>Certificate Issues</p> </li> <li>Verify TLS configuration in .env</li> <li>Check certificate paths</li> <li>Ensure proper permissions on certificate files</li> </ol>"},{"location":"archive/development-notes/docker-initialization/#maintenance","title":"Maintenance","text":""},{"location":"archive/development-notes/docker-initialization/#database-backups","title":"Database Backups","text":"<p>PostgreSQL data is persisted in a named volume. To backup: <pre><code>docker-compose exec krakenhashes pg_dump -U krakenhashes &gt; backup.sql\n</code></pre></p>"},{"location":"archive/development-notes/docker-initialization/#log-rotation","title":"Log Rotation","text":"<p>Logs are automatically rotated using logrotate with the following policy: - Maximum size: 100MB - Retention: 30 days - Compression: enabled</p>"},{"location":"archive/development-notes/docker-initialization/#security-notes","title":"Security Notes","text":"<ol> <li>Change default passwords in .env</li> <li>Secure the log directory permissions</li> <li>Regular security updates</li> <li>Monitor log files for suspicious activity </li> </ol>"},{"location":"archive/development-notes/hashlists/","title":"Hashlists","text":"<p>Hashlists are fundamental to KrakenHashes, representing collections of hashes uploaded for cracking jobs or analysis. This document outlines how hashlists are uploaded, processed, and managed within the system.</p>"},{"location":"archive/development-notes/hashlists/#overview","title":"Overview","text":"<ul> <li>Definition: A hashlist is a file containing multiple lines, where each line typically represents a single hash (and potentially its cracked password).</li> <li>Association: Each hashlist is associated with a specific Hash Type (e.g., NTLM, SHA1) and can optionally be linked to a Client/Engagement.</li> <li>Processing: Uploaded hashlists undergo an asynchronous background processing workflow to ingest the hashes into the central database.</li> <li>Storage: Hashlist files are stored on the backend server in a configured directory.</li> </ul>"},{"location":"archive/development-notes/hashlists/#uploading-hashlists","title":"Uploading Hashlists","text":"<p>Hashlists are typically uploaded through the frontend UI.</p> <ol> <li>Navigate: Go to the \"Hashlists\" section of the dashboard.</li> <li>Initiate Upload: Click the \"Upload Hashlist\" button.</li> <li>Fill Details: In the dialog, provide:<ul> <li>Name: A descriptive name for the hashlist.</li> <li>Hash Type: Select the correct hash type from the dropdown. This list is populated from the <code>hash_types</code> table in the database (see Hash Types below). The format displayed is <code>ID - Name</code> (e.g., <code>1000 - NTLM</code>).</li> <li>Client: (Optional) Select an existing client to associate this hashlist with, or create a new one on the fly.</li> <li>File: Choose the hashlist file from your local machine.</li> </ul> </li> <li>Submit: Click the upload button in the dialog.</li> </ol>"},{"location":"archive/development-notes/hashlists/#api-endpoint","title":"API Endpoint","text":"<p>The frontend interacts with the <code>POST /api/hashlists</code> endpoint. This endpoint expects a <code>multipart/form-data</code> request containing the fields mentioned above (name, hash_type_id, client_id) and the hashlist file itself.</p>"},{"location":"archive/development-notes/hashlists/#file-storage","title":"File Storage","text":"<ul> <li>Uploaded hashlist files are stored on the backend server.</li> <li>The base directory for uploads is configured via the <code>KH_DATA_DIR</code> environment variable.</li> <li>Within the data directory, hashlists are stored in a specific subdirectory, typically <code>hashlist_uploads</code>, but configurable via <code>KH_HASH_UPLOAD_DIR</code>.</li> <li>The maximum allowed upload size is determined by the <code>KH_MAX_UPLOAD_SIZE_MB</code> environment variable (default: 32 MiB).</li> </ul>"},{"location":"archive/development-notes/hashlists/#hashlist-processing","title":"Hashlist Processing","text":"<p>Once a hashlist file is uploaded and initial metadata is saved, it enters an asynchronous processing queue.</p>"},{"location":"archive/development-notes/hashlists/#status-workflow","title":"Status Workflow","text":"<p>A hashlist progresses through the following statuses:</p> <ol> <li><code>uploading</code>: Initial state when the upload request is received.</li> <li><code>processing</code>: The backend worker has picked up the hashlist and is actively reading the file and ingesting hashes.</li> <li><code>ready</code>: Processing completed successfully. All valid lines have been processed and stored. The hashlist is now available for use in cracking jobs.</li> <li><code>ready_with_errors</code>: Processing finished, but one or more lines in the file could not be processed correctly (e.g., invalid format for the selected hash type). Valid lines were still ingested. Check backend logs for details on specific line errors. (Not fully implemented)    `</li> <li><code>error</code>: A fatal error occurred during processing (e.g., file unreadable, database error during batch insert). The <code>error_message</code> field on the hashlist provides a general reason. Check backend logs for more details.</li> </ol>"},{"location":"archive/development-notes/hashlists/#processing-steps","title":"Processing Steps","text":"<p>The backend processor performs the following steps:</p> <ol> <li>Fetch Details: Retrieves the hashlist metadata (ID, file path, hash type ID) from the database.</li> <li>Open File: Opens the stored hashlist file.</li> <li>Scan Line by Line: Reads the file line by line.<ul> <li>Empty lines are skipped.</li> <li>Lines starting with <code>#</code> are treated as comments and skipped.</li> </ul> </li> <li>Extract Hash/Password:<ul> <li>Default: Checks for a colon (<code>:</code>) separator. If found, the part before the colon is treated as the hash, and the part after is treated as the pre-cracked password (<code>is_cracked</code> = true). If no colon is found, the entire line is treated as the hash (<code>is_cracked</code> = false).</li> <li>Type-Specific Processing: For certain hash types (e.g., <code>1000 - NTLM</code>), specific processing logic might be applied to extract the canonical hash format from more complex lines (like <code>user:sid:LM:NT:::</code>). This logic is determined by the <code>needs_processing</code> flag and potentially the <code>processing_logic</code> field in the <code>hash_types</code> table.</li> </ul> </li> <li>Batching: Hashes are collected into batches (size configured by <code>KH_HASHLIST_BATCH_SIZE</code>, default: 1000).</li> <li>Database Insertion: Each batch is processed:<ul> <li>The system checks if any hashes in the batch already exist in the central <code>hashes</code> table (based on hash value and hash type ID).</li> <li>New, unique hashes are inserted into the <code>hashes</code> table.</li> <li>Entries are created in the <code>hashlist_hashes</code> join table to link both new and existing hashes from the batch to the current hashlist.</li> <li>If a hash being added includes a pre-cracked password, the corresponding record in the <code>hashes</code> table is updated (<code>is_cracked</code>=true, <code>password</code>=...).</li> </ul> </li> <li>Update Status: Once the entire file is processed, the hashlist status is updated to <code>ready</code>, <code>ready_with_errors</code>, or <code>error</code>, along with the final <code>total_hashes</code> and <code>cracked_hashes</code> counts.</li> </ol>"},{"location":"archive/development-notes/hashlists/#supported-input-formats","title":"Supported Input Formats","text":"<p>The processor primarily expects:</p> <ul> <li>One hash per line.</li> <li>Optional: <code>hash:password</code> format for lines containing already cracked hashes.</li> <li>Lines starting with <code>#</code> are ignored.</li> <li>Empty lines are ignored.</li> <li>Specific formats handled by type-specific processors (e.g., NTLM).</li> </ul>"},{"location":"archive/development-notes/hashlists/#hash-types","title":"Hash Types","text":"<ul> <li>Supported hash types are defined in the <code>hash_types</code> database table.</li> <li>This table is populated by a database migration (<code>000016_add_hashcat_hash_types.up.sql</code>) which includes common types and examples sourced from the Hashcat wiki.</li> <li>Each type has an ID (corresponding to the Hashcat mode), Name, Description (optional), Example (optional), and flags indicating if it needs special processing (<code>needs_processing</code>, <code>processing_logic</code>) or is enabled (<code>is_enabled</code>).</li> <li>The frontend uses the <code>GET /api/hashtypes</code> endpoint (filtered by <code>is_enabled=true</code> by default) to populate the selection dropdown during hashlist upload.</li> </ul>"},{"location":"archive/development-notes/hashlists/#managing-hashlists","title":"Managing Hashlists","text":"<ul> <li>Viewing: The \"Hashlists\" dashboard provides a sortable and filterable view of all accessible hashlists, showing Name, Client, Status, Progress (% Cracked), and Creation Date.</li> <li>Downloading: Use the download icon on the dashboard or the <code>GET /api/hashlists/{id}/download</code> endpoint to retrieve the original uploaded hashlist file.</li> <li>Deleting: Use the delete icon on the dashboard or the <code>DELETE /api/hashlists/{id}</code> endpoint. Deleting a hashlist removes its entry from the <code>hashlists</code> table, removes associated entries from the <code>hashlist_hashes</code> table, and deletes the original hashlist file from the backend storage. It does not delete the individual hashes from the central <code>hashes</code> table, as they might be referenced by other hashlists.</li> </ul>"},{"location":"archive/development-notes/hashlists/#data-retention","title":"Data Retention","text":"<p>Uploaded hashlists and their associated data are subject to the system's data retention policies. Old hashlists may be automatically purged based on client-specific or default retention settings configured by an administrator. See Admin Settings documentation for details. </p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>Complete guide for deploying KrakenHashes in various environments.</p>"},{"location":"deployment/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Docker Deployment</p> <p>Deploy using Docker containers with initialization</p> </li> <li> <p> Docker Compose</p> <p>Multi-container deployment with orchestration</p> </li> <li> <p> Production Best Practices</p> <p>Security, performance, and reliability guidelines</p> </li> <li> <p> Update Procedures</p> <p>Safely updating KrakenHashes components</p> </li> </ul>"},{"location":"deployment/#deployment-options","title":"Deployment Options","text":""},{"location":"deployment/#quick-deployment","title":"Quick Deployment","text":"<p>For testing or small deployments, use our pre-configured Docker Compose: <pre><code>docker-compose up -d\n</code></pre></p>"},{"location":"deployment/#production-deployment","title":"Production Deployment","text":"<p>For production environments, follow our Production Best Practices guide for: - High availability setup - Security hardening - Performance optimization - Monitoring configuration</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":""},{"location":"deployment/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: Linux (recommended), Windows Server 2019+, or macOS</li> <li>CPU: 4+ cores recommended</li> <li>RAM: 8GB minimum, 16GB+ recommended</li> <li>Storage: 50GB+ for application and data</li> <li>Network: Stable internet connection for updates</li> </ul>"},{"location":"deployment/#software-requirements","title":"Software Requirements","text":"<ul> <li>Docker 20.10+ and Docker Compose 2.0+</li> <li>PostgreSQL 15+ (or use included container)</li> <li>Valid SSL/TLS certificates for production</li> </ul>"},{"location":"deployment/#deployment-checklist","title":"Deployment Checklist","text":"<p>Before You Deploy</p> <ul> <li> Review system requirements</li> <li> Plan network architecture</li> <li> Configure firewall rules</li> <li> Prepare SSL/TLS certificates</li> <li> Set up backup storage</li> <li> Plan monitoring strategy</li> <li> Review security guidelines</li> </ul>"},{"location":"deployment/#support-matrix","title":"Support Matrix","text":"Component Docker Bare Metal Kubernetes Cloud Backend \u2705 Full \u26a0\ufe0f Manual \ud83d\udea7 Planned \u2705 Yes Frontend \u2705 Full \u2705 Full \ud83d\udea7 Planned \u2705 Yes Database \u2705 Full \u2705 Full \ud83d\udea7 Planned \u2705 Yes Agent \u2705 Full \u2705 Full \u274c No \u26a0\ufe0f Limited"},{"location":"deployment/#security-considerations","title":"Security Considerations","text":"<p>Production Security</p> <p>Always follow these security practices:</p> <ul> <li>Enable TLS/SSL for all connections</li> <li>Use strong passwords and rotate regularly</li> <li>Enable MFA for all admin accounts</li> <li>Restrict network access with firewalls</li> <li>Regular security updates</li> <li>Monitor logs for suspicious activity</li> </ul>"},{"location":"deployment/#getting-help","title":"Getting Help","text":"<ul> <li>Check our Troubleshooting Guide</li> <li>Join our Discord Community</li> <li>Review GitHub Issues</li> </ul>"},{"location":"deployment/docker-compose/","title":"Docker Compose Deployment Guide","text":"<p>\u26a0\ufe0f Version Requirement: Docker Compose v2.0+ is required. Use <code>docker compose</code> (space), not <code>docker-compose</code> (hyphen).</p> <p>This guide covers deploying KrakenHashes using Docker Compose, including configuration options, network setup, volume management, and common customizations.</p>"},{"location":"deployment/docker-compose/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Configuration Options</li> <li>Network Setup</li> <li>Volume Management</li> <li>Deployment Steps</li> <li>Scaling Considerations</li> <li>Common Customizations</li> <li>Troubleshooting</li> <li>Security Considerations</li> </ul>"},{"location":"deployment/docker-compose/#overview","title":"Overview","text":"<p>KrakenHashes uses Docker Compose to orchestrate multiple services:</p> <ul> <li>PostgreSQL: Database backend for storing hashlists, jobs, and system data</li> <li>KrakenHashes App: Combined backend API and frontend served through nginx</li> </ul> <p>The Docker Compose setup provides: - Automatic service dependency management - Health checks for service readiness - Persistent data storage through Docker volumes - Environment-based configuration - Isolated networking</p>"},{"location":"deployment/docker-compose/#prerequisites","title":"Prerequisites","text":"<p>Before deploying with Docker Compose:</p> <ol> <li>Docker Engine: Version 20.10 or higher</li> <li>Docker Compose: Version 2.0 or higher (included with Docker Desktop)</li> <li>System Requirements:</li> <li>4GB RAM minimum (8GB recommended)</li> <li>10GB free disk space</li> <li> <p>Linux, macOS, or Windows with WSL2</p> </li> <li> <p>Network Ports:</p> </li> <li>443 (HTTPS frontend)</li> <li>1337 (HTTP API)</li> <li>31337 (HTTPS API)</li> <li>5432 (PostgreSQL - optional, can be internal only)</li> </ol>"},{"location":"deployment/docker-compose/#configuration-options","title":"Configuration Options","text":""},{"location":"deployment/docker-compose/#environment-variables","title":"Environment Variables","text":"<p>Create a <code>.env</code> file in the project root with the following variables:</p> <pre><code># Database Configuration\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\n\n# Port Configuration\nFRONTEND_PORT=443\nKH_PORT=1337\nKH_HTTPS_PORT=31337\n\n# Directory Configuration\nLOG_DIR=/var/log/krakenhashes\nKH_CONFIG_DIR_HOST=/etc/krakenhashes\nKH_DATA_DIR_HOST=/var/lib/krakenhashes\n\n# User/Group IDs (for file permissions)\nPUID=1000\nPGID=1000\n\n# TLS Configuration\nKH_TLS_MODE=self-signed\nKH_CERT_KEY_SIZE=4096\nKH_CERT_VALIDITY_DAYS=365\nKH_CA_VALIDITY_DAYS=3650\n</code></pre>"},{"location":"deployment/docker-compose/#service-specific-configuration","title":"Service-Specific Configuration","text":""},{"location":"deployment/docker-compose/#postgresql-service","title":"PostgreSQL Service","text":"<p>The PostgreSQL service is configured with: - Alpine-based image for smaller footprint - Health checks using <code>pg_isready</code> - Persistent data storage in Docker volume - Configurable credentials via environment variables</p>"},{"location":"deployment/docker-compose/#krakenhashes-application","title":"KrakenHashes Application","text":"<p>The main application container includes: - Multi-stage build for optimized image size - Combined backend and frontend services - nginx reverse proxy for frontend - TLS/SSL support with multiple modes - File storage for binaries, wordlists, and hashlists</p>"},{"location":"deployment/docker-compose/#network-setup","title":"Network Setup","text":""},{"location":"deployment/docker-compose/#default-network","title":"Default Network","text":"<p>Docker Compose creates an isolated bridge network <code>krakenhashes-net</code>:</p> <pre><code>networks:\n  krakenhashes-net:\n    driver: bridge\n</code></pre> <p>This provides: - Service discovery by container name - Isolation from other Docker networks - Internal DNS resolution</p>"},{"location":"deployment/docker-compose/#service-communication","title":"Service Communication","text":"<ul> <li>Backend connects to PostgreSQL using hostname <code>postgres</code></li> <li>All services communicate over the internal network</li> <li>Only required ports are exposed to the host</li> </ul>"},{"location":"deployment/docker-compose/#custom-network-configuration","title":"Custom Network Configuration","text":"<p>To use an existing network or customize settings:</p> <pre><code>networks:\n  krakenhashes-net:\n    external: true\n    name: my-existing-network\n</code></pre> <p>Or with custom subnet:</p> <pre><code>networks:\n  krakenhashes-net:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n          gateway: 172.20.0.1\n</code></pre>"},{"location":"deployment/docker-compose/#volume-management","title":"Volume Management","text":""},{"location":"deployment/docker-compose/#persistent-volumes","title":"Persistent Volumes","text":"<p>KrakenHashes uses named volumes for data persistence:</p> <ol> <li>postgres_data: PostgreSQL database files</li> <li>krakenhashes_data: Application data (wordlists, rules, hashlists)</li> </ol>"},{"location":"deployment/docker-compose/#volume-locations","title":"Volume Locations","text":"<p>Default volume storage locations: - Docker managed: <code>/var/lib/docker/volumes/</code> - Named volumes:   - <code>krakenhashes_postgres_data</code>   - <code>krakenhashes_app_data</code></p>"},{"location":"deployment/docker-compose/#bind-mounts","title":"Bind Mounts","text":"<p>The compose file uses bind mounts for: - Logs: <code>${LOG_DIR:-/var/log/krakenhashes}</code> - Config: <code>${KH_CONFIG_DIR_HOST:-/etc/krakenhashes}</code> - Data: <code>${KH_DATA_DIR_HOST:-/var/lib/krakenhashes}</code></p>"},{"location":"deployment/docker-compose/#backup-strategy","title":"Backup Strategy","text":"<p>To backup volumes:</p> <pre><code># Backup PostgreSQL data\ndocker run --rm -v krakenhashes_postgres_data:/data \\\n  -v $(pwd):/backup alpine tar czf /backup/postgres-backup.tar.gz -C /data .\n\n# Backup application data\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $(pwd):/backup alpine tar czf /backup/app-backup.tar.gz -C /data .\n</code></pre>"},{"location":"deployment/docker-compose/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/docker-compose/#initial-deployment","title":"Initial Deployment","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Create environment file:    <pre><code>cp .env.example .env\n# Edit .env with your configuration\n</code></pre></p> </li> <li> <p>Create required directories:    <pre><code>sudo mkdir -p /var/log/krakenhashes\nsudo mkdir -p /etc/krakenhashes\nsudo mkdir -p /var/lib/krakenhashes\nsudo chown -R 1000:1000 /var/log/krakenhashes\nsudo chown -R 1000:1000 /etc/krakenhashes\nsudo chown -R 1000:1000 /var/lib/krakenhashes\n</code></pre></p> </li> <li> <p>Build and start services:    <pre><code>docker-compose up -d --build\n</code></pre></p> </li> <li> <p>Verify deployment:    <pre><code>docker-compose ps\ndocker-compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#updating-deployment","title":"Updating Deployment","text":"<ol> <li> <p>Pull latest changes:    <pre><code>git pull origin main\n</code></pre></p> </li> <li> <p>Rebuild and restart:    <pre><code>docker-compose down\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Check migration status:    <pre><code>docker-compose logs krakenhashes | grep -i migration\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"deployment/docker-compose/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>While the current setup runs as a single instance, you can prepare for scaling:</p> <ol> <li>Database Scaling:</li> <li>Use external PostgreSQL for production</li> <li>Consider connection pooling with PgBouncer</li> <li> <p>Implement read replicas for reporting</p> </li> <li> <p>Application Scaling:</p> </li> <li>Use external load balancer (nginx, HAProxy)</li> <li>Share file storage (NFS, S3-compatible)</li> <li>Implement Redis for session storage</li> </ol>"},{"location":"deployment/docker-compose/#resource-limits","title":"Resource Limits","text":"<p>Add resource constraints to prevent container resource exhaustion:</p> <pre><code>services:\n  krakenhashes:\n    deploy:\n      resources:\n        limits:\n          cpus: '2'\n          memory: 4G\n        reservations:\n          cpus: '1'\n          memory: 2G\n</code></pre>"},{"location":"deployment/docker-compose/#common-customizations","title":"Common Customizations","text":""},{"location":"deployment/docker-compose/#development-mode","title":"Development Mode","text":"<p>For development, uncomment restart policies and expose additional ports:</p> <pre><code>services:\n  postgres:\n    restart: unless-stopped\n    ports:\n      - \"5432:5432\"  # Direct database access\n\n  krakenhashes:\n    restart: unless-stopped\n    environment:\n      - DEBUG=true\n      - LOG_LEVEL=debug\n</code></pre>"},{"location":"deployment/docker-compose/#production-optimizations","title":"Production Optimizations","text":"<ol> <li> <p>Remove unnecessary port exposures:    <pre><code>services:\n  postgres:\n    # Remove ports section for internal-only access\n</code></pre></p> </li> <li> <p>Enable restart policies:    <pre><code>restart: always\n</code></pre></p> </li> <li> <p>Use specific image tags:    <pre><code>image: postgres:15.4-alpine\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#custom-tls-certificates","title":"Custom TLS Certificates","text":"<p>To use your own certificates:</p> <ol> <li>Place certificates in <code>/etc/krakenhashes/certs/</code></li> <li>Set environment variables:    <pre><code>KH_TLS_MODE=provided\nKH_TLS_CERT_PATH=/etc/krakenhashes/certs/server.crt\nKH_TLS_KEY_PATH=/etc/krakenhashes/certs/server.key\n</code></pre></li> </ol>"},{"location":"deployment/docker-compose/#external-database","title":"External Database","text":"<p>To use an external PostgreSQL instance:</p> <ol> <li>Remove the postgres service from docker-compose.yml</li> <li>Update environment variables:    <pre><code>DB_HOST=your-database-host.com\nDB_PORT=5432\nDB_NAME=krakenhashes\nDB_USER=krakenhashes\nDB_PASSWORD=your-password\n</code></pre></li> </ol>"},{"location":"deployment/docker-compose/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/docker-compose/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Container fails to start:    <pre><code># Check logs\ndocker-compose logs krakenhashes\n\n# Check health status\ndocker-compose ps\n</code></pre></p> </li> <li> <p>Database connection errors:    <pre><code># Test database connectivity\ndocker-compose exec krakenhashes pg_isready -h postgres -U krakenhashes\n\n# Check PostgreSQL logs\ndocker-compose logs postgres\n</code></pre></p> </li> <li> <p>Permission issues:    <pre><code># Fix ownership\nsudo chown -R 1000:1000 /var/lib/krakenhashes\nsudo chown -R 1000:1000 /var/log/krakenhashes\n</code></pre></p> </li> <li> <p>Port conflicts:    <pre><code># Check port usage\nsudo netstat -tlnp | grep -E '(443|1337|31337|5432)'\n\n# Change ports in .env file\nFRONTEND_PORT=8443\nKH_PORT=8337\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker-compose/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code># In .env file\nLOG_LEVEL=debug\nDEBUG=true\n\n# Restart services\ndocker-compose restart krakenhashes\n</code></pre>"},{"location":"deployment/docker-compose/#health-checks","title":"Health Checks","text":"<p>Monitor service health:</p> <pre><code># Check all services\ndocker-compose ps\n\n# Detailed health info\ndocker inspect krakenhashes-postgres | jq '.[0].State.Health'\n</code></pre>"},{"location":"deployment/docker-compose/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/docker-compose/#network-security","title":"Network Security","text":"<ol> <li>Firewall Rules:</li> <li>Only expose necessary ports</li> <li>Use firewall to restrict access</li> <li> <p>Consider VPN for administrative access</p> </li> <li> <p>TLS/SSL:</p> </li> <li>Always use HTTPS in production</li> <li>Regularly update certificates</li> <li>Use strong cipher suites</li> </ol>"},{"location":"deployment/docker-compose/#container-security","title":"Container Security","text":"<ol> <li>Run as non-root:</li> <li>Containers use UID/GID 1000 by default</li> <li> <p>Avoid running as root user</p> </li> <li> <p>Image Security:    <pre><code># Scan for vulnerabilities\ndocker scan krakenhashes:latest\n</code></pre></p> </li> <li> <p>Secrets Management:</p> </li> <li>Use Docker secrets for sensitive data</li> <li>Rotate database passwords regularly</li> <li>Never commit .env files to version control</li> </ol>"},{"location":"deployment/docker-compose/#backup-and-recovery","title":"Backup and Recovery","text":"<ol> <li> <p>Regular Backups:    <pre><code># Automated backup script\n#!/bin/bash\nBACKUP_DIR=\"/backup/krakenhashes/$(date +%Y%m%d)\"\nmkdir -p $BACKUP_DIR\n\n# Backup database\ndocker-compose exec -T postgres pg_dump -U krakenhashes &gt; $BACKUP_DIR/database.sql\n\n# Backup volumes\ndocker run --rm -v krakenhashes_app_data:/data \\\n  -v $BACKUP_DIR:/backup alpine \\\n  tar czf /backup/app-data.tar.gz -C /data .\n</code></pre></p> </li> <li> <p>Test Recovery:</p> </li> <li>Regularly test backup restoration</li> <li>Document recovery procedures</li> <li>Keep multiple backup generations</li> </ol>"},{"location":"deployment/docker-compose/#monitoring","title":"Monitoring","text":"<p>Implement monitoring for: - Container health and restarts - Resource usage (CPU, memory, disk) - Application logs and errors - Database performance - TLS certificate expiration</p> <p>Consider using: - Prometheus + Grafana for metrics - ELK stack for log aggregation - Uptime monitoring services</p>"},{"location":"deployment/docker/","title":"Docker Initialization Guide","text":""},{"location":"deployment/docker/#overview","title":"Overview","text":"<p>This guide explains how to initialize and run KrakenHashes using Docker. KrakenHashes runs as a single container that includes the frontend, backend, and PostgreSQL database.</p>"},{"location":"deployment/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker 20.10.0 or higher</li> <li>Docker Compose v2.0.0 or higher</li> <li>At least 4GB of free disk space</li> </ul>"},{"location":"deployment/docker/#environment-configuration","title":"Environment Configuration","text":"<p>The application uses a single <code>.env</code> file for all configuration. Copy the <code>.env.example</code> file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"deployment/docker/#important-environment-variables","title":"Important Environment Variables","text":""},{"location":"deployment/docker/#logging-configuration","title":"Logging Configuration","text":"<ul> <li><code>DEBUG</code>: Set to 'true' or '1' to enable debug output</li> <li><code>LOG_LEVEL</code>: Controls message verbosity (DEBUG, INFO, WARNING, ERROR)</li> <li><code>DEBUG_SQL</code>: Enable SQL query logging</li> <li><code>DEBUG_HTTP</code>: Enable HTTP request/response logging</li> <li><code>DEBUG_WEBSOCKET</code>: Enable WebSocket message logging</li> <li><code>DEBUG_AUTH</code>: Enable authentication debugging</li> <li><code>DEBUG_JOBS</code>: Enable job processing debugging</li> </ul>"},{"location":"deployment/docker/#log-file-locations","title":"Log File Locations","text":"<p>All logs are written to both stdout/stderr and files within the container: - <code>/var/log/krakenhashes/</code>: Base log directory mounted from host   - <code>backend/</code>: Backend service logs   - <code>frontend/</code>: Frontend service logs   - <code>nginx/</code>: Nginx access and error logs   - <code>postgres/</code>: PostgreSQL logs</p>"},{"location":"deployment/docker/#starting-the-service","title":"Starting the Service","text":"<ol> <li> <p>Build and start the service:    <pre><code>docker-compose up --build\n</code></pre></p> </li> <li> <p>Verify the service is running:    <pre><code>docker-compose ps\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#debugging","title":"Debugging","text":""},{"location":"deployment/docker/#viewing-logs","title":"Viewing Logs","text":"<ol> <li> <p>Real-time container logs:    <pre><code>docker-compose logs -f\n</code></pre></p> </li> <li> <p>Access log files directly from host machine:    <pre><code>ls /var/log/krakenhashes/\n</code></pre></p> </li> </ol>"},{"location":"deployment/docker/#common-issues","title":"Common Issues","text":"<ol> <li>Database Connection Issues</li> <li>Check PostgreSQL logs in <code>/var/log/krakenhashes/postgres/</code></li> <li>Verify database credentials in .env</li> <li> <p>Ensure database port is not in use</p> </li> <li> <p>Certificate Issues</p> </li> <li>Verify TLS configuration in .env</li> <li>Check certificate paths</li> <li>Ensure proper permissions on certificate files</li> </ol>"},{"location":"deployment/docker/#maintenance","title":"Maintenance","text":""},{"location":"deployment/docker/#database-backups","title":"Database Backups","text":"<p>PostgreSQL data is persisted in a named volume. To backup: <pre><code>docker-compose exec krakenhashes pg_dump -U krakenhashes &gt; backup.sql\n</code></pre></p>"},{"location":"deployment/docker/#log-rotation","title":"Log Rotation","text":"<p>Logs are automatically rotated using logrotate with the following policy: - Maximum size: 100MB - Retention: 30 days - Compression: enabled</p>"},{"location":"deployment/docker/#security-notes","title":"Security Notes","text":"<ol> <li>Change default passwords in .env</li> <li>Secure the log directory permissions</li> <li>Regular security updates</li> <li>Monitor log files for suspicious activity </li> </ol>"},{"location":"deployment/production/","title":"Production Best Practices Guide","text":"<p>This guide provides comprehensive recommendations for deploying and operating KrakenHashes in production environments. Following these practices ensures security, reliability, performance, and compliance for your password cracking infrastructure.</p>"},{"location":"deployment/production/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Infrastructure Requirements and Recommendations</li> <li>Security Hardening Checklist</li> <li>Performance Optimization</li> <li>High Availability Setup</li> <li>Monitoring and Alerting</li> <li>Backup and Disaster Recovery</li> <li>Compliance Considerations</li> </ol>"},{"location":"deployment/production/#infrastructure-requirements-and-recommendations","title":"Infrastructure Requirements and Recommendations","text":""},{"location":"deployment/production/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"deployment/production/#backend-server-minimum","title":"Backend Server (Minimum)","text":"<ul> <li>CPU: 8 cores (16+ recommended for large deployments)</li> <li>RAM: 16GB (32GB+ recommended)</li> <li>Storage: </li> <li>System: 50GB SSD</li> <li>Data: 500GB+ SSD (scales with hashlist/wordlist size)</li> <li>Database: 100GB+ SSD with high IOPS</li> <li>Network: 1Gbps connection minimum</li> </ul>"},{"location":"deployment/production/#backend-server-recommended","title":"Backend Server (Recommended)","text":"<ul> <li>CPU: 16-32 cores (AMD EPYC or Intel Xeon)</li> <li>RAM: 64GB ECC memory</li> <li>Storage:</li> <li>System: 2x 250GB SSD in RAID 1</li> <li>Data: 2TB+ NVMe SSD array (RAID 10)</li> <li>Database: Dedicated 500GB+ enterprise SSD</li> <li>Network: 10Gbps connection for agent communication</li> </ul>"},{"location":"deployment/production/#agent-hardware-per-agent","title":"Agent Hardware (Per Agent)","text":"<ul> <li>CPU: 8+ cores for hashcat coordination</li> <li>RAM: 16GB minimum (32GB+ for large wordlists)</li> <li>GPU: NVIDIA RTX 3090/4090 or better</li> <li>Storage: 250GB SSD for local caching</li> <li>Network: 1Gbps stable connection</li> </ul>"},{"location":"deployment/production/#network-architecture","title":"Network Architecture","text":"<pre><code>Internet\n    \u2502\n    \u251c\u2500\u2500\u2500 Firewall/WAF\n    \u2502         \u2502\n    \u2502    Load Balancer (Optional for HA)\n    \u2502         \u2502\n    \u2502    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502    \u2502 Backend \u2502 \u2190\u2500\u2500 Port 31337 (HTTPS API)\n    \u2502    \u2502 Server  \u2502 \u2190\u2500\u2500 Port 1337 (HTTP CA cert)\n    \u2502    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502         \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500 PostgreSQL (Port 5432)\n    \u2502\n    \u2514\u2500\u2500\u2500 Agent Network \u2190\u2500\u2500 WebSocket connections\n</code></pre>"},{"location":"deployment/production/#firewall-rules","title":"Firewall Rules","text":""},{"location":"deployment/production/#inbound-rules","title":"Inbound Rules","text":"<pre><code># Public Access\n443/tcp  \u2192 Load Balancer       # HTTPS (if using reverse proxy)\n31337/tcp \u2192 Backend Server      # HTTPS API\n1337/tcp  \u2192 Backend Server      # CA Certificate endpoint\n\n# Internal Only\n5432/tcp  \u2192 PostgreSQL          # Database (restrict to backend only)\n22/tcp    \u2192 All Servers         # SSH (restrict source IPs)\n</code></pre>"},{"location":"deployment/production/#outbound-rules","title":"Outbound Rules","text":"<pre><code>443/tcp   \u2192 Internet            # Updates, external services\n80/tcp    \u2192 Internet            # Package updates\n53/udp    \u2192 DNS Servers         # DNS resolution\n123/udp   \u2192 NTP Servers         # Time synchronization\n</code></pre>"},{"location":"deployment/production/#storage-recommendations","title":"Storage Recommendations","text":""},{"location":"deployment/production/#file-system-layout","title":"File System Layout","text":"<pre><code>/var/lib/krakenhashes/          # Main data directory\n\u251c\u2500\u2500 binaries/                   # Hashcat binaries (10GB)\n\u251c\u2500\u2500 wordlists/                  # Wordlist storage (100GB+)\n\u2502   \u251c\u2500\u2500 general/               \n\u2502   \u251c\u2500\u2500 specialized/           \n\u2502   \u251c\u2500\u2500 targeted/              \n\u2502   \u2514\u2500\u2500 custom/                \n\u251c\u2500\u2500 rules/                      # Rule files (10GB)\n\u2502   \u251c\u2500\u2500 hashcat/               \n\u2502   \u251c\u2500\u2500 john/                  \n\u2502   \u2514\u2500\u2500 custom/                \n\u251c\u2500\u2500 hashlists/                  # User hashlists (100GB+)\n\u2514\u2500\u2500 hashlist_uploads/           # Temporary uploads (50GB)\n\n/var/lib/postgresql/            # Database files (separate disk)\n/var/log/krakenhashes/          # Application logs\n/backup/krakenhashes/           # Backup storage (separate disk/NAS)\n</code></pre>"},{"location":"deployment/production/#storage-best-practices","title":"Storage Best Practices","text":"<ul> <li>Use separate disks/volumes for database, data, and backups</li> <li>Implement RAID for redundancy (RAID 10 recommended)</li> <li>Monitor disk usage and set alerts at 80% capacity</li> <li>Use SSD/NVMe for database and frequently accessed data</li> <li>Consider object storage (S3-compatible) for large wordlists</li> </ul>"},{"location":"deployment/production/#security-hardening-checklist","title":"Security Hardening Checklist","text":""},{"location":"deployment/production/#authentication-and-access-control","title":"Authentication and Access Control","text":""},{"location":"deployment/production/#1-strong-authentication-configuration","title":"1. Strong Authentication Configuration","text":"<pre><code># Configure in Admin Panel \u2192 Authentication Settings\n\n\u2713 Minimum Password Length: 20 characters\n\u2713 Require Uppercase: Enabled\n\u2713 Require Lowercase: Enabled\n\u2713 Require Numbers: Enabled\n\u2713 Require Special Characters: Enabled\n\u2713 Maximum Failed Login Attempts: 3\n\u2713 Account Lockout Duration: 30 minutes\n\u2713 JWT Token Expiry: 15 minutes (production)\n</code></pre>"},{"location":"deployment/production/#2-multi-factor-authentication-mfa","title":"2. Multi-Factor Authentication (MFA)","text":"<pre><code>\u2713 Require MFA for All Users: Enabled\n\u2713 Allowed Methods:\n  - Email Authentication: Enabled (backup only)\n  - Authenticator Apps: Enabled (primary)\n\u2713 Email Code Validity: 5 minutes\n\u2713 Code Cooldown Period: 2 minutes\n\u2713 Maximum Code Attempts: 3\n\u2713 Number of Backup Codes: 10\n</code></pre>"},{"location":"deployment/production/#3-user-account-security","title":"3. User Account Security","text":"<ul> <li>Enforce unique usernames (no shared accounts)</li> <li>Implement role-based access control (RBAC)</li> <li>Regular access reviews (quarterly)</li> <li>Disable default accounts</li> <li>Audit privileged account usage</li> </ul>"},{"location":"deployment/production/#network-security","title":"Network Security","text":""},{"location":"deployment/production/#1-tlsssl-configuration","title":"1. TLS/SSL Configuration","text":"<pre><code># Production TLS Setup (use provided certificates)\nexport KH_TLS_MODE=provided\nexport KH_TLS_CERT_PATH=/etc/krakenhashes/certs/server.crt\nexport KH_TLS_KEY_PATH=/etc/krakenhashes/certs/server.key\nexport KH_TLS_CA_PATH=/etc/krakenhashes/certs/ca.crt\n\n# Or use Let's Encrypt\nexport KH_TLS_MODE=certbot\nexport KH_CERTBOT_DOMAIN=krakenhashes.example.com\nexport KH_CERTBOT_EMAIL=admin@example.com\n</code></pre>"},{"location":"deployment/production/#2-network-hardening","title":"2. Network Hardening","text":"<pre><code># Disable unnecessary services\nsystemctl disable avahi-daemon\nsystemctl disable cups\nsystemctl disable bluetooth\n\n# Configure iptables/firewalld\nfirewall-cmd --permanent --add-service=https\nfirewall-cmd --permanent --add-port=31337/tcp\nfirewall-cmd --permanent --add-port=1337/tcp\nfirewall-cmd --permanent --remove-service=ssh # Use specific source IPs\nfirewall-cmd --reload\n\n# Enable fail2ban for SSH and API endpoints\napt-get install fail2ban\n</code></pre>"},{"location":"deployment/production/#3-api-security","title":"3. API Security","text":"<ul> <li>Implement rate limiting (100 requests/minute per IP)</li> <li>Enable CORS with specific allowed origins</li> <li>Validate all input data</li> <li>Use prepared statements for database queries</li> <li>Implement request signing for agent communication</li> </ul>"},{"location":"deployment/production/#database-security","title":"Database Security","text":""},{"location":"deployment/production/#1-postgresql-hardening","title":"1. PostgreSQL Hardening","text":"<pre><code>-- Restrict connections\nALTER SYSTEM SET listen_addresses = 'localhost';\nALTER SYSTEM SET max_connections = 200;\n\n-- Enable SSL\nALTER SYSTEM SET ssl = on;\nALTER SYSTEM SET ssl_cert_file = '/etc/postgresql/server.crt';\nALTER SYSTEM SET ssl_key_file = '/etc/postgresql/server.key';\n\n-- Configure authentication\n-- Edit pg_hba.conf\nhostssl krakenhashes krakenhashes 127.0.0.1/32 scram-sha-256\nhostssl krakenhashes krakenhashes ::1/128 scram-sha-256\n\n-- Set strong passwords\nALTER USER krakenhashes WITH PASSWORD 'use-a-very-strong-password-here';\nALTER USER postgres WITH PASSWORD 'another-very-strong-password';\n\n-- Revoke unnecessary permissions\nREVOKE CREATE ON SCHEMA public FROM PUBLIC;\n</code></pre>"},{"location":"deployment/production/#2-database-access-control","title":"2. Database Access Control","text":"<ul> <li>Use application-specific database users</li> <li>Implement least privilege principle</li> <li>Regular password rotation (90 days)</li> <li>Audit database access logs</li> <li>Encrypt sensitive columns (future feature)</li> </ul>"},{"location":"deployment/production/#file-system-security","title":"File System Security","text":""},{"location":"deployment/production/#1-directory-permissions","title":"1. Directory Permissions","text":"<pre><code># Set proper ownership\nchown -R krakenhashes:krakenhashes /var/lib/krakenhashes\nchown -R postgres:postgres /var/lib/postgresql\n\n# Set restrictive permissions\nchmod 750 /var/lib/krakenhashes\nchmod 750 /var/lib/krakenhashes/binaries\nchmod 750 /var/lib/krakenhashes/wordlists\nchmod 750 /var/lib/krakenhashes/rules\nchmod 750 /var/lib/krakenhashes/hashlists\nchmod 1777 /var/lib/krakenhashes/hashlist_uploads  # Sticky bit for uploads\n\n# Protect configuration\nchmod 600 /etc/krakenhashes/config.env\nchmod 700 /etc/krakenhashes/certs\nchmod 600 /etc/krakenhashes/certs/*\n</code></pre>"},{"location":"deployment/production/#2-file-integrity-monitoring","title":"2. File Integrity Monitoring","text":"<pre><code># Install AIDE or similar\napt-get install aide\naide --init\naide --check\n\n# Monitor critical files\n/usr/local/bin/krakenhashes\n/etc/krakenhashes/*\n/var/lib/krakenhashes/binaries/*\n</code></pre>"},{"location":"deployment/production/#container-security-docker-deployments","title":"Container Security (Docker Deployments)","text":""},{"location":"deployment/production/#1-docker-hardening","title":"1. Docker Hardening","text":"<pre><code># docker-compose.yml security additions\nservices:\n  backend:\n    security_opt:\n      - no-new-privileges:true\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /var/run\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE\n    user: \"1000:1000\"\n</code></pre>"},{"location":"deployment/production/#2-image-security","title":"2. Image Security","text":"<pre><code># Scan images for vulnerabilities\ndocker scan krakenhashes/backend:latest\n\n# Use specific versions, not 'latest'\nimage: krakenhashes/backend:v0.1.0-alpha\n\n# Sign images\nexport DOCKER_CONTENT_TRUST=1\n</code></pre>"},{"location":"deployment/production/#performance-optimization","title":"Performance Optimization","text":""},{"location":"deployment/production/#database-optimization","title":"Database Optimization","text":""},{"location":"deployment/production/#1-postgresql-tuning","title":"1. PostgreSQL Tuning","text":"<pre><code>-- Memory settings (for 64GB RAM server)\nALTER SYSTEM SET shared_buffers = '16GB';\nALTER SYSTEM SET effective_cache_size = '48GB';\nALTER SYSTEM SET maintenance_work_mem = '2GB';\nALTER SYSTEM SET work_mem = '256MB';\n\n-- Connection pooling\nALTER SYSTEM SET max_connections = 200;\nALTER SYSTEM SET max_prepared_transactions = 200;\n\n-- Write performance\nALTER SYSTEM SET checkpoint_completion_target = 0.9;\nALTER SYSTEM SET wal_buffers = '16MB';\nALTER SYSTEM SET default_statistics_target = 100;\n\n-- Query optimization\nALTER SYSTEM SET random_page_cost = 1.1;  -- For SSD\nALTER SYSTEM SET effective_io_concurrency = 200;  -- For SSD\n</code></pre>"},{"location":"deployment/production/#2-index-optimization","title":"2. Index Optimization","text":"<pre><code>-- Analyze query patterns and create appropriate indexes\nCREATE INDEX CONCURRENTLY idx_hashes_hashlist_cracked \n    ON hashes(hashlist_id, is_cracked);\n\nCREATE INDEX CONCURRENTLY idx_job_tasks_status \n    ON job_tasks(job_execution_id, status);\n\nCREATE INDEX CONCURRENTLY idx_agent_performance_metrics_lookup \n    ON agent_performance_metrics(agent_id, metric_type, timestamp);\n\n-- Regular maintenance\nVACUUM ANALYZE;\nREINDEX CONCURRENTLY;\n</code></pre>"},{"location":"deployment/production/#application-performance","title":"Application Performance","text":""},{"location":"deployment/production/#1-backend-optimization","title":"1. Backend Optimization","text":"<pre><code># Environment variables for performance\nexport GOMAXPROCS=16                    # Match CPU cores\nexport KH_HASHLIST_BATCH_SIZE=5000      # Increase for better throughput\nexport KH_DB_MAX_OPEN_CONNS=50          # Database connection pool\nexport KH_DB_MAX_IDLE_CONNS=25\nexport KH_WEBSOCKET_BUFFER_SIZE=8192    # Larger WebSocket buffers\n</code></pre>"},{"location":"deployment/production/#2-caching-strategy","title":"2. Caching Strategy","text":"<ul> <li>Implement Redis for session caching</li> <li>Cache agent benchmark results (24 hours)</li> <li>Cache frequently accessed wordlist metadata</li> <li>Use CDN for static assets</li> </ul>"},{"location":"deployment/production/#3-job-processing-optimization","title":"3. Job Processing Optimization","text":"<pre><code># Configure job chunking for optimal performance\nRule Splitting Threshold: 10000         # Split large rule files\nKeyspace Chunk Size: 1 hour            # Balanced chunk duration\nMaximum Concurrent Jobs: 100            # Per-agent limit\nTask Dispatch Batch Size: 10            # Parallel task dispatch\n</code></pre>"},{"location":"deployment/production/#network-optimization","title":"Network Optimization","text":""},{"location":"deployment/production/#1-websocket-tuning","title":"1. WebSocket Tuning","text":"<pre><code># Nginx configuration for WebSocket\nlocation /ws {\n    proxy_pass https://backend:31337;\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection \"upgrade\";\n    proxy_read_timeout 3600s;\n    proxy_send_timeout 3600s;\n    proxy_buffer_size 64k;\n    proxy_buffers 16 32k;\n}\n</code></pre>"},{"location":"deployment/production/#2-load-balancing","title":"2. Load Balancing","text":"<pre><code>upstream krakenhashes_backend {\n    least_conn;\n    server backend1:31337 max_fails=3 fail_timeout=30s;\n    server backend2:31337 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n</code></pre>"},{"location":"deployment/production/#high-availability-setup","title":"High Availability Setup","text":""},{"location":"deployment/production/#architecture-overview","title":"Architecture Overview","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Load Balancer\u2502\n                    \u2502   (HAProxy)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Backend 1  \u2502      \u2502  Backend 2  \u2502\n         \u2502  (Active)   \u2502      \u2502  (Active)   \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502     PostgreSQL Cluster          \u2502\n         \u2502   (Primary + Replica)           \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"deployment/production/#database-high-availability","title":"Database High Availability","text":""},{"location":"deployment/production/#1-postgresql-replication-setup","title":"1. PostgreSQL Replication Setup","text":"<pre><code># On Primary\npostgresql.conf:\nwal_level = replica\nmax_wal_senders = 3\nwal_keep_segments = 64\nsynchronous_commit = on\nsynchronous_standby_names = 'replica1'\n\n# On Replica\nrecovery.conf:\nstandby_mode = 'on'\nprimary_conninfo = 'host=primary port=5432 user=replicator'\ntrigger_file = '/tmp/postgresql.trigger'\n</code></pre>"},{"location":"deployment/production/#2-connection-pooling-with-pgbouncer","title":"2. Connection Pooling with PgBouncer","text":"<pre><code># pgbouncer.ini\n[databases]\nkrakenhashes = host=primary port=5432 dbname=krakenhashes\n\n[pgbouncer]\npool_mode = transaction\nmax_client_conn = 1000\ndefault_pool_size = 50\nmin_pool_size = 10\nreserve_pool_size = 5\nreserve_pool_timeout = 3\nserver_lifetime = 3600\nserver_idle_timeout = 600\n</code></pre>"},{"location":"deployment/production/#application-high-availability","title":"Application High Availability","text":""},{"location":"deployment/production/#1-backend-clustering","title":"1. Backend Clustering","text":"<pre><code># docker-compose-ha.yml\nservices:\n  backend1:\n    image: krakenhashes/backend:v0.1.0\n    environment:\n      - KH_CLUSTER_MODE=true\n      - KH_NODE_ID=backend1\n      - KH_CLUSTER_PEERS=backend2:31337\n    volumes:\n      - shared-data:/var/lib/krakenhashes\n\n  backend2:\n    image: krakenhashes/backend:v0.1.0\n    environment:\n      - KH_CLUSTER_MODE=true\n      - KH_NODE_ID=backend2\n      - KH_CLUSTER_PEERS=backend1:31337\n    volumes:\n      - shared-data:/var/lib/krakenhashes\n</code></pre>"},{"location":"deployment/production/#2-load-balancer-configuration","title":"2. Load Balancer Configuration","text":"<pre><code># haproxy.cfg\nglobal\n    maxconn 4096\n    log stdout local0\n\ndefaults\n    mode http\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n    option httplog\n\nfrontend krakenhashes_front\n    bind *:443 ssl crt /etc/ssl/krakenhashes.pem\n    default_backend krakenhashes_back\n\nbackend krakenhashes_back\n    balance leastconn\n    option httpchk GET /api/health\n    server backend1 backend1:31337 check ssl verify none\n    server backend2 backend2:31337 check ssl verify none\n</code></pre>"},{"location":"deployment/production/#storage-high-availability","title":"Storage High Availability","text":""},{"location":"deployment/production/#1-distributed-file-system","title":"1. Distributed File System","text":"<pre><code># GlusterFS setup for shared storage\ngluster volume create krakenhashes-data replica 2 \\\n    server1:/data/gluster/krakenhashes \\\n    server2:/data/gluster/krakenhashes\n\ngluster volume start krakenhashes-data\nmount -t glusterfs server1:/krakenhashes-data /var/lib/krakenhashes\n</code></pre>"},{"location":"deployment/production/#2-object-storage-integration","title":"2. Object Storage Integration","text":"<pre><code># S3-compatible storage for large files\nexport KH_STORAGE_TYPE=s3\nexport KH_S3_ENDPOINT=https://s3.example.com\nexport KH_S3_BUCKET=krakenhashes-data\nexport KH_S3_ACCESS_KEY=your-access-key\nexport KH_S3_SECRET_KEY=your-secret-key\n</code></pre>"},{"location":"deployment/production/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"deployment/production/#metrics-collection","title":"Metrics Collection","text":""},{"location":"deployment/production/#1-prometheus-integration","title":"1. Prometheus Integration","text":"<pre><code># prometheus.yml\nscrape_configs:\n  - job_name: 'krakenhashes'\n    static_configs:\n      - targets: ['backend1:31337', 'backend2:31337']\n    metrics_path: '/metrics'\n    scheme: 'https'\n    tls_config:\n      insecure_skip_verify: true\n</code></pre>"},{"location":"deployment/production/#2-key-metrics-to-monitor","title":"2. Key Metrics to Monitor","text":"<pre><code># System Metrics\n- CPU usage per core\n- Memory usage and available\n- Disk I/O and latency\n- Network throughput and errors\n\n# Application Metrics\n- Request rate and latency\n- Error rate by endpoint\n- Active WebSocket connections\n- Database connection pool usage\n\n# Business Metrics\n- Jobs queued/running/completed\n- Hash crack rate\n- Agent utilization\n- Storage usage growth\n</code></pre>"},{"location":"deployment/production/#alerting-configuration","title":"Alerting Configuration","text":""},{"location":"deployment/production/#1-critical-alerts-immediate-response","title":"1. Critical Alerts (Immediate Response)","text":"<pre><code>groups:\n  - name: critical\n    rules:\n      - alert: ServiceDown\n        expr: up{job=\"krakenhashes\"} == 0\n        for: 2m\n\n      - alert: DatabaseDown\n        expr: pg_up == 0\n        for: 1m\n\n      - alert: HighErrorRate\n        expr: rate(http_requests_total{status=~\"5..\"}[5m]) &gt; 0.05\n        for: 5m\n\n      - alert: DiskSpaceCritical\n        expr: disk_free_percentage &lt; 10\n        for: 5m\n</code></pre>"},{"location":"deployment/production/#2-warning-alerts-business-hours","title":"2. Warning Alerts (Business Hours)","text":"<pre><code>groups:\n  - name: warnings\n    rules:\n      - alert: HighCPUUsage\n        expr: cpu_usage_percentage &gt; 80\n        for: 15m\n\n      - alert: MemoryPressure\n        expr: memory_available_percentage &lt; 20\n        for: 10m\n\n      - alert: SlowQueries\n        expr: pg_slow_queries_rate &gt; 10\n        for: 10m\n\n      - alert: AgentDisconnections\n        expr: rate(agent_disconnections[5m]) &gt; 5\n        for: 5m\n</code></pre>"},{"location":"deployment/production/#logging-strategy","title":"Logging Strategy","text":""},{"location":"deployment/production/#1-centralized-logging","title":"1. Centralized Logging","text":"<pre><code># Fluentd configuration\n&lt;source&gt;\n  @type tail\n  path /var/log/krakenhashes/backend/*.log\n  pos_file /var/log/fluentd/krakenhashes.pos\n  tag krakenhashes.backend\n  &lt;parse&gt;\n    @type json\n  &lt;/parse&gt;\n&lt;/source&gt;\n\n&lt;match krakenhashes.**&gt;\n  @type elasticsearch\n  host elasticsearch.example.com\n  port 9200\n  logstash_format true\n  logstash_prefix krakenhashes\n&lt;/match&gt;\n</code></pre>"},{"location":"deployment/production/#2-log-retention-policy","title":"2. Log Retention Policy","text":"<ul> <li>Application logs: 30 days</li> <li>Security logs: 90 days</li> <li>Audit logs: 365 days</li> <li>Performance metrics: 90 days aggregated</li> </ul>"},{"location":"deployment/production/#dashboard-setup","title":"Dashboard Setup","text":""},{"location":"deployment/production/#1-grafana-dashboards","title":"1. Grafana Dashboards","text":"<ul> <li>System Overview Dashboard</li> <li>Job Performance Dashboard</li> <li>Agent Monitoring Dashboard</li> <li>Security Events Dashboard</li> <li>Database Performance Dashboard</li> </ul>"},{"location":"deployment/production/#2-real-time-monitoring","title":"2. Real-time Monitoring","text":"<pre><code># Custom monitoring endpoints\nGET /api/admin/metrics/realtime\nGET /api/admin/health/detailed\nGET /api/admin/agents/status\nGET /api/admin/jobs/statistics\n</code></pre>"},{"location":"deployment/production/#backup-and-disaster-recovery","title":"Backup and Disaster Recovery","text":""},{"location":"deployment/production/#backup-strategy","title":"Backup Strategy","text":""},{"location":"deployment/production/#1-automated-backup-schedule","title":"1. Automated Backup Schedule","text":"<pre><code># Database backups - every 4 hours\n0 */4 * * * /usr/local/bin/krakenhashes-db-backup.sh\n\n# File system backups - daily at 2 AM\n0 2 * * * /usr/local/bin/krakenhashes-file-backup.sh\n\n# Configuration backups - weekly\n0 3 * * 0 /usr/local/bin/krakenhashes-config-backup.sh\n\n# Off-site sync - daily at 4 AM\n0 4 * * * /usr/local/bin/krakenhashes-offsite-sync.sh\n</code></pre>"},{"location":"deployment/production/#2-backup-verification","title":"2. Backup Verification","text":"<pre><code>#!/bin/bash\n# Automated backup verification\nBACKUP_DIR=\"/backup/krakenhashes\"\nLOG_FILE=\"/var/log/krakenhashes-backup-verify.log\"\n\n# Verify latest backups\nfor backup_type in postgres files config; do\n    latest=$(find $BACKUP_DIR/$backup_type -name \"*.gz\" -mtime -1 | head -1)\n    if [ -z \"$latest\" ]; then\n        echo \"ERROR: No recent $backup_type backup found\" &gt;&gt; $LOG_FILE\n        # Send alert\n    fi\ndone\n</code></pre>"},{"location":"deployment/production/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":""},{"location":"deployment/production/#1-rtorpo-targets","title":"1. RTO/RPO Targets","text":"<ul> <li>Database RTO: 30 minutes</li> <li>Database RPO: 4 hours</li> <li>Full System RTO: 2 hours</li> <li>Full System RPO: 24 hours</li> </ul>"},{"location":"deployment/production/#2-recovery-procedures","title":"2. Recovery Procedures","text":"<pre><code># Quick recovery checklist\n1. Assess damage and determine recovery scope\n2. Provision replacement infrastructure\n3. Restore database from latest backup\n4. Restore file system data\n5. Update DNS/load balancer configuration\n6. Verify system functionality\n7. Communicate with users\n</code></pre>"},{"location":"deployment/production/#3-disaster-recovery-testing","title":"3. Disaster Recovery Testing","text":"<ul> <li>Monthly backup restoration tests</li> <li>Quarterly full DR drills</li> <li>Annual infrastructure failover test</li> <li>Document lessons learned</li> </ul>"},{"location":"deployment/production/#compliance-considerations","title":"Compliance Considerations","text":""},{"location":"deployment/production/#data-protection","title":"Data Protection","text":""},{"location":"deployment/production/#1-gdpr-compliance","title":"1. GDPR Compliance","text":"<ul> <li>Implement right to erasure (data deletion)</li> <li>Maintain data processing records</li> <li>Encrypt personal data at rest and in transit</li> <li>Implement data retention policies</li> <li>Regular privacy impact assessments</li> </ul>"},{"location":"deployment/production/#2-data-retention-policy","title":"2. Data Retention Policy","text":"<pre><code>-- Automated data retention\nDELETE FROM hashes \nWHERE is_cracked = true \n  AND cracked_at &lt; NOW() - INTERVAL '90 days'\n  AND hashlist_id IN (\n    SELECT id FROM hashlists \n    WHERE retention_days = 90\n  );\n\nDELETE FROM job_executions \nWHERE completed_at &lt; NOW() - INTERVAL '180 days';\n\nDELETE FROM agent_performance_metrics \nWHERE timestamp &lt; NOW() - INTERVAL '90 days';\n</code></pre>"},{"location":"deployment/production/#security-compliance","title":"Security Compliance","text":""},{"location":"deployment/production/#1-access-control-compliance","title":"1. Access Control Compliance","text":"<ul> <li>Implement least privilege access</li> <li>Regular access reviews</li> <li>Multi-factor authentication</li> <li>Session management controls</li> <li>Audit trail for all admin actions</li> </ul>"},{"location":"deployment/production/#2-audit-logging","title":"2. Audit Logging","text":"<pre><code>-- Audit table structure\nCREATE TABLE audit_logs (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER,\n    action VARCHAR(100),\n    resource_type VARCHAR(50),\n    resource_id INTEGER,\n    details JSONB,\n    ip_address INET,\n    user_agent TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Index for efficient querying\nCREATE INDEX idx_audit_logs_user_action \n    ON audit_logs(user_id, action, created_at);\n</code></pre>"},{"location":"deployment/production/#industry-standards","title":"Industry Standards","text":""},{"location":"deployment/production/#1-password-handling","title":"1. Password Handling","text":"<ul> <li>Never store plaintext passwords</li> <li>Use bcrypt with cost factor 12+</li> <li>Implement password history</li> <li>Enforce password complexity</li> <li>Regular password policy reviews</li> </ul>"},{"location":"deployment/production/#2-cryptographic-standards","title":"2. Cryptographic Standards","text":"<ul> <li>TLS 1.2 minimum (prefer TLS 1.3)</li> <li>Strong cipher suites only</li> <li>2048-bit RSA minimum (prefer 4096-bit)</li> <li>Regular certificate rotation</li> <li>Hardware security module (HSM) for keys</li> </ul>"},{"location":"deployment/production/#compliance-reporting","title":"Compliance Reporting","text":""},{"location":"deployment/production/#1-regular-reports","title":"1. Regular Reports","text":"<ul> <li>Monthly security metrics</li> <li>Quarterly compliance audits</li> <li>Annual penetration testing</li> <li>Incident response reports</li> <li>User access reviews</li> </ul>"},{"location":"deployment/production/#2-documentation-requirements","title":"2. Documentation Requirements","text":"<ul> <li>System architecture diagrams</li> <li>Data flow documentation</li> <li>Security policies and procedures</li> <li>Incident response plan</li> <li>Business continuity plan</li> </ul>"},{"location":"deployment/production/#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"deployment/production/#change-management","title":"Change Management","text":""},{"location":"deployment/production/#1-deployment-process","title":"1. Deployment Process","text":"<pre><code># Pre-deployment checklist\n- [ ] Code review completed\n- [ ] Security scan passed\n- [ ] Performance testing done\n- [ ] Backup taken\n- [ ] Rollback plan ready\n- [ ] Maintenance window scheduled\n- [ ] User notification sent\n</code></pre>"},{"location":"deployment/production/#2-version-control","title":"2. Version Control","text":"<ul> <li>Tag all production releases</li> <li>Maintain detailed changelogs</li> <li>Document breaking changes</li> <li>Test upgrade paths</li> <li>Keep rollback scripts ready</li> </ul>"},{"location":"deployment/production/#capacity-planning","title":"Capacity Planning","text":""},{"location":"deployment/production/#1-growth-monitoring","title":"1. Growth Monitoring","text":"<pre><code>-- Monitor growth trends\nSELECT \n    DATE_TRUNC('month', created_at) as month,\n    COUNT(*) as hashlists_created,\n    SUM(total_hashes) as total_hashes_added\nFROM hashlists\nGROUP BY month\nORDER BY month;\n</code></pre>"},{"location":"deployment/production/#2-scaling-triggers","title":"2. Scaling Triggers","text":"<ul> <li>CPU usage &gt; 70% sustained</li> <li>Memory usage &gt; 80%</li> <li>Storage usage &gt; 75%</li> <li>Response time &gt; 2 seconds</li> <li>Queue depth &gt; 1000 jobs</li> </ul>"},{"location":"deployment/production/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"deployment/production/#1-scheduled-maintenance","title":"1. Scheduled Maintenance","text":"<ul> <li>Weekly: Log rotation, temp file cleanup</li> <li>Monthly: Database optimization, index rebuilds</li> <li>Quarterly: Security updates, certificate renewal</li> <li>Annually: Major version upgrades</li> </ul>"},{"location":"deployment/production/#2-emergency-maintenance","title":"2. Emergency Maintenance","text":"<ul> <li>Critical security patches: Immediate</li> <li>Data corruption: Immediate</li> <li>Performance degradation: Within 4 hours</li> <li>Non-critical bugs: Next maintenance window</li> </ul>"},{"location":"deployment/production/#summary","title":"Summary","text":"<p>This production deployment guide provides a comprehensive framework for operating KrakenHashes at scale. Key takeaways:</p> <ol> <li>Security First: Implement defense in depth with multiple security layers</li> <li>High Availability: Design for failure with redundancy at every level</li> <li>Performance: Optimize continuously based on monitoring data</li> <li>Compliance: Maintain audit trails and follow data protection regulations</li> <li>Automation: Automate routine tasks to reduce human error</li> <li>Documentation: Keep all procedures documented and up to date</li> </ol> <p>Regular review and updates of these practices ensure your KrakenHashes deployment remains secure, performant, and reliable as your organization's needs evolve.</p>"},{"location":"deployment/updates/","title":"Update Procedures Guide","text":"<p>This guide covers the procedures for updating KrakenHashes deployments, including pre-update checks, update processes, and rollback procedures.</p> <p>\u26a0\ufe0f IMPORTANT: KrakenHashes is currently in v0.1.0-alpha. Breaking changes are expected between versions. Always review release notes and test updates in a non-production environment first.</p>"},{"location":"deployment/updates/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Pre-Update Checklist</li> <li>Updating Docker Deployments</li> <li>Database Migration Procedures</li> <li>Agent Update Process</li> <li>Rollback Procedures</li> <li>Version Compatibility</li> <li>Post-Update Verification</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/updates/#pre-update-checklist","title":"Pre-Update Checklist","text":"<p>Before beginning any update, complete the following checklist:</p>"},{"location":"deployment/updates/#1-review-release-notes","title":"1. Review Release Notes","text":"<ul> <li> Check the release notes for breaking changes</li> <li> Review migration scripts included in the release</li> <li> Identify any configuration changes required</li> <li> Note any new environment variables or removed features</li> </ul>"},{"location":"deployment/updates/#2-backup-current-system","title":"2. Backup Current System","text":"<pre><code># Backup database\ndocker-compose exec postgres pg_dump -U krakenhashes krakenhashes &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n\n# Backup configuration files\ncp -r /home/zerkereod/Programming/passwordCracking/krakenhashes/.env backup/.env.$(date +%Y%m%d_%H%M%S)\ncp -r /home/zerkereod/Programming/passwordCracking/kh-backend/config backup/config_$(date +%Y%m%d_%H%M%S)\n\n# Backup data directory\ntar -czf backup/data_$(date +%Y%m%d_%H%M%S).tar.gz /home/zerkereod/Programming/passwordCracking/kh-backend/data\n</code></pre>"},{"location":"deployment/updates/#3-check-system-health","title":"3. Check System Health","text":"<pre><code># Check service status\ndocker-compose ps\n\n# Verify no active jobs\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/health\n\n# Check agent connections\ndocker-compose logs backend | grep -i \"agent.*connected\" | tail -n 20\n</code></pre>"},{"location":"deployment/updates/#4-document-current-version","title":"4. Document Current Version","text":"<pre><code># Record current versions\ndocker-compose exec backend /app/krakenhashes --version &gt; current_version.txt\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1;\"\n</code></pre>"},{"location":"deployment/updates/#5-plan-maintenance-window","title":"5. Plan Maintenance Window","text":"<ul> <li> Notify users of planned downtime</li> <li> Schedule update during low-activity period</li> <li> Prepare rollback plan</li> <li> Assign responsible personnel</li> </ul>"},{"location":"deployment/updates/#updating-docker-deployments","title":"Updating Docker Deployments","text":""},{"location":"deployment/updates/#standard-update-process","title":"Standard Update Process","text":"<ol> <li> <p>Stop Current Services <pre><code>cd /home/zerkereod/Programming/passwordCracking/krakenhashes\ndocker-compose down\n</code></pre></p> </li> <li> <p>Pull Latest Code <pre><code>git fetch origin\ngit checkout tags/v0.2.0  # Replace with target version\n# OR for latest development\ngit checkout master\ngit pull origin master\n</code></pre></p> </li> <li> <p>Review Configuration Changes <pre><code># Check for new environment variables\ndiff .env.example .env\n\n# Apply any new required variables\nnano .env\n</code></pre></p> </li> <li> <p>Build and Start Services <pre><code># Build new images\ndocker-compose build --no-cache\n\n# Start services\ndocker-compose up -d\n\n# Monitor startup\ndocker-compose logs -f\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#incremental-updates-development","title":"Incremental Updates (Development)","text":"<p>For development environments with frequent updates:</p> <pre><code># Quick rebuild and restart\ndocker-compose down\ngit pull origin master\ndocker-compose up -d --build backend\ndocker-compose logs -f backend\n</code></pre>"},{"location":"deployment/updates/#database-migration-procedures","title":"Database Migration Procedures","text":""},{"location":"deployment/updates/#automatic-migrations","title":"Automatic Migrations","text":"<p>Migrations are automatically applied on backend startup. Monitor the process:</p> <pre><code># Watch migration logs\ndocker-compose logs backend | grep -i migration\n\n# Verify migration status\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT version, dirty FROM schema_migrations ORDER BY version DESC LIMIT 5;\"\n</code></pre>"},{"location":"deployment/updates/#manual-migration-control","title":"Manual Migration Control","text":"<p>For production environments requiring manual migration control:</p> <ol> <li> <p>Disable Auto-Migration <pre><code># In .env, set:\nAUTO_MIGRATE=false\n</code></pre></p> </li> <li> <p>Apply Migrations Manually <pre><code>cd backend\n\n# View pending migrations\nmake migrate-status\n\n# Apply all pending migrations\nmake migrate-up\n\n# Apply specific version\nmigrate -path db/migrations -database \"$DATABASE_URL\" goto 20240115120000\n</code></pre></p> </li> <li> <p>Verify Migration Success <pre><code># Check migration history\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM schema_migrations;\"\n\n# Test critical tables\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"\\dt\"\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#handling-failed-migrations","title":"Handling Failed Migrations","text":"<p>If a migration fails:</p> <ol> <li> <p>Check Migration Status <pre><code># Check if migration is dirty\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM schema_migrations WHERE dirty = true;\"\n</code></pre></p> </li> <li> <p>Fix Dirty Migration <pre><code># Force version (use with caution)\ncd backend\nmigrate -path db/migrations -database \"$DATABASE_URL\" force 20240115120000\n\n# Then retry\nmake migrate-up\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#agent-update-process","title":"Agent Update Process","text":""},{"location":"deployment/updates/#coordinated-agent-updates","title":"Coordinated Agent Updates","text":"<ol> <li> <p>Prepare New Agent Binary <pre><code># New agent binaries are typically included in backend updates\n# Verify new version is available\ndocker-compose exec backend ls -la /data/krakenhashes/binaries/\n</code></pre></p> </li> <li> <p>Notify Connected Agents <pre><code># Agents will receive update notifications via WebSocket\n# Monitor agent update status in backend logs\ndocker-compose logs -f backend | grep -i \"agent.*update\"\n</code></pre></p> </li> <li> <p>Manual Agent Update (if auto-update fails) <pre><code># On each agent machine\ncd /path/to/agent\n./update.sh  # If provided\n\n# Or manually:\nsystemctl stop krakenhashes-agent\nwget https://your-server/api/v1/binaries/agent/latest -O krakenhashes-agent\nchmod +x krakenhashes-agent\nsystemctl start krakenhashes-agent\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#agent-compatibility-check","title":"Agent Compatibility Check","text":"<p>Before updating: <pre><code># Check agent versions\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/agents | jq '.[] | {id, version, last_seen}'\n\n# Verify compatibility matrix in release notes\n</code></pre></p>"},{"location":"deployment/updates/#rollback-procedures","title":"Rollback Procedures","text":""},{"location":"deployment/updates/#quick-rollback-docker","title":"Quick Rollback (Docker)","text":"<ol> <li> <p>Stop Current Services <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>Restore Previous Version <pre><code># Checkout previous version\ngit checkout tags/v0.1.0  # Previous version\n\n# Restore configuration\ncp backup/.env.20240115_120000 .env\n\n# Rebuild and start\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Restore Database (if schema changed) <pre><code># Stop backend to prevent connections\ndocker-compose stop backend\n\n# Restore database\ndocker-compose exec -T postgres psql -U krakenhashes -d krakenhashes &lt; backup_20240115_120000.sql\n\n# Restart backend\ndocker-compose start backend\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#rollback-with-data-preservation","title":"Rollback with Data Preservation","text":"<p>For rollbacks that need to preserve new data:</p> <ol> <li> <p>Export New Data <pre><code># Export specific tables with new data\ndocker-compose exec postgres pg_dump -U krakenhashes -t job_executions -t hashes --data-only krakenhashes &gt; new_data.sql\n</code></pre></p> </li> <li> <p>Perform Rollback Follow standard rollback procedure</p> </li> <li> <p>Reimport Preserved Data <pre><code># Carefully reimport compatible data\ndocker-compose exec -T postgres psql -U krakenhashes -d krakenhashes &lt; new_data.sql\n</code></pre></p> </li> </ol>"},{"location":"deployment/updates/#version-compatibility","title":"Version Compatibility","text":""},{"location":"deployment/updates/#compatibility-matrix","title":"Compatibility Matrix","text":"Component Backend Agent Frontend Database Schema v0.1.0 0.1.0 0.1.0 0.1.0 19 v0.2.0 0.2.0 0.1.0-0.2.0 0.2.0 22 v1.0.0 1.0.0 1.0.0 1.0.0 30 <p>Note: During alpha, assume all components must be updated together unless release notes specify otherwise.</p>"},{"location":"deployment/updates/#checking-compatibility","title":"Checking Compatibility","text":"<pre><code># Check all component versions\ndocker-compose exec backend /app/krakenhashes --version\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/system/info\n\n# Check schema version\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT MAX(version) FROM schema_migrations;\"\n</code></pre>"},{"location":"deployment/updates/#post-update-verification","title":"Post-Update Verification","text":""},{"location":"deployment/updates/#1-system-health-checks","title":"1. System Health Checks","text":"<pre><code># Backend health\ncurl -s https://localhost:8443/api/v1/health | jq .\n\n# Database connectivity\ndocker-compose exec backend curl -s http://localhost:8080/api/v1/system/db-check\n\n# Frontend accessibility\ncurl -I https://localhost:8443\n</code></pre>"},{"location":"deployment/updates/#2-functional-verification","title":"2. Functional Verification","text":"<pre><code># Test authentication\ncurl -X POST https://localhost:8443/api/v1/auth/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\":\"admin\",\"password\":\"your-password\"}'\n\n# Check agent connectivity\ndocker-compose logs backend | grep -i \"websocket.*agent\" | tail -n 10\n\n# Verify job creation (with auth token)\ncurl -X GET https://localhost:8443/api/v1/jobs \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre>"},{"location":"deployment/updates/#3-data-integrity-checks","title":"3. Data Integrity Checks","text":"<pre><code>-- Connect to database\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes\n\n-- Check critical tables\nSELECT COUNT(*) FROM users;\nSELECT COUNT(*) FROM agents;\nSELECT COUNT(*) FROM hashlists;\nSELECT COUNT(*) FROM job_executions WHERE status = 'running';\n\n-- Verify migrations\nSELECT * FROM schema_migrations ORDER BY version DESC LIMIT 5;\n</code></pre>"},{"location":"deployment/updates/#4-performance-verification","title":"4. Performance Verification","text":"<pre><code># Check resource usage\ndocker stats --no-stream\n\n# Monitor logs for errors\ndocker-compose logs --tail=100 backend | grep -i error\n\n# Check response times\ntime curl -s https://localhost:8443/api/v1/health\n</code></pre>"},{"location":"deployment/updates/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/updates/#common-update-issues","title":"Common Update Issues","text":""},{"location":"deployment/updates/#1-migration-failures","title":"1. Migration Failures","text":"<pre><code># Check migration logs\ndocker-compose logs backend | grep -E \"(migration|migrate)\"\n\n# Reset dirty migration\ncd backend\nmigrate -path db/migrations -database \"$DATABASE_URL\" force VERSION_NUMBER\n</code></pre>"},{"location":"deployment/updates/#2-container-start-failures","title":"2. Container Start Failures","text":"<pre><code># Check detailed logs\ndocker-compose logs backend\ndocker-compose logs postgres\n\n# Verify file permissions\nls -la /home/zerkereod/Programming/passwordCracking/kh-backend/data\n\n# Check disk space\ndf -h\n</code></pre>"},{"location":"deployment/updates/#3-agent-connection-issues","title":"3. Agent Connection Issues","text":"<pre><code># Restart agent connections\ndocker-compose restart backend\n\n# Check WebSocket logs\ndocker-compose logs backend | grep -i websocket\n\n# Verify agent API keys are still valid\ndocker-compose exec postgres psql -U krakenhashes -c \"SELECT * FROM agents WHERE active = true;\"\n</code></pre>"},{"location":"deployment/updates/#4-frontend-loading-issues","title":"4. Frontend Loading Issues","text":"<pre><code># Clear browser cache and cookies\n# Rebuild frontend\ndocker-compose up -d --build app\n\n# Check nginx logs\ndocker-compose logs app\n</code></pre>"},{"location":"deployment/updates/#emergency-procedures","title":"Emergency Procedures","text":"<p>If the system becomes unresponsive:</p> <ol> <li> <p>Preserve Logs <pre><code>docker-compose logs &gt; emergency_logs_$(date +%Y%m%d_%H%M%S).txt\n</code></pre></p> </li> <li> <p>Force Stop <pre><code>docker-compose down -v\n</code></pre></p> </li> <li> <p>Clean Start <pre><code>docker system prune -f\ndocker-compose up -d --build\n</code></pre></p> </li> <li> <p>Contact Support</p> </li> <li>Provide emergency logs</li> <li>Document steps leading to failure</li> <li>Note any error messages</li> </ol>"},{"location":"deployment/updates/#best-practices","title":"Best Practices","text":"<ol> <li>Always Test Updates</li> <li>Use staging environment</li> <li>Test core functionality</li> <li> <p>Verify agent connectivity</p> </li> <li> <p>Schedule Wisely</p> </li> <li>Update during maintenance windows</li> <li>Avoid updates during active cracking jobs</li> <li> <p>Coordinate with users</p> </li> <li> <p>Document Everything</p> </li> <li>Record version changes</li> <li>Note configuration modifications</li> <li> <p>Log any issues encountered</p> </li> <li> <p>Monitor Post-Update</p> </li> <li>Watch logs for 24 hours</li> <li>Check performance metrics</li> <li> <p>Gather user feedback</p> </li> <li> <p>Maintain Backups</p> </li> <li>Keep multiple backup versions</li> <li>Test restore procedures regularly</li> <li>Store backups securely</li> </ol>"},{"location":"deployment/updates/#conclusion","title":"Conclusion","text":"<p>Updating KrakenHashes requires careful planning and execution, especially during the alpha phase. Always prioritize data safety and system availability. When in doubt, test in a non-production environment first.</p> <p>For additional support or questions about specific update scenarios, consult the documentation or contact the development team.</p>"},{"location":"developer/","title":"Developer Guide","text":"<p>Documentation for developers working on or extending KrakenHashes.</p>"},{"location":"developer/#in-this-section","title":"In This Section","text":"<ul> <li> <p>:material-architecture:{ .lg .middle } System Architecture</p> <p>Understanding the overall system design and components</p> </li> <li> <p> Backend Development</p> <p>Working with the Go backend, APIs, and services</p> </li> <li> <p> Frontend Development</p> <p>React frontend, Material-UI components, and state management</p> </li> <li> <p> Agent Development</p> <p>Agent architecture, hardware detection, and job execution</p> </li> </ul>"},{"location":"developer/#getting-started","title":"Getting Started","text":""},{"location":"developer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21+ for backend development</li> <li>Node.js 18+ for frontend development  </li> <li>Docker and Docker Compose for testing</li> <li>PostgreSQL 15+ for database</li> <li>Git for version control</li> </ul>"},{"location":"developer/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/ZerkerEOD/krakenhashes.git\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Set up development environment</p> </li> <li>Copy <code>.env.example</code> to <code>.env</code></li> <li>Configure database connection</li> <li> <p>Set development flags</p> </li> <li> <p>Start services <pre><code>docker-compose up -d\n</code></pre></p> </li> </ol>"},{"location":"developer/#development-workflow","title":"Development Workflow","text":""},{"location":"developer/#code-organization","title":"Code Organization","text":"<pre><code>krakenhashes/\n\u251c\u2500\u2500 backend/         # Go backend service\n\u251c\u2500\u2500 frontend/        # React frontend\n\u251c\u2500\u2500 agent/          # Go agent system\n\u251c\u2500\u2500 docs/           # Documentation\n\u2514\u2500\u2500 scripts/        # Utility scripts\n</code></pre>"},{"location":"developer/#key-technologies","title":"Key Technologies","text":"<ul> <li>Backend: Go, Gin, GORM, JWT, WebSocket</li> <li>Frontend: React, TypeScript, Material-UI, React Query</li> <li>Database: PostgreSQL with migrations</li> <li>Agent: Go, Hashcat integration, Hardware detection</li> <li>Communication: REST API, WebSocket, TLS</li> </ul>"},{"location":"developer/#contributing-guidelines","title":"Contributing Guidelines","text":"<p>Pre-v1.0 Status</p> <p>External contributions are not being accepted until v1.0 release. This documentation is for understanding the codebase structure.</p>"},{"location":"developer/#code-standards","title":"Code Standards","text":"<ul> <li>Follow Go conventions for backend code</li> <li>Use TypeScript strictly in frontend</li> <li>Write tests for new functionality</li> <li>Document complex algorithms</li> <li>Keep security in mind</li> </ul>"},{"location":"developer/#testing","title":"Testing","text":"<ul> <li>Unit tests alongside code (<code>*_test.go</code>)</li> <li>Integration tests in <code>integration_test/</code></li> <li>Frontend tests with React Testing Library</li> <li>Manual testing with Docker environment</li> </ul>"},{"location":"developer/#architecture-principles","title":"Architecture Principles","text":"<ol> <li>Separation of Concerns</li> <li>Clear boundaries between layers</li> <li>Repository pattern for data access</li> <li> <p>Service layer for business logic</p> </li> <li> <p>Security First</p> </li> <li>Authentication on all endpoints</li> <li>Input validation and sanitization</li> <li> <p>Secure communication channels</p> </li> <li> <p>Scalability</p> </li> <li>Distributed agent architecture</li> <li>Efficient job scheduling</li> <li> <p>Resource pooling</p> </li> <li> <p>Maintainability</p> </li> <li>Clear code organization</li> <li>Comprehensive error handling</li> <li>Extensive logging</li> </ol>"},{"location":"developer/#need-help","title":"Need Help?","text":"<ul> <li>Check existing code patterns</li> <li>Review test files for examples</li> <li>Join Discord development channel</li> </ul>"},{"location":"developer/agent/","title":"KrakenHashes Agent Development Guide","text":""},{"location":"developer/agent/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Agent Architecture Overview</li> <li>Hardware Detection System</li> <li>Job Execution Flow</li> <li>WebSocket Communication</li> <li>File Synchronization</li> <li>Metrics Collection</li> <li>Adding New Features</li> <li>Testing Agents</li> </ol>"},{"location":"developer/agent/#agent-architecture-overview","title":"Agent Architecture Overview","text":"<p>The KrakenHashes agent is a distributed compute node that executes password cracking jobs using hashcat. It's built with Go and designed to be cross-platform, supporting various GPU and CPU configurations.</p>"},{"location":"developer/agent/#core-components","title":"Core Components","text":"<pre><code>agent/\n\u251c\u2500\u2500 cmd/agent/          # Entry point and initialization\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 agent/         # Core agent logic and WebSocket connection\n\u2502   \u251c\u2500\u2500 auth/          # API key and certificate management\n\u2502   \u251c\u2500\u2500 config/        # Configuration management\n\u2502   \u251c\u2500\u2500 hardware/      # GPU/CPU detection using hashcat\n\u2502   \u251c\u2500\u2500 jobs/          # Job execution and hashcat management\n\u2502   \u251c\u2500\u2500 metrics/       # System metrics collection\n\u2502   \u251c\u2500\u2500 sync/          # File synchronization with backend\n\u2502   \u2514\u2500\u2500 status/        # Agent status management\n\u2514\u2500\u2500 pkg/debug/         # Debug logging utilities\n</code></pre>"},{"location":"developer/agent/#agent-lifecycle","title":"Agent Lifecycle","text":"<pre><code>// From cmd/agent/main.go\nfunc main() {\n    // 1. Initialize debug logging\n    debug.Reinitialize()\n\n    // 2. Load configuration from .env\n    cfg := loadConfig()\n\n    // 3. Initialize data directories\n    dataDirs, err := config.GetDataDirs()\n\n    // 4. Create metrics collector\n    collector, err := metrics.New(metrics.Config{\n        CollectionInterval: time.Duration(cfg.heartbeatInterval) * time.Second,\n        EnableGPU:          true,\n    })\n\n    // 5. Load or register agent credentials\n    agentID, cert, err := agent.LoadCredentials()\n\n    // 6. Create WebSocket connection\n    conn, err := agent.NewConnection(urlConfig)\n\n    // 7. Create job manager with hardware monitor\n    hwMonitor := conn.GetHardwareMonitor()\n    jobManager = jobs.NewJobManager(agentConfig, nil, hwMonitor)\n\n    // 8. Start connection and maintenance\n    conn.Start()\n\n    // 9. Detect and send device information\n    conn.DetectAndSendDevices()\n}\n</code></pre>"},{"location":"developer/agent/#hardware-detection-system","title":"Hardware Detection System","text":"<p>The agent uses hashcat's built-in device detection to identify available compute devices (GPUs and CPUs).</p>"},{"location":"developer/agent/#device-detection-implementation","title":"Device Detection Implementation","text":"<pre><code>// From internal/hardware/hashcat_detector.go\ntype HashcatDetector struct {\n    binaryPath     string\n    dataDirectory  string\n}\n\nfunc (d *HashcatDetector) DetectDevices() (*types.DeviceDetectionResult, error) {\n    // Build hashcat command with device info flags\n    args := []string{\"-I\", \"--machine-readable\", \"--quiet\"}\n\n    cmd := exec.CommandContext(ctx, d.binaryPath, args...)\n    output, err := cmd.Output()\n\n    // Parse hashcat output\n    devices, backends, err := d.parseHashcatOutput(string(output))\n\n    return &amp;types.DeviceDetectionResult{\n        Devices:          devices,\n        OpenCLBackends:   backends,\n        DetectedAt:       time.Now(),\n        HashcatVersion:   d.getHashcatVersion(),\n    }, nil\n}\n</code></pre>"},{"location":"developer/agent/#device-structure","title":"Device Structure","text":"<pre><code>// From internal/hardware/types/device.go\ntype Device struct {\n    ID          int    `json:\"id\"`\n    Type        string `json:\"type\"`        // \"gpu\" or \"cpu\"\n    Brand       string `json:\"brand\"`       // \"nvidia\", \"amd\", \"intel\"\n    Name        string `json:\"name\"`\n    Processor   string `json:\"processor\"`\n    Memory      int64  `json:\"memory\"`      // In bytes\n    DriverVersion string `json:\"driver_version\"`\n\n    // Performance characteristics\n    PCIeBus     string `json:\"pcie_bus\"`\n    CoreCount   int    `json:\"core_count\"`\n    ClockSpeed  int    `json:\"clock_speed\"` // In MHz\n\n    // Runtime state\n    Enabled     bool   `json:\"enabled\"`\n}\n</code></pre>"},{"location":"developer/agent/#hardware-monitor","title":"Hardware Monitor","text":"<pre><code>// From internal/hardware/monitor.go\ntype Monitor struct {\n    mu             sync.RWMutex\n    devices        []types.Device\n    hashcatDetector *HashcatDetector\n}\n\n// Detect devices using hashcat\nfunc (m *Monitor) DetectDevices() (*types.DeviceDetectionResult, error) {\n    result, err := m.hashcatDetector.DetectDevices()\n    if err != nil {\n        return nil, err\n    }\n\n    // Store devices in monitor\n    m.mu.Lock()\n    m.devices = result.Devices\n    m.mu.Unlock()\n\n    return result, nil\n}\n\n// Get enabled device flags for hashcat (-d parameter)\nfunc (m *Monitor) GetEnabledDeviceFlags() string {\n    m.mu.RLock()\n    defer m.mu.RUnlock()\n\n    return BuildDeviceFlags(m.devices)\n}\n</code></pre>"},{"location":"developer/agent/#job-execution-flow","title":"Job Execution Flow","text":"<p>The agent executes hashcat jobs based on task assignments from the backend.</p>"},{"location":"developer/agent/#job-manager","title":"Job Manager","text":"<pre><code>// From internal/jobs/jobs.go\ntype JobManager struct {\n    executor         *HashcatExecutor\n    config           *config.Config\n    progressCallback func(*JobProgress)\n    outputCallback   func(taskID string, output string, isError bool)\n    fileSync         *filesync.FileSync\n    hwMonitor        HardwareMonitor\n\n    mutex           sync.RWMutex\n    activeJobs      map[string]*JobExecution\n}\n\n// Process a job assignment from the backend\nfunc (jm *JobManager) ProcessJobAssignment(ctx context.Context, assignmentData []byte) error {\n    var assignment JobTaskAssignment\n    err := json.Unmarshal(assignmentData, &amp;assignment)\n\n    // 1. Ensure hashlist is available\n    err = jm.ensureHashlist(ctx, &amp;assignment)\n\n    // 2. Ensure rule chunks are available if needed\n    err = jm.ensureRuleChunks(ctx, &amp;assignment)\n\n    // 3. Start job execution\n    process, err := jm.executor.ExecuteTask(ctx, &amp;assignment)\n\n    // 4. Monitor job progress\n    go jm.monitorJobProgress(ctx, jobExecution)\n\n    return nil\n}\n</code></pre>"},{"location":"developer/agent/#hashcat-executor","title":"Hashcat Executor","text":"<pre><code>// From internal/jobs/hashcat_executor.go\ntype HashcatExecutor struct {\n    dataDirectory      string\n    activeProcesses    map[string]*HashcatProcess\n    mu                 sync.RWMutex\n    agentExtraParams   string\n    deviceFlagsCallback func() string\n}\n\n// Execute a hashcat task\nfunc (e *HashcatExecutor) ExecuteTask(ctx context.Context, assignment *JobTaskAssignment) (*HashcatProcess, error) {\n    // 1. Build hashcat command\n    args := e.buildHashcatCommand(assignment)\n\n    // 2. Create process with output capture\n    cmd := exec.CommandContext(ctx, binaryPath, args...)\n\n    // 3. Set up output pipes\n    stdout, _ := cmd.StdoutPipe()\n    stderr, _ := cmd.StderrPipe()\n\n    // 4. Start the process\n    err := cmd.Start()\n\n    // 5. Monitor output and parse progress\n    go e.monitorOutput(process, stdout, stderr)\n\n    return process, nil\n}\n\n// Build hashcat command with all parameters\nfunc (e *HashcatExecutor) buildHashcatCommand(assignment *JobTaskAssignment) []string {\n    args := []string{\n        \"-m\", strconv.Itoa(assignment.HashType),\n        \"-a\", strconv.Itoa(assignment.AttackMode),\n        \"--status\",\n        \"--status-json\",\n        \"--status-timer\", strconv.Itoa(assignment.ReportInterval),\n    }\n\n    // Add device flags if callback is set\n    if e.deviceFlagsCallback != nil {\n        deviceFlags := e.deviceFlagsCallback()\n        if deviceFlags != \"\" {\n            args = append(args, \"-d\", deviceFlags)\n        }\n    }\n\n    // Add skip and limit for keyspace splitting\n    if assignment.KeyspaceStart &gt; 0 {\n        args = append(args, \"-s\", strconv.FormatInt(assignment.KeyspaceStart, 10))\n    }\n    if assignment.KeyspaceEnd &gt; assignment.KeyspaceStart {\n        limit := assignment.KeyspaceEnd - assignment.KeyspaceStart\n        args = append(args, \"-l\", strconv.FormatInt(limit, 10))\n    }\n\n    return args\n}\n</code></pre>"},{"location":"developer/agent/#job-progress-tracking","title":"Job Progress Tracking","text":"<pre><code>// From internal/jobs/types.go\ntype JobProgress struct {\n    TaskID            string         `json:\"task_id\"`\n    Status            string         `json:\"status\"`\n    Progress          float64        `json:\"progress\"`\n    Speed             int64          `json:\"speed\"`\n    DeviceSpeeds      []DeviceSpeed  `json:\"device_speeds\"`\n    TimeRemaining     int            `json:\"time_remaining\"`\n    KeyspaceProcessed int64          `json:\"keyspace_processed\"`\n    CrackedCount      int            `json:\"cracked_count\"`\n    CrackedHashes     []CrackedHash  `json:\"cracked_hashes,omitempty\"`\n    ErrorMessage      string         `json:\"error_message,omitempty\"`\n}\n</code></pre>"},{"location":"developer/agent/#websocket-communication","title":"WebSocket Communication","text":"<p>The agent maintains a persistent WebSocket connection with the backend for real-time communication.</p>"},{"location":"developer/agent/#connection-management","title":"Connection Management","text":"<pre><code>// From internal/agent/connection.go\ntype Connection struct {\n    ws              *websocket.Conn\n    urlConfig       *config.URLConfig\n    hwMonitor       *hardware.Monitor\n    outbound        chan *WSMessage\n    done            chan struct{}\n    isConnected     atomic.Bool\n    tlsConfig       *tls.Config\n    fileSync        *filesync.FileSync\n    jobManager      JobManager\n}\n\n// WebSocket message types\ntype WSMessageType string\n\nconst (\n    WSTypeHardwareInfo      WSMessageType = \"hardware_info\"\n    WSTypeMetrics           WSMessageType = \"metrics\"\n    WSTypeHeartbeat         WSMessageType = \"heartbeat\"\n    WSTypeAgentStatus       WSMessageType = \"agent_status\"\n    WSTypeFileSyncRequest   WSMessageType = \"file_sync_request\"\n    WSTypeFileSyncResponse  WSMessageType = \"file_sync_response\"\n    WSTypeTaskAssignment    WSMessageType = \"task_assignment\"\n    WSTypeJobProgress       WSMessageType = \"job_progress\"\n    WSTypeJobStop           WSMessageType = \"job_stop\"\n    WSTypeBenchmarkRequest  WSMessageType = \"benchmark_request\"\n    WSTypeBenchmarkResult   WSMessageType = \"benchmark_result\"\n    WSTypeHashcatOutput     WSMessageType = \"hashcat_output\"\n    WSTypeDeviceDetection   WSMessageType = \"device_detection\"\n    WSTypeDeviceUpdate      WSMessageType = \"device_update\"\n)\n</code></pre>"},{"location":"developer/agent/#message-handling","title":"Message Handling","text":"<pre><code>// From internal/agent/connection.go - readPump method\nfunc (c *Connection) readPump() {\n    for {\n        var msg WSMessage\n        err := c.ws.ReadJSON(&amp;msg)\n\n        switch msg.Type {\n        case WSTypeTaskAssignment:\n            // Process job assignment\n            if err := c.jobManager.ProcessJobAssignment(ctx, msg.Payload); err != nil {\n                debug.Error(\"Failed to process job assignment: %v\", err)\n            }\n\n        case WSTypeFileSyncRequest:\n            // Handle file sync request\n            go c.handleFileSyncAsync(requestPayload)\n\n        case WSTypeBenchmarkRequest:\n            // Run speed test\n            go func() {\n                totalSpeed, deviceSpeeds, err := executor.RunSpeedTest(ctx, assignment, testDuration)\n                // Send results back\n            }()\n\n        case WSTypeDeviceUpdate:\n            // Update device enabled/disabled state\n            c.hwMonitor.UpdateDeviceStatus(updatePayload.DeviceID, updatePayload.Enabled)\n        }\n    }\n}\n</code></pre>"},{"location":"developer/agent/#sending-updates","title":"Sending Updates","text":"<pre><code>// Send job progress to backend\nfunc (c *Connection) SendJobProgress(progress *jobs.JobProgress) error {\n    progressJSON, err := json.Marshal(progress)\n\n    msg := &amp;WSMessage{\n        Type:      WSTypeJobProgress,\n        Payload:   progressJSON,\n        Timestamp: time.Now(),\n    }\n\n    select {\n    case c.outbound &lt;- msg:\n        return nil\n    case &lt;-time.After(5 * time.Second):\n        return fmt.Errorf(\"failed to queue job progress: channel blocked\")\n    }\n}\n</code></pre>"},{"location":"developer/agent/#file-synchronization","title":"File Synchronization","text":"<p>The agent synchronizes wordlists, rules, and binaries with the backend.</p>"},{"location":"developer/agent/#file-sync-implementation","title":"File Sync Implementation","text":"<pre><code>// From internal/sync/sync.go\ntype FileSync struct {\n    client     *http.Client\n    urlConfig  *config.URLConfig\n    dataDirs   *config.DataDirs\n    sem        chan struct{} // Semaphore for concurrent downloads\n    maxRetries int\n    apiKey     string\n    agentID    string\n}\n\n// Download a file from the backend\nfunc (fs *FileSync) DownloadFileFromInfo(ctx context.Context, fileInfo *FileInfo) error {\n    // 1. Check if file already exists with correct hash\n    if fs.fileExistsWithHash(fileInfo) {\n        return nil\n    }\n\n    // 2. Build download URL\n    url := fs.buildDownloadURL(fileInfo)\n\n    // 3. Create authenticated request\n    req, _ := http.NewRequestWithContext(ctx, \"GET\", url, nil)\n    req.Header.Set(\"X-API-Key\", fs.apiKey)\n    req.Header.Set(\"X-Agent-ID\", fs.agentID)\n\n    // 4. Download to temporary file\n    tempPath := finalPath + \".tmp\"\n    err := fs.downloadToFile(req, tempPath)\n\n    // 5. Verify hash\n    if !fs.verifyFileHash(tempPath, fileInfo.MD5Hash) {\n        return fmt.Errorf(\"hash mismatch\")\n    }\n\n    // 6. Move to final location\n    os.Rename(tempPath, finalPath)\n\n    // 7. Extract if it's a binary archive\n    if fileInfo.FileType == \"binary\" &amp;&amp; strings.HasSuffix(fileInfo.Name, \".7z\") {\n        fs.ExtractBinary7z(finalPath, targetDir)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/agent/#directory-structure","title":"Directory Structure","text":"<pre><code>// From internal/config/dirs.go\ntype DataDirs struct {\n    Binaries   string // /path/to/data/binaries\n    Wordlists  string // /path/to/data/wordlists\n    Rules      string // /path/to/data/rules\n    Hashlists  string // /path/to/data/hashlists\n}\n\n// Wordlist categories:\n// - general/     # Common wordlists\n// - specialized/ # Domain-specific lists\n// - targeted/    # Custom lists for specific targets\n// - custom/      # User-uploaded lists\n\n// Rule categories:\n// - hashcat/     # Hashcat rule files\n// - john/        # John the Ripper rules\n// - custom/      # User-created rules\n// - chunks/      # Split rule files for distributed processing\n</code></pre>"},{"location":"developer/agent/#metrics-collection","title":"Metrics Collection","text":"<p>The agent collects system metrics for monitoring and optimization.</p>"},{"location":"developer/agent/#metrics-collector","title":"Metrics Collector","text":"<pre><code>// From internal/metrics/collector.go\ntype Collector struct {\n    interval   time.Duration\n    gpuEnabled bool\n}\n\n// Collect system metrics\nfunc (c *Collector) Collect() (*SystemMetrics, error) {\n    metrics := &amp;SystemMetrics{}\n\n    // CPU metrics using gopsutil\n    percentage, _ := cpu.Percent(time.Second, false)\n    metrics.CPUUsage = percentage[0]\n\n    // Memory metrics\n    vmem, _ := mem.VirtualMemory()\n    metrics.MemoryUsage = vmem.UsedPercent\n\n    // GPU metrics come from hashcat JSON status\n    // during job execution\n\n    return metrics, nil\n}\n</code></pre>"},{"location":"developer/agent/#metrics-data-structure","title":"Metrics Data Structure","text":"<pre><code>// From internal/agent/connection.go\ntype MetricsData struct {\n    AgentID     int               `json:\"agent_id\"`\n    CollectedAt time.Time         `json:\"collected_at\"`\n    CPUs        []CPUMetrics      `json:\"cpus\"`\n    GPUs        []GPUMetrics      `json:\"gpus\"`\n    Memory      MemoryMetrics     `json:\"memory\"`\n    Disk        []DiskMetrics     `json:\"disk\"`\n    Network     []NetworkMetrics  `json:\"network\"`\n    Process     []ProcessMetrics  `json:\"process\"`\n}\n</code></pre>"},{"location":"developer/agent/#adding-new-features","title":"Adding New Features","text":""},{"location":"developer/agent/#1-adding-a-new-websocket-message-type","title":"1. Adding a New WebSocket Message Type","text":"<pre><code>// 1. Define the message type in connection.go\nconst WSTypeNewFeature WSMessageType = \"new_feature\"\n\n// 2. Create payload structures\ntype NewFeatureRequest struct {\n    Parameter1 string `json:\"parameter1\"`\n    Parameter2 int    `json:\"parameter2\"`\n}\n\n// 3. Add handler in readPump\ncase WSTypeNewFeature:\n    var payload NewFeatureRequest\n    if err := json.Unmarshal(msg.Payload, &amp;payload); err != nil {\n        debug.Error(\"Failed to parse new feature: %v\", err)\n        continue\n    }\n\n    // Handle the feature\n    go c.handleNewFeature(payload)\n\n// 4. Implement the handler\nfunc (c *Connection) handleNewFeature(payload NewFeatureRequest) {\n    // Implementation\n}\n</code></pre>"},{"location":"developer/agent/#2-adding-hardware-support","title":"2. Adding Hardware Support","text":"<pre><code>// 1. Update device detection in hashcat_detector.go\nfunc (d *HashcatDetector) parseHashcatOutput(output string) ([]types.Device, []types.OpenCLBackend, error) {\n    // Add parsing for new hardware types\n\n    // Example: Detect new GPU vendor\n    if strings.Contains(line, \"NewVendor\") {\n        device.Brand = \"newvendor\"\n        device.Type = \"gpu\"\n    }\n}\n\n// 2. Add device-specific handling\nfunc BuildDeviceFlags(devices []types.Device) string {\n    // Add logic for new device types\n}\n</code></pre>"},{"location":"developer/agent/#3-adding-job-features","title":"3. Adding Job Features","text":"<pre><code>// 1. Update JobTaskAssignment structure\ntype JobTaskAssignment struct {\n    // Existing fields...\n\n    NewFeature string `json:\"new_feature\"`\n}\n\n// 2. Update hashcat command building\nfunc (e *HashcatExecutor) buildHashcatCommand(assignment *JobTaskAssignment) []string {\n    // Add new hashcat parameters\n    if assignment.NewFeature != \"\" {\n        args = append(args, \"--new-feature\", assignment.NewFeature)\n    }\n}\n\n// 3. Update progress monitoring if needed\nfunc (e *HashcatExecutor) parseHashcatStatus(status map[string]interface{}) *JobProgress {\n    // Parse new status fields\n}\n</code></pre>"},{"location":"developer/agent/#testing-agents","title":"Testing Agents","text":""},{"location":"developer/agent/#unit-testing","title":"Unit Testing","text":"<pre><code>// Example from agent_test.go\nfunc TestGetAgentID(t *testing.T) {\n    tests := []struct {\n        name        string\n        setupFunc   func(configDir string) error\n        expectedID  int\n        wantErr     bool\n    }{\n        {\n            name: \"successful ID retrieval\",\n            setupFunc: func(configDir string) error {\n                return auth.SaveAgentKey(configDir, \"test-api-key\", \"456\")\n            },\n            expectedID: 456,\n            wantErr:    false,\n        },\n    }\n\n    for _, tt := range tests {\n        t.Run(tt.name, func(t *testing.T) {\n            tempDir := t.TempDir()\n            t.Setenv(\"KH_CONFIG_DIR\", tempDir)\n\n            if tt.setupFunc != nil {\n                err := tt.setupFunc(tempDir)\n                require.NoError(t, err)\n            }\n\n            id, err := GetAgentID()\n\n            if tt.wantErr {\n                assert.Error(t, err)\n            } else {\n                assert.NoError(t, err)\n                assert.Equal(t, tt.expectedID, id)\n            }\n        })\n    }\n}\n</code></pre>"},{"location":"developer/agent/#integration-testing","title":"Integration Testing","text":"<pre><code>// Test WebSocket connection\nfunc TestWebSocketConnection(t *testing.T) {\n    // Create test server\n    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        upgrader := websocket.Upgrader{}\n        conn, _ := upgrader.Upgrade(w, r, nil)\n        defer conn.Close()\n\n        // Test message exchange\n        var msg WSMessage\n        conn.ReadJSON(&amp;msg)\n        assert.Equal(t, WSTypeHeartbeat, msg.Type)\n\n        // Send response\n        conn.WriteJSON(WSMessage{\n            Type: WSTypeHeartbeat,\n            Timestamp: time.Now(),\n        })\n    }))\n    defer server.Close()\n\n    // Test connection\n    conn, err := NewConnection(urlConfig)\n    assert.NoError(t, err)\n\n    err = conn.Start()\n    assert.NoError(t, err)\n}\n</code></pre>"},{"location":"developer/agent/#mock-testing","title":"Mock Testing","text":"<pre><code>// From internal/mocks/\ntype MockJobManager struct {\n    mock.Mock\n}\n\nfunc (m *MockJobManager) ProcessJobAssignment(ctx context.Context, data []byte) error {\n    args := m.Called(ctx, data)\n    return args.Error(0)\n}\n\n// Use in tests\nfunc TestJobProcessing(t *testing.T) {\n    mockJM := new(MockJobManager)\n    mockJM.On(\"ProcessJobAssignment\", mock.Anything, mock.Anything).Return(nil)\n\n    conn := &amp;Connection{\n        jobManager: mockJM,\n    }\n\n    // Test job processing\n    msg := WSMessage{\n        Type: WSTypeTaskAssignment,\n        Payload: json.RawMessage(`{\"task_id\": \"test-123\"}`),\n    }\n\n    // Process message\n    // Assert expectations\n    mockJM.AssertExpectations(t)\n}\n</code></pre>"},{"location":"developer/agent/#performance-testing","title":"Performance Testing","text":"<pre><code>func BenchmarkHashcatCommandBuilding(b *testing.B) {\n    executor := NewHashcatExecutor(\"/data\")\n    assignment := &amp;JobTaskAssignment{\n        TaskID:     \"bench-task\",\n        HashType:   0,\n        AttackMode: 0,\n        WordlistPaths: []string{\"wordlist1.txt\", \"wordlist2.txt\"},\n        RulePaths:     []string{\"rule1.rule\", \"rule2.rule\"},\n    }\n\n    b.ResetTimer()\n    for i := 0; i &lt; b.N; i++ {\n        _ = executor.buildHashcatCommand(assignment)\n    }\n}\n</code></pre>"},{"location":"developer/agent/#manual-testing","title":"Manual Testing","text":"<pre><code># Build and run agent locally\ncd agent\ngo build -o krakenhashes-agent cmd/agent/main.go\n\n# Run with test configuration\n./krakenhashes-agent -host localhost:8080 -claim TEST-CLAIM-CODE -debug\n\n# Test specific components\n# Device detection\n./krakenhashes-agent -test-devices\n\n# File sync\n./krakenhashes-agent -test-sync wordlists\n\n# Benchmark\n./krakenhashes-agent -test-benchmark -m 0 -a 0\n</code></pre>"},{"location":"developer/agent/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Error Handling: Always use the debug package for logging    <pre><code>debug.Error(\"Operation failed: %v\", err)\ndebug.Info(\"Operation completed successfully\")\n</code></pre></p> </li> <li> <p>Resource Management: Always clean up resources    <pre><code>defer func() {\n  if conn != nil {\n      conn.Close()\n  }\n}()\n</code></pre></p> </li> <li> <p>Concurrent Operations: Use proper synchronization    <pre><code>type SafeMap struct {\n    mu   sync.RWMutex\n    data map[string]interface{}\n}\n</code></pre></p> </li> <li> <p>Context Usage: Respect context cancellation    <pre><code>select {\ncase &lt;-ctx.Done():\n    return ctx.Err()\ncase result := &lt;-resultChan:\n    return result\n}\n</code></pre></p> </li> <li> <p>Configuration: Use environment variables    <pre><code>value := os.Getenv(\"KH_SETTING\")\nif value == \"\" {\n    value = \"default\"\n}\n</code></pre></p> </li> </ol>"},{"location":"developer/agent/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Certificate Errors: Agent will attempt to renew certificates automatically</li> <li>Connection Drops: Automatic reconnection with exponential backoff</li> <li>File Sync Failures: Automatic retry with hash verification</li> <li>Hashcat Errors: Check device permissions and driver installation</li> <li>Memory Issues: Monitor system resources during large jobs</li> </ol> <p>For debugging, enable debug mode: <pre><code>export DEBUG=true\nexport LOG_LEVEL=DEBUG\n</code></pre></p>"},{"location":"developer/architecture/","title":"KrakenHashes System Architecture","text":""},{"location":"developer/architecture/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>High-Level Architecture</li> <li>Backend Architecture</li> <li>Frontend Architecture</li> <li>Agent Architecture</li> <li>Communication Protocols</li> <li>Database Schema</li> <li>Security Architecture</li> <li>File Storage Architecture</li> <li>Deployment Architecture</li> </ol>"},{"location":"developer/architecture/#overview","title":"Overview","text":"<p>KrakenHashes is a distributed password cracking management system designed to orchestrate and manage hashcat operations across multiple compute agents. The system follows a client-server architecture with a centralized backend, web-based frontend, and distributed agent nodes.</p>"},{"location":"developer/architecture/#key-components","title":"Key Components","text":"<ul> <li>Backend Server (Go): REST API server managing job orchestration, user authentication, and agent coordination</li> <li>Frontend (React/TypeScript): Web UI for system management and monitoring</li> <li>Agent (Go): Distributed compute nodes executing hashcat jobs</li> <li>PostgreSQL Database: Persistent storage for system data</li> <li>File Storage: Centralized storage for binaries, wordlists, rules, and hashlists</li> </ul>"},{"location":"developer/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            Frontend (React)                              \u2502\n\u2502                         Material-UI Components                           \u2502\n\u2502                      React Query + TypeScript                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 HTTPS/REST API\n                                 \u2502 WebSocket\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Backend Server (Go)                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Handlers   \u2502  \u2502   Services   \u2502  \u2502 Repositories \u2502  \u2502 Middleware \u2502 \u2502\n\u2502  \u2502  (HTTP/WS)   \u2502  \u2502 (Business)   \u2502  \u2502   (Data)     \u2502  \u2502   (Auth)   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502 SQL\n                                 \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         PostgreSQL Database                              \u2502\n\u2502                    (Users, Agents, Jobs, Hashlists)                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Agent Nodes (Go)                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Hardware   \u2502  \u2502     Job      \u2502  \u2502     Sync     \u2502  \u2502 Heartbeat  \u2502 \u2502\n\u2502  \u2502  Detection   \u2502  \u2502  Execution   \u2502  \u2502   Manager    \u2502  \u2502  Manager   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer/architecture/#backend-architecture","title":"Backend Architecture","text":""},{"location":"developer/architecture/#layered-architecture","title":"Layered Architecture","text":"<p>The backend follows a clean layered architecture with clear separation of concerns:</p>"},{"location":"developer/architecture/#1-presentation-layer-internalhandlers","title":"1. Presentation Layer (<code>internal/handlers/</code>)","text":"<ul> <li>HTTP request handlers organized by domain</li> <li>WebSocket handlers for real-time communication</li> <li>Request validation and response formatting</li> </ul> <p>Key Packages: - <code>admin/</code> - Administrative functions (users, clients, settings) - <code>agent/</code> - Agent management and registration - <code>auth/</code> - Authentication and authorization - <code>hashlist/</code> - Hashlist management - <code>jobs/</code> - Job execution and monitoring - <code>websocket/</code> - WebSocket connection handling</p>"},{"location":"developer/architecture/#2-service-layer-internalservices","title":"2. Service Layer (<code>internal/services/</code>)","text":"<ul> <li>Business logic implementation</li> <li>Transaction management</li> <li>Cross-cutting concerns (scheduling, monitoring)</li> </ul> <p>Key Services: - <code>AgentService</code> - Agent lifecycle management - <code>JobExecutionService</code> - Job orchestration - <code>JobSchedulingService</code> - Task distribution - <code>ClientService</code> - Customer management - <code>RetentionService</code> - Automated data purging with secure deletion - <code>WebSocketService</code> - Real-time communication hub - <code>HashlistSyncService</code> - File synchronization to agents - <code>MetricsCleanupService</code> - Agent metrics pruning</p>"},{"location":"developer/architecture/#3-repository-layer-internalrepository","title":"3. Repository Layer (<code>internal/repository/</code>)","text":"<ul> <li>Database access abstraction</li> <li>SQL query execution</li> <li>Data mapping</li> </ul> <p>Key Repositories: - <code>UserRepository</code> - User account management - <code>AgentRepository</code> - Agent registration and status - <code>HashlistRepository</code> - Hashlist storage - <code>JobExecutionRepository</code> - Job tracking - <code>JobTaskRepository</code> - Task management</p>"},{"location":"developer/architecture/#4-infrastructure-layer","title":"4. Infrastructure Layer","text":"<ul> <li>Database connections (<code>internal/database/</code>)</li> <li>File storage (<code>internal/binary/</code>, <code>internal/wordlist/</code>, <code>internal/rule/</code>)</li> <li>External integrations (email providers)</li> <li>TLS/SSL management (<code>internal/tls/</code>)</li> </ul>"},{"location":"developer/architecture/#design-patterns","title":"Design Patterns","text":"<ol> <li>Repository Pattern: All database operations go through repository interfaces</li> <li>Service Layer Pattern: Business logic separated from data access</li> <li>Middleware Pattern: Cross-cutting concerns (auth, logging, CORS)</li> <li>Hub Pattern: Central WebSocket hub for agent connections</li> <li>Factory Pattern: TLS provider creation, GPU detector creation</li> </ol>"},{"location":"developer/architecture/#key-backend-features","title":"Key Backend Features","text":"<ul> <li>JWT Authentication: Access/refresh token pattern</li> <li>Multi-Factor Authentication: TOTP, email, backup codes</li> <li>Role-Based Access Control: user, admin, agent, system roles</li> <li>Job Scheduling: Dynamic task distribution with chunking</li> <li>File Synchronization: Agent-backend file sync</li> <li>Monitoring: System metrics and heartbeat management</li> <li>Data Retention: Configurable retention policies</li> <li>Accurate Keyspace Tracking: Captures real keyspace from hashcat <code>progress[1]</code> values for precise progress reporting</li> </ul>"},{"location":"developer/architecture/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"developer/architecture/#component-structure","title":"Component Structure","text":"<p>The frontend uses React with TypeScript and follows a component-based architecture:</p>"},{"location":"developer/architecture/#1-pages-srcpages","title":"1. Pages (<code>src/pages/</code>)","text":"<ul> <li>Top-level route components</li> <li>Page-specific business logic</li> <li>Component composition</li> </ul> <p>Key Pages: - <code>Dashboard</code> - System overview - <code>AgentManagement</code> - Agent monitoring - <code>Jobs/</code> - Job execution interface - <code>AdminSettings/</code> - System configuration - <code>Login</code> - Authentication</p>"},{"location":"developer/architecture/#2-components-srccomponents","title":"2. Components (<code>src/components/</code>)","text":"<ul> <li>Reusable UI components</li> <li>Domain-specific components</li> <li>Common UI patterns</li> </ul> <p>Component Categories: - <code>admin/</code> - Administrative UI components - <code>agent/</code> - Agent-related components - <code>auth/</code> - Authentication components - <code>common/</code> - Shared components - <code>hashlist/</code> - Hashlist management UI</p>"},{"location":"developer/architecture/#3-services-srcservices","title":"3. Services (<code>src/services/</code>)","text":"<ul> <li>API communication layer</li> <li>HTTP request handling</li> <li>Response transformation</li> </ul> <p>Key Services: - <code>api.ts</code> - Base API configuration - <code>auth.ts</code> - Authentication API - <code>jobSettings.ts</code> - Job configuration - <code>systemSettings.ts</code> - System settings</p>"},{"location":"developer/architecture/#4-state-management","title":"4. State Management","text":"<ul> <li>React Context: Authentication state (<code>AuthContext</code>)</li> <li>React Query: Server state management with caching</li> <li>Local State: Component-specific state with hooks</li> </ul>"},{"location":"developer/architecture/#5-type-system-srctypes","title":"5. Type System (<code>src/types/</code>)","text":"<ul> <li>TypeScript interfaces and types</li> <li>API response types</li> <li>Domain models</li> </ul>"},{"location":"developer/architecture/#frontend-technologies","title":"Frontend Technologies","text":"<ul> <li>React 18: Component framework</li> <li>TypeScript: Type safety</li> <li>Material-UI: Component library</li> <li>React Query: Data fetching and caching</li> <li>React Router: Client-side routing</li> <li>Axios: HTTP client</li> </ul>"},{"location":"developer/architecture/#agent-architecture","title":"Agent Architecture","text":""},{"location":"developer/architecture/#core-modules","title":"Core Modules","text":""},{"location":"developer/architecture/#1-agent-core-internalagent","title":"1. Agent Core (<code>internal/agent/</code>)","text":"<ul> <li>WebSocket connection management</li> <li>Registration with claim codes</li> <li>Heartbeat maintenance</li> <li>Message routing</li> </ul>"},{"location":"developer/architecture/#2-hardware-detection-internalhardware","title":"2. Hardware Detection (<code>internal/hardware/</code>)","text":"<ul> <li>GPU detection (NVIDIA, AMD, Intel)</li> <li>System resource monitoring</li> <li>Hashcat availability checking</li> <li>Device capability reporting</li> </ul> <p>GPU Detectors: - <code>gpu/nvidia.go</code> - NVIDIA GPU detection - <code>gpu/amd.go</code> - AMD GPU detection - <code>gpu/intel.go</code> - Intel GPU detection - <code>gpu/detector.go</code> - Detection orchestration</p>"},{"location":"developer/architecture/#3-job-execution-internaljobs","title":"3. Job Execution (<code>internal/jobs/</code>)","text":"<ul> <li>Hashcat process management</li> <li>Job progress tracking</li> <li>Output parsing</li> <li>Error handling</li> </ul>"},{"location":"developer/architecture/#4-file-synchronization-internalsync","title":"4. File Synchronization (<code>internal/sync/</code>)","text":"<ul> <li>Binary synchronization</li> <li>Wordlist management</li> <li>Rule file handling</li> <li>Hashlist retrieval</li> </ul>"},{"location":"developer/architecture/#5-metrics-collection-internalmetrics","title":"5. Metrics Collection (<code>internal/metrics/</code>)","text":"<ul> <li>System resource monitoring</li> <li>GPU utilization tracking</li> <li>Performance metrics reporting</li> </ul>"},{"location":"developer/architecture/#agent-lifecycle","title":"Agent Lifecycle","text":"<ol> <li>Registration Phase</li> <li>Claim code validation</li> <li>API key generation</li> <li>Certificate exchange</li> <li> <p>Initial synchronization</p> </li> <li> <p>Active Phase</p> </li> <li>Heartbeat maintenance</li> <li>Job reception and execution</li> <li>Progress reporting</li> <li> <p>File synchronization</p> </li> <li> <p>Execution Phase</p> </li> <li>Task assignment reception</li> <li>Hashcat process spawning</li> <li>Progress monitoring</li> <li>Result reporting</li> </ol>"},{"location":"developer/architecture/#communication-protocols","title":"Communication Protocols","text":""},{"location":"developer/architecture/#rest-api","title":"REST API","text":"<p>The system uses RESTful APIs for standard CRUD operations:</p> <p>Endpoint Structure: <pre><code>/api/v1/auth/*         - Authentication endpoints\n/api/v1/admin/*        - Administrative functions\n/api/v1/agents/*       - Agent management\n/api/v1/hashlists/*    - Hashlist operations\n/api/v1/jobs/*         - Job management\n/api/v1/wordlists/*    - Wordlist management\n/api/v1/rules/*        - Rule file management\n</code></pre></p> <p>Authentication: - JWT Bearer tokens - API key authentication (agents) - Refresh token rotation</p>"},{"location":"developer/architecture/#websocket-protocol","title":"WebSocket Protocol","text":"<p>Real-time communication uses WebSocket with JSON message format:</p> <p>Message Structure: <pre><code>{\n  \"type\": \"message_type\",\n  \"payload\": { ... },\n  \"timestamp\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre></p> <p>Agent \u2192 Server Messages: - <code>heartbeat</code> - Keep-alive signal - <code>task_status</code> - Task execution status - <code>job_progress</code> - Job progress updates - <code>benchmark_result</code> - Benchmark results - <code>hardware_info</code> - Hardware capabilities - <code>hashcat_output</code> - Hashcat output streams - <code>device_update</code> - Device status changes</p> <p>Server \u2192 Agent Messages: - <code>task_assignment</code> - New task assignment - <code>job_stop</code> - Stop job execution - <code>benchmark_request</code> - Request benchmark - <code>config_update</code> - Configuration changes - <code>file_sync_request</code> - File sync command - <code>force_cleanup</code> - Force cleanup command</p>"},{"location":"developer/architecture/#file-transfer-protocol","title":"File Transfer Protocol","text":"<p>File synchronization uses HTTP(S) with the following endpoints:</p> <ul> <li><code>GET /api/v1/sync/binaries/:name</code> - Download binaries</li> <li><code>GET /api/v1/sync/wordlists/:id</code> - Download wordlists</li> <li><code>GET /api/v1/sync/rules/:id</code> - Download rules</li> <li><code>GET /api/v1/sync/hashlists/:id</code> - Download hashlists</li> </ul>"},{"location":"developer/architecture/#database-schema","title":"Database Schema","text":""},{"location":"developer/architecture/#core-tables","title":"Core Tables","text":""},{"location":"developer/architecture/#user-management","title":"User Management","text":"<ul> <li><code>users</code> - User accounts with roles and preferences</li> <li><code>auth_tokens</code> - JWT refresh tokens</li> <li><code>mfa_methods</code> - Multi-factor authentication settings</li> <li><code>mfa_backup_codes</code> - MFA recovery codes</li> </ul>"},{"location":"developer/architecture/#agent-management","title":"Agent Management","text":"<ul> <li><code>agents</code> - Registered compute agents</li> <li><code>agent_devices</code> - GPU/compute devices per agent</li> <li><code>agent_schedules</code> - Agent availability schedules</li> <li><code>agent_hashlists</code> - Agent-hashlist assignments</li> </ul>"},{"location":"developer/architecture/#job-management","title":"Job Management","text":"<ul> <li><code>job_workflows</code> - Attack strategy definitions</li> <li><code>preset_jobs</code> - Predefined job templates</li> <li><code>job_executions</code> - Active job instances</li> <li><code>job_tasks</code> - Individual task assignments</li> <li><code>performance_metrics</code> - Task performance data</li> </ul>"},{"location":"developer/architecture/#data-management","title":"Data Management","text":"<ul> <li><code>hashlists</code> - Password hash collections</li> <li><code>hashes</code> - Individual password hashes</li> <li><code>clients</code> - Customer/engagement tracking</li> <li><code>wordlists</code> - Dictionary files</li> <li><code>rules</code> - Rule files for mutations</li> </ul>"},{"location":"developer/architecture/#system-management","title":"System Management","text":"<ul> <li><code>vouchers</code> - Agent registration codes</li> <li><code>binary_versions</code> - Hashcat binary versions</li> <li><code>system_settings</code> - Global configuration</li> <li><code>client_settings</code> - Per-client settings</li> </ul>"},{"location":"developer/architecture/#key-relationships","title":"Key Relationships","text":"<pre><code>users \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 agents (owner_id)\n               \u251c\u2500\u2500\u2500\u2500 hashlists (created_by)\n               \u2514\u2500\u2500\u2500\u2500 job_executions (created_by)\n\nagents \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 agent_devices\n               \u251c\u2500\u2500\u2500\u2500 agent_schedules\n               \u2514\u2500\u2500\u2500\u2500 job_tasks\n\nhashlists \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500 hashes\n               \u251c\u2500\u2500\u2500\u2500 job_executions\n               \u2514\u2500\u2500\u2500\u2500 clients\n\njob_workflows \u2500\u2500\u252c\u2500\u2500\u2500 preset_jobs\n                \u2514\u2500\u2500\u2500 job_executions \u2500\u2500\u2500\u2500 job_tasks\n</code></pre>"},{"location":"developer/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"developer/architecture/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"developer/architecture/#multi-layer-authentication","title":"Multi-Layer Authentication","text":"<ol> <li>User Authentication</li> <li>Username/password with bcrypt hashing</li> <li>JWT access/refresh token pattern</li> <li> <p>Session management with token rotation</p> </li> <li> <p>Multi-Factor Authentication</p> </li> <li>TOTP (Time-based One-Time Passwords)</li> <li>Email-based verification</li> <li>Backup codes for recovery</li> <li> <p>Configurable MFA policies</p> </li> <li> <p>Agent Authentication</p> </li> <li>Claim code registration</li> <li>API key authentication</li> <li>Certificate-based trust</li> </ol>"},{"location":"developer/architecture/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Roles: - <code>user</code> - Standard user access - <code>admin</code> - Administrative privileges - <code>agent</code> - Agent-specific operations - <code>system</code> - System-level operations</p> <p>Middleware Chain: <pre><code>AuthMiddleware \u2192 RoleMiddleware \u2192 ResourceMiddleware \u2192 Handler\n</code></pre></p>"},{"location":"developer/architecture/#transport-security","title":"Transport Security","text":""},{"location":"developer/architecture/#tlsssl-configuration","title":"TLS/SSL Configuration","text":"<p>Supported Modes: 1. Self-Signed Certificates    - Automatic generation with CA    - Configurable validity periods    - SAN extension support</p> <ol> <li>Provided Certificates</li> <li>Custom certificate installation</li> <li> <p>Certificate chain validation</p> </li> <li> <p>Let's Encrypt (Certbot)</p> </li> <li>Automatic certificate renewal</li> <li>ACME protocol support</li> </ol> <p>Certificate Features: - RSA 2048/4096 bit keys - Multiple DNS names and IP addresses - Proper certificate chain delivery - Browser-compatible extensions</p>"},{"location":"developer/architecture/#data-security","title":"Data Security","text":"<ol> <li>Password Storage</li> <li>bcrypt with configurable cost factor</li> <li> <p>No plaintext storage</p> </li> <li> <p>Token Security</p> </li> <li>Short-lived access tokens (15 minutes)</li> <li>Refresh token rotation</li> <li> <p>Secure token storage</p> </li> <li> <p>File Access Control</p> </li> <li>Path sanitization</li> <li>Directory restrictions</li> <li> <p>User-based access control</p> </li> <li> <p>API Security</p> </li> <li>Rate limiting</li> <li>Request validation</li> <li>CORS configuration</li> </ol>"},{"location":"developer/architecture/#file-storage-architecture","title":"File Storage Architecture","text":""},{"location":"developer/architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/         # Hashcat binaries\n\u2502   \u251c\u2500\u2500 hashcat-linux-x64/\n\u2502   \u251c\u2500\u2500 hashcat-windows-x64/\n\u2502   \u2514\u2500\u2500 hashcat-darwin-x64/\n\u251c\u2500\u2500 wordlists/        # Dictionary files\n\u2502   \u251c\u2500\u2500 general/      # Common wordlists\n\u2502   \u251c\u2500\u2500 specialized/  # Domain-specific\n\u2502   \u251c\u2500\u2500 targeted/     # Custom lists\n\u2502   \u2514\u2500\u2500 custom/       # User uploads\n\u251c\u2500\u2500 rules/            # Mutation rules\n\u2502   \u251c\u2500\u2500 hashcat/      # Hashcat rules\n\u2502   \u251c\u2500\u2500 john/         # John rules\n\u2502   \u2514\u2500\u2500 custom/       # Custom rules\n\u2514\u2500\u2500 hashlists/        # Hash files\n    \u2514\u2500\u2500 {client_id}/  # Per-client storage\n</code></pre>"},{"location":"developer/architecture/#storage-management","title":"Storage Management","text":"<ul> <li>Upload Processing: Files are uploaded to temporary storage, processed, then moved to permanent locations</li> <li>Deduplication: Files are tracked by MD5 hash to prevent duplicates</li> <li>Synchronization: Agent sync service ensures agents have required files</li> <li>Cleanup: Automated retention policies remove expired data</li> </ul>"},{"location":"developer/architecture/#data-lifecycle-management","title":"Data Lifecycle Management","text":""},{"location":"developer/architecture/#retention-system","title":"Retention System","text":"<p>The system implements comprehensive data lifecycle management with automated retention policies:</p>"},{"location":"developer/architecture/#backend-retention-service","title":"Backend Retention Service","text":"<ul> <li>Automatic Purging: Runs daily at midnight and on startup</li> <li>Client-Specific Policies: Each client can have custom retention periods</li> <li>Secure Deletion Process:</li> <li>Transaction-based database cleanup</li> <li>Secure file overwriting with random data</li> <li>PostgreSQL VACUUM to prevent recovery</li> <li>Comprehensive audit logging</li> </ul>"},{"location":"developer/architecture/#agent-cleanup-service","title":"Agent Cleanup Service","text":"<ul> <li>3-Day Retention: Temporary files removed after 3 days</li> <li>Automatic Cleanup: Runs every 6 hours</li> <li>File Types Managed:</li> <li>Hashlist files (after inactivity)</li> <li>Rule chunks (temporary segments)</li> <li>Chunk ID tracking files</li> <li>Preserved Files: Base rules, wordlists, and binaries</li> </ul>"},{"location":"developer/architecture/#potfile-exclusion","title":"Potfile Exclusion","text":"<p>Important</p> <p>The potfile (<code>/var/lib/krakenhashes/wordlists/custom/potfile.txt</code>) containing plaintext passwords is NOT managed by the retention system. It requires separate manual management for compliance with data protection regulations.</p>"},{"location":"developer/architecture/#data-security_1","title":"Data Security","text":""},{"location":"developer/architecture/#secure-deletion","title":"Secure Deletion","text":"<ul> <li>Files overwritten with random data before removal</li> <li>VACUUM ANALYZE on PostgreSQL tables</li> <li>Prevention of WAL (Write-Ahead Log) recovery</li> <li>Transaction safety for atomic operations</li> </ul>"},{"location":"developer/architecture/#audit-trail","title":"Audit Trail","text":"<ul> <li>All deletion operations logged</li> <li>Retention compliance tracking</li> <li> <p>Last purge timestamp recording</p> </li> <li> <p>File Organization</p> </li> <li>Client-based isolation</li> <li>Category-based grouping</li> <li> <p>Version tracking</p> </li> <li> <p>Synchronization</p> </li> <li>Delta-based updates</li> <li>Checksum verification</li> <li> <p>Compression support</p> </li> <li> <p>Retention Policies</p> </li> <li>Configurable retention periods</li> <li>Automatic cleanup</li> <li>Archive support</li> </ul>"},{"location":"developer/architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"developer/architecture/#docker-based-deployment","title":"Docker-Based Deployment","text":"<pre><code>Services:\n- backend    # Go backend server\n- postgres   # PostgreSQL database\n- app        # Nginx + React frontend\n\nNetworks:\n- krakenhashes_default # Internal network\n\nVolumes:\n- postgres_data     # Database persistence\n- kh_config        # Configuration files\n- kh_data          # Application data\n- kh_logs          # Log files\n</code></pre>"},{"location":"developer/architecture/#production-considerations","title":"Production Considerations","text":"<ol> <li>Scalability</li> <li>Horizontal agent scaling</li> <li>Database connection pooling</li> <li> <p>Load balancer ready</p> </li> <li> <p>Monitoring</p> </li> <li>Health check endpoints</li> <li>Metrics collection</li> <li> <p>Log aggregation</p> </li> <li> <p>Backup &amp; Recovery</p> </li> <li>Database backups</li> <li>File system snapshots</li> <li> <p>Configuration backup</p> </li> <li> <p>High Availability</p> </li> <li>Database replication support</li> <li>Stateless backend design</li> <li>Agent failover handling</li> </ol>"},{"location":"developer/architecture/#environment-configuration","title":"Environment Configuration","text":"<p>Key Environment Variables: <pre><code># Database\nDB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_NAME\n\n# Security\nJWT_SECRET, JWT_REFRESH_SECRET\n\n# TLS/SSL\nKH_TLS_MODE, KH_CERT_KEY_SIZE\n\n# Directories\nKH_CONFIG_DIR, KH_DATA_DIR\n\n# Ports\nKH_HTTP_PORT, KH_HTTPS_PORT\n</code></pre></p>"},{"location":"developer/architecture/#conclusion","title":"Conclusion","text":"<p>KrakenHashes implements a robust distributed architecture designed for scalability, security, and maintainability. The system's modular design allows for independent scaling of components while maintaining clear separation of concerns throughout the stack.</p>"},{"location":"developer/backend/","title":"Backend Development Guide","text":"<p>This guide covers the KrakenHashes backend development, including environment setup, architecture, coding patterns, and common development tasks.</p>"},{"location":"developer/backend/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Development Environment Setup</li> <li>Code Structure and Architecture</li> <li>Core Conventions and Patterns</li> <li>Adding New Endpoints</li> <li>Database Operations</li> <li>Authentication and Authorization</li> <li>WebSocket Development</li> <li>Testing Strategies</li> <li>Common Patterns and Utilities</li> <li>Debugging and Logging</li> </ol>"},{"location":"developer/backend/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"developer/backend/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose (primary development method)</li> <li>Go 1.21+ (for IDE support and running tests locally)</li> <li>PostgreSQL client tools (optional, for database inspection)</li> <li>Make (for running build commands)</li> </ul>"},{"location":"developer/backend/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository <pre><code>git clone &lt;repository-url&gt;\ncd krakenhashes\n</code></pre></p> </li> <li> <p>Set up environment variables <pre><code># Copy the example environment file\ncp .env.example .env\n\n# Edit .env with your configuration\n# Required variables:\nDB_HOST=postgres\nDB_PORT=5432\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\nJWT_SECRET=your-jwt-secret\nKH_TLS_MODE=self-signed\n</code></pre></p> </li> <li> <p>Start the development environment <pre><code># Build and start all services\ndocker-compose down &amp;&amp; docker-compose up -d --build\n\n# View logs\ndocker-compose logs -f backend\n</code></pre></p> </li> <li> <p>Verify the setup <pre><code># Check backend health\ncurl -k https://localhost:8443/api/status\n\n# Check database migrations\ndocker-compose exec backend ls -la /app/db/migrations\n</code></pre></p> </li> </ol>"},{"location":"developer/backend/#docker-development-workflow","title":"Docker Development Workflow","text":"<p>Important: Always use Docker for building and testing. Never use <code>go build</code> directly as it creates binaries in the project directory.</p> <pre><code># Rebuild backend only\ndocker-compose up -d --build backend\n\n# Run database migrations\ncd backend &amp;&amp; make migrate-up\n\n# View structured logs\ndocker-compose logs backend | grep -E \"ERROR|WARNING|INFO\"\n\n# Access backend container\ndocker-compose exec backend sh\n</code></pre>"},{"location":"developer/backend/#code-structure-and-architecture","title":"Code Structure and Architecture","text":"<p>The backend follows a layered architecture with clear separation of concerns:</p> <pre><code>backend/\n\u251c\u2500\u2500 cmd/\n\u2502   \u251c\u2500\u2500 server/          # Main application entry point\n\u2502   \u2514\u2500\u2500 migrate/         # Database migration tool\n\u251c\u2500\u2500 internal/            # Private application code\n\u2502   \u251c\u2500\u2500 config/          # Configuration management\n\u2502   \u251c\u2500\u2500 db/              # Database wrapper and utilities\n\u2502   \u251c\u2500\u2500 handlers/        # HTTP request handlers (controllers)\n\u2502   \u251c\u2500\u2500 middleware/      # HTTP middleware\n\u2502   \u251c\u2500\u2500 models/          # Domain models and types\n\u2502   \u251c\u2500\u2500 repository/      # Data access layer\n\u2502   \u251c\u2500\u2500 services/        # Business logic layer\n\u2502   \u251c\u2500\u2500 websocket/       # WebSocket handlers\n\u2502   \u2514\u2500\u2500 routes/          # Route configuration\n\u251c\u2500\u2500 pkg/                 # Public packages\n\u2502   \u251c\u2500\u2500 debug/           # Debug logging utilities\n\u2502   \u251c\u2500\u2500 jwt/             # JWT token handling\n\u2502   \u2514\u2500\u2500 httputil/        # HTTP utilities\n\u2514\u2500\u2500 db/\n    \u2514\u2500\u2500 migrations/      # SQL migration files\n</code></pre>"},{"location":"developer/backend/#key-architecture-patterns","title":"Key Architecture Patterns","text":"<ol> <li>Repository Pattern: All database access through repositories</li> <li>Service Layer: Business logic separated from handlers</li> <li>Dependency Injection: Dependencies passed through constructors</li> <li>Middleware Chain: Composable middleware for cross-cutting concerns</li> <li>Context Propagation: Request context flows through all layers</li> </ol>"},{"location":"developer/backend/#core-conventions-and-patterns","title":"Core Conventions and Patterns","text":""},{"location":"developer/backend/#database-access-pattern","title":"Database Access Pattern","text":"<p>The backend uses a custom DB wrapper instead of sqlx directly:</p> <pre><code>// internal/db/db.go\ntype DB struct {\n    *sql.DB\n}\n\n// Repository pattern\ntype UserRepository struct {\n    db *db.DB\n}\n\nfunc NewUserRepository(db *db.DB) *UserRepository {\n    return &amp;UserRepository{db: db}\n}\n\n// Use standard database/sql methods\nfunc (r *UserRepository) GetByID(ctx context.Context, id uuid.UUID) (*models.User, error) {\n    user := &amp;models.User{}\n    err := r.db.QueryRowContext(ctx, queries.GetUserByID, id).Scan(\n        &amp;user.ID,\n        &amp;user.Username,\n        // ... other fields\n    )\n    if err == sql.ErrNoRows {\n        return nil, fmt.Errorf(\"user not found: %s\", id)\n    }\n    return user, err\n}\n</code></pre>"},{"location":"developer/backend/#service-layer-pattern","title":"Service Layer Pattern","text":"<p>Services contain business logic and orchestrate multiple repositories:</p> <pre><code>// internal/services/client/client_service.go\ntype ClientService struct {\n    clientRepo         *repository.ClientRepository\n    hashlistRepo       *repository.HashListRepository\n    clientSettingsRepo *repository.ClientSettingsRepository\n    retentionService   *retention.RetentionService\n}\n\nfunc (s *ClientService) DeleteClient(ctx context.Context, clientID uuid.UUID) error {\n    // Begin transaction\n    tx, err := s.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to start transaction: %w\", err)\n    }\n    defer tx.Rollback()\n\n    // Business logic here...\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#error-handling","title":"Error Handling","text":"<p>Use wrapped errors for better error tracking:</p> <pre><code>// Wrap errors with context\nif err != nil {\n    return fmt.Errorf(\"failed to create user: %w\", err)\n}\n\n// Custom error types\nvar (\n    ErrNotFound = errors.New(\"resource not found\")\n    ErrUnauthorized = errors.New(\"unauthorized\")\n)\n\n// Check error types\nif errors.Is(err, repository.ErrNotFound) {\n    http.Error(w, \"Not found\", http.StatusNotFound)\n    return\n}\n</code></pre>"},{"location":"developer/backend/#adding-new-endpoints","title":"Adding New Endpoints","text":""},{"location":"developer/backend/#step-1-define-the-model","title":"Step 1: Define the Model","text":"<pre><code>// internal/models/example.go\npackage models\n\nimport (\n    \"time\"\n    \"github.com/google/uuid\"\n)\n\ntype Example struct {\n    ID          uuid.UUID  `json:\"id\"`\n    Name        string     `json:\"name\"`\n    Description string     `json:\"description\"`\n    CreatedAt   time.Time  `json:\"created_at\"`\n    UpdatedAt   time.Time  `json:\"updated_at\"`\n}\n</code></pre>"},{"location":"developer/backend/#step-2-create-the-repository","title":"Step 2: Create the Repository","text":"<pre><code>// internal/repository/example_repository.go\npackage repository\n\ntype ExampleRepository struct {\n    db *db.DB\n}\n\nfunc NewExampleRepository(db *db.DB) *ExampleRepository {\n    return &amp;ExampleRepository{db: db}\n}\n\nfunc (r *ExampleRepository) Create(ctx context.Context, example *models.Example) error {\n    query := `\n        INSERT INTO examples (id, name, description, created_at, updated_at)\n        VALUES ($1, $2, $3, $4, $5)\n    `\n    _, err := r.db.ExecContext(ctx, query,\n        example.ID,\n        example.Name,\n        example.Description,\n        example.CreatedAt,\n        example.UpdatedAt,\n    )\n    return err\n}\n\nfunc (r *ExampleRepository) GetByID(ctx context.Context, id uuid.UUID) (*models.Example, error) {\n    example := &amp;models.Example{}\n    query := `SELECT id, name, description, created_at, updated_at FROM examples WHERE id = $1`\n\n    err := r.db.QueryRowContext(ctx, query, id).Scan(\n        &amp;example.ID,\n        &amp;example.Name,\n        &amp;example.Description,\n        &amp;example.CreatedAt,\n        &amp;example.UpdatedAt,\n    )\n\n    if err == sql.ErrNoRows {\n        return nil, ErrNotFound\n    }\n\n    return example, err\n}\n</code></pre>"},{"location":"developer/backend/#step-3-create-the-service-if-needed","title":"Step 3: Create the Service (if needed)","text":"<pre><code>// internal/services/example_service.go\npackage services\n\ntype ExampleService struct {\n    repo *repository.ExampleRepository\n}\n\nfunc NewExampleService(repo *repository.ExampleRepository) *ExampleService {\n    return &amp;ExampleService{repo: repo}\n}\n\nfunc (s *ExampleService) CreateExample(ctx context.Context, name, description string) (*models.Example, error) {\n    example := &amp;models.Example{\n        ID:          uuid.New(),\n        Name:        name,\n        Description: description,\n        CreatedAt:   time.Now(),\n        UpdatedAt:   time.Now(),\n    }\n\n    if err := s.repo.Create(ctx, example); err != nil {\n        return nil, fmt.Errorf(\"failed to create example: %w\", err)\n    }\n\n    return example, nil\n}\n</code></pre>"},{"location":"developer/backend/#step-4-create-the-handler","title":"Step 4: Create the Handler","text":"<pre><code>// internal/handlers/example/handler.go\npackage example\n\ntype Handler struct {\n    service *services.ExampleService\n}\n\nfunc NewHandler(service *services.ExampleService) *Handler {\n    return &amp;Handler{service: service}\n}\n\nfunc (h *Handler) Create(w http.ResponseWriter, r *http.Request) {\n    var req struct {\n        Name        string `json:\"name\"`\n        Description string `json:\"description\"`\n    }\n\n    if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {\n        http.Error(w, \"Invalid request\", http.StatusBadRequest)\n        return\n    }\n\n    // Get user ID from context (set by auth middleware)\n    userID := r.Context().Value(\"user_id\").(uuid.UUID)\n\n    example, err := h.service.CreateExample(r.Context(), req.Name, req.Description)\n    if err != nil {\n        debug.Error(\"Failed to create example: %v\", err)\n        http.Error(w, \"Internal server error\", http.StatusInternalServerError)\n        return\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(example)\n}\n\nfunc (h *Handler) GetByID(w http.ResponseWriter, r *http.Request) {\n    vars := mux.Vars(r)\n    id, err := uuid.Parse(vars[\"id\"])\n    if err != nil {\n        http.Error(w, \"Invalid ID\", http.StatusBadRequest)\n        return\n    }\n\n    example, err := h.service.repo.GetByID(r.Context(), id)\n    if err != nil {\n        if errors.Is(err, repository.ErrNotFound) {\n            http.Error(w, \"Not found\", http.StatusNotFound)\n            return\n        }\n        debug.Error(\"Failed to get example: %v\", err)\n        http.Error(w, \"Internal server error\", http.StatusInternalServerError)\n        return\n    }\n\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    json.NewEncoder(w).Encode(example)\n}\n</code></pre>"},{"location":"developer/backend/#step-5-register-routes","title":"Step 5: Register Routes","text":"<pre><code>// internal/routes/routes.go\n// In SetupRoutes function:\n\n// Initialize repository and service\nexampleRepo := repository.NewExampleRepository(database)\nexampleService := services.NewExampleService(exampleRepo)\nexampleHandler := example.NewHandler(exampleService)\n\n// Register routes with authentication\njwtRouter.HandleFunc(\"/examples\", exampleHandler.Create).Methods(\"POST\")\njwtRouter.HandleFunc(\"/examples/{id}\", exampleHandler.GetByID).Methods(\"GET\")\n</code></pre>"},{"location":"developer/backend/#database-operations","title":"Database Operations","text":""},{"location":"developer/backend/#creating-migrations","title":"Creating Migrations","text":"<pre><code># Create a new migration\nmake migration name=add_example_table\n\n# This creates two files:\n# - db/migrations/XXXXXX_add_example_table.up.sql\n# - db/migrations/XXXXXX_add_example_table.down.sql\n</code></pre> <p>Example migration:</p> <pre><code>-- XXXXXX_add_example_table.up.sql\nCREATE TABLE IF NOT EXISTS examples (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    created_by UUID REFERENCES users(id) ON DELETE SET NULL\n);\n\nCREATE INDEX idx_examples_created_by ON examples(created_by);\n\n-- Add trigger for updated_at\nCREATE TRIGGER update_examples_updated_at BEFORE UPDATE ON examples\nFOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n\n-- XXXXXX_add_example_table.down.sql\nDROP TRIGGER IF EXISTS update_examples_updated_at ON examples;\nDROP TABLE IF EXISTS examples;\n</code></pre>"},{"location":"developer/backend/#transaction-management","title":"Transaction Management","text":"<pre><code>// Use transactions for complex operations\nfunc (s *Service) ComplexOperation(ctx context.Context) error {\n    tx, err := s.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to begin transaction: %w\", err)\n    }\n    defer func() {\n        if err != nil {\n            if rbErr := tx.Rollback(); rbErr != nil {\n                debug.Error(\"Failed to rollback: %v\", rbErr)\n            }\n        }\n    }()\n\n    // Perform operations using tx\n    if err = s.repo.CreateWithTx(tx, data); err != nil {\n        return err\n    }\n\n    if err = s.repo.UpdateWithTx(tx, id, updates); err != nil {\n        return err\n    }\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#query-patterns","title":"Query Patterns","text":"<pre><code>// Parameterized queries (always use placeholders)\nquery := `\n    SELECT h.id, h.hash_value, h.is_cracked, h.plain_text\n    FROM hashes h\n    WHERE h.hashlist_id = $1\n    AND h.created_at &gt; $2\n    ORDER BY h.created_at DESC\n    LIMIT $3\n`\n\nrows, err := db.QueryContext(ctx, query, hashlistID, since, limit)\nif err != nil {\n    return nil, fmt.Errorf(\"failed to query hashes: %w\", err)\n}\ndefer rows.Close()\n\nvar hashes []models.Hash\nfor rows.Next() {\n    var hash models.Hash\n    err := rows.Scan(&amp;hash.ID, &amp;hash.HashValue, &amp;hash.IsCracked, &amp;hash.PlainText)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to scan hash: %w\", err)\n    }\n    hashes = append(hashes, hash)\n}\n\nif err = rows.Err(); err != nil {\n    return nil, fmt.Errorf(\"error iterating hash rows: %w\", err)\n}\n</code></pre>"},{"location":"developer/backend/#authentication-and-authorization","title":"Authentication and Authorization","text":""},{"location":"developer/backend/#jwt-authentication-flow","title":"JWT Authentication Flow","text":"<ol> <li>Login: User provides credentials \u2192 Validate \u2192 Generate JWT \u2192 Set cookie</li> <li>Request: Extract token from cookie \u2192 Validate JWT \u2192 Check database \u2192 Add to context</li> <li>Logout: Remove token from database \u2192 Clear cookie</li> </ol>"},{"location":"developer/backend/#middleware-stack","title":"Middleware Stack","text":"<pre><code>// internal/middleware/auth.go\nfunc RequireAuth(database *db.DB) func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            // Skip for OPTIONS requests\n            if r.Method == \"OPTIONS\" {\n                next.ServeHTTP(w, r)\n                return\n            }\n\n            // Get token from cookie\n            cookie, err := r.Cookie(\"token\")\n            if err != nil {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Validate token\n            userID, err := jwt.ValidateJWT(cookie.Value)\n            if err != nil {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Verify token exists in database\n            exists, err := database.TokenExists(cookie.Value)\n            if !exists {\n                http.Error(w, \"Unauthorized\", http.StatusUnauthorized)\n                return\n            }\n\n            // Add to context\n            ctx := context.WithValue(r.Context(), \"user_id\", userID)\n            ctx = context.WithValue(ctx, \"user_role\", role)\n            r = r.WithContext(ctx)\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n</code></pre>"},{"location":"developer/backend/#role-based-access-control","title":"Role-Based Access Control","text":"<pre><code>// internal/middleware/admin.go\nfunc RequireAdmin() func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            role := r.Context().Value(\"user_role\").(string)\n\n            if role != \"admin\" {\n                http.Error(w, \"Forbidden\", http.StatusForbidden)\n                return\n            }\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n\n// Usage in routes\nadminRouter := jwtRouter.PathPrefix(\"/admin\").Subrouter()\nadminRouter.Use(middleware.RequireAdmin())\n</code></pre>"},{"location":"developer/backend/#api-key-authentication-agents","title":"API Key Authentication (Agents)","text":"<pre><code>// internal/handlers/auth/api/middleware.go\nfunc RequireAPIKey(agentService *services.AgentService) func(http.Handler) http.Handler {\n    return func(next http.Handler) http.Handler {\n        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n            apiKey := r.Header.Get(\"X-API-Key\")\n            agentIDStr := r.Header.Get(\"X-Agent-ID\")\n\n            if apiKey == \"\" || agentIDStr == \"\" {\n                http.Error(w, \"API Key and Agent ID required\", http.StatusUnauthorized)\n                return\n            }\n\n            agent, err := agentService.GetByAPIKey(r.Context(), apiKey)\n            if err != nil {\n                http.Error(w, \"Invalid API Key\", http.StatusUnauthorized)\n                return\n            }\n\n            ctx := context.WithValue(r.Context(), \"agent_id\", agent.ID)\n            r = r.WithContext(ctx)\n\n            next.ServeHTTP(w, r)\n        })\n    }\n}\n</code></pre>"},{"location":"developer/backend/#websocket-development","title":"WebSocket Development","text":""},{"location":"developer/backend/#websocket-handler-pattern","title":"WebSocket Handler Pattern","text":"<pre><code>// internal/websocket/agent_updates.go\ntype AgentUpdateHandler struct {\n    db           *db.DB\n    agentService *services.AgentService\n    upgrader     websocket.Upgrader\n}\n\nfunc (h *AgentUpdateHandler) HandleUpdates(w http.ResponseWriter, r *http.Request) {\n    // Authenticate before upgrading\n    apiKey := r.Header.Get(\"X-API-Key\")\n    agent, err := h.agentService.GetByAPIKey(r.Context(), apiKey)\n    if err != nil {\n        http.Error(w, \"Invalid API Key\", http.StatusUnauthorized)\n        return\n    }\n\n    // Upgrade connection\n    conn, err := h.upgrader.Upgrade(w, r, nil)\n    if err != nil {\n        debug.Error(\"Failed to upgrade connection: %v\", err)\n        return\n    }\n    defer conn.Close()\n\n    // Configure connection\n    conn.SetReadLimit(maxMessageSize)\n    conn.SetReadDeadline(time.Now().Add(pongWait))\n    conn.SetPongHandler(func(string) error {\n        conn.SetReadDeadline(time.Now().Add(pongWait))\n        return nil\n    })\n\n    // Start ping ticker\n    ticker := time.NewTicker(pingPeriod)\n    defer ticker.Stop()\n\n    // Message handling loop\n    for {\n        messageType, message, err := conn.ReadMessage()\n        if err != nil {\n            if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway) {\n                debug.Error(\"WebSocket error: %v\", err)\n            }\n            break\n        }\n\n        // Process message\n        if err := h.processMessage(agent.ID, message); err != nil {\n            debug.Error(\"Failed to process message: %v\", err)\n        }\n    }\n}\n</code></pre>"},{"location":"developer/backend/#message-processing-with-transactions","title":"Message Processing with Transactions","text":"<pre><code>func (h *AgentUpdateHandler) processCrackUpdate(ctx context.Context, agentID int, msg CrackUpdateMessage) error {\n    tx, err := h.db.BeginTx(ctx, nil)\n    if err != nil {\n        return fmt.Errorf(\"failed to start transaction: %w\", err)\n    }\n    defer func() {\n        if err != nil {\n            tx.Rollback()\n        }\n    }()\n\n    // Update hash status\n    err = h.hashRepo.UpdateCrackStatus(tx, msg.HashID, msg.Password)\n    if err != nil {\n        return err\n    }\n\n    // Update hashlist count\n    err = h.hashlistRepo.IncrementCrackedCountTx(tx, msg.HashlistID, 1)\n    if err != nil {\n        return err\n    }\n\n    return tx.Commit()\n}\n</code></pre>"},{"location":"developer/backend/#testing-strategies","title":"Testing Strategies","text":""},{"location":"developer/backend/#unit-testing","title":"Unit Testing","text":"<pre><code>// internal/handlers/auth/handler_test.go\nfunc TestLoginHandler(t *testing.T) {\n    // Setup\n    testutil.SetTestJWTSecret(t)\n    db := testutil.SetupTestDB(t)\n    emailService := testutil.NewMockEmailService()\n    handler := NewHandler(db, emailService)\n\n    // Create test user\n    testUser := testutil.CreateTestUser(t, db, \"testuser\", \"test@example.com\", \"password\", \"user\")\n\n    // Test successful login\n    t.Run(\"successful login\", func(t *testing.T) {\n        body := map[string]string{\n            \"username\": \"testuser\",\n            \"password\": \"password\",\n        }\n        jsonBody, _ := json.Marshal(body)\n\n        req := httptest.NewRequest(\"POST\", \"/api/login\", bytes.NewBuffer(jsonBody))\n        rr := httptest.NewRecorder()\n\n        handler.Login(rr, req)\n\n        assert.Equal(t, http.StatusOK, rr.Code)\n\n        var resp models.LoginResponse\n        json.Unmarshal(rr.Body.Bytes(), &amp;resp)\n        assert.True(t, resp.Success)\n        assert.NotEmpty(t, resp.Token)\n    })\n}\n</code></pre>"},{"location":"developer/backend/#integration-testing","title":"Integration Testing","text":"<pre><code>// internal/integration_test/auth_integration_test.go\nfunc TestAuthenticationFlow(t *testing.T) {\n    // Setup test environment\n    db := testutil.SetupTestDB(t)\n    router := setupTestRouter(db)\n\n    // Register user\n    registerResp := testutil.RegisterUser(t, router, \"testuser\", \"test@example.com\", \"password\")\n    assert.Equal(t, http.StatusOK, registerResp.Code)\n\n    // Login\n    loginResp := testutil.Login(t, router, \"testuser\", \"password\")\n    assert.Equal(t, http.StatusOK, loginResp.Code)\n\n    // Extract token\n    token := testutil.ExtractTokenFromResponse(t, loginResp)\n\n    // Access protected endpoint\n    req := httptest.NewRequest(\"GET\", \"/api/dashboard\", nil)\n    req.AddCookie(&amp;http.Cookie{Name: \"token\", Value: token})\n    rr := httptest.NewRecorder()\n\n    router.ServeHTTP(rr, req)\n    assert.Equal(t, http.StatusOK, rr.Code)\n}\n</code></pre>"},{"location":"developer/backend/#mock-services","title":"Mock Services","text":"<pre><code>// internal/testutil/mocks.go\ntype MockEmailService struct {\n    SentEmails []SentEmail\n}\n\nfunc (m *MockEmailService) SendMFACode(ctx context.Context, email, code string) error {\n    m.SentEmails = append(m.SentEmails, SentEmail{\n        To:      email,\n        Subject: \"MFA Code\",\n        Body:    code,\n    })\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#database-testing","title":"Database Testing","text":"<pre><code>// internal/testutil/db.go\nfunc SetupTestDB(t *testing.T) *db.DB {\n    // Connect to test database\n    testDB := os.Getenv(\"TEST_DATABASE_URL\")\n    if testDB == \"\" {\n        testDB = \"postgres://test:test@localhost/krakenhashes_test\"\n    }\n\n    sqlDB, err := sql.Open(\"postgres\", testDB)\n    require.NoError(t, err)\n\n    // Run migrations\n    err = database.RunMigrations()\n    require.NoError(t, err)\n\n    // Clean up after test\n    t.Cleanup(func() {\n        // Truncate all tables\n        tables := []string{\"users\", \"agents\", \"hashlists\", \"hashes\"}\n        for _, table := range tables {\n            sqlDB.Exec(fmt.Sprintf(\"TRUNCATE TABLE %s CASCADE\", table))\n        }\n        sqlDB.Close()\n    })\n\n    return &amp;db.DB{DB: sqlDB}\n}\n</code></pre>"},{"location":"developer/backend/#common-patterns-and-utilities","title":"Common Patterns and Utilities","text":""},{"location":"developer/backend/#debug-logging","title":"Debug Logging","text":"<pre><code>// Use the debug package for structured logging\nimport \"github.com/ZerkerEOD/krakenhashes/backend/pkg/debug\"\n\n// Log levels\ndebug.Debug(\"Processing request for user: %s\", userID)\ndebug.Info(\"Server starting on port %d\", port)\ndebug.Warning(\"Rate limit approaching for user: %s\", userID)\ndebug.Error(\"Failed to connect to database: %v\", err)\n\n// Conditional debug logging\nif debug.IsDebugEnabled() {\n    debug.Debug(\"Detailed request info: %+v\", req)\n}\n</code></pre>"},{"location":"developer/backend/#http-utilities","title":"HTTP Utilities","text":"<pre><code>// internal/pkg/httputil/httputil.go\nfunc WriteJSON(w http.ResponseWriter, status int, data interface{}) {\n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.WriteHeader(status)\n    if err := json.NewEncoder(w).Encode(data); err != nil {\n        debug.Error(\"Failed to encode JSON response: %v\", err)\n    }\n}\n\nfunc ReadJSON(r *http.Request, dest interface{}) error {\n    if r.Header.Get(\"Content-Type\") != \"application/json\" {\n        return errors.New(\"content-type must be application/json\")\n    }\n\n    decoder := json.NewDecoder(r.Body)\n    decoder.DisallowUnknownFields()\n    return decoder.Decode(dest)\n}\n</code></pre>"},{"location":"developer/backend/#context-values","title":"Context Values","text":"<pre><code>// pkg/jwt/context.go\ntype contextKey string\n\nconst (\n    userIDKey   contextKey = \"user_id\"\n    userRoleKey contextKey = \"user_role\"\n    agentIDKey  contextKey = \"agent_id\"\n)\n\nfunc GetUserID(ctx context.Context) (uuid.UUID, bool) {\n    id, ok := ctx.Value(userIDKey).(uuid.UUID)\n    return id, ok\n}\n\nfunc GetUserRole(ctx context.Context) (string, bool) {\n    role, ok := ctx.Value(userRoleKey).(string)\n    return role, ok\n}\n</code></pre>"},{"location":"developer/backend/#file-operations","title":"File Operations","text":"<pre><code>// Use the centralized data directory\nfunc SaveUploadedFile(file multipart.File, filename string) error {\n    dataDir := config.GetDataDir()\n    destPath := filepath.Join(dataDir, \"uploads\", filename)\n\n    // Ensure directory exists\n    if err := os.MkdirAll(filepath.Dir(destPath), 0755); err != nil {\n        return fmt.Errorf(\"failed to create directory: %w\", err)\n    }\n\n    // Create destination file\n    dest, err := os.Create(destPath)\n    if err != nil {\n        return fmt.Errorf(\"failed to create file: %w\", err)\n    }\n    defer dest.Close()\n\n    // Copy content\n    if _, err := io.Copy(dest, file); err != nil {\n        return fmt.Errorf(\"failed to save file: %w\", err)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#validation-helpers","title":"Validation Helpers","text":"<pre><code>// Validate request data\nfunc ValidateCreateUserRequest(req *CreateUserRequest) error {\n    if req.Username == \"\" {\n        return errors.New(\"username is required\")\n    }\n\n    if len(req.Username) &lt; 3 || len(req.Username) &gt; 50 {\n        return errors.New(\"username must be between 3 and 50 characters\")\n    }\n\n    if !emailRegex.MatchString(req.Email) {\n        return errors.New(\"invalid email format\")\n    }\n\n    if err := password.Validate(req.Password); err != nil {\n        return fmt.Errorf(\"invalid password: %w\", err)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"developer/backend/#debugging-and-logging","title":"Debugging and Logging","text":""},{"location":"developer/backend/#environment-variables-for-debugging","title":"Environment Variables for Debugging","text":"<pre><code># Enable debug logging\nKH_DEBUG=true\n\n# Set log level (DEBUG, INFO, WARNING, ERROR)\nKH_LOG_LEVEL=DEBUG\n\n# Enable SQL query logging\nKH_LOG_SQL=true\n</code></pre>"},{"location":"developer/backend/#debugging-database-queries","title":"Debugging Database Queries","text":"<pre><code>// Log SQL queries in development\nif debug.IsDebugEnabled() {\n    debug.Debug(\"Executing query: %s with args: %v\", query, args)\n}\n\n// Time query execution\nstart := time.Now()\nrows, err := db.QueryContext(ctx, query, args...)\ndebug.Debug(\"Query executed in %v\", time.Since(start))\n</code></pre>"},{"location":"developer/backend/#requestresponse-logging-middleware","title":"Request/Response Logging Middleware","text":"<pre><code>func loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n\n        // Wrap response writer to capture status\n        wrapped := &amp;responseWriter{ResponseWriter: w, statusCode: http.StatusOK}\n\n        // Log request\n        debug.Info(\"[%s] %s %s\", r.Method, r.URL.Path, r.RemoteAddr)\n\n        next.ServeHTTP(wrapped, r)\n\n        // Log response\n        duration := time.Since(start)\n        debug.Info(\"[%s] %s %s - %d (%v)\", \n            r.Method, r.URL.Path, r.RemoteAddr, \n            wrapped.statusCode, duration)\n    })\n}\n</code></pre>"},{"location":"developer/backend/#common-debugging-commands","title":"Common Debugging Commands","text":"<pre><code># View backend logs with context\ndocker-compose logs backend | grep -A 5 -B 5 \"ERROR\"\n\n# Monitor real-time logs\ndocker-compose logs -f backend | grep -E \"user_id|agent_id\"\n\n# Check database state\ndocker-compose exec postgres psql -U krakenhashes -d krakenhashes \\\n  -c \"SELECT * FROM users WHERE created_at &gt; NOW() - INTERVAL '1 hour';\"\n\n# Test endpoint with curl\ncurl -k -X POST https://localhost:8443/api/login \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\":\"test\",\"password\":\"test\"}' \\\n  -c cookies.txt -v\n\n# Use saved cookies for authenticated requests\ncurl -k https://localhost:8443/api/dashboard \\\n  -b cookies.txt -v\n</code></pre>"},{"location":"developer/backend/#best-practices","title":"Best Practices","text":"<ol> <li>Always use context: Pass context through all function calls for cancellation and timeouts</li> <li>Handle errors explicitly: Never ignore errors, always log or return them</li> <li>Use transactions: For operations that modify multiple tables</li> <li>Validate input: Validate all user input at the handler level</li> <li>Log appropriately: Use debug for development, info for important events, error for failures</li> <li>Test thoroughly: Write unit tests for business logic, integration tests for workflows</li> <li>Document APIs: Add comments to handlers explaining request/response formats</li> <li>Use prepared statements: Always use parameterized queries to prevent SQL injection</li> <li>Close resources: Always close database rows, files, and connections</li> <li>Follow Go conventions: Use gofmt, follow effective Go guidelines</li> </ol>"},{"location":"developer/backend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/backend/#common-issues","title":"Common Issues","text":"<ol> <li>Database connection errors</li> <li>Check DATABASE_URL environment variable</li> <li>Ensure PostgreSQL is running</li> <li> <p>Verify network connectivity in Docker</p> </li> <li> <p>Migration failures</p> </li> <li>Check migration syntax</li> <li>Ensure migrations are sequential</li> <li> <p>Verify database permissions</p> </li> <li> <p>Authentication issues</p> </li> <li>Check JWT_SECRET is set</li> <li>Verify token exists in database</li> <li> <p>Check cookie settings (secure, httpOnly)</p> </li> <li> <p>WebSocket connection failures</p> </li> <li>Verify TLS certificates</li> <li>Check CORS settings</li> <li> <p>Ensure proper authentication headers</p> </li> <li> <p>File upload issues</p> </li> <li>Check data directory permissions</li> <li>Verify multipart form parsing</li> <li>Check file size limits</li> </ol>"},{"location":"developer/backend/#debug-mode-features","title":"Debug Mode Features","text":"<p>When <code>KH_DEBUG=true</code>: - Detailed SQL query logging - Request/response body logging - Performance timing information - Stack traces on errors - WebSocket message logging</p> <p>Remember to disable debug mode in production for security and performance reasons.</p>"},{"location":"developer/frontend/","title":"KrakenHashes Frontend Development Guide","text":"<p>This guide provides comprehensive documentation for developing the KrakenHashes frontend application. The frontend is built with React, TypeScript, Material-UI, and React Query.</p>"},{"location":"developer/frontend/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Development Environment Setup</li> <li>Project Structure and Organization</li> <li>Component Development Patterns</li> <li>State Management with React Query</li> <li>API Integration and Services</li> <li>Material-UI Theming and Styling</li> <li>TypeScript Conventions</li> <li>Testing Approaches</li> </ol>"},{"location":"developer/frontend/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"developer/frontend/#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18+ and npm 9+</li> <li>Docker and Docker Compose (for backend services)</li> <li>A modern web browser with developer tools</li> </ul>"},{"location":"developer/frontend/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes/frontend\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>npm install\n</code></pre></p> </li> <li> <p>Configure environment variables:    Create a <code>.env.local</code> file in the frontend directory:    <pre><code>REACT_APP_API_URL=https://localhost:31337\nREACT_APP_WS_URL=wss://localhost:31337\n</code></pre></p> </li> <li> <p>Start the development server: <pre><code>npm start\n</code></pre>    The application will be available at <code>http://localhost:3000</code></p> </li> </ol>"},{"location":"developer/frontend/#running-with-backend-services","title":"Running with Backend Services","text":"<p>For full functionality, run the backend services using Docker:</p> <pre><code># From the project root\ndocker-compose up -d --build\n</code></pre>"},{"location":"developer/frontend/#development-scripts","title":"Development Scripts","text":"<pre><code>npm start       # Start development server (port 3000)\nnpm run build   # Production build\nnpm test        # Run tests\nnpm run eject   # Eject from Create React App (use with caution)\n</code></pre>"},{"location":"developer/frontend/#project-structure-and-organization","title":"Project Structure and Organization","text":"<p>The frontend follows a feature-based organization pattern:</p> <pre><code>frontend/src/\n\u251c\u2500\u2500 api/                 # API version checking\n\u2502   \u2514\u2500\u2500 version.ts\n\u251c\u2500\u2500 components/          # Reusable components\n\u2502   \u251c\u2500\u2500 admin/          # Admin-specific components\n\u2502   \u251c\u2500\u2500 agent/          # Agent management components\n\u2502   \u251c\u2500\u2500 auth/           # Authentication components\n\u2502   \u251c\u2500\u2500 common/         # Shared common components\n\u2502   \u251c\u2500\u2500 hashlist/       # Hashlist management components\n\u2502   \u251c\u2500\u2500 pot/            # Cracked passwords components\n\u2502   \u2514\u2500\u2500 settings/       # Settings components\n\u251c\u2500\u2500 contexts/           # React contexts\n\u2502   \u2514\u2500\u2500 AuthContext.tsx # Authentication context provider\n\u251c\u2500\u2500 hooks/              # Custom React hooks\n\u2502   \u251c\u2500\u2500 useAuth.tsx     # Authentication hook\n\u2502   \u251c\u2500\u2500 useConfirm.tsx  # Confirmation dialog hook\n\u2502   \u251c\u2500\u2500 useDebounce.ts  # Debounce hook\n\u2502   \u2514\u2500\u2500 useVouchers.ts  # Voucher management hook\n\u251c\u2500\u2500 pages/              # Page components (routes)\n\u2502   \u251c\u2500\u2500 admin/          # Admin pages\n\u2502   \u251c\u2500\u2500 settings/       # Settings pages\n\u2502   \u2514\u2500\u2500 ...             # Other page components\n\u251c\u2500\u2500 services/           # API service layer\n\u2502   \u251c\u2500\u2500 api.ts          # Axios configuration\n\u2502   \u251c\u2500\u2500 auth.ts         # Authentication services\n\u2502   \u2514\u2500\u2500 ...             # Domain-specific services\n\u251c\u2500\u2500 styles/             # Global styles\n\u2502   \u2514\u2500\u2500 theme.ts        # Material-UI theme\n\u251c\u2500\u2500 types/              # TypeScript type definitions\n\u2502   \u251c\u2500\u2500 agent.ts        # Agent types\n\u2502   \u251c\u2500\u2500 auth.ts         # Authentication types\n\u2502   \u2514\u2500\u2500 ...             # Domain-specific types\n\u251c\u2500\u2500 utils/              # Utility functions\n\u2502   \u251c\u2500\u2500 formatters.ts   # Data formatting utilities\n\u2502   \u251c\u2500\u2500 validation.ts   # Validation utilities\n\u2502   \u2514\u2500\u2500 ...             # Other utilities\n\u251c\u2500\u2500 App.tsx             # Root application component\n\u251c\u2500\u2500 config.ts           # Application configuration\n\u2514\u2500\u2500 index.tsx           # Application entry point\n</code></pre>"},{"location":"developer/frontend/#component-development-patterns","title":"Component Development Patterns","text":""},{"location":"developer/frontend/#component-structure","title":"Component Structure","text":"<p>Components follow a consistent structure with TypeScript interfaces and comprehensive documentation:</p> <pre><code>/**\n * ComponentName - Brief description of the component\n * \n * Features:\n *   - Key feature 1\n *   - Key feature 2\n * \n * Dependencies:\n *   - External libraries used\n *   - Internal components/hooks\n * \n * Error Scenarios:\n *   - Possible error conditions\n *   - How they're handled\n */\n\nimport React, { useState, useCallback } from 'react';\nimport { Box, Typography } from '@mui/material';\n\ninterface ComponentNameProps {\n  title: string;\n  onAction?: () =&gt; void;\n}\n\nconst ComponentName: React.FC&lt;ComponentNameProps&gt; = ({ title, onAction }) =&gt; {\n  const [state, setState] = useState&lt;string&gt;('');\n\n  const handleAction = useCallback(() =&gt; {\n    // Handle action\n    onAction?.();\n  }, [onAction]);\n\n  return (\n    &lt;Box&gt;\n      &lt;Typography variant=\"h5\"&gt;{title}&lt;/Typography&gt;\n      {/* Component content */}\n    &lt;/Box&gt;\n  );\n};\n\nexport default ComponentName;\n</code></pre>"},{"location":"developer/frontend/#layout-components","title":"Layout Components","text":"<p>The application uses a main Layout component with navigation:</p> <pre><code>// components/Layout.tsx\nconst Layout: React.FC = () =&gt; {\n  const [open, setOpen] = useState&lt;boolean&gt;(true);\n  const navigate = useNavigate();\n  const { userRole } = useAuth();\n\n  return (\n    &lt;Box sx={{ display: 'flex', minHeight: '100vh' }}&gt;\n      &lt;AppBar position=\"fixed\"&gt;\n        &lt;Toolbar&gt;\n          &lt;Typography variant=\"h6\"&gt;KrakenHashes&lt;/Typography&gt;\n          &lt;UserMenu /&gt;\n        &lt;/Toolbar&gt;\n      &lt;/AppBar&gt;\n      &lt;Drawer variant=\"permanent\" open={open}&gt;\n        &lt;List&gt;\n          {menuItems.map((item) =&gt; (\n            &lt;ListItem button key={item.text} onClick={() =&gt; navigate(item.path)}&gt;\n              &lt;ListItemIcon&gt;{item.icon}&lt;/ListItemIcon&gt;\n              &lt;ListItemText primary={item.text} /&gt;\n            &lt;/ListItem&gt;\n          ))}\n        &lt;/List&gt;\n        {userRole === 'admin' &amp;&amp; &lt;AdminMenu /&gt;}\n      &lt;/Drawer&gt;\n      &lt;Box component=\"main\" sx={{ flexGrow: 1, p: 3 }}&gt;\n        &lt;Outlet /&gt;\n      &lt;/Box&gt;\n    &lt;/Box&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#form-components","title":"Form Components","text":"<p>Forms use React Hook Form with Material-UI integration:</p> <pre><code>import { useForm } from 'react-hook-form';\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport * as z from 'zod';\n\nconst schema = z.object({\n  name: z.string().min(1, 'Name is required'),\n  email: z.string().email('Invalid email'),\n});\n\ntype FormData = z.infer&lt;typeof schema&gt;;\n\nconst FormComponent: React.FC = () =&gt; {\n  const { register, handleSubmit, formState: { errors } } = useForm&lt;FormData&gt;({\n    resolver: zodResolver(schema),\n  });\n\n  const onSubmit = (data: FormData) =&gt; {\n    // Handle form submission\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit(onSubmit)}&gt;\n      &lt;TextField\n        {...register('name')}\n        error={!!errors.name}\n        helperText={errors.name?.message}\n        label=\"Name\"\n        fullWidth\n      /&gt;\n      &lt;Button type=\"submit\" variant=\"contained\"&gt;\n        Submit\n      &lt;/Button&gt;\n    &lt;/form&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#state-management-with-react-query","title":"State Management with React Query","text":""},{"location":"developer/frontend/#query-client-configuration","title":"Query Client Configuration","text":"<p>The application uses React Query for server state management:</p> <pre><code>// App.tsx\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: {\n      staleTime: 5 * 60 * 1000, // 5 minutes\n      refetchOnWindowFocus: true,\n    },\n  },\n});\n\nfunction App() {\n  return (\n    &lt;QueryClientProvider client={queryClient}&gt;\n      {/* Application components */}\n    &lt;/QueryClientProvider&gt;\n  );\n}\n</code></pre>"},{"location":"developer/frontend/#using-queries","title":"Using Queries","text":"<p>Example of fetching data with React Query:</p> <pre><code>import { useQuery } from '@tanstack/react-query';\nimport { getWordlists } from '../services/wordlists';\n\nconst WordlistsComponent: React.FC = () =&gt; {\n  const { data, isLoading, error, refetch } = useQuery({\n    queryKey: ['wordlists'],\n    queryFn: getWordlists,\n  });\n\n  if (isLoading) return &lt;CircularProgress /&gt;;\n  if (error) return &lt;Alert severity=\"error\"&gt;Failed to load wordlists&lt;/Alert&gt;;\n\n  return (\n    &lt;Box&gt;\n      {data?.map((wordlist) =&gt; (\n        &lt;WordlistItem key={wordlist.id} wordlist={wordlist} /&gt;\n      ))}\n    &lt;/Box&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#using-mutations","title":"Using Mutations","text":"<p>Example of mutating data with React Query:</p> <pre><code>import { useMutation, useQueryClient } from '@tanstack/react-query';\nimport { updateWordlist } from '../services/wordlists';\n\nconst EditWordlistDialog: React.FC&lt;{ wordlist: Wordlist }&gt; = ({ wordlist }) =&gt; {\n  const queryClient = useQueryClient();\n  const { enqueueSnackbar } = useSnackbar();\n\n  const mutation = useMutation({\n    mutationFn: (data: UpdateWordlistData) =&gt; updateWordlist(wordlist.id, data),\n    onSuccess: () =&gt; {\n      queryClient.invalidateQueries({ queryKey: ['wordlists'] });\n      enqueueSnackbar('Wordlist updated successfully', { variant: 'success' });\n    },\n    onError: (error) =&gt; {\n      enqueueSnackbar('Failed to update wordlist', { variant: 'error' });\n    },\n  });\n\n  const handleSubmit = (data: UpdateWordlistData) =&gt; {\n    mutation.mutate(data);\n  };\n\n  return (\n    &lt;Dialog open onClose={onClose}&gt;\n      &lt;DialogContent&gt;\n        {/* Form fields */}\n      &lt;/DialogContent&gt;\n      &lt;DialogActions&gt;\n        &lt;Button onClick={() =&gt; handleSubmit(formData)} disabled={mutation.isPending}&gt;\n          Save\n        &lt;/Button&gt;\n      &lt;/DialogActions&gt;\n    &lt;/Dialog&gt;\n  );\n};\n</code></pre>"},{"location":"developer/frontend/#api-integration-and-services","title":"API Integration and Services","text":""},{"location":"developer/frontend/#axios-configuration","title":"Axios Configuration","text":"<p>The API service layer uses Axios with interceptors:</p> <pre><code>// services/api.ts\nimport axios from 'axios';\n\nconst API_URL = process.env.REACT_APP_API_URL || 'https://localhost:31337';\n\nexport const api = axios.create({\n  baseURL: API_URL,\n  withCredentials: true, // Required for cookies/session\n});\n\n// Request interceptor\napi.interceptors.request.use((config) =&gt; {\n  // Log requests in development\n  if (process.env.NODE_ENV === 'development') {\n    console.debug(`[API] ${config.method?.toUpperCase()} ${config.url}`, config.data);\n  }\n  return config;\n});\n\n// Response interceptor\napi.interceptors.response.use(\n  (response) =&gt; response,\n  async (error) =&gt; {\n    if (error.response?.status === 401) {\n      // Handle authentication errors\n      window.location.href = '/login';\n    }\n    return Promise.reject(error);\n  }\n);\n</code></pre>"},{"location":"developer/frontend/#service-layer-pattern","title":"Service Layer Pattern","text":"<p>Services encapsulate API calls:</p> <pre><code>// services/wordlists.ts\nimport { api } from './api';\nimport { Wordlist, WordlistUploadData } from '../types/wordlists';\n\nexport const wordlistService = {\n  getWordlists: async (): Promise&lt;Wordlist[]&gt; =&gt; {\n    const response = await api.get('/api/wordlists');\n    return response.data;\n  },\n\n  uploadWordlist: async (\n    formData: FormData,\n    onProgress?: (progress: number) =&gt; void\n  ): Promise&lt;Wordlist&gt; =&gt; {\n    const response = await api.post('/api/wordlists/upload', formData, {\n      headers: { 'Content-Type': 'multipart/form-data' },\n      onUploadProgress: (progressEvent) =&gt; {\n        if (progressEvent.total) {\n          const progress = Math.round((progressEvent.loaded * 100) / progressEvent.total);\n          onProgress?.(progress);\n        }\n      },\n    });\n    return response.data;\n  },\n\n  updateWordlist: async (id: string, data: Partial&lt;Wordlist&gt;): Promise&lt;Wordlist&gt; =&gt; {\n    const response = await api.put(`/api/wordlists/${id}`, data);\n    return response.data;\n  },\n\n  deleteWordlist: async (id: string): Promise&lt;void&gt; =&gt; {\n    await api.delete(`/api/wordlists/${id}`);\n  },\n};\n</code></pre>"},{"location":"developer/frontend/#authentication-service","title":"Authentication Service","text":"<p>Authentication is handled through a dedicated service:</p> <pre><code>// services/auth.ts\nimport { api } from './api';\nimport { LoginCredentials, AuthResponse } from '../types/auth';\n\nexport const authService = {\n  login: async (credentials: LoginCredentials): Promise&lt;AuthResponse&gt; =&gt; {\n    const response = await api.post('/api/login', credentials);\n    return response.data;\n  },\n\n  logout: async (): Promise&lt;void&gt; =&gt; {\n    await api.post('/api/logout');\n  },\n\n  isAuthenticated: async (): Promise&lt;{ authenticated: boolean; role?: string }&gt; =&gt; {\n    const response = await api.get('/api/check-auth');\n    return response.data;\n  },\n\n  refreshToken: async (): Promise&lt;void&gt; =&gt; {\n    await api.post('/api/refresh-token');\n  },\n};\n</code></pre>"},{"location":"developer/frontend/#material-ui-theming-and-styling","title":"Material-UI Theming and Styling","text":""},{"location":"developer/frontend/#theme-configuration","title":"Theme Configuration","text":"<p>The application uses a dark theme with custom overrides:</p> <pre><code>// styles/theme.ts\nimport { createTheme, Theme } from '@mui/material/styles';\n\nconst theme: Theme = createTheme({\n  palette: {\n    mode: 'dark',\n    primary: {\n      main: '#ff0000',\n    },\n    background: {\n      default: '#000000',\n      paper: '#121212',\n    },\n    text: {\n      primary: '#ffffff',\n    },\n  },\n  components: {\n    MuiCssBaseline: {\n      styleOverrides: {\n        body: {\n          backgroundColor: '#000000',\n          color: '#ffffff',\n        },\n      },\n    },\n    MuiDataGrid: {\n      styleOverrides: {\n        root: {\n          border: 'none',\n          backgroundColor: '#121212',\n          '&amp; .MuiDataGrid-columnHeaders': {\n            backgroundColor: 'rgba(255, 255, 255, 0.08)',\n          },\n          '&amp; .MuiDataGrid-row:hover': {\n            backgroundColor: 'rgba(255, 255, 255, 0.05)',\n          },\n        },\n      },\n    },\n  },\n});\n\nexport default theme;\n</code></pre>"},{"location":"developer/frontend/#component-styling","title":"Component Styling","text":"<p>Use the <code>sx</code> prop for component-specific styles:</p> <pre><code>&lt;Box\n  sx={{\n    display: 'flex',\n    flexDirection: 'column',\n    gap: 2,\n    p: 3,\n    backgroundColor: 'background.paper',\n    borderRadius: 1,\n    '&amp;:hover': {\n      backgroundColor: 'action.hover',\n    },\n  }}\n&gt;\n  {/* Content */}\n&lt;/Box&gt;\n</code></pre>"},{"location":"developer/frontend/#responsive-design","title":"Responsive Design","text":"<p>Use Material-UI's responsive utilities:</p> <pre><code>&lt;Grid container spacing={2}&gt;\n  &lt;Grid item xs={12} sm={6} md={4}&gt;\n    {/* Content adapts to screen size */}\n  &lt;/Grid&gt;\n&lt;/Grid&gt;\n\n&lt;Box\n  sx={{\n    width: { xs: '100%', sm: '60%', md: '40%' },\n    display: { xs: 'none', md: 'block' },\n  }}\n&gt;\n  {/* Responsive visibility and sizing */}\n&lt;/Box&gt;\n</code></pre>"},{"location":"developer/frontend/#typescript-conventions","title":"TypeScript Conventions","text":""},{"location":"developer/frontend/#type-definitions","title":"Type Definitions","text":"<p>All types are defined in the <code>types/</code> directory:</p> <pre><code>// types/wordlists.ts\nexport enum WordlistType {\n  GENERAL = 'general',\n  SPECIALIZED = 'specialized',\n  TARGETED = 'targeted',\n  CUSTOM = 'custom',\n}\n\nexport interface Wordlist {\n  id: string;\n  name: string;\n  description: string;\n  wordlist_type: WordlistType;\n  file_size: number;\n  word_count: number;\n  verification_status: 'pending' | 'verified' | 'failed';\n  created_at: string;\n  updated_at: string;\n}\n\nexport interface WordlistUploadData {\n  file: File;\n  name?: string;\n  description?: string;\n  wordlist_type: WordlistType;\n}\n</code></pre>"},{"location":"developer/frontend/#component-props","title":"Component Props","text":"<p>Always define interfaces for component props:</p> <pre><code>interface TableProps {\n  data: Wordlist[];\n  onEdit?: (wordlist: Wordlist) =&gt; void;\n  onDelete?: (id: string) =&gt; void;\n  loading?: boolean;\n}\n\nconst WordlistTable: React.FC&lt;TableProps&gt; = ({ \n  data, \n  onEdit, \n  onDelete, \n  loading = false \n}) =&gt; {\n  // Component implementation\n};\n</code></pre>"},{"location":"developer/frontend/#api-response-types","title":"API Response Types","text":"<p>Define types for API responses:</p> <pre><code>// types/api.ts\nexport interface ApiResponse&lt;T&gt; {\n  data: T;\n  message?: string;\n}\n\nexport interface PaginatedResponse&lt;T&gt; {\n  data: T[];\n  total: number;\n  page: number;\n  pageSize: number;\n}\n\nexport interface ApiError {\n  error: string;\n  message: string;\n  statusCode: number;\n}\n</code></pre>"},{"location":"developer/frontend/#utility-types","title":"Utility Types","text":"<p>Use TypeScript utility types effectively:</p> <pre><code>// Partial for update operations\ntype UpdateWordlistData = Partial&lt;Omit&lt;Wordlist, 'id' | 'created_at' | 'updated_at'&gt;&gt;;\n\n// Pick for specific field selections\ntype WordlistSummary = Pick&lt;Wordlist, 'id' | 'name' | 'word_count'&gt;;\n\n// Union types for status\ntype JobStatus = 'pending' | 'running' | 'completed' | 'failed';\n</code></pre>"},{"location":"developer/frontend/#testing-approaches","title":"Testing Approaches","text":""},{"location":"developer/frontend/#unit-testing-with-jest-and-react-testing-library","title":"Unit Testing with Jest and React Testing Library","text":"<p>Although the project doesn't have extensive tests yet, here's the recommended approach:</p> <pre><code>// WordlistTable.test.tsx\nimport { render, screen, fireEvent } from '@testing-library/react';\nimport { QueryClient, QueryClientProvider } from '@tanstack/react-query';\nimport WordlistTable from './WordlistTable';\n\nconst queryClient = new QueryClient({\n  defaultOptions: {\n    queries: { retry: false },\n  },\n});\n\nconst wrapper = ({ children }: { children: React.ReactNode }) =&gt; (\n  &lt;QueryClientProvider client={queryClient}&gt;\n    {children}\n  &lt;/QueryClientProvider&gt;\n);\n\ndescribe('WordlistTable', () =&gt; {\n  const mockWordlists: Wordlist[] = [\n    {\n      id: '1',\n      name: 'Test Wordlist',\n      description: 'Test description',\n      wordlist_type: WordlistType.GENERAL,\n      file_size: 1024,\n      word_count: 100,\n      verification_status: 'verified',\n      created_at: '2024-01-01',\n      updated_at: '2024-01-01',\n    },\n  ];\n\n  it('renders wordlist data correctly', () =&gt; {\n    render(\n      &lt;WordlistTable data={mockWordlists} /&gt;,\n      { wrapper }\n    );\n\n    expect(screen.getByText('Test Wordlist')).toBeInTheDocument();\n    expect(screen.getByText('100')).toBeInTheDocument();\n  });\n\n  it('calls onEdit when edit button is clicked', () =&gt; {\n    const handleEdit = jest.fn();\n    render(\n      &lt;WordlistTable data={mockWordlists} onEdit={handleEdit} /&gt;,\n      { wrapper }\n    );\n\n    fireEvent.click(screen.getByLabelText('Edit'));\n    expect(handleEdit).toHaveBeenCalledWith(mockWordlists[0]);\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#integration-testing","title":"Integration Testing","text":"<p>Test components with API interactions:</p> <pre><code>// WordlistsManagement.integration.test.tsx\nimport { render, screen, waitFor } from '@testing-library/react';\nimport { rest } from 'msw';\nimport { setupServer } from 'msw/node';\nimport WordlistsManagement from './WordlistsManagement';\n\nconst server = setupServer(\n  rest.get('/api/wordlists', (req, res, ctx) =&gt; {\n    return res(\n      ctx.json({\n        data: [\n          {\n            id: '1',\n            name: 'Test Wordlist',\n            word_count: 100,\n          },\n        ],\n      })\n    );\n  })\n);\n\nbeforeAll(() =&gt; server.listen());\nafterEach(() =&gt; server.resetHandlers());\nafterAll(() =&gt; server.close());\n\ntest('loads and displays wordlists', async () =&gt; {\n  render(&lt;WordlistsManagement /&gt;);\n\n  await waitFor(() =&gt; {\n    expect(screen.getByText('Test Wordlist')).toBeInTheDocument();\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#testing-custom-hooks","title":"Testing Custom Hooks","text":"<pre><code>// useDebounce.test.ts\nimport { renderHook, act } from '@testing-library/react-hooks';\nimport useDebounce from './useDebounce';\n\ndescribe('useDebounce', () =&gt; {\n  jest.useFakeTimers();\n\n  it('returns initial value immediately', () =&gt; {\n    const { result } = renderHook(() =&gt; useDebounce('test', 500));\n    expect(result.current).toBe('test');\n  });\n\n  it('debounces value changes', () =&gt; {\n    const { result, rerender } = renderHook(\n      ({ value, delay }) =&gt; useDebounce(value, delay),\n      { initialProps: { value: 'test', delay: 500 } }\n    );\n\n    rerender({ value: 'updated', delay: 500 });\n    expect(result.current).toBe('test');\n\n    act(() =&gt; {\n      jest.advanceTimersByTime(500);\n    });\n\n    expect(result.current).toBe('updated');\n  });\n});\n</code></pre>"},{"location":"developer/frontend/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nnpm test\n\n# Run tests in watch mode\nnpm test -- --watch\n\n# Run tests with coverage\nnpm test -- --coverage\n\n# Run specific test file\nnpm test WordlistTable.test.tsx\n</code></pre>"},{"location":"developer/frontend/#best-practices","title":"Best Practices","text":""},{"location":"developer/frontend/#component-guidelines","title":"Component Guidelines","text":"<ol> <li>Keep components focused: Each component should have a single responsibility</li> <li>Use TypeScript strictly: Enable strict mode and avoid <code>any</code> types</li> <li>Implement proper error handling: Use error boundaries and display user-friendly messages</li> <li>Optimize performance: Use React.memo, useCallback, and useMemo appropriately</li> <li>Follow accessibility standards: Use semantic HTML and ARIA attributes</li> </ol>"},{"location":"developer/frontend/#code-organization","title":"Code Organization","text":"<ol> <li>Consistent file naming: Use PascalCase for components, camelCase for utilities</li> <li>Co-locate related files: Keep tests, styles, and types near their components</li> <li>Extract reusable logic: Create custom hooks for shared functionality</li> <li>Document complex logic: Add JSDoc comments for non-trivial functions</li> </ol>"},{"location":"developer/frontend/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Lazy load routes: Use React.lazy for code splitting</li> <li>Optimize re-renders: Use React Query's staleTime and cacheTime</li> <li>Virtualize long lists: Use react-window for large datasets</li> <li>Debounce user input: Use the useDebounce hook for search fields</li> </ol>"},{"location":"developer/frontend/#security-considerations","title":"Security Considerations","text":"<ol> <li>Sanitize user input: Validate and sanitize all user-provided data</li> <li>Use HTTPS: Ensure all API calls use secure connections</li> <li>Handle authentication properly: Store tokens securely and implement refresh logic</li> <li>Implement CSP: Configure Content Security Policy headers</li> </ol>"},{"location":"developer/frontend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developer/frontend/#common-issues","title":"Common Issues","text":"<ol> <li>Certificate errors: The application includes a certificate check on startup</li> <li>CORS issues: Ensure the backend is configured to accept frontend origin</li> <li>Authentication loops: Check token refresh logic and API interceptors</li> <li>Build failures: Clear node_modules and package-lock.json, then reinstall</li> </ol>"},{"location":"developer/frontend/#debug-tips","title":"Debug Tips","text":"<ol> <li>Enable React Developer Tools</li> <li>Use Redux DevTools for React Query debugging</li> <li>Check network tab for API call failures</li> <li>Review console for error messages</li> </ol>"},{"location":"developer/frontend/#resources","title":"Resources","text":"<ul> <li>React Documentation</li> <li>Material-UI Documentation</li> <li>React Query Documentation</li> <li>TypeScript Documentation</li> <li>React Hook Form</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to KrakenHashes! This section will help you get up and running quickly.</p>"},{"location":"getting-started/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Overview</p> <p>Learn what KrakenHashes is and how it can help your security operations</p> </li> <li> <p> Quick Start</p> <p>Get KrakenHashes running in under 5 minutes with Docker</p> </li> <li> <p> Installation</p> <p>Detailed installation guide for production deployments</p> </li> <li> <p> First Password Crack</p> <p>Step-by-step tutorial for your first successful password crack</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li> Docker and Docker Compose installed</li> <li> At least 4GB of RAM</li> <li> 10GB of free disk space</li> <li> Linux-based system (recommended) or Windows with WSL2</li> <li>:material-gpu: GPU with CUDA support (optional but recommended)</li> </ul>"},{"location":"getting-started/#choose-your-path","title":"Choose Your Path","text":""},{"location":"getting-started/#i-want-to-try-it-quickly","title":"I want to try it quickly","text":"<p>\u2192 Go to Quick Start for a 5-minute Docker setup</p>"},{"location":"getting-started/#im-deploying-to-production","title":"I'm deploying to production","text":"<p>\u2192 Read the Installation Guide for detailed setup options</p>"},{"location":"getting-started/#i-want-to-understand-the-system","title":"I want to understand the system","text":"<p>\u2192 Start with the Overview to learn about KrakenHashes</p>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check our Troubleshooting Guide</li> <li>Join our Discord Community</li> <li>Report issues on GitHub</li> </ul>"},{"location":"getting-started/first-crack/","title":"Your First Password Crack","text":"<p>This guide walks you through performing your first password crack with KrakenHashes. By the end of this tutorial, you'll understand the complete workflow from setup to viewing results.</p> <p>Prerequisites</p> <ul> <li>KrakenHashes is installed and running</li> <li>You're logged in as an admin user</li> <li>You have access to a hashcat binary file</li> <li>At least one agent machine is available (can be the same as the server)</li> </ul>"},{"location":"getting-started/first-crack/#step-1-upload-hashcat-binary","title":"Step 1: Upload Hashcat Binary","text":"<p>First, we need to upload the hashcat binary that agents will use to perform the actual cracking.</p>"},{"location":"getting-started/first-crack/#navigate-to-binary-management","title":"Navigate to Binary Management","text":"<ol> <li>From the main menu, click Admin \u2192 Binary Management</li> <li>Click the Add Binary button in the top right</li> </ol>"},{"location":"getting-started/first-crack/#upload-the-binary","title":"Upload the Binary","text":"<ol> <li>In the upload dialog:</li> <li>Click Choose File or drag and drop your hashcat binary</li> <li>Supported formats: <code>.7z</code>, <code>.zip</code>, <code>.tar.gz</code></li> <li> <p>The binary should be compressed and include all necessary files</p> </li> <li> <p>Click Upload and wait for the process to complete</p> </li> </ol> <p>Getting Hashcat</p> <p>If you don't have hashcat, download it from hashcat.net/hashcat/</p> <p>For this tutorial, download the appropriate binary for your system: - Linux: <code>hashcat-6.2.6.7z</code> - Windows: <code>hashcat-6.2.6.exe</code> (compress to .zip first)</p> <ol> <li>After upload, the system will automatically verify the binary</li> <li>Status should show as verified \u2713</li> <li>Note the Binary ID for later reference</li> </ol> <p>Why Binary Upload is First</p> <p>Uploading a hashcat binary is required before most system features become available. Specifically: - The potfile preset job is created only after a binary exists - Agents need binaries to perform any cracking operations - Job creation requires selecting a binary version</p>"},{"location":"getting-started/first-crack/#step-2-upload-a-simple-wordlist","title":"Step 2: Upload a Simple Wordlist","text":"<p>Next, let's create a basic wordlist for our first crack attempt.</p>"},{"location":"getting-started/first-crack/#navigate-to-wordlist-management","title":"Navigate to Wordlist Management","text":"<ol> <li>From the main menu, click Resources \u2192 Wordlists</li> <li>Click the Add Wordlist button</li> </ol>"},{"location":"getting-started/first-crack/#create-a-test-wordlist","title":"Create a Test Wordlist","text":"<p>For this tutorial, let's create a simple wordlist file:</p> <ol> <li> <p>Create a text file named <code>common-passwords.txt</code> with these contents:    <pre><code>password\n123456\npassword123\nadmin\nletmein\nwelcome\nmonkey\ndragon\n</code></pre></p> </li> <li> <p>In the upload dialog:</p> </li> <li>Name: \"Common Passwords Tutorial\"</li> <li>Description: \"Basic wordlist for first crack tutorial\"</li> <li>Type: General</li> <li> <p>File: Select your <code>common-passwords.txt</code></p> </li> <li> <p>Click Upload</p> </li> </ol> <p>Wordlist Types</p> <ul> <li>General: Standard password lists</li> <li>Usernames: Lists of common usernames</li> <li>Custom: Specialized wordlists for specific targets</li> </ul>"},{"location":"getting-started/first-crack/#step-3-create-a-test-hashlist","title":"Step 3: Create a Test Hashlist","text":"<p>Now let's create some password hashes to crack. We'll use known passwords so you can verify the results.</p>"},{"location":"getting-started/first-crack/#prepare-test-hashes","title":"Prepare Test Hashes","text":"<ol> <li>Create a file named <code>test-hashes.txt</code> with these MD5 hashes:    <pre><code>5f4dcc3b5aa765d61d8327deb882cf99\ne10adc3949ba59abbe56e057f20f883e\n482c811da5d5b4bc6d497ffa98491e38\n</code></pre></li> </ol> <p>What are these hashes?</p> <p>These are MD5 hashes of: - <code>password</code> \u2192 5f4dcc3b5aa765d61d8327deb882cf99 - <code>123456</code> \u2192 e10adc3949ba59abbe56e057f20f883e - <code>password123</code> \u2192 482c811da5d5b4bc6d497ffa98491e38</p>"},{"location":"getting-started/first-crack/#upload-the-hashlist","title":"Upload the Hashlist","text":"<ol> <li>Navigate to Hashlists from the main menu</li> </ol> <p> Hashlist Management page with UPLOAD HASHLIST button highlighted, showing the interface where users can view existing hashlists and initiate new uploads</p> <ol> <li>Click Upload Hashlist</li> </ol> <p></p> <ol> <li>Fill in the details:</li> <li>Name: \"Tutorial Test Hashes\"</li> <li>Description: \"MD5 hashes for first crack tutorial\"</li> <li>Hash Type: MD5 (mode 0)</li> <li>Client: (Optional - leave blank for tutorial)</li> <li> <p>File: Select your <code>test-hashes.txt</code></p> </li> <li> <p>Click Upload</p> </li> </ol> <p>The system will process your hashlist and show: - Total hashes: 3 - Unique hashes: 3 - Status: Active</p>"},{"location":"getting-started/first-crack/#step-4-connect-an-agent","title":"Step 4: Connect an Agent","text":"<p>Agents are the worker machines that perform the actual password cracking. Let's connect one.</p>"},{"location":"getting-started/first-crack/#generate-a-claim-code","title":"Generate a Claim Code","text":"<ol> <li>Navigate to Admin \u2192 Agent Management</li> <li>Click Generate Claim Code</li> </ol> <ol> <li>A claim code will be generated (e.g., <code>ABCD-EFGH-IJKL</code>)</li> <li>Copy this code - you'll need it for the agent</li> </ol>"},{"location":"getting-started/first-crack/#install-and-configure-the-agent","title":"Install and Configure the Agent","text":"<p>On your agent machine (can be the same as the server):</p> <ol> <li>Download the KrakenHashes agent for your platform</li> <li>Extract it to a directory</li> <li> <p>Run the registration command:    <pre><code>./krakenhashes-agent --host your-server:31337 --claim ABCD-EFGH-IJKL\n</code></pre></p> </li> <li> <p>The agent will:</p> </li> <li>Register with the server</li> <li>Receive an API key</li> <li>Start heartbeat communication</li> <li>Sync required files (binaries, wordlists)</li> </ol> <p>Agent Connected!</p> <p>Once registered, you'll see the agent appear in the Agent Management page with: - Status: Online \ud83d\udfe2 - Hardware info (GPU details if available) - Last heartbeat timestamp</p>"},{"location":"getting-started/first-crack/#step-5-create-and-run-a-job","title":"Step 5: Create and Run a Job","text":"<p>Now for the exciting part - let's crack those passwords!</p>"},{"location":"getting-started/first-crack/#create-a-new-job","title":"Create a New Job","text":"<ol> <li>Go back to Hashlists</li> <li>Find your \"Tutorial Test Hashes\" and click on it</li> <li>Click Create Job button</li> </ol>"},{"location":"getting-started/first-crack/#configure-the-job","title":"Configure the Job","text":"<p>You have three options for creating jobs:</p> Quick Job (Recommended for Tutorial)Custom JobWorkflow <ol> <li>Select the Preset Jobs tab</li> <li>Choose \"Quick Dictionary Attack\"</li> <li>This preset includes:</li> <li>Dictionary attack with your wordlists</li> <li>Basic rules for variations</li> <li>Optimized for speed</li> </ol> <ol> <li>Select the Custom tab</li> <li>Configure:</li> <li>Name: \"Tutorial First Crack\"</li> <li>Attack Mode: Dictionary (0)</li> <li>Wordlists: Select \"Common Passwords Tutorial\"</li> <li>Rules: (Optional - leave empty for now)</li> </ol> <ol> <li>Select the Workflows tab</li> <li>Choose a predefined workflow that runs multiple attack strategies</li> </ol>"},{"location":"getting-started/first-crack/#start-the-job","title":"Start the Job","text":"<ol> <li>Review your settings</li> <li>Click Create Job</li> </ol> <p>The job will be: - Added to the queue - Assigned to an available agent - Started automatically</p>"},{"location":"getting-started/first-crack/#step-6-monitor-progress","title":"Step 6: Monitor Progress","text":""},{"location":"getting-started/first-crack/#view-job-status","title":"View Job Status","text":"<ol> <li>Navigate to Jobs from the main menu</li> <li>Find your job in the list</li> </ol> <p>You'll see: - Status: Running \ud83d\udd04 - Progress: Percentage complete - Speed: Hashes per second - Time: Elapsed and estimated remaining</p>"},{"location":"getting-started/first-crack/#real-time-updates","title":"Real-time Updates","text":"<p>The dashboard updates in real-time showing: - Candidates tested - Passwords cracked - Current speed - Agent assignment</p> <p>Understanding Speed</p> <p>Speed is measured in H/s (hashes per second). Higher is better! - CPU only: 1,000 - 100,000 H/s - Single GPU: 1,000,000+ H/s - Multiple GPUs: Much higher!</p>"},{"location":"getting-started/first-crack/#step-7-view-results","title":"Step 7: View Results","text":"<p>Once the job completes (should be quick for this tutorial):</p>"},{"location":"getting-started/first-crack/#check-the-results","title":"Check the Results","text":"<ol> <li>Go back to your hashlist</li> <li>You'll see the status has updated:</li> <li>Cracked: 3/3 (100%)</li> <li>Status indicators next to each hash</li> </ol>"},{"location":"getting-started/first-crack/#view-cracked-passwords","title":"View Cracked Passwords","text":"<ol> <li>Click on a cracked hash to see details</li> <li>The plaintext password will be displayed</li> <li>You should see:</li> <li><code>5f4dcc3b5aa765d61d8327deb882cf99</code> \u2192 <code>password</code></li> <li><code>e10adc3949ba59abbe56e057f20f883e</code> \u2192 <code>123456</code></li> <li><code>482c811da5d5b4bc6d497ffa98491e38</code> \u2192 <code>password123</code></li> </ol> <p> Hashlist view showing successfully cracked passwords with their plaintext values revealed</p>"},{"location":"getting-started/first-crack/#export-results","title":"Export Results","text":"<ol> <li>Click Export button</li> <li>Choose format:</li> <li>CSV: For spreadsheets</li> <li>JSON: For programming</li> <li> <p>Hashcat Potfile: hash:plain format</p> </li> <li> <p>Download includes:</p> </li> <li>Original hashes</li> <li>Cracked plaintexts</li> <li>Crack timestamps</li> <li>Metadata</li> </ol>"},{"location":"getting-started/first-crack/#understanding-what-happened","title":"Understanding What Happened","text":"<p>Let's review the complete workflow:</p> <pre><code>graph LR\n    A[Upload Binary] --&gt; B[Upload Wordlist]\n    B --&gt; C[Create Hashlist]\n    C --&gt; D[Connect Agent]\n    D --&gt; E[Create Job]\n    E --&gt; F[Agent Executes]\n    F --&gt; G[View Results]</code></pre>"},{"location":"getting-started/first-crack/#key-concepts","title":"Key Concepts","text":"<ol> <li>Hashcat Binary: The actual cracking engine</li> <li>Agents download and use this</li> <li> <p>Supports many hash types and attack modes</p> </li> <li> <p>Wordlists: Lists of potential passwords</p> </li> <li>Can be general or targeted</li> <li> <p>Quality matters more than quantity</p> </li> <li> <p>Hashlists: Your target hashes to crack</p> </li> <li>Organized by hash type</li> <li> <p>Can contain thousands or millions</p> </li> <li> <p>Agents: Distributed workers</p> </li> <li>Can be anywhere with internet</li> <li>Automatically sync files</li> <li> <p>Report progress in real-time</p> </li> <li> <p>Jobs: Work assignments</p> </li> <li>Define what to try (wordlists, rules, masks)</li> <li>Can be simple or complex workflows</li> <li>Automatically distributed to agents</li> </ol> <p> Main dashboard view showing hashlist management with crack statistics and job execution interface - what you'll see after completing your first successful password crack</p>"},{"location":"getting-started/first-crack/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first crack:</p>"},{"location":"getting-started/first-crack/#1-try-more-complex-attacks","title":"1. Try More Complex Attacks","text":"<ul> <li>Add Rules: Transform wordlists (password \u2192 Password123!)</li> <li>Use Masks: Pattern-based attacks (?u?l?l?l?d?d?d?d)</li> <li>Combination Attacks: Combine multiple wordlists</li> </ul>"},{"location":"getting-started/first-crack/#2-scale-up","title":"2. Scale Up","text":"<ul> <li>Add More Agents: Distribute work across multiple machines</li> <li>Use GPUs: Dramatically increase cracking speed</li> <li>Upload Larger Wordlists: Try rockyou.txt or custom lists</li> </ul>"},{"location":"getting-started/first-crack/#3-create-workflows","title":"3. Create Workflows","text":"<p>Build multi-stage attacks: 1. Quick dictionary attack 2. Dictionary with rules 3. Targeted masks 4. Brute force (last resort)</p>"},{"location":"getting-started/first-crack/#4-explore-features","title":"4. Explore Features","text":"<ul> <li>Client Management: Organize hashlists by client/project</li> <li>Scheduling: Control when agents run jobs</li> <li>Retention Policies: Automatic cleanup of old data</li> <li>Team Collaboration: Share access with team members</li> </ul>"},{"location":"getting-started/first-crack/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/first-crack/#common-issues","title":"Common Issues","text":"Agent won't connect <ul> <li>Check firewall rules (port 8443)</li> <li>Verify server URL is correct</li> <li>Ensure claim code hasn't expired</li> <li>Check agent logs for errors</li> </ul> Job stuck in queue <ul> <li>Verify agent is online</li> <li>Check agent has required hardware</li> <li>Ensure binary is verified</li> <li>Look for error messages</li> </ul> No passwords cracked <ul> <li>Try a different wordlist</li> <li>Add rules for variations</li> <li>Check hash type is correct</li> <li>Verify hash format</li> </ul> Slow performance <ul> <li>Use GPU-enabled agents</li> <li>Optimize wordlists (remove duplicates)</li> <li>Check network connectivity</li> <li>Monitor system resources</li> </ul>"},{"location":"getting-started/first-crack/#summary","title":"Summary","text":"<p>Congratulations! You've successfully:</p> <ul> <li>\u2705 Uploaded a hashcat binary</li> <li>\u2705 Created a wordlist</li> <li>\u2705 Uploaded target hashes</li> <li>\u2705 Connected an agent</li> <li>\u2705 Ran a cracking job</li> <li>\u2705 Retrieved cracked passwords</li> </ul> <p>You now understand the fundamental workflow of KrakenHashes. From here, you can scale up to crack real-world password hashes with more sophisticated attacks and distributed agents.</p> <p>Ready for Real Work!</p> <p>You're now equipped to:</p> <ul> <li>Handle client hashlist submissions</li> <li>Build effective attack strategies  </li> <li>Manage distributed cracking operations</li> <li>Deliver results efficiently</li> </ul>"},{"location":"getting-started/first-crack/#additional-resources","title":"Additional Resources","text":"<ul> <li>Understanding Hash Types</li> <li>Building Better Wordlists</li> <li>Advanced Attack Strategies</li> <li>Agent Deployment Guide</li> </ul>"},{"location":"getting-started/installation/","title":"KrakenHashes Installation Guide","text":"<p>This guide covers both production and development installation of KrakenHashes.</p>"},{"location":"getting-started/installation/#production-installation","title":"Production Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+ and Docker Compose 2.0+</li> <li>4GB+ RAM recommended</li> <li>20GB+ disk space for hash files, wordlists, and rules (Very dependent on your wordlist size)</li> <li>Linux host (Ubuntu 20.04+, Debian 11+, RHEL 8+, or similar)</li> </ul>"},{"location":"getting-started/installation/#docker-compose-v2-requirements","title":"Docker Compose v2 Requirements","text":"<p>KrakenHashes requires Docker Compose v2.0 or higher due to: - Advanced environment variable interpolation syntax - Improved health check support - Better service dependency handling</p> <p>The docker compose.yml uses syntax like <code>${LOG_DIR:-./logs}/postgres</code> which requires v2.</p>"},{"location":"getting-started/installation/#installing-docker-compose-v2","title":"Installing Docker Compose v2","text":"<pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install docker compose-plugin\n\n# CentOS/RHEL/Fedora\nsudo yum install docker compose-plugin\n\n# Manual installation (all systems)\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker compose\nsudo chmod +x /usr/local/bin/docker compose\n\n# Verify installation\ndocker compose version\n</code></pre>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>If you see this error: <pre><code>ERROR: Invalid interpolation format for \"postgres\" option in service \"services\"\n</code></pre></p> <p>You're using the old docker compose v1. Install v2 and use <code>docker compose</code> (with a space).</p>"},{"location":"getting-started/installation/#quick-start-with-docker-hub","title":"Quick Start with Docker Hub","text":"<p>The easiest way to run KrakenHashes is using the pre-built Docker image from Docker Hub.</p>"},{"location":"getting-started/installation/#1-create-a-docker-composeyml-file","title":"1. Create a docker compose.yml file","text":"<pre><code>services:\n    postgres:\n        image: postgres:15-alpine\n        container_name: krakenhashes-postgres\n        volumes:\n            - postgres_data:/var/lib/postgresql/data\n        environment:\n            - POSTGRES_USER=krakenhashes\n            - POSTGRES_PASSWORD=changeme # CHANGE THIS!\n            - POSTGRES_DB=krakenhashes\n        restart: unless-stopped\n        healthcheck:\n            test: [\"CMD-SHELL\", \"pg_isready -U krakenhashes\"]\n            interval: 5s\n            timeout: 5s\n            retries: 5\n\n    krakenhashes:\n        image: zerkereod/krakenhashes:latest\n        container_name: krakenhashes-app\n        depends_on:\n            postgres:\n                condition: service_healthy\n        ports:\n            - \"443:443\" # Frontend HTTPS\n            - \"31337:31337\" # Backend API HTTPS\n            - \"1337:1337\" # Backend API HTTP\n        volumes:\n            - krakenhashes_data:/var/lib/krakenhashes\n            - ./logs:/var/log/krakenhashes\n        environment:\n            - DB_HOST=postgres\n            - DB_PORT=5432\n            - DB_NAME=krakenhashes\n            - DB_USER=krakenhashes\n            - DB_PASSWORD=changeme # CHANGE THIS!\n            - JWT_SECRET=your-secret-key-here # CHANGE THIS!\n            - TLS_MODE=self-signed # Options: self-signed, user-provided, certbot\n        restart: unless-stopped\n\nvolumes:\n    postgres_data:\n    krakenhashes_data:\n</code></pre>"},{"location":"getting-started/installation/#2-create-a-env-file-optional-but-recommended","title":"2. Create a .env file (optional but recommended)","text":"<pre><code># Database Configuration\nDB_USER=krakenhashes\nDB_PASSWORD=your-secure-password\nDB_NAME=krakenhashes\n\n# Security\nJWT_SECRET=your-very-long-random-string\n\n# TLS Configuration\nTLS_MODE=self-signed\n\n# Ports (optional, defaults shown)\nFRONTEND_PORT=443\nKH_HTTPS_PORT=31337\nKH_PORT=1337\n</code></pre>"},{"location":"getting-started/installation/#3-start-krakenhashes","title":"3. Start KrakenHashes","text":"<pre><code># Pull the latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Start the services\ndocker compose up -d\n\n# Check logs\ndocker compose logs -f\n</code></pre>"},{"location":"getting-started/installation/#4-access-the-application","title":"4. Access the Application","text":"<ul> <li>Frontend: https://localhost:443</li> <li>Backend API: https://localhost:31337</li> <li>Default admin credentials: admin:KrakenHashes1!</li> </ul>"},{"location":"getting-started/installation/#production-configuration","title":"Production Configuration","text":""},{"location":"getting-started/installation/#tlsssl-options","title":"TLS/SSL Options","text":"<p>KrakenHashes supports three TLS modes:</p> <ol> <li>self-signed (default) - Automatically generates self-signed certificates</li> <li>user-provided - Use your own certificates</li> <li>certbot - Automatically obtain Let's Encrypt certificates (tested and working)</li> </ol>"},{"location":"getting-started/installation/#using-your-own-certificates","title":"Using Your Own Certificates","text":"<pre><code>krakenhashes:\n    environment:\n        - TLS_MODE=user-provided\n    volumes:\n        - ./certs/server.crt:/etc/krakenhashes/certs/server.crt:ro\n        - ./certs/server.key:/etc/krakenhashes/certs/server.key:ro\n        - ./certs/ca.crt:/etc/krakenhashes/certs/ca.crt:ro # Optional\n</code></pre>"},{"location":"getting-started/installation/#using-lets-encrypt-certbot","title":"Using Let's Encrypt (Certbot)","text":"<p>Important Limitation</p> <p>Certbot cannot add IP addresses to certificates. You must access the system through the domain name for the certificate to be trusted. If you need IP access, use self-signed or user-provided certificates instead.</p> <pre><code>krakenhashes:\n    environment:\n        - TLS_MODE=certbot\n        - CERTBOT_EMAIL=admin@example.com\n        - CERTBOT_DOMAIN=krakenhashes.example.com\n</code></pre>"},{"location":"getting-started/installation/#data-persistence","title":"Data Persistence","text":"<p>Important directories that should be persisted:</p> <ul> <li><code>/var/lib/krakenhashes</code> - Application data (binaries, wordlists, rules, hashlists)</li> <li><code>/var/log/krakenhashes</code> - Application logs</li> <li>PostgreSQL data volume</li> </ul>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>DB_HOST</code> localhost PostgreSQL hostname <code>DB_PORT</code> 5432 PostgreSQL port <code>DB_NAME</code> krakenhashes Database name <code>DB_USER</code> krakenhashes Database user <code>DB_PASSWORD</code> krakenhashes Database password <code>JWT_SECRET</code> (random) JWT signing secret <code>TLS_MODE</code> self-signed TLS certificate mode <code>PUID</code> 1000 User ID for file permissions <code>PGID</code> 1000 Group ID for file permissions"},{"location":"getting-started/installation/#logging-configuration","title":"Logging Configuration","text":"<p>KrakenHashes provides comprehensive logging with configurable levels and component-specific debugging:</p>"},{"location":"getting-started/installation/#log-levels","title":"Log Levels","text":"<p>Set the <code>LOG_LEVEL</code> environment variable to control logging verbosity:</p> <ul> <li><code>DEBUG</code> - Detailed debugging information (verbose)</li> <li><code>INFO</code> - General information and status updates (default)</li> <li><code>WARNING</code> - Warning messages that need attention</li> <li><code>ERROR</code> - Error messages only</li> </ul>"},{"location":"getting-started/installation/#debug-flags","title":"Debug Flags","text":"<p>Enable component-specific debugging with these environment variables:</p> Flag Description <code>DEBUG_SQL</code> Log all SQL queries and parameters <code>DEBUG_HTTP</code> Log HTTP requests and responses <code>DEBUG_WEBSOCKET</code> Log WebSocket messages <code>DEBUG_AUTH</code> Log authentication attempts and JWT validation <code>DEBUG_JOBS</code> Log job processing and scheduling"},{"location":"getting-started/installation/#log-storage","title":"Log Storage","text":"<p>Logs are stored in the following directory structure:</p> <pre><code>$HOME/krakenhashes/logs/\n\u251c\u2500\u2500 backend/      # Backend application logs\n\u251c\u2500\u2500 frontend/     # Nginx access and error logs\n\u251c\u2500\u2500 nginx/        # Nginx configuration logs\n\u2514\u2500\u2500 postgres/     # PostgreSQL database logs\n</code></pre> <p>To view logs in real-time:</p> <pre><code># All logs\ndocker compose logs -f\n\n# Specific service\ndocker compose logs -f backend\n\n# Check for errors\ndocker compose logs | grep -i error\n</code></pre>"},{"location":"getting-started/installation/#production-best-practices","title":"Production Best Practices","text":"<ol> <li> <p>Security</p> <ul> <li>Always change default passwords</li> <li>Use a strong JWT_SECRET (minimum 32 characters)</li> <li>Use proper TLS certificates for production</li> <li>Restrict network access to necessary ports only</li> </ul> </li> <li> <p>Backup</p> <ul> <li>Regular PostgreSQL backups: <code>docker exec krakenhashes-postgres pg_dump -U krakenhashes krakenhashes &gt; backup.sql</code></li> <li>Backup the data volume: <code>/var/lib/krakenhashes</code></li> </ul> </li> <li> <p>Monitoring</p> <ul> <li>Monitor logs in <code>/var/log/krakenhashes</code></li> <li>Set up health checks for the application endpoints</li> <li>Monitor disk space for hash storage</li> </ul> </li> <li> <p>Updates</p> <pre><code># Pull latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Recreate container with new image\ndocker compose up -d --force-recreate krakenhashes\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#troubleshooting-production-issues","title":"Troubleshooting Production Issues","text":""},{"location":"getting-started/installation/#container-wont-start","title":"Container won't start","text":"<pre><code># Check logs\ndocker compose logs krakenhashes\n\n# Check if ports are already in use\nnetstat -tlnp | grep -E \"443|31337|1337|5432\"\n</code></pre>"},{"location":"getting-started/installation/#database-connection-issues","title":"Database connection issues","text":"<pre><code># Test database connectivity\ndocker exec krakenhashes-app nc -zv postgres 5432\n\n# Check database logs\ndocker compose logs postgres\n</code></pre>"},{"location":"getting-started/installation/#permission-issues","title":"Permission issues","text":"<pre><code># Fix ownership (adjust PUID/PGID as needed)\ndocker exec krakenhashes-app chown -R 1000:1000 /var/lib/krakenhashes\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":""},{"location":"getting-started/installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker Engine 20.10+ and Docker Compose 2.0+</li> <li>Go 1.23.1+</li> <li>Node.js 20+</li> <li>Git</li> <li>8GB+ RAM recommended for development</li> </ul>"},{"location":"getting-started/installation/#development-setup-options","title":"Development Setup Options","text":""},{"location":"getting-started/installation/#option-1-docker-development-environment-recommended","title":"Option 1: Docker Development Environment (Recommended)","text":"<p>This setup provides hot-reloading for both backend and frontend.</p> <ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre> </li> <li> <p>Start development environment</p> <pre><code># Start all services with hot-reloading\ndocker compose -f docker compose.dev.yml up\n\n# Or run in background\ndocker compose -f docker compose.dev.yml up -d\n</code></pre> </li> <li> <p>Access the services</p> <ul> <li>Frontend: http://localhost:3000 (with hot-reload)</li> <li>Backend API: https://localhost:31337</li> <li>PostgreSQL: localhost:5432</li> </ul> </li> <li> <p>View logs</p> <pre><code># All services\ndocker compose -f docker compose.dev.yml logs -f\n\n# Specific service\ndocker compose -f docker compose.dev.yml logs -f backend\n</code></pre> </li> </ol> <p>The development environment features:</p> <ul> <li>Backend: Uses Air for Go hot-reloading</li> <li>Frontend: Uses React development server with hot-reload</li> <li>Database: PostgreSQL with persistent volume</li> <li>Volumes: Source code mounted for live updates</li> </ul>"},{"location":"getting-started/installation/#option-2-local-development-traditional","title":"Option 2: Local Development (Traditional)","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/yourusername/krakenhashes.git\ncd krakenhashes\n</code></pre> </li> <li> <p>Start PostgreSQL</p> <pre><code>cd backend\ndocker compose up -d\ncd ..\n</code></pre> </li> <li> <p>Run the backend</p> <pre><code>cd backend\ngo mod download\ngo run cmd/server/main.go\n</code></pre> </li> <li> <p>Run the frontend <pre><code>cd frontend\nnpm install\nnpm start\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#development-configuration","title":"Development Configuration","text":""},{"location":"getting-started/installation/#backend-development","title":"Backend Development","text":"<p>The backend uses Air for hot-reloading. Configuration is in <code>backend/.air.toml</code>:</p> <pre><code>[build]\n  cmd = \"go build -o ./tmp/main ./cmd/server\"\n  bin = \"./tmp/main\"\n  include_ext = [\"go\", \"tpl\", \"tmpl\", \"html\"]\n</code></pre> <p>Environment variables for development:</p> <pre><code>export DB_HOST=localhost\nexport DB_PORT=5432\nexport DB_NAME=krakenhashes\nexport DB_USER=krakenhashes\nexport DB_PASSWORD=krakenhashes\nexport JWT_SECRET=dev_jwt_secret\nexport DEBUG=true\nexport LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"getting-started/installation/#frontend-development","title":"Frontend Development","text":"<p>The frontend uses Create React App with environment variables:</p> <pre><code>REACT_APP_API_URL=https://localhost:31337\nREACT_APP_WS_URL=wss://localhost:31337\nREACT_APP_DEBUG=true\n</code></pre>"},{"location":"getting-started/installation/#development-workflows","title":"Development Workflows","text":""},{"location":"getting-started/installation/#running-tests","title":"Running Tests","text":"<pre><code># Backend tests\ncd backend\ngo test ./...\ngo test -v ./internal/services\n\n# Frontend tests\ncd frontend\nnpm test\n</code></pre>"},{"location":"getting-started/installation/#building-for-production","title":"Building for Production","text":"<pre><code># Build production Docker image\ndocker build -f Dockerfile.prod -t krakenhashes:local .\n\n# Test production build locally\ndocker compose down\ndocker compose up -d\n</code></pre>"},{"location":"getting-started/installation/#database-migrations","title":"Database Migrations","text":"<pre><code># Apply migrations\ncd backend\nmake migrate-up\n\n# Rollback migrations\nmake migrate-down\n\n# Create new migration\nmake migrate-create name=add_new_table\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":""},{"location":"getting-started/installation/#logging","title":"Logging","text":"<p>View development logs with filtering:</p> <pre><code># Backend logs\ntail -f logs/backend.log | grep -i error\n\n# Frontend logs\ntail -f logs/frontend.log\n\n# All logs\ngrep -i error logs/*.log\n</code></pre>"},{"location":"getting-started/installation/#database-access","title":"Database Access","text":"<pre><code># Connect to development database\ndocker exec -it krakenhashes-postgres-dev psql -U krakenhashes -d krakenhashes\n\n# Quick query\ndocker exec krakenhashes-postgres-dev psql -U krakenhashes -d krakenhashes -c \"SELECT * FROM users;\"\n</code></pre>"},{"location":"getting-started/installation/#switching-between-development-and-production","title":"Switching Between Development and Production","text":"<pre><code># Stop development environment\ndocker compose -f docker compose.dev.yml down\n\n# Start production environment\ndocker compose up -d\n\n# Switch back to development\ndocker compose down\ndocker compose -f docker compose.dev.yml up\n</code></pre>"},{"location":"getting-started/installation/#common-development-issues","title":"Common Development Issues","text":""},{"location":"getting-started/installation/#port-conflicts","title":"Port conflicts","text":"<pre><code># Check what's using the ports\nlsof -i :3000   # Frontend\nlsof -i :31337  # Backend HTTPS\nlsof -i :5432   # PostgreSQL\n</code></pre>"},{"location":"getting-started/installation/#go-module-issues","title":"Go module issues","text":"<pre><code># Clear module cache\ngo clean -modcache\n\n# Update dependencies\ngo mod tidy\ngo mod download\n</code></pre>"},{"location":"getting-started/installation/#frontend-dependency-issues","title":"Frontend dependency issues","text":"<pre><code># Clear npm cache\ncd frontend\nrm -rf node_modules package-lock.json\nnpm cache clean --force\nnpm install\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Review the configuration settings in your .env file</li> <li>Check the Admin Documentation for system administration</li> <li>See User Documentation for using KrakenHashes</li> <li>Join our community for support and updates</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#what-is-krakenhashes","title":"What is KrakenHashes?","text":"<p>KrakenHashes is a distributed password cracking management system that transforms the power of Hashcat into an enterprise-ready platform. While Hashcat excels as a command-line tool for password recovery, KrakenHashes adds the orchestration, management, and collaboration features needed for professional security teams.</p> <p>Think of KrakenHashes as the control center for your password auditing operations \u2013 coordinating multiple machines, managing vast collections of hashes, and providing actionable insights through an intuitive web interface.</p>"},{"location":"getting-started/overview/#why-krakenhashes","title":"Why KrakenHashes?","text":""},{"location":"getting-started/overview/#the-challenge-with-direct-hashcat-usage","title":"The Challenge with Direct Hashcat Usage","text":"<p>Running Hashcat directly presents several challenges for security teams:</p> <ul> <li>Single Machine Limitation: Hashcat runs on one machine at a time</li> <li>Manual Coordination: No built-in way to distribute work across multiple systems</li> <li>Limited Visibility: Command-line output makes it hard to track multiple jobs</li> <li>No Collaboration: Difficult for teams to share resources and results</li> <li>Manual Management: Wordlists, rules, and results require manual organization</li> </ul>"},{"location":"getting-started/overview/#the-krakenhashes-solution","title":"The KrakenHashes Solution","text":"<p>KrakenHashes addresses these limitations by providing:</p> <ul> <li>Distributed Processing: Coordinate multiple agents across your infrastructure</li> <li>Centralized Management: Single interface for all your cracking operations</li> <li>Team Collaboration: Share resources, results, and strategies</li> <li>Automated Workflows: Pre-configured attack patterns and job templates</li> <li>Enterprise Features: Authentication, authorization, and audit trails</li> </ul> <p> KrakenHashes main dashboard showing the hashlist overview table with crack progress and the jobs management interface with dark theme</p>"},{"location":"getting-started/overview/#key-features-and-capabilities","title":"Key Features and Capabilities","text":""},{"location":"getting-started/overview/#distributed-architecture","title":"\ud83d\ude80 Distributed Architecture","text":"<ul> <li> <p>Multi-Agent Support</p> <p>Deploy agents on multiple machines to leverage all available hardware</p> </li> <li> <p>Automatic Load Balancing</p> <p>Jobs are intelligently distributed based on agent capabilities</p> </li> <li> <p>Dynamic Scaling</p> <p>Add or remove agents without interrupting active jobs</p> </li> <li> <p>Resource Scheduling</p> <p>Configure when agents are available for processing</p> </li> </ul>"},{"location":"getting-started/overview/#job-management","title":"\ud83d\udcca Job Management","text":"<p>KrakenHashes transforms password cracking from ad-hoc commands into managed operations:</p> <ul> <li>Job Templates: Pre-configured attack strategies for common scenarios</li> <li>Workflows: Chain multiple attack types for comprehensive audits</li> <li>Priority System: Ensure critical jobs get resources first (0-1000 scale)</li> <li>Progress Tracking: Real-time visibility into job status and ETA</li> <li>Automatic Chunking: Large jobs are split for optimal distribution</li> </ul>"},{"location":"getting-started/overview/#resource-organization","title":"\ud83d\uddc2\ufe0f Resource Organization","text":"<p>Centralized management of all cracking resources:</p> <ul> <li>Wordlist Library: Upload once, use across all agents</li> <li>Rule Management: Organize and version your rule files</li> <li>Binary Versioning: Ensure all agents run compatible Hashcat versions</li> <li>Hash Organization: Group hashes by client, project, or engagement</li> <li>Result Storage: Automatic capture and organization of cracked passwords</li> </ul>"},{"location":"getting-started/overview/#security-compliance","title":"\ud83d\udd12 Security &amp; Compliance","text":"<p>Built with enterprise security requirements in mind:</p> <ul> <li>Multi-Factor Authentication: TOTP, email, and backup codes</li> <li>Role-Based Access: Admin and user roles with granular permissions</li> <li>TLS/SSL Support: Multiple certificate options including self-signed</li> <li>Audit Logging: Track all actions for compliance requirements</li> <li>Data Retention: Configurable policies for automatic cleanup</li> <li>Client Isolation: Keep customer data properly segregated</li> </ul>"},{"location":"getting-started/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Web Interface\"\n        UI[React Frontend]\n    end\n\n    subgraph \"Backend Services\"\n        API[REST API]\n        WS[WebSocket Hub]\n        SCH[Job Scheduler]\n        FS[File Storage]\n    end\n\n    subgraph \"Data Layer\"\n        PG[(PostgreSQL)]\n    end\n\n    subgraph \"Agent Pool\"\n        A1[Agent 1&lt;br/&gt;GPU: RTX 4090]\n        A2[Agent 2&lt;br/&gt;GPU: RTX 3080]\n        A3[Agent N&lt;br/&gt;CPU Only]\n    end\n\n    subgraph \"Hashcat\"\n        HC1[Hashcat Instance]\n        HC2[Hashcat Instance]\n        HC3[Hashcat Instance]\n    end\n\n    UI --&gt;|HTTPS| API\n    API --&gt; PG\n    API --&gt; FS\n    API --&gt; SCH\n    SCH --&gt; WS\n    WS &lt;--&gt;|WebSocket| A1\n    WS &lt;--&gt;|WebSocket| A2\n    WS &lt;--&gt;|WebSocket| A3\n    A1 --&gt; HC1\n    A2 --&gt; HC2\n    A3 --&gt; HC3</code></pre>"},{"location":"getting-started/overview/#component-breakdown","title":"Component Breakdown","text":"<p>Backend Service (Go)</p> <p>The brain of KrakenHashes, handling:</p> <ul> <li>RESTful API for all client operations</li> <li>WebSocket management for real-time agent communication</li> <li>Job scheduling with intelligent distribution algorithms</li> <li>File synchronization to ensure agents have required resources</li> <li>Database operations for persistent storage</li> </ul> <p>Agent System (Go)</p> <p>Distributed workers that:</p> <ul> <li>Execute Hashcat with optimized parameters</li> <li>Monitor hardware health (temperature, usage)</li> <li>Report progress in real-time</li> <li>Automatically sync required files from backend</li> <li>Support scheduling for resource availability</li> </ul> <p>Web Interface (React)</p> <p>Modern, responsive UI featuring:</p> <ul> <li>Real-time job monitoring dashboards</li> <li>Drag-and-drop file uploads</li> <li>Interactive result analysis</li> <li>Administrative controls</li> <li>Mobile-friendly responsive design</li> </ul> <p>PostgreSQL Database</p> <p>Reliable data storage for:</p> <ul> <li>User accounts and permissions</li> <li>Job definitions and results</li> <li>Hash lists and cracked passwords</li> <li>Agent registrations and metrics</li> <li>Audit logs and system configuration</li> </ul>"},{"location":"getting-started/overview/#typical-use-cases","title":"Typical Use Cases","text":""},{"location":"getting-started/overview/#penetration-testing","title":"\ud83d\udd0d Penetration Testing","text":"<p>During security assessments, teams often need to:</p> <ul> <li>Test password strength across multiple client environments</li> <li>Maintain separation between different client data</li> <li>Generate compliance reports showing password vulnerabilities</li> <li>Coordinate efforts across team members</li> </ul> <p>KrakenHashes Solution: Create separate clients for each engagement, assign team members appropriately, and use preset workflows to ensure consistent testing methodology.</p>"},{"location":"getting-started/overview/#incident-response","title":"\ud83d\udea8 Incident Response","text":"<p>When responding to security incidents:</p> <ul> <li>Time is critical \u2013 need results fast</li> <li>May have various hash formats from different systems</li> <li>Need to document the recovery process</li> <li>Multiple analysts working simultaneously</li> </ul> <p>KrakenHashes Solution: Use high-priority jobs to get critical results first, leverage distributed agents for speed, and maintain audit trails for evidence handling.</p>"},{"location":"getting-started/overview/#security-research","title":"\ud83d\udd2c Security Research","text":"<p>Researchers analyzing password security need:</p> <ul> <li>Consistent benchmarking across hash types</li> <li>Ability to test new attack methodologies</li> <li>Performance metrics and statistics</li> <li>Reproducible results</li> </ul> <p>KrakenHashes Solution: Built-in benchmarking system, custom workflow creation, detailed metrics collection, and comprehensive result export options.</p>"},{"location":"getting-started/overview/#corporate-security-audits","title":"\ud83c\udfe2 Corporate Security Audits","text":"<p>Internal security teams conducting password audits require:</p> <ul> <li>Regular testing of AD password dumps</li> <li>Tracking improvement over time</li> <li>Executive-friendly reporting</li> <li>Automated testing schedules</li> </ul> <p>KrakenHashes Solution: Scheduled jobs, historical tracking, client-based organization for different departments, and retention policies for compliance.</p>"},{"location":"getting-started/overview/#workflows-attack-strategy-made-simple","title":"Workflows: Attack Strategy Made Simple","text":"<p>One of KrakenHashes' most powerful features is the workflow system. Instead of manually running different Hashcat commands, you can define comprehensive attack strategies:</p>"},{"location":"getting-started/overview/#example-standard-password-audit-workflow","title":"Example: Standard Password Audit Workflow","text":"<pre><code>graph LR\n    A[Quick Wins] --&gt;|Found: 40%| B[Common Passwords]\n    B --&gt;|Found: 15%| C[Rule-Based Attacks]\n    C --&gt;|Found: 20%| D[Hybrid Attacks]\n    D --&gt;|Found: 10%| E[Brute Force]\n\n    style A fill:#90EE90\n    style B fill:#87CEEB\n    style C fill:#FFB6C1\n    style D fill:#FFE4B5\n    style E fill:#F0E68C</code></pre> <ol> <li>Quick Wins - Try the most common passwords first</li> <li>Common Passwords - Test against known breach compilations</li> <li>Rule-Based - Apply transformations to wordlists</li> <li>Hybrid Attacks - Combine wordlists with masks</li> <li>Brute Force - Last resort for remaining hashes</li> </ol>"},{"location":"getting-started/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"getting-started/overview/#data-protection","title":"\ud83d\udee1\ufe0f Data Protection","text":"<ul> <li>All agent communication is encrypted</li> <li>Passwords are never logged in plaintext</li> <li>Database connections use SSL/TLS</li> <li>File transfers are authenticated and encrypted</li> </ul>"},{"location":"getting-started/overview/#access-control","title":"\ud83d\udd10 Access Control","text":"<ul> <li>JWT-based authentication with refresh tokens</li> <li>Multi-factor authentication options</li> <li>Role-based permissions (Admin/User)</li> <li>Secure session management with token validation</li> </ul>"},{"location":"getting-started/overview/#compliance","title":"\ud83d\udcdd Compliance","text":"<ul> <li>Comprehensive audit logging</li> <li>Data retention policies</li> <li>Client data isolation</li> <li>Export capabilities for reporting</li> </ul>"},{"location":"getting-started/overview/#operational-security","title":"\u26a0\ufe0f Operational Security","text":"<p>Important Security Notes</p> <ul> <li>Deploy behind a firewall - never expose directly to internet</li> <li>Use strong passwords for all accounts</li> <li>Enable MFA for all users, especially administrators  </li> <li>Regularly review audit logs</li> <li>Keep agents and Hashcat binaries updated</li> <li>Use TLS certificates (self-signed minimum)</li> </ul>"},{"location":"getting-started/overview/#getting-started","title":"Getting Started","text":"<p>Ready to deploy KrakenHashes? Here's your path forward:</p> <ol> <li>Quick Start Guide - Get running in 5 minutes with Docker</li> <li>Installation Guide - Detailed setup for production</li> <li>First Crack Tutorial - Step-by-step first password recovery</li> <li>User Guide - Learn all features and capabilities</li> </ol>"},{"location":"getting-started/overview/#comparison-krakenhashes-vs-direct-hashcat","title":"Comparison: KrakenHashes vs Direct Hashcat","text":"Feature Hashcat KrakenHashes Setup Simple binary Docker deployment Interface Command line Web UI + API Multi-machine Manual coordination Automatic distribution Job Management Scripts/manual Database-backed queue Progress Tracking Terminal output Real-time dashboard Result Storage Text files Structured database Team Collaboration File sharing Built-in sharing Resource Management Manual copying Automatic sync User Management OS-level Application-level Audit Trail None Comprehensive Scheduling Cron jobs Built-in scheduler MFA Support N/A TOTP, Email, Backup codes"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand what KrakenHashes offers, choose your next step:</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in minutes with our Docker setup</p> </li> <li> <p> User Guide</p> <p>Deep dive into features and capabilities</p> </li> <li> <p> Security Setup</p> <p>Configure TLS and authentication for production</p> </li> <li> <p> Agent Deployment</p> <p>Learn how to deploy and manage agents</p> </li> </ul> <p>Version Note</p> <p>This documentation reflects KrakenHashes v0.1.0-alpha. As the project is under active development, features may change. Always refer to the GitHub repository for the latest updates.</p>"},{"location":"getting-started/quick-start/","title":"KrakenHashes Quick Start Guide","text":"<p>Get KrakenHashes up and running in under 5 minutes!</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Engine 19.03.0+ and Docker Compose v2.0+ installed</li> <li>4GB RAM minimum</li> <li>Linux-based system recommended</li> </ul>"},{"location":"getting-started/quick-start/#verify-docker-compose-version","title":"Verify Docker Compose Version","text":"<pre><code># Check your version - should be 2.0 or higher\ndocker compose version\n\n# If you have the old docker-compose v1, you'll need to upgrade\n</code></pre> <p>\u26a0\ufe0f Important: This project requires Docker Compose v2 (plugin). The legacy <code>docker-compose</code> command will not work.</p>"},{"location":"getting-started/quick-start/#1-download-configuration-template","title":"1. Download configuration template","text":"<pre><code># Create a directory for KrakenHashes\nmkdir krakenhashes &amp;&amp; cd krakenhashes\n\n# Download the environment template\nwget https://raw.githubusercontent.com/ZerkerEOD/krakenhashes/master/.env.example\ncp .env.example .env\n\n# Edit the .env file and change these at minimum:\n# - DB_PASSWORD (from default)\n# - JWT_SECRET (from default)\n# - PUID/PGID (to match your user: run 'id -u' and 'id -g')\n</code></pre>"},{"location":"getting-started/quick-start/#2-create-docker-composeyml","title":"2. Create docker-compose.yml","text":"<p>Create this <code>docker-compose.yml</code> file:</p> <pre><code>services:\n    postgres:\n        image: postgres:15-alpine\n        container_name: krakenhashes-postgres\n        volumes:\n            - postgres_data:/var/lib/postgresql/data\n        environment:\n            - POSTGRES_USER=${DB_USER}\n            - POSTGRES_PASSWORD=${DB_PASSWORD}\n            - POSTGRES_DB=${DB_NAME}\n        restart: unless-stopped\n        healthcheck:\n            test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER}\"]\n            interval: 5s\n            retries: 5\n\n    krakenhashes:\n        image: zerkereod/krakenhashes:latest\n        container_name: krakenhashes-app\n        depends_on:\n            postgres:\n                condition: service_healthy\n        env_file:\n            - .env\n        ports:\n            - \"${FRONTEND_PORT:-443}:443\"\n            - \"${KH_HTTPS_PORT:-31337}:31337\"\n        volumes:\n            - krakenhashes_data:/var/lib/krakenhashes\n            - ./logs:/var/log/krakenhashes\n        environment:\n            - DB_HOST=postgres # Override to use container name\n            - PUID=${PUID}\n            - PGID=${PGID}\n        restart: unless-stopped\n\nvolumes:\n    postgres_data:\n    krakenhashes_data:\n</code></pre>"},{"location":"getting-started/quick-start/#3-start-krakenhashes","title":"3. Start KrakenHashes","text":"<pre><code># Start the application\ndocker compose up -d\n\n# Wait for initialization (about 30 seconds)\ndocker compose logs -f krakenhashes\n</code></pre>"},{"location":"getting-started/quick-start/#4-access-the-application","title":"4. Access the Application","text":"<p>Open your browser and navigate to:</p> <ul> <li>https://localhost (redirects to port 443)</li> </ul> <p>Note: You'll see a certificate warning because we're using self-signed certificates. This is normal for local development.</p>"},{"location":"getting-started/quick-start/#5-first-login","title":"5. First Login","text":"<ol> <li>Log in with the default admin credentials:</li> <li>Username: <code>admin</code></li> <li> <p>Password: <code>KrakenHashes1!</code></p> </li> <li> <p>Important: Change the admin password immediately after first login for security</p> </li> </ol>"},{"location":"getting-started/quick-start/#6-quick-test","title":"6. Quick Test","text":"<ol> <li> <p>Upload hashcat binary (required first):</p> <ul> <li>Navigate to Admin \u2192 Binaries</li> <li>Click \"Upload Binary\"</li> <li>Upload your hashcat binary archive (download from https://hashcat.net/hashcat/)</li> <li>The archive contains binaries for all platforms (Linux, Windows, macOS)</li> </ul> <p>System Initialization</p> <p>After uploading your first binary, the system automatically: - Verifies the binary integrity - Creates the potfile preset job (if not already exists) - Makes the binary available to agents</p> </li> <li> <p>Upload a wordlist:</p> <ul> <li>Navigate to Admin \u2192 Wordlists</li> <li>Click \"Upload Wordlist\"</li> <li>Upload a small wordlist file (e.g., rockyou.txt or a test file)</li> </ul> </li> <li> <p>Create a test hashlist:</p> <ul> <li>Navigate to Hashlists</li> <li>Click \"Create Hashlist\"</li> <li>Add a few test hashes (e.g., MD5 hashes like <code>5f4dcc3b5aa765d61d8327deb882cf99</code> for \"password\")</li> </ul> </li> <li> <p>Create a job (when agents are connected):</p> <ul> <li>Navigate to your hashlist from the Hashlists page</li> <li>Click on your hashlist to view its details</li> <li>Click \"Create Job\" from the hashlist management page</li> <li>Choose a preset job template or configure custom settings</li> <li>Start the job</li> </ul> </li> </ol> <p>Note: Jobs require at least one connected agent to execute. Without agents, jobs will remain in pending status.</p>"},{"location":"getting-started/quick-start/#common-tasks","title":"Common Tasks","text":""},{"location":"getting-started/quick-start/#view-logs","title":"View Logs","text":"<p>Log files are stored in <code>$HOME/krakenhashes/logs/</code> by default. You can view them in several ways:</p> <pre><code># Live logs from Docker\ndocker compose logs -f\n\n# Backend logs only\ndocker compose logs -f krakenhashes\n\n# Check stored log files\ntail -f $HOME/krakenhashes/logs/backend/*.log\n\n# Check for errors across all logs\ngrep -i error $HOME/krakenhashes/logs/*/*.log\n</code></pre> <p>For debugging, you can enable verbose logging by setting <code>LOG_LEVEL=DEBUG</code> in your <code>.env</code> file.</p>"},{"location":"getting-started/quick-start/#stop-the-application","title":"Stop the Application","text":"<pre><code>docker compose down\n</code></pre>"},{"location":"getting-started/quick-start/#update-to-latest-version","title":"Update to Latest Version","text":"<pre><code># Pull latest image\ndocker pull zerkereod/krakenhashes:latest\n\n# Restart with new image\ndocker compose up -d\n</code></pre>"},{"location":"getting-started/quick-start/#backup-database","title":"Backup Database","text":"<pre><code>docker exec krakenhashes-postgres pg_dump -U krakenhashes krakenhashes &gt; backup.sql\n</code></pre>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#cannot-access-the-web-interface","title":"Cannot access the web interface","text":"<ol> <li>Check if containers are running: <code>docker compose ps</code></li> <li>Check logs for errors: <code>docker compose logs</code></li> <li>Ensure ports 443 and 31337 are not in use: <code>netstat -tlnp | grep -E \"443|31337\"</code></li> </ol>"},{"location":"getting-started/quick-start/#database-connection-errors","title":"Database connection errors","text":"<ol> <li>Ensure PostgreSQL is healthy: <code>docker compose ps</code></li> <li>Check database logs: <code>docker compose logs postgres</code></li> <li>Verify environment variables match in both services</li> </ol>"},{"location":"getting-started/quick-start/#certificate-warnings","title":"Certificate warnings","text":"<p>This is normal with self-signed certificates. For production, see the Installation Guide for proper TLS setup.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>For Users: Read Understanding Jobs and Workflows</li> <li>For Admins: Review the full Installation Guide for production setup</li> <li>For Developers: See Development Setup</li> </ul>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Check the full documentation</li> <li>Report issues on GitHub</li> <li>Join our community chat (Discord): KrakenHashes</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Quick reference guides and technical specifications for KrakenHashes.</p>"},{"location":"reference/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Environment Variables</p> <p>Complete list of all configuration options</p> </li> <li> <p> Database Schema</p> <p>Table structures and relationships</p> </li> <li> <p> Error Codes</p> <p>Error codes and their meanings</p> </li> <li> <p> Glossary</p> <p>Technical terms and concepts explained</p> </li> </ul>"},{"location":"reference/#quick-links","title":"Quick Links","text":""},{"location":"reference/#configuration-reference","title":"Configuration Reference","text":"<ul> <li>Backend Environment Variables</li> <li>Frontend Environment Variables</li> <li>Agent Environment Variables</li> <li>Docker Environment Variables</li> </ul>"},{"location":"reference/#database-reference","title":"Database Reference","text":"<ul> <li>Core Tables</li> <li>Relationships</li> <li>Indexes</li> <li>Migration History</li> </ul>"},{"location":"reference/#error-reference","title":"Error Reference","text":"<ul> <li>HTTP Status Codes</li> <li>Application Error Codes</li> <li>Agent Error Codes</li> <li>WebSocket Error Codes</li> </ul>"},{"location":"reference/#common-terms","title":"Common Terms","text":"<ul> <li>Password Cracking Terms</li> <li>System Architecture Terms</li> <li>Security Terms</li> <li>Performance Terms</li> </ul>"},{"location":"reference/#usage-tips","title":"Usage Tips","text":"<p>Finding Information</p> <ul> <li>Use <code>Ctrl+F</code> to search within pages</li> <li>Check the glossary for unfamiliar terms</li> <li>Environment variables are grouped by component</li> <li>Error codes include resolution steps</li> </ul>"},{"location":"reference/#contributing","title":"Contributing","text":"<p>Found an error or missing information? Please report it on GitHub.</p>"},{"location":"reference/database/","title":"KrakenHashes Database Schema Reference","text":"<p>This document provides a comprehensive reference for the KrakenHashes database schema, extracted from migration files (v0.1.0-alpha).</p>"},{"location":"reference/database/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Tables</li> <li>users</li> <li>teams</li> <li>user_teams</li> <li>Authentication &amp; Security</li> <li>auth_tokens</li> <li>mfa_methods</li> <li>mfa_backup_codes</li> <li>login_attempts</li> <li>security_events</li> <li>Agent Management</li> <li>agents</li> <li>agent_metrics</li> <li>agent_teams</li> <li>claim_vouchers</li> <li>claim_voucher_usage</li> <li>Email System</li> <li>email_config</li> <li>email_templates</li> <li>email_usage</li> <li>Hash Management</li> <li>hashlists</li> <li>hashes</li> <li>hashcat_hash_types</li> <li>Job Management</li> <li>job_workflows</li> <li>job_executions</li> <li>job_tasks</li> <li>job_execution_settings</li> <li>Resource Management</li> <li>wordlists</li> <li>rules</li> <li>binary_versions</li> <li>Client &amp; Settings</li> <li>clients</li> <li>client_settings</li> <li>system_settings</li> <li>Performance &amp; Scheduling</li> <li>performance_metrics</li> <li>agent_scheduling</li> <li>Migration History</li> </ol>"},{"location":"reference/database/#core-tables","title":"Core Tables","text":""},{"location":"reference/database/#users","title":"users","text":"<p>User accounts for the system, including the special system user with UUID 00000000-0000-0000-0000-000000000000.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique user identifier username VARCHAR(255) UNIQUE NOT NULL Username for login first_name VARCHAR(255) User's first name last_name VARCHAR(255) User's last name email VARCHAR(255) UNIQUE NOT NULL User's email address password_hash VARCHAR(255) NOT NULL Bcrypt password hash role VARCHAR(50) NOT NULL, CHECK 'user' Role: user, admin, agent, system status VARCHAR(50) NOT NULL 'active' Account status created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Account creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_users_username (username) - idx_users_email (email) - idx_users_role (role)</p> <p>Triggers: - update_users_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#teams","title":"teams","text":"<p>Organizational teams for grouping users.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Unique team identifier name VARCHAR(100) NOT NULL, UNIQUE Team name description TEXT Team description created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Team creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_teams_name (name)</p> <p>Triggers: - update_teams_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#user_teams","title":"user_teams","text":"<p>Junction table for user-team relationships.</p> Column Type Constraints Default Description user_id UUID NOT NULL, FK \u2192 users(id) User reference team_id UUID NOT NULL, FK \u2192 teams(id) Team reference role VARCHAR(50) NOT NULL, CHECK 'member' Role in team: member, admin joined_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Join timestamp <p>Primary Key: (user_id, team_id)</p> <p>Indexes: - idx_user_teams_user_id (user_id) - idx_user_teams_team_id (team_id)</p>"},{"location":"reference/database/#authentication-security","title":"Authentication &amp; Security","text":""},{"location":"reference/database/#auth_tokens","title":"auth_tokens","text":"<p>Stores refresh tokens for JWT authentication.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Token identifier user_id UUID NOT NULL, FK \u2192 users(id) User reference token VARCHAR(255) NOT NULL, UNIQUE Refresh token value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Token creation time <p>Indexes: - idx_auth_tokens_token (token) - idx_auth_tokens_user_id (user_id)</p>"},{"location":"reference/database/#agent-management","title":"Agent Management","text":""},{"location":"reference/database/#agents","title":"agents","text":"<p>Registered compute agents for distributed processing.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Agent identifier name VARCHAR(255) NOT NULL Agent name status VARCHAR(50) NOT NULL 'inactive' Agent status last_heartbeat TIMESTAMP WITH TIME ZONE Last heartbeat received version VARCHAR(50) NOT NULL Agent version hardware JSONB NOT NULL Hardware configuration os_info JSONB NOT NULL '{}' Operating system info created_by_id UUID NOT NULL, FK \u2192 users(id) Creator user created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time api_key VARCHAR(64) UNIQUE Agent API key api_key_created_at TIMESTAMP WITH TIME ZONE API key creation time api_key_last_used TIMESTAMP WITH TIME ZONE API key last usage last_error TEXT Last error message metadata JSONB '{}' Additional metadata owner_id UUID FK \u2192 users(id) Agent owner (added in migration 30) extra_parameters TEXT Extra hashcat parameters (added in migration 30) is_enabled BOOLEAN NOT NULL true Agent enabled status (added in migration 31) <p>Indexes: - idx_agents_status (status) - idx_agents_created_by (created_by_id) - idx_agents_last_heartbeat (last_heartbeat) - idx_agents_api_key (api_key) - idx_agents_owner_id (owner_id)</p> <p>Triggers: - update_agents_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#agent_metrics","title":"agent_metrics","text":"<p>Time-series metrics data for agents.</p> Column Type Constraints Default Description agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference cpu_usage FLOAT NOT NULL CPU usage percentage gpu_utilization FLOAT NOT NULL GPU utilization percentage gpu_temp FLOAT NOT NULL GPU temperature memory_usage FLOAT NOT NULL Memory usage percentage gpu_metrics JSONB NOT NULL '{}' Additional GPU metrics timestamp TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Metric timestamp <p>Primary Key: (agent_id, timestamp)</p> <p>Indexes: - idx_agent_metrics_timestamp (timestamp)</p>"},{"location":"reference/database/#agent_teams","title":"agent_teams","text":"<p>Junction table for agent-team associations.</p> Column Type Constraints Default Description agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference team_id UUID NOT NULL, FK \u2192 teams(id) Team reference <p>Primary Key: (agent_id, team_id)</p>"},{"location":"reference/database/#claim_vouchers","title":"claim_vouchers","text":"<p>Stores active agent registration vouchers.</p> Column Type Constraints Default Description code VARCHAR(50) PRIMARY KEY Voucher code created_by_id UUID NOT NULL, FK \u2192 users(id) Creator user created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time is_continuous BOOLEAN NOT NULL false Can be used multiple times is_active BOOLEAN NOT NULL true Voucher active status used_at TIMESTAMP WITH TIME ZONE Usage timestamp used_by_agent_id INTEGER FK \u2192 agents(id) Agent that used voucher <p>Indexes: - idx_claim_vouchers_code (code) - idx_claim_vouchers_active (is_active) - idx_claim_vouchers_created_by (created_by_id)</p> <p>Triggers: - update_claim_vouchers_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#claim_voucher_usage","title":"claim_voucher_usage","text":"<p>Tracks usage attempts of claim vouchers.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Usage record ID voucher_code VARCHAR(50) NOT NULL, FK \u2192 claim_vouchers(code) Voucher reference attempted_by_id UUID NOT NULL, FK \u2192 users(id) User who attempted attempted_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Attempt timestamp success BOOLEAN NOT NULL false Success status ip_address VARCHAR(45) Client IP address user_agent TEXT Client user agent error_message TEXT Error message if failed <p>Indexes: - idx_claim_voucher_usage_voucher (voucher_code) - idx_claim_voucher_usage_attempted_by (attempted_by_id)</p>"},{"location":"reference/database/#email-system","title":"Email System","text":""},{"location":"reference/database/#email_config","title":"email_config","text":"<p>Email provider configuration.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Config ID provider_type email_provider_type NOT NULL Provider: mailgun, sendgrid, mailchimp, gmail api_key TEXT NOT NULL Provider API key additional_config JSONB Additional configuration monthly_limit INTEGER Monthly email limit reset_date TIMESTAMP WITH TIME ZONE Limit reset date is_active BOOLEAN NOT NULL false Active status created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last update time <p>Triggers: - update_email_config_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#email_templates","title":"email_templates","text":"<p>Email template definitions.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Template ID template_type email_template_type NOT NULL Type: security_event, job_completion, admin_error, mfa_code name VARCHAR(255) NOT NULL Template name subject VARCHAR(255) NOT NULL Email subject html_content TEXT NOT NULL HTML template text_content TEXT NOT NULL Plain text template created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last update time last_modified_by UUID FK \u2192 users(id) Last modifier <p>Indexes: - idx_email_templates_type (template_type)</p> <p>Triggers: - update_email_templates_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#email_usage","title":"email_usage","text":"<p>Tracks email usage for rate limiting.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Usage record ID month_year DATE NOT NULL, UNIQUE Month/year for tracking count INTEGER NOT NULL 0 Email count last_reset TIMESTAMP WITH TIME ZONE NOT NULL NOW() Last reset time <p>Indexes: - idx_email_usage_month_year (month_year)</p>"},{"location":"reference/database/#hash-management","title":"Hash Management","text":""},{"location":"reference/database/#clients","title":"clients","text":"<p>Stores information about clients for whom hashlists are processed.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Client identifier name VARCHAR(255) NOT NULL, UNIQUE Client name description TEXT Client description contact_info TEXT Contact information created_at TIMESTAMPTZ NOT NULL NOW() Creation time updated_at TIMESTAMPTZ NOT NULL NOW() Last update time data_retention_months INT NULL Data retention policy (NULL = system default, 0 = keep forever) <p>Data Retention Notes: - <code>data_retention_months</code> overrides system default retention policy - NULL means use system default (<code>client_settings.default_data_retention_months</code>) - 0 means keep data forever (no automatic deletion) - Positive integers specify months to retain data after creation - When retention period expires, hashlists and associated data are securely deleted</p> <p>Indexes: - idx_clients_name (name)</p> <p>Triggers: - update_clients_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#hash_types","title":"hash_types","text":"<p>Stores information about supported hash types, keyed by hashcat mode ID.</p> Column Type Constraints Default Description id INT PRIMARY KEY Hashcat mode number name VARCHAR(255) NOT NULL Hash type name description TEXT Hash type description example TEXT Example hash needs_processing BOOLEAN NOT NULL FALSE Requires preprocessing processing_logic JSONB Processing rules as JSON is_enabled BOOLEAN NOT NULL TRUE Hash type enabled slow BOOLEAN NOT NULL FALSE Slow hash algorithm"},{"location":"reference/database/#hashlists","title":"hashlists","text":"<p>Stores metadata about uploaded hash lists.</p> Column Type Constraints Default Description id BIGSERIAL PRIMARY KEY Hashlist identifier name VARCHAR(255) NOT NULL Hashlist name user_id UUID NOT NULL, FK \u2192 users(id) Owner user client_id UUID FK \u2192 clients(id) Associated client hash_type_id INT NOT NULL, FK \u2192 hash_types(id) Hash type file_path VARCHAR(1024) File storage path total_hashes INT NOT NULL 0 Total hash count cracked_hashes INT NOT NULL 0 Cracked hash count created_at TIMESTAMPTZ NOT NULL NOW() Creation time updated_at TIMESTAMPTZ NOT NULL NOW() Last update time status TEXT NOT NULL, CHECK Status: uploading, processing, ready, error error_message TEXT Error details <p>Retention &amp; Deletion Behavior: - Deletion is CASCADE - removing a hashlist deletes:   - All associations in <code>hashlist_hashes</code>   - Related <code>agent_hashlists</code> entries   - Related <code>job_executions</code> and their <code>job_tasks</code> - File at <code>file_path</code> is securely overwritten with random data before deletion - Orphaned hashes (not linked to any other hashlist) are automatically deleted - VACUUM ANALYZE runs after deletion to prevent WAL recovery</p> <p>Indexes: - idx_hashlists_user_id (user_id) - idx_hashlists_client_id (client_id) - idx_hashlists_hash_type_id (hash_type_id) - idx_hashlists_status (status)</p> <p>Triggers: - update_hashlists_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#hashes","title":"hashes","text":"<p>Stores individual hash entries.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Hash identifier hash_value TEXT NOT NULL Hash value original_hash TEXT Original hash if processed username TEXT Associated username hash_type_id INT NOT NULL, FK \u2192 hash_types(id) Hash type is_cracked BOOLEAN NOT NULL FALSE Crack status password TEXT Cracked password last_updated TIMESTAMPTZ NOT NULL NOW() Last update time <p>Indexes: - idx_hashes_hash_value (hash_value)</p> <p>Triggers: - update_hashes_last_updated: Updates last_updated on row modification</p>"},{"location":"reference/database/#hashlist_hashes","title":"hashlist_hashes","text":"<p>Junction table for the many-to-many relationship between hashlists and hashes.</p> Column Type Constraints Default Description hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference hash_id UUID NOT NULL, FK \u2192 hashes(id) Hash reference <p>Primary Key: (hashlist_id, hash_id)</p> <p>Indexes: - idx_hashlist_hashes_hashlist_id (hashlist_id) - idx_hashlist_hashes_hash_id (hash_id)</p>"},{"location":"reference/database/#hashcat_hash_types","title":"hashcat_hash_types","text":"<p>Stores hashcat-specific hash type information (added in migration 16).</p> Column Type Constraints Default Description mode INT PRIMARY KEY Hashcat mode number name VARCHAR(255) NOT NULL Hash type name category VARCHAR(100) Hash category slow_hash BOOLEAN FALSE Is slow hash password_length_min INT Minimum password length password_length_max INT Maximum password length supports_brain BOOLEAN FALSE Supports brain feature example_hash_format TEXT Example hash format benchmark_mask VARCHAR(255) Benchmark mask benchmark_charset1 VARCHAR(255) Benchmark charset 1 autodetect_regex TEXT Regex for autodetection potfile_regex TEXT Regex for potfile parsing test_hash TEXT Test hash value test_password VARCHAR(255) Test password valid_hash_regex TEXT Valid hash format regex"},{"location":"reference/database/#job-management","title":"Job Management","text":""},{"location":"reference/database/#preset_jobs","title":"preset_jobs","text":"<p>Stores predefined job configurations.</p> Column Type Constraints Default Description id UUID PRIMARY KEY uuid_generate_v4() Job identifier name TEXT UNIQUE NOT NULL Job name wordlist_ids JSONB NOT NULL '[]' Array of wordlist IDs rule_ids JSONB NOT NULL '[]' Array of rule IDs attack_mode INTEGER NOT NULL, CHECK 0 Attack mode: 0,1,3,6,7,9 priority INTEGER NOT NULL Job priority chunk_size_seconds INTEGER NOT NULL Chunk duration status_updates_enabled BOOLEAN NOT NULL true Enable status updates is_small_job BOOLEAN NOT NULL false Small job flag allow_high_priority_override BOOLEAN NOT NULL false Allows this job to interrupt lower priority running jobs when no agents available binary_version_id INTEGER NOT NULL, FK \u2192 binary_versions(id) Binary version mask TEXT NULL Mask pattern created_at TIMESTAMPTZ NOW() Creation time updated_at TIMESTAMPTZ NOW() Last update time keyspace_limit BIGINT Keyspace limit (added in migration 32) max_agents INTEGER Max agents allowed (added in migration 32) <p>Triggers: - update_preset_jobs_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_workflows","title":"job_workflows","text":"<p>Stores workflow definitions for multi-step attacks.</p> Column Type Constraints Default Description id UUID PRIMARY KEY uuid_generate_v4() Workflow identifier name TEXT UNIQUE NOT NULL Workflow name created_at TIMESTAMPTZ NOW() Creation time updated_at TIMESTAMPTZ NOW() Last update time <p>Triggers: - update_job_workflows_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_workflow_steps","title":"job_workflow_steps","text":"<p>Defines steps within a workflow.</p> Column Type Constraints Default Description id BIGSERIAL PRIMARY KEY Step identifier job_workflow_id UUID NOT NULL, FK \u2192 job_workflows(id) Workflow reference preset_job_id UUID NOT NULL, FK \u2192 preset_jobs(id) Preset job reference step_order INTEGER NOT NULL Execution order <p>Unique Constraint: (job_workflow_id, step_order)</p> <p>Indexes: - idx_job_workflow_steps_job_workflow_id (job_workflow_id) - idx_job_workflow_steps_preset_job_id (preset_job_id)</p>"},{"location":"reference/database/#job_executions","title":"job_executions","text":"<p>Tracks actual job runs.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Execution identifier preset_job_id UUID NOT NULL, FK \u2192 preset_jobs(id) Preset job reference hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference status VARCHAR(50) NOT NULL, CHECK 'pending' Status: pending, running, completed, failed, cancelled, interrupted (Note: interrupted jobs return to pending) priority INT NOT NULL 0 Execution priority total_keyspace BIGINT Total keyspace size processed_keyspace BIGINT 0 Processed keyspace attack_mode INT NOT NULL Attack mode created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time started_at TIMESTAMP WITH TIME ZONE Start time completed_at TIMESTAMP WITH TIME ZONE Completion time error_message TEXT Error details interrupted_by UUID FK \u2192 job_executions(id) ID of the higher priority job that interrupted this one created_by UUID FK \u2192 users(id) Creator user (added in migration 33) chunk_size INTEGER Chunk size override (added in migration 34) chunk_overlap INTEGER 0 Chunk overlap (added in migration 34) dispatched_keyspace BIGINT 0 Dispatched keyspace (added in migration 40) progress NUMERIC(6,3) 0 Progress percentage (added in migration 36, updated in migration 38) consecutive_failures INTEGER 0 Consecutive failure count (added in migration 37) last_failure_at TIMESTAMP WITH TIME ZONE Last failure time (added in migration 37) is_accurate_keyspace BOOLEAN false True when keyspace is from hashcat progress[1] values (added in migration 63) avg_rule_multiplier FLOAT Actual/estimated keyspace ratio for improving future estimates (added in migration 63) <p>Indexes: - idx_job_executions_status (status) - idx_job_executions_priority (priority, created_at) - idx_job_executions_created_by (created_by) - idx_job_executions_consecutive_failures (consecutive_failures)</p>"},{"location":"reference/database/#job_tasks","title":"job_tasks","text":"<p>Individual chunks assigned to agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Task identifier job_execution_id UUID NOT NULL, FK \u2192 job_executions(id) Job execution reference agent_id INTEGER FK \u2192 agents(id) Assigned agent (nullable in migration 35) status VARCHAR(50) NOT NULL, CHECK 'pending' Status: pending, assigned, running, completed, failed, cancelled keyspace_start BIGINT NOT NULL Keyspace start keyspace_end BIGINT NOT NULL Keyspace end keyspace_processed BIGINT 0 Processed amount benchmark_speed BIGINT Hashes per second chunk_duration INT NOT NULL Duration in seconds assigned_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Assignment time started_at TIMESTAMP WITH TIME ZONE Start time completed_at TIMESTAMP WITH TIME ZONE Completion time last_checkpoint TIMESTAMP WITH TIME ZONE Last checkpoint error_message TEXT Error details created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time (added in migration 25) updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time (added in migration 26) progress NUMERIC(6,3) 0 Progress percentage (added in migration 36, updated in migration 38) consecutive_failures INTEGER 0 Consecutive failure count (added in migration 37) last_failure_at TIMESTAMP WITH TIME ZONE Last failure time (added in migration 37) chunk_number INTEGER Chunk number for rule splits (added in migration 44) effective_keyspace BIGINT Effective keyspace size (added in migration 47) is_actual_keyspace BOOLEAN false True when task has actual keyspace from hashcat progress[1] (added in migration 63) chunk_actual_keyspace BIGINT Immutable chunk size from hashcat progress[1] for accurate keyspace tracking (added in migration 64) <p>Indexes: - idx_job_tasks_agent_status (agent_id, status) - idx_job_tasks_execution (job_execution_id) - idx_job_tasks_consecutive_failures (consecutive_failures) - idx_job_tasks_chunk_number (job_execution_id, chunk_number)</p> <p>Triggers: - update_job_tasks_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#job_execution_settings","title":"job_execution_settings","text":"<p>Settings for job executions (added in migration 21).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Settings ID name VARCHAR(255) NOT NULL, UNIQUE Setting name value TEXT NOT NULL Setting value description TEXT Setting description data_type VARCHAR(50) NOT NULL 'string' Data type created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Indexes: - idx_job_execution_settings_name (name)</p> <p>Triggers: - update_job_execution_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#resource-management","title":"Resource Management","text":""},{"location":"reference/database/#binary_versions","title":"binary_versions","text":"<p>Stores information about different versions of hash cracking binaries.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Version ID binary_type binary_type NOT NULL Type: hashcat, john compression_type compression_type NOT NULL Compression: 7z, zip, tar.gz, tar.xz source_url TEXT NOT NULL Download URL file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user is_active BOOLEAN true Active status last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed <p>Indexes: - idx_binary_versions_type_active (binary_type) WHERE is_active = true - idx_binary_versions_verification (verification_status)</p>"},{"location":"reference/database/#binary_version_audit_log","title":"binary_version_audit_log","text":"<p>Tracks all changes and actions performed on binary versions.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID binary_version_id INTEGER NOT NULL, FK \u2192 binary_versions(id) Binary version reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_binary_version_audit_binary_id (binary_version_id) - idx_binary_version_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#wordlists","title":"wordlists","text":"<p>Stores information about wordlists used for password cracking.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Wordlist ID name VARCHAR(255) NOT NULL Wordlist name description TEXT Description wordlist_type wordlist_type NOT NULL Type: general, specialized, targeted, custom format wordlist_format NOT NULL 'plaintext' Format: plaintext, compressed file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes word_count BIGINT Number of words created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed <p>Indexes: - idx_wordlists_name (name) - idx_wordlists_type (wordlist_type) - idx_wordlists_verification (verification_status) - idx_wordlists_md5 (md5_hash)</p>"},{"location":"reference/database/#wordlist_audit_log","title":"wordlist_audit_log","text":"<p>Tracks all changes and actions performed on wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_wordlist_audit_wordlist_id (wordlist_id) - idx_wordlist_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#wordlist_tags","title":"wordlist_tags","text":"<p>Stores tags associated with wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Tag ID wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference tag VARCHAR(50) NOT NULL Tag value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user <p>Unique Index: idx_wordlist_tags_unique (wordlist_id, tag)</p> <p>Indexes: - idx_wordlist_tags_tag (tag)</p>"},{"location":"reference/database/#rules","title":"rules","text":"<p>Stores information about rules used for password cracking.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Rule ID name VARCHAR(255) NOT NULL Rule name description TEXT Description rule_type rule_type NOT NULL Type: hashcat, john file_name VARCHAR(255) NOT NULL File name md5_hash VARCHAR(32) NOT NULL MD5 hash file_size BIGINT NOT NULL File size in bytes rule_count INTEGER Number of rules created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater last_verified_at TIMESTAMP WITH TIME ZONE Last verification time verification_status VARCHAR(50) 'pending' Status: pending, verified, failed estimated_keyspace_multiplier FLOAT Keyspace multiplier estimate <p>Indexes: - idx_rules_name (name) - idx_rules_type (rule_type) - idx_rules_verification (verification_status) - idx_rules_md5 (md5_hash)</p>"},{"location":"reference/database/#rule_audit_log","title":"rule_audit_log","text":"<p>Tracks all changes and actions performed on rules.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Audit log ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference action VARCHAR(50) NOT NULL Action performed performed_by UUID NOT NULL, FK \u2192 users(id) User who performed action performed_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Action timestamp details JSONB Additional details <p>Indexes: - idx_rule_audit_rule_id (rule_id) - idx_rule_audit_performed_at (performed_at)</p>"},{"location":"reference/database/#rule_tags","title":"rule_tags","text":"<p>Stores tags associated with rules.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Tag ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference tag VARCHAR(50) NOT NULL Tag value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user <p>Unique Index: idx_rule_tags_unique (rule_id, tag)</p> <p>Indexes: - idx_rule_tags_tag (tag)</p>"},{"location":"reference/database/#rule_wordlist_compatibility","title":"rule_wordlist_compatibility","text":"<p>Stores compatibility information between rules and wordlists.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Compatibility ID rule_id INTEGER NOT NULL, FK \u2192 rules(id) Rule reference wordlist_id INTEGER NOT NULL, FK \u2192 wordlists(id) Wordlist reference compatibility_score FLOAT NOT NULL 1.0 Score from 0.0 to 1.0 notes TEXT Compatibility notes created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time created_by UUID NOT NULL, FK \u2192 users(id) Creator user updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time updated_by UUID FK \u2192 users(id) Last updater <p>Unique Index: idx_rule_wordlist_unique (rule_id, wordlist_id)</p> <p>Indexes: - idx_rule_wordlist_rule (rule_id) - idx_rule_wordlist_wordlist (wordlist_id)</p>"},{"location":"reference/database/#client-settings","title":"Client &amp; Settings","text":""},{"location":"reference/database/#client_settings","title":"client_settings","text":"<p>Stores client-specific settings (added in migration 17). Also used for system-wide settings without a client_id.</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Settings ID client_id UUID NOT NULL, FK \u2192 clients(id) Client reference key VARCHAR(255) NOT NULL Setting key value TEXT Setting value data_type VARCHAR(50) NOT NULL 'string' Data type created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Important System-Wide Settings: - <code>default_data_retention_months</code> - Default retention period for all hashlists (when client_id is NULL) - <code>last_purge_run</code> - Timestamp of last retention purge execution</p> <p>Unique Constraint: (client_id, key)</p> <p>Indexes: - idx_client_settings_client (client_id)</p> <p>Triggers: - update_client_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#system_settings","title":"system_settings","text":"<p>Stores global system-wide settings.</p> Column Type Constraints Default Description key VARCHAR(255) PRIMARY KEY Setting key value TEXT Setting value description TEXT Setting description data_type VARCHAR(50) NOT NULL 'string' Data type: string, integer, boolean, float updated_at TIMESTAMPTZ NOT NULL NOW() Last update time <p>Default Settings: - max_job_priority: 1000 (integer) - agent_scheduling_enabled: false (boolean) - added in migration 42 - hashcat_speedtest_timeout: 300 (integer) - added in migration 39 - task_heartbeat_timeout: 300 (integer) - added in migration 46</p> <p>Triggers: - update_system_settings_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#performance-scheduling","title":"Performance &amp; Scheduling","text":""},{"location":"reference/database/#agent_benchmarks","title":"agent_benchmarks","text":"<p>Stores benchmark results for agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Benchmark ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference attack_mode INT NOT NULL Attack mode hash_type INT NOT NULL Hash type speed BIGINT NOT NULL Hashes per second created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, attack_mode, hash_type)</p> <p>Indexes: - idx_agent_benchmarks_lookup (agent_id, attack_mode, hash_type)</p>"},{"location":"reference/database/#agent_performance_metrics","title":"agent_performance_metrics","text":"<p>Historical performance tracking for agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference metric_type VARCHAR(50) NOT NULL, CHECK Type: hash_rate, utilization, temperature, power_usage value NUMERIC NOT NULL Metric value timestamp TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Metric timestamp aggregation_level VARCHAR(20) NOT NULL, CHECK 'realtime' Level: realtime, daily, weekly period_start TIMESTAMP WITH TIME ZONE Aggregation period start period_end TIMESTAMP WITH TIME ZONE Aggregation period end <p>Indexes: - idx_agent_metrics_lookup (agent_id, metric_type, timestamp) - idx_agent_metrics_aggregation (aggregation_level, timestamp)</p>"},{"location":"reference/database/#performance_metrics","title":"performance_metrics","text":"<p>Detailed performance metrics (added in migration 41).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID job_task_id UUID FK \u2192 job_tasks(id) Job task reference agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference device_id INTEGER Device ID device_name VARCHAR(255) Device name timestamp TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Metric timestamp hash_rate BIGINT Current hash rate utilization FLOAT GPU utilization % temperature FLOAT Temperature in Celsius power_usage FLOAT Power usage in watts memory_used BIGINT Memory used in bytes memory_total BIGINT Total memory in bytes fan_speed FLOAT Fan speed % core_clock INTEGER Core clock in MHz memory_clock INTEGER Memory clock in MHz pcie_rx BIGINT PCIe RX throughput pcie_tx BIGINT PCIe TX throughput <p>Indexes: - idx_performance_metrics_timestamp (timestamp) - idx_performance_metrics_agent (agent_id, timestamp) - idx_performance_metrics_job_task (job_task_id) - idx_performance_metrics_device (agent_id, device_id, timestamp)</p>"},{"location":"reference/database/#job_performance_metrics","title":"job_performance_metrics","text":"<p>Job-level performance tracking.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Metric ID job_execution_id UUID NOT NULL, FK \u2192 job_executions(id) Job execution reference metric_type VARCHAR(50) NOT NULL, CHECK Type: hash_rate, progress_percentage, cracks_found value NUMERIC NOT NULL Metric value timestamp TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Metric timestamp aggregation_level VARCHAR(20) NOT NULL, CHECK 'realtime' Level: realtime, daily, weekly period_start TIMESTAMP WITH TIME ZONE Aggregation period start period_end TIMESTAMP WITH TIME ZONE Aggregation period end <p>Indexes: - idx_job_metrics_lookup (job_execution_id, metric_type, timestamp)</p>"},{"location":"reference/database/#agent_hashlists","title":"agent_hashlists","text":"<p>Tracks hashlist distribution to agents.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Record ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference hashlist_id BIGINT NOT NULL, FK \u2192 hashlists(id) Hashlist reference file_path TEXT NOT NULL Local file path downloaded_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Download time last_used_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last usage time file_hash VARCHAR(32) MD5 hash for verification <p>Unique Constraint: (agent_id, hashlist_id)</p> <p>Indexes: - idx_agent_hashlists_cleanup (last_used_at)</p>"},{"location":"reference/database/#agent_devices","title":"agent_devices","text":"<p>Tracks individual compute devices (added in migration 29).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Device record ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference device_id INTEGER NOT NULL Device ID device_name VARCHAR(255) NOT NULL Device name device_type VARCHAR(50) NOT NULL Type: GPU or CPU enabled BOOLEAN NOT NULL TRUE Device enabled status created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, device_id)</p> <p>Indexes: - idx_agent_devices_agent_id (agent_id) - idx_agent_devices_enabled (agent_id, enabled)</p> <p>Triggers: - update_agent_devices_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#agent_schedules","title":"agent_schedules","text":"<p>Stores daily scheduling information for agents (added in migration 42).</p> Column Type Constraints Default Description id SERIAL PRIMARY KEY Schedule ID agent_id INTEGER NOT NULL, FK \u2192 agents(id) Agent reference day_of_week INTEGER NOT NULL, CHECK Day: 0=Sunday...6=Saturday start_time TIME NOT NULL Start time in UTC end_time TIME NOT NULL End time in UTC timezone VARCHAR(50) NOT NULL 'UTC' Original timezone is_active BOOLEAN NOT NULL true Schedule active status created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time updated_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Last update time <p>Unique Constraint: (agent_id, day_of_week)</p> <p>Check Constraint: end_time != start_time (allows overnight schedules)</p> <p>Indexes: - idx_agent_schedules_agent_id (agent_id) - idx_agent_schedules_day_active (day_of_week, is_active)</p> <p>Triggers: - update_agent_schedules_updated_at: Updates updated_at on row modification</p>"},{"location":"reference/database/#authentication-security-extended","title":"Authentication &amp; Security (Extended)","text":"<p>The users table has been extended with additional security columns added through migrations:</p>"},{"location":"reference/database/#additional-users-columns","title":"Additional users columns","text":"Column Type Constraints Default Description mfa_enabled BOOLEAN FALSE MFA enabled status mfa_type text[] CHECK ARRAY['email'] MFA types enabled mfa_secret TEXT MFA secret backup_codes TEXT[] Hashed backup codes last_password_change TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last password change failed_login_attempts INT 0 Failed login count last_failed_attempt TIMESTAMP WITH TIME ZONE Last failed attempt account_locked BOOLEAN FALSE Account lock status account_locked_until TIMESTAMP WITH TIME ZONE Lock expiration account_enabled BOOLEAN TRUE Account enabled status last_login TIMESTAMP WITH TIME ZONE Last successful login disabled_reason TEXT Reason for disabling disabled_at TIMESTAMP WITH TIME ZONE Disable timestamp disabled_by UUID FK \u2192 users(id) Who disabled account preferred_mfa_method VARCHAR(20) Preferred MFA method"},{"location":"reference/database/#tokens","title":"tokens","text":"<p>JWT token storage (added in migration 7).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Token ID user_id UUID NOT NULL, FK \u2192 users(id) User reference token TEXT NOT NULL, UNIQUE Token value created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Creation time last_used_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last usage time expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time revoked BOOLEAN FALSE Revocation status revoked_at TIMESTAMP WITH TIME ZONE Revocation time revoked_reason TEXT Revocation reason <p>Indexes: - idx_tokens_token (token) - idx_tokens_user_id (user_id) - idx_tokens_revoked (revoked)</p>"},{"location":"reference/database/#auth_settings","title":"auth_settings","text":"<p>Stores global authentication and security settings.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Settings ID min_password_length INT 15 Minimum password length require_uppercase BOOLEAN TRUE Require uppercase letters require_lowercase BOOLEAN TRUE Require lowercase letters require_numbers BOOLEAN TRUE Require numbers require_special_chars BOOLEAN TRUE Require special characters max_failed_attempts INT 5 Max failed login attempts lockout_duration_minutes INT 60 Account lockout duration require_mfa BOOLEAN FALSE Require MFA for all users jwt_expiry_minutes INT 60 JWT token expiry display_timezone VARCHAR(50) 'UTC' Display timezone notification_aggregation_minutes INT 60 Notification aggregation period allowed_mfa_methods JSONB '[\"email\", \"authenticator\"]' Allowed MFA methods email_code_validity_minutes INT 5 Email code validity backup_codes_count INT 8 Number of backup codes mfa_code_cooldown_minutes INT 1 MFA code cooldown mfa_code_expiry_minutes INT 5 MFA code expiry mfa_max_attempts INT 3 Max MFA attempts"},{"location":"reference/database/#login_attempts","title":"login_attempts","text":"<p>Tracks login attempts for security monitoring.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Attempt ID user_id UUID FK \u2192 users(id) User reference (nullable) username VARCHAR(255) Attempted username ip_address INET NOT NULL Client IP address user_agent TEXT Client user agent success BOOLEAN NOT NULL Success status failure_reason TEXT Failure reason attempted_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Attempt time notified BOOLEAN FALSE Notification sent <p>Indexes: - idx_login_attempts_user_id (user_id) - idx_login_attempts_attempted_at (attempted_at) - idx_login_attempts_notified (notified)</p>"},{"location":"reference/database/#active_sessions","title":"active_sessions","text":"<p>Tracks active user sessions.</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Session ID user_id UUID FK \u2192 users(id) User reference ip_address INET NOT NULL Session IP address user_agent TEXT Client user agent created_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Session start last_active_at TIMESTAMP WITH TIME ZONE CURRENT_TIMESTAMP Last activity <p>Indexes: - idx_active_sessions_user_id (user_id) - idx_active_sessions_last_active (last_active_at)</p>"},{"location":"reference/database/#pending_mfa_setup","title":"pending_mfa_setup","text":"<p>Tracks pending MFA setup processes (added in migration 8).</p> Column Type Constraints Default Description user_id UUID PRIMARY KEY, FK \u2192 users(id) User reference method VARCHAR(20) NOT NULL, CHECK Method: email, authenticator secret TEXT MFA secret created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_pending_mfa_created_at (created_at)</p>"},{"location":"reference/database/#email_mfa_codes","title":"email_mfa_codes","text":"<p>Stores temporary MFA codes sent via email (added in migration 8).</p> Column Type Constraints Default Description user_id UUID PRIMARY KEY, FK \u2192 users(id) User reference code VARCHAR(6) NOT NULL MFA code attempts INT NOT NULL 0 Attempt count expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_email_mfa_expires_at (expires_at)</p>"},{"location":"reference/database/#mfa_methods","title":"mfa_methods","text":"<p>Stores user MFA method configurations (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Method ID user_id UUID NOT NULL, FK \u2192 users(id) User reference method VARCHAR(20) NOT NULL, CHECK Method: email, authenticator is_primary BOOLEAN FALSE Primary method flag created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time last_used_at TIMESTAMP WITH TIME ZONE Last usage time metadata JSONB Method-specific data <p>Unique Constraint: (user_id, method)</p> <p>Indexes: - idx_mfa_methods_user (user_id) - idx_mfa_methods_primary (user_id, is_primary)</p>"},{"location":"reference/database/#mfa_backup_codes","title":"mfa_backup_codes","text":"<p>Stores MFA backup codes (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Code ID user_id UUID NOT NULL, FK \u2192 users(id) User reference code_hash VARCHAR(255) NOT NULL Hashed backup code used_at TIMESTAMP WITH TIME ZONE Usage timestamp created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Creation time <p>Indexes: - idx_mfa_backup_codes_user (user_id) - idx_mfa_backup_codes_unused (user_id, used_at) WHERE used_at IS NULL</p>"},{"location":"reference/database/#mfa_sessions","title":"mfa_sessions","text":"<p>Tracks MFA verification sessions during login (added in migration 11).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Session ID user_id UUID NOT NULL, FK \u2192 users(id) User reference session_token TEXT NOT NULL Session token expires_at TIMESTAMP WITH TIME ZONE NOT NULL Expiration time attempts INT NOT NULL 0 Failed attempts created_at TIMESTAMP WITH TIME ZONE NOT NULL NOW() Creation time <p>Indexes: - idx_mfa_sessions_user_id (user_id) - idx_mfa_sessions_session_token (session_token) - idx_mfa_sessions_expires_at (expires_at)</p> <p>Triggers: - enforce_mfa_max_attempts_trigger: Enforces max attempts limit - cleanup_expired_mfa_sessions_trigger: Cleans up expired sessions</p>"},{"location":"reference/database/#security_events","title":"security_events","text":"<p>Logs security-related events (added in migration 8).</p> Column Type Constraints Default Description id UUID PRIMARY KEY gen_random_uuid() Event ID user_id UUID FK \u2192 users(id) User reference event_type VARCHAR(50) NOT NULL Event type ip_address INET Client IP address user_agent TEXT Client user agent details JSONB Event details created_at TIMESTAMP WITH TIME ZONE NOT NULL CURRENT_TIMESTAMP Event time <p>Indexes: - idx_security_events_user (user_id) - idx_security_events_type (event_type) - idx_security_events_created (created_at)</p>"},{"location":"reference/database/#potfile-initialization-sequence","title":"Potfile Initialization Sequence","text":"<p>The potfile system initializes in stages during server startup:</p>"},{"location":"reference/database/#1-on-server-startup","title":"1. On Server Startup","text":"<ul> <li>Creates <code>/data/krakenhashes/wordlists/custom/potfile.txt</code> if missing</li> <li>Creates potfile wordlist entry in database with <code>is_potfile = true</code></li> <li>Attempts to create \"Potfile Run\" preset job</li> </ul>"},{"location":"reference/database/#2-binary-dependency","title":"2. Binary Dependency","text":"<ul> <li>Preset jobs require a <code>binary_version_id</code> (NOT NULL constraint in database)</li> <li>If no binaries exist, preset job creation is deferred</li> <li>A background monitor runs every 5 seconds checking for binary availability</li> <li>Monitor stops once preset job is successfully created</li> </ul>"},{"location":"reference/database/#3-completion","title":"3. Completion","text":"<ul> <li>Once a binary is uploaded and verified, the preset job is created</li> <li>System settings are updated with both <code>potfile_wordlist_id</code> and <code>potfile_preset_job_id</code></li> <li>The potfile system is fully operational</li> </ul>"},{"location":"reference/database/#related-tables","title":"Related Tables","text":"<ul> <li>wordlists: Contains potfile entry with <code>is_potfile = true</code></li> <li>preset_jobs: Contains \"Potfile Run\" job (once binary available)</li> <li>potfile_staging: Temporary storage for passwords before batch processing</li> <li>system_settings: Stores <code>potfile_wordlist_id</code> and <code>potfile_preset_job_id</code></li> </ul>"},{"location":"reference/database/#migration-history","title":"Migration History","text":"<p>The database schema has evolved through 63 migrations:</p> <ol> <li>000001: Initial schema - users, teams, user_teams</li> <li>000002: Add auth_tokens table</li> <li>000003: Create agents system</li> <li>000004: Create voucher system</li> <li>000005: Add email system</li> <li>000006: Add email templates (enhancement)</li> <li>000007: Auth security infrastructure</li> <li>000008: Add MFA tables</li> <li>000009: Update auth settings</li> <li>000010: Add preferred MFA method</li> <li>000011: Add MFA session</li> <li>000012: Add binary versions</li> <li>000013: Add wordlists</li> <li>000014: Add rules</li> <li>000015: Add hashlist tables</li> <li>000016: Add hashcat hash types</li> <li>000017: Add client settings</li> <li>000018: Add job workflows</li> <li>000019: Add system settings</li> <li>000020: Add job execution (fixed)</li> <li>000021: Add job execution settings</li> <li>000022: Enhance job tasks and system settings</li> <li>000023: Add max_agents column</li> <li>000024: Add interrupted status</li> <li>000025: Add job_tasks created_at</li> <li>000026: Add job_tasks updated_at</li> <li>000027: Fix hashes trigger</li> <li>000028: Fix cracked counts</li> <li>000029: Add agent devices</li> <li>000030: Add agent owner and extra parameters</li> <li>000031: Add agent is_enabled</li> <li>000032: Add preset job keyspace and max_agents</li> <li>000033: Add job created_by</li> <li>000034: Add enhanced chunking support</li> <li>000035: Make agent_id nullable in job_tasks</li> <li>000036: Add progress tracking</li> <li>000037: Add consecutive failures tracking</li> <li>000038: Update progress precision</li> <li>000039: Add speedtest timeout setting</li> <li>000040: Add dispatched_keyspace to job_executions</li> <li>000041: Add device tracking to performance_metrics</li> <li>000042: Add agent scheduling</li> <li>000043: Set owner_id for existing agents</li> <li>000044: Add chunk_number to job_tasks</li> <li>000045: Fix total_keyspace for rule split jobs</li> <li>000046: Add task heartbeat timeout setting</li> <li>000047: Add effective_keyspace to job_tasks</li> <li>000048: Add potfile support</li> <li>000049: Make job executions self-contained</li> <li>000050: Add reconnect_pending status</li> <li>000051: Add monitoring settings</li> <li>000052: Remove is_small_job column</li> <li>000053: Add binary default system</li> <li>000054: Add auth token last activity tracking</li> <li>000055: Add job notification tracking</li> <li>000056: Add reconnect grace period setting</li> <li>000057: Add agent download settings</li> <li>000058: Add agent sync status</li> <li>000059: Add average speed to tasks</li> <li>000060: Add missing hash types</li> <li>000061: Add hashlist potfile exclusion</li> <li>000062: Add client potfile exclusion</li> <li>000063: Add accurate keyspace tracking</li> <li>000064: Add chunk_actual_keyspace tracking</li> </ol>"},{"location":"reference/database/#enums-and-custom-types","title":"Enums and Custom Types","text":""},{"location":"reference/database/#email_provider_type","title":"email_provider_type","text":"<ul> <li>mailgun</li> <li>sendgrid</li> <li>mailchimp</li> <li>gmail</li> </ul>"},{"location":"reference/database/#email_template_type","title":"email_template_type","text":"<ul> <li>security_event</li> <li>job_completion</li> <li>admin_error</li> <li>mfa_code</li> </ul>"},{"location":"reference/database/#binary_type","title":"binary_type","text":"<ul> <li>hashcat</li> <li>john</li> </ul>"},{"location":"reference/database/#compression_type","title":"compression_type","text":"<ul> <li>7z</li> <li>zip</li> <li>tar.gz</li> <li>tar.xz</li> </ul>"},{"location":"reference/database/#wordlist_type","title":"wordlist_type","text":"<ul> <li>general</li> <li>specialized</li> <li>targeted</li> <li>custom</li> </ul>"},{"location":"reference/database/#wordlist_format","title":"wordlist_format","text":"<ul> <li>plaintext</li> <li>compressed</li> </ul>"},{"location":"reference/database/#rule_type","title":"rule_type","text":"<ul> <li>hashcat</li> <li>john</li> </ul>"},{"location":"reference/database/#key-relationships","title":"Key Relationships","text":"<ol> <li>User System: users \u2194 teams (many-to-many via user_teams)</li> <li>Agent System: agents \u2192 users (created_by), agents \u2194 teams (many-to-many via agent_teams)</li> <li>Hash Management: hashlists \u2192 users, hashlists \u2192 clients, hashlists \u2194 hashes (many-to-many via hashlist_hashes)</li> <li>Job System: preset_jobs \u2192 binary_versions, job_executions \u2192 preset_jobs + hashlists, job_tasks \u2192 job_executions + agents</li> <li>Resource Management: wordlists/rules \u2192 users (created_by), rules \u2194 wordlists (compatibility)</li> <li>Authentication: Various MFA and security tables \u2192 users</li> </ol>"},{"location":"reference/database/#data-lifecycle-security","title":"Data Lifecycle &amp; Security","text":""},{"location":"reference/database/#data-retention-system","title":"Data Retention System","text":"<p>The database implements a comprehensive data retention system with automatic purging:</p> <ol> <li>Retention Policy Hierarchy</li> <li>System default: <code>client_settings.default_data_retention_months</code> (when client_id is NULL)</li> <li>Client-specific: <code>clients.data_retention_months</code> overrides system default</li> <li> <p>Special values: NULL = use system default, 0 = keep forever</p> </li> <li> <p>Automatic Purge Process</p> </li> <li>Runs daily at midnight and on backend startup</li> <li>Processes hashlists older than retention period based on <code>created_at</code></li> <li>Executes within database transactions for atomicity</li> <li> <p>Logs all deletions for audit compliance</p> </li> <li> <p>Secure Deletion Process</p> </li> <li>Database: Transactional deletion with CASCADE to dependent tables</li> <li>Filesystem: Files overwritten with random data before removal</li> <li>PostgreSQL: VACUUM ANALYZE on affected tables to prevent WAL recovery</li> <li> <p>Orphan Cleanup: Automatic removal of hashes not linked to any hashlist</p> </li> <li> <p>Affected Tables During Purge</p> </li> <li><code>hashlists</code> - Primary deletion target</li> <li><code>hashlist_hashes</code> - Junction table entries removed</li> <li><code>hashes</code> - Orphaned entries deleted</li> <li><code>agent_hashlists</code> - CASCADE deletion</li> <li><code>job_executions</code> - CASCADE deletion</li> <li><code>job_tasks</code> - CASCADE deletion via job_executions</li> </ol>"},{"location":"reference/database/#security-features","title":"Security Features","text":"<ol> <li>Deletion Security</li> <li>Files are securely overwritten with random data to prevent recovery</li> <li>VACUUM ANALYZE prevents recovery from PostgreSQL dead tuples</li> <li> <p>Audit trail maintained for compliance verification</p> </li> <li> <p>CASCADE Deletion Paths <pre><code>hashlists deletion triggers:\n\u251c\u2500\u2500 hashlist_hashes (explicit deletion)\n\u251c\u2500\u2500 hashes (orphan cleanup)\n\u251c\u2500\u2500 agent_hashlists (CASCADE)\n\u2514\u2500\u2500 job_executions (CASCADE)\n    \u2514\u2500\u2500 job_tasks (CASCADE)\n</code></pre></p> </li> <li> <p>Agent-Side Cleanup</p> </li> <li>Agents automatically clean files older than 3 days</li> <li>Prevents storage accumulation on compute nodes</li> <li>Preserves base resources (binaries, wordlists, rules)</li> </ol>"},{"location":"reference/database/#important-notes","title":"Important Notes","text":"<ol> <li>UUID Usage: Most primary keys use UUID except for legacy/performance-critical tables (agents, hashlists use SERIAL/BIGSERIAL)</li> <li>Soft Deletes: Not implemented - uses CASCADE deletes for referential integrity</li> <li>Audit Trails: Separate audit tables for binary_versions, wordlists, and rules</li> <li>Time Zones: All timestamps stored as TIMESTAMP WITH TIME ZONE</li> <li>JSON Storage: Heavy use of JSONB for flexible metadata storage</li> <li>System User: Special user with UUID 00000000-0000-0000-0000-000000000000 for system operations</li> <li>Data Retention: Automatic purging with secure deletion and WAL protection</li> </ol>"},{"location":"reference/environment/","title":"Environment Variables Reference","text":"<p>This document provides a comprehensive reference for all environment variables used in the KrakenHashes system.</p>"},{"location":"reference/environment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Backend Server</li> <li>Frontend Application</li> <li>Agent</li> <li>Docker &amp; Deployment</li> <li>Database</li> <li>Authentication &amp; Security</li> <li>TLS/SSL Configuration</li> <li>Logging &amp; Debugging</li> <li>WebSocket Configuration</li> </ul>"},{"location":"reference/environment/#backend-server","title":"Backend Server","text":""},{"location":"reference/environment/#core-configuration","title":"Core Configuration","text":"Variable Type Default Required Description <code>KH_HOST</code> string <code>localhost</code> (or <code>0.0.0.0</code> in Docker) No Host address for the server to bind to <code>KH_HTTPS_PORT</code> integer <code>31337</code> No Port for HTTPS API server <code>KH_HTTP_PORT</code> integer <code>1337</code> No Port for HTTP server (CA certificate distribution) <code>KH_IN_DOCKER</code> boolean <code>false</code> No Set to <code>TRUE</code> when running in Docker container"},{"location":"reference/environment/#data-storage","title":"Data &amp; Storage","text":"Variable Type Default Required Description <code>KH_CONFIG_DIR</code> string <code>~/.krakenhashes</code> No Base directory for configuration files <code>KH_DATA_DIR</code> string <code>~/.krakenhashes-data</code> No Base directory for mutable data (uploads, binaries, etc.) <code>KH_HASHLIST_BATCH_SIZE</code> integer <code>1000</code> No Maximum number of hashes to process in one database batch <code>KH_MAX_UPLOAD_SIZE_MB</code> integer <code>32</code> No Maximum file upload size in megabytes <code>KH_HASH_UPLOAD_DIR</code> string <code>{KH_DATA_DIR}/hashlist_uploads</code> No Directory for storing uploaded hashlists"},{"location":"reference/environment/#directory-structure","title":"Directory Structure","text":"<p>The backend automatically creates the following subdirectories under <code>KH_DATA_DIR</code>: - <code>binaries/</code> - Executable files (hashcat, john, etc.) - <code>wordlists/</code> - Wordlist files with subdirectories:   - <code>general/</code> - Common wordlists   - <code>specialized/</code> - Domain-specific wordlists   - <code>targeted/</code> - Client/project-specific wordlists   - <code>custom/</code> - User-created wordlists - <code>rules/</code> - Rule files with subdirectories:   - <code>hashcat/</code> - Hashcat-compatible rules   - <code>john/</code> - John the Ripper rules   - <code>custom/</code> - User-created rules - <code>hashlists/</code> - Hash files and crack results</p>"},{"location":"reference/environment/#frontend-application","title":"Frontend Application","text":""},{"location":"reference/environment/#api-configuration","title":"API Configuration","text":"Variable Type Default Required Description <code>REACT_APP_API_URL</code> string <code>https://localhost:31337</code> Yes HTTPS API endpoint URL <code>REACT_APP_HTTP_API_URL</code> string <code>http://localhost:1337</code> No HTTP API endpoint URL (for CA cert download) <code>REACT_APP_WS_URL</code> string <code>wss://localhost:31337</code> Yes WebSocket endpoint URL <code>REACT_APP_VERSION</code> string (from versions.json) No Frontend version (set during build)"},{"location":"reference/environment/#development-server","title":"Development Server","text":"Variable Type Default Required Description <code>HTTPS</code> boolean <code>true</code> No Enable HTTPS for development server <code>SSL_CRT_FILE</code> string - No Path to SSL certificate for dev server <code>SSL_KEY_FILE</code> string - No Path to SSL key for dev server <code>HOST</code> string <code>0.0.0.0</code> No Development server host <code>PORT</code> integer <code>3000</code> No Development server port <code>NODE_ENV</code> string <code>development</code> No Node environment <code>BROWSER</code> string <code>none</code> No Browser launch behavior"},{"location":"reference/environment/#debug-configuration","title":"Debug Configuration","text":"Variable Type Default Required Description <code>REACT_APP_DEBUG</code> boolean <code>false</code> No Enable debug mode in React app <code>REACT_APP_DEBUG_REDUX</code> boolean <code>false</code> No Enable Redux debugging"},{"location":"reference/environment/#agent","title":"Agent","text":""},{"location":"reference/environment/#core-configuration_1","title":"Core Configuration","text":"Variable Type Default Required Description <code>KH_DATA_DIR</code> string <code>{executable_dir}/data</code> No Base directory for agent data <code>KH_CONFIG_DIR</code> string <code>{executable_dir}/config</code> No Directory for agent configuration files <code>HASHCAT_EXTRA_PARAMS</code> string - No Extra parameters to pass to hashcat (e.g., <code>-O -w 3</code>) <p>The agent creates the same directory structure as the backend under its data directory.</p>"},{"location":"reference/environment/#docker-deployment","title":"Docker &amp; Deployment","text":""},{"location":"reference/environment/#container-configuration","title":"Container Configuration","text":"Variable Type Default Required Description <code>PUID</code> integer <code>1000</code> No User ID for file permissions <code>PGID</code> integer <code>1000</code> No Group ID for file permissions <code>TZ</code> string <code>UTC</code> No Container timezone"},{"location":"reference/environment/#volume-mounts","title":"Volume Mounts","text":"Variable Type Default Required Description <code>LOG_DIR</code> string <code>/var/log/krakenhashes</code> No Base directory for log files <code>KH_CONFIG_DIR_HOST</code> string <code>/etc/krakenhashes</code> No Host path for config directory <code>KH_DATA_DIR_HOST</code> string <code>/var/lib/krakenhashes</code> No Host path for data directory"},{"location":"reference/environment/#port-mappings","title":"Port Mappings","text":"Variable Type Default Required Description <code>FRONTEND_PORT</code> integer <code>443</code> No Host port for frontend (nginx)"},{"location":"reference/environment/#database","title":"Database","text":""},{"location":"reference/environment/#connection-settings","title":"Connection Settings","text":"Variable Type Default Required Description <code>DATABASE_URL</code> string - Yes* Full PostgreSQL connection string <code>DB_CONNECTION_STRING</code> string - Yes* Alternative to DATABASE_URL <code>DB_HOST</code> string <code>localhost</code> Yes** Database host <code>DB_PORT</code> integer <code>5432</code> Yes** Database port <code>DB_NAME</code> string <code>krakenhashes</code> Yes** Database name <code>DB_USER</code> string <code>krakenhashes</code> Yes** Database username <code>DB_PASSWORD</code> string <code>krakenhashes</code> Yes** Database password <p>* Either <code>DATABASE_URL</code> or individual DB_* variables must be set ** Required if <code>DATABASE_URL</code> is not provided</p>"},{"location":"reference/environment/#authentication-security","title":"Authentication &amp; Security","text":""},{"location":"reference/environment/#jwt-configuration","title":"JWT Configuration","text":"Variable Type Default Required Description <code>JWT_SECRET</code> string - Yes Secret key for JWT token signing <code>JWT_EXPIRATION</code> string <code>24h</code> No JWT token expiration time <code>DEFAULT_ADMIN_ID</code> string - No User ID of the default admin"},{"location":"reference/environment/#cors-configuration","title":"CORS Configuration","text":"Variable Type Default Required Description <code>CORS_ALLOWED_ORIGIN</code> string <code>https://localhost:443</code> No Allowed CORS origin <code>ALLOWED_ORIGINS</code> string <code>*</code> No Comma-separated list of allowed origins"},{"location":"reference/environment/#tlsssl-configuration","title":"TLS/SSL Configuration","text":""},{"location":"reference/environment/#certificate-mode","title":"Certificate Mode","text":"Variable Type Default Required Description <code>KH_TLS_MODE</code> string <code>self-signed</code> No TLS mode: <code>self-signed</code>, <code>provided</code>, or <code>certbot</code> <code>KH_CERTS_DIR</code> string <code>{KH_CONFIG_DIR}/certs</code> No Directory for storing certificates"},{"location":"reference/environment/#certificate-details","title":"Certificate Details","text":"Variable Type Default Required Description <code>KH_ADDITIONAL_DNS_NAMES</code> string - No Comma-separated additional DNS names for certificates <code>KH_ADDITIONAL_IP_ADDRESSES</code> string - No Comma-separated additional IP addresses for certificates <code>KH_KEY_SIZE</code> integer <code>4096</code> No RSA key size (2048 or 4096) <code>KH_SERVER_CERT_VALIDITY</code> integer <code>365</code> No Server certificate validity in days <code>KH_CA_CERT_VALIDITY</code> integer <code>3650</code> No CA certificate validity in days"},{"location":"reference/environment/#self-signed-ca-configuration","title":"Self-Signed CA Configuration","text":"Variable Type Default Required Description <code>KH_CA_COUNTRY</code> string <code>US</code> No CA certificate country code <code>KH_CA_ORGANIZATION</code> string <code>KrakenHashes</code> No CA organization name <code>KH_CA_ORGANIZATIONAL_UNIT</code> string <code>KrakenHashes CA</code> No CA organizational unit <code>KH_CA_COMMON_NAME</code> string <code>KrakenHashes Root CA</code> No CA common name"},{"location":"reference/environment/#user-provided-certificates","title":"User-Provided Certificates","text":"Variable Type Default Required Description <code>KH_CERT_FILE</code> string <code>{KH_CERTS_DIR}/server.crt</code> Yes* Path to certificate file <code>KH_KEY_FILE</code> string <code>{KH_CERTS_DIR}/server.key</code> Yes* Path to private key file <code>KH_CA_FILE</code> string <code>{KH_CERTS_DIR}/ca.crt</code> No Path to CA certificate file <p>* Required when <code>KH_TLS_MODE=provided</code></p>"},{"location":"reference/environment/#lets-encrypt-certbot-configuration","title":"Let's Encrypt (Certbot) Configuration","text":"Variable Type Default Required Description <code>KH_CERTBOT_DOMAIN</code> string - Yes* Domain name for Let's Encrypt <code>KH_CERTBOT_EMAIL</code> string - Yes* Email for Let's Encrypt notifications <code>KH_CERTBOT_STAGING</code> boolean <code>false</code> No Use Let's Encrypt staging server <code>KH_CERTBOT_AUTO_RENEW</code> boolean <code>true</code> No Enable automatic renewal <code>KH_CERTBOT_RENEW_HOOK</code> string - No Custom hook script after renewal <code>CLOUDFLARE_API_TOKEN</code> string - Yes** Cloudflare API token for DNS-01 challenge <p>* Required when <code>KH_TLS_MODE=certbot</code> ** Required for DNS-01 challenge with Cloudflare</p>"},{"location":"reference/environment/#logging-debugging","title":"Logging &amp; Debugging","text":""},{"location":"reference/environment/#debug-flags","title":"Debug Flags","text":"Variable Type Default Required Description <code>DEBUG</code> boolean <code>false</code> No Enable global debug output <code>LOG_LEVEL</code> string <code>INFO</code> No Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code> <code>DEBUG_SQL</code> boolean <code>false</code> No Enable SQL query logging <code>DEBUG_HTTP</code> boolean <code>false</code> No Enable HTTP request/response logging <code>DEBUG_WEBSOCKET</code> boolean <code>false</code> No Enable WebSocket message logging <code>DEBUG_AUTH</code> boolean <code>false</code> No Enable authentication debugging <code>DEBUG_JOBS</code> boolean <code>false</code> No Enable job processing debugging"},{"location":"reference/environment/#log-directories","title":"Log Directories","text":"Variable Type Default Required Description <code>BACKEND_LOG_DIR</code> string <code>${LOG_DIR}/backend</code> No Backend log directory <code>FRONTEND_LOG_DIR</code> string <code>${LOG_DIR}/frontend</code> No Frontend log directory <code>NGINX_LOG_DIR</code> string <code>${LOG_DIR}/nginx</code> No Nginx log directory <code>POSTGRES_LOG_DIR</code> string <code>${LOG_DIR}/postgres</code> No PostgreSQL log directory"},{"location":"reference/environment/#nginx-logging","title":"Nginx Logging","text":"Variable Type Default Required Description <code>NGINX_ACCESS_LOG_LEVEL</code> string <code>info</code> No Nginx access log level <code>NGINX_ERROR_LOG_LEVEL</code> string <code>warn</code> No Nginx error log level <code>NGINX_CLIENT_MAX_BODY_SIZE</code> string <code>50M</code> No Maximum client body size"},{"location":"reference/environment/#websocket-configuration","title":"WebSocket Configuration","text":"Variable Type Default Required Description <code>KH_WRITE_WAIT</code> duration <code>10s</code> No Time allowed to write messages <code>KH_PONG_WAIT</code> duration <code>60s</code> No Time to wait for pong response <code>KH_PING_PERIOD</code> duration <code>54s</code> No How often to send pings <p>Duration format: <code>10s</code>, <code>5m</code>, <code>1h</code>, etc.</p>"},{"location":"reference/environment/#environment-variable-priority","title":"Environment Variable Priority","text":"<ol> <li>Explicit environment variables take precedence</li> <li>Docker environment files (<code>.env</code>) are loaded next</li> <li>Default values are used as fallback</li> </ol>"},{"location":"reference/environment/#best-practices","title":"Best Practices","text":"<ol> <li>Security: Never commit sensitive values (passwords, JWT secrets) to version control</li> <li>Production: Always set strong values for <code>JWT_SECRET</code>, <code>DB_PASSWORD</code>, and certificate configurations</li> <li>Development: Use <code>.env</code> files for local development configuration</li> <li>Docker: Mount configuration directories to persist data between container restarts</li> <li>Paths: Use absolute paths for file and directory configurations</li> <li>Validation: The backend validates critical environment variables on startup</li> </ol>"},{"location":"reference/environment/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/environment/#minimal-development-setup","title":"Minimal Development Setup","text":"<pre><code># .env\nDB_CONNECTION_STRING=postgres://krakenhashes:krakenhashes@localhost:5432/krakenhashes?sslmode=disable\nJWT_SECRET=dev-secret-change-in-production\nDEBUG=true\n</code></pre>"},{"location":"reference/environment/#production-docker-setup","title":"Production Docker Setup","text":"<pre><code># .env.production\nPUID=1000\nPGID=1000\nDB_HOST=postgres\nDB_PASSWORD=strong-random-password\nJWT_SECRET=very-long-random-secret\nKH_TLS_MODE=certbot\nKH_CERTBOT_DOMAIN=kraken.example.com\nKH_CERTBOT_EMAIL=admin@example.com\nCLOUDFLARE_API_TOKEN=your-cloudflare-api-token\nDEBUG=false\nLOG_LEVEL=WARNING\n</code></pre>"},{"location":"reference/environment/#agent-configuration","title":"Agent Configuration","text":"<pre><code># Agent environment\nKH_DATA_DIR=/opt/krakenhashes-agent/data\nKH_CONFIG_DIR=/opt/krakenhashes-agent/config\nHASHCAT_EXTRA_PARAMS=-O -w 3\n</code></pre>"},{"location":"reference/errors/","title":"KrakenHashes Error Codes Reference","text":"<p>This document provides a comprehensive reference for all error codes, HTTP status codes, and error conditions used throughout the KrakenHashes system.</p>"},{"location":"reference/errors/#table-of-contents","title":"Table of Contents","text":"<ul> <li>HTTP Status Codes</li> <li>Application Error Types</li> <li>Agent Error Conditions</li> <li>WebSocket Error Messages</li> <li>Common Error Scenarios</li> </ul>"},{"location":"reference/errors/#http-status-codes","title":"HTTP Status Codes","text":"<p>The KrakenHashes API uses standard HTTP status codes to indicate the success or failure of requests.</p>"},{"location":"reference/errors/#success-codes-2xx","title":"Success Codes (2xx)","text":"Code Name Usage 200 OK Standard successful response for GET, PUT, DELETE requests 201 Created Resource successfully created (POST requests) 204 No Content Successful request with no response body (DELETE requests)"},{"location":"reference/errors/#client-error-codes-4xx","title":"Client Error Codes (4xx)","text":"Code Name Common Usage 400 Bad Request Invalid request format, missing required fields, validation errors 401 Unauthorized Missing or invalid authentication credentials 403 Forbidden Valid credentials but insufficient permissions 404 Not Found Requested resource does not exist 409 Conflict Conflict with current state (e.g., duplicate records)"},{"location":"reference/errors/#server-error-codes-5xx","title":"Server Error Codes (5xx)","text":"Code Name Usage 500 Internal Server Error Unexpected server error, database errors, system failures 502 Bad Gateway WebSocket upgrade failures, proxy errors 503 Service Unavailable Server temporarily unavailable (maintenance, overload)"},{"location":"reference/errors/#application-error-types","title":"Application Error Types","text":""},{"location":"reference/errors/#model-layer-errors-backendinternalmodelserrorsgo","title":"Model Layer Errors (<code>backend/internal/models/errors.go</code>)","text":"<pre><code>var (\n    ErrNotFound     = errors.New(\"record not found\")\n    ErrInvalidInput = errors.New(\"invalid input\")\n)\n</code></pre>"},{"location":"reference/errors/#repository-layer-errors-backendinternalrepositoryerrorsgo","title":"Repository Layer Errors (<code>backend/internal/repository/errors.go</code>)","text":"<pre><code>var (\n    // Resource Errors\n    ErrNotFound         = errors.New(\"resource not found\")\n    ErrDuplicateRecord  = errors.New(\"duplicate record\")\n\n    // Validation Errors\n    ErrInvalidStatus    = errors.New(\"invalid status\")\n    ErrInvalidToken     = errors.New(\"invalid token\")\n    ErrInvalidHardware  = errors.New(\"invalid hardware information\")\n    ErrInvalidMetrics   = errors.New(\"invalid metrics\")\n\n    // Voucher Errors\n    ErrInvalidVoucher      = errors.New(\"invalid or expired voucher\")\n    ErrVoucherAlreadyUsed  = errors.New(\"voucher has already been used\")\n    ErrVoucherDeactivated  = errors.New(\"voucher has been deactivated\")\n    ErrVoucherExpired      = errors.New(\"voucher has expired\")\n\n    // Agent Errors\n    ErrDuplicateToken  = errors.New(\"agent token already exists\")\n    ErrAgentNotFound   = errors.New(\"agent not found\")\n)\n</code></pre>"},{"location":"reference/errors/#agent-error-conditions","title":"Agent Error Conditions","text":""},{"location":"reference/errors/#registration-errors","title":"Registration Errors","text":"Error HTTP Status Description Invalid claim code 400 The provided claim code is invalid or has been used Expired claim code 400 The claim code has expired Registration failed 500 Server error during agent registration"},{"location":"reference/errors/#authentication-errors","title":"Authentication Errors","text":"Error HTTP Status Description Missing API key 401 API key header not provided Invalid API key 401 API key does not match any registered agent Agent ID mismatch 401 API key does not match the provided agent ID TLS required 400 WebSocket connection requires TLS"},{"location":"reference/errors/#connection-errors","title":"Connection Errors","text":"Error Description WebSocket upgrade failed Failed to upgrade HTTP connection to WebSocket Connection timeout Agent failed to send heartbeat within timeout period Invalid message format WebSocket message does not match expected JSON format"},{"location":"reference/errors/#websocket-error-messages","title":"WebSocket Error Messages","text":""},{"location":"reference/errors/#message-types","title":"Message Types","text":"<p>The WebSocket protocol uses typed messages for communication between agents and the server.</p>"},{"location":"reference/errors/#server-to-agent-error-messages","title":"Server to Agent Error Messages","text":"<pre><code>{\n    \"type\": \"error_report\",\n    \"payload\": {\n        \"error\": \"error_message\",\n        \"code\": \"ERROR_CODE\",\n        \"details\": {}\n    }\n}\n</code></pre>"},{"location":"reference/errors/#agent-to-server-error-messages","title":"Agent to Server Error Messages","text":"<pre><code>{\n    \"type\": \"error_report\",\n    \"payload\": {\n        \"agent_id\": 123,\n        \"error\": \"error description\",\n        \"stack\": \"stack trace if available\",\n        \"context\": {},\n        \"reported_at\": \"2025-01-20T10:30:00Z\"\n    }\n}\n</code></pre>"},{"location":"reference/errors/#websocket-message-types","title":"WebSocket Message Types","text":""},{"location":"reference/errors/#agent-server-messages","title":"Agent \u2192 Server Messages","text":"Type Purpose <code>heartbeat</code> Regular heartbeat to maintain connection <code>task_status</code> Task execution status updates <code>job_progress</code> Job progress updates <code>benchmark_result</code> GPU benchmark results <code>agent_status</code> Agent status changes <code>error_report</code> Error reporting <code>hardware_info</code> Hardware capability updates <code>file_sync_response</code> File synchronization responses <code>file_sync_status</code> File sync progress updates <code>hashcat_output</code> Hashcat execution output <code>device_detection</code> GPU device detection results <code>device_update</code> GPU device status updates"},{"location":"reference/errors/#server-agent-messages","title":"Server \u2192 Agent Messages","text":"Type Purpose <code>task_assignment</code> New task assignment <code>job_stop</code> Stop job execution <code>benchmark_request</code> Request GPU benchmark <code>agent_command</code> Generic agent command <code>config_update</code> Configuration updates <code>file_sync_request</code> Request file inventory <code>file_sync_command</code> File download commands <code>force_cleanup</code> Force cleanup of resources"},{"location":"reference/errors/#common-error-scenarios","title":"Common Error Scenarios","text":""},{"location":"reference/errors/#authentication-flow-errors","title":"Authentication Flow Errors","text":"<ol> <li>Login Failures</li> <li>Invalid credentials \u2192 401 Unauthorized</li> <li>System user login attempt \u2192 401 Unauthorized</li> <li> <p>MFA required but not provided \u2192 Response with MFA session</p> </li> <li> <p>Token Errors</p> </li> <li>Expired access token \u2192 401 Unauthorized</li> <li>Invalid refresh token \u2192 401 Unauthorized</li> <li> <p>Expired refresh token \u2192 401 Unauthorized</p> </li> <li> <p>MFA Errors</p> </li> <li>Invalid TOTP code \u2192 400 Bad Request</li> <li>Invalid email code \u2192 400 Bad Request</li> <li>Expired MFA session \u2192 401 Unauthorized</li> <li>Invalid backup code \u2192 400 Bad Request</li> </ol>"},{"location":"reference/errors/#file-operation-errors","title":"File Operation Errors","text":"<ol> <li>Upload Errors</li> <li>File too large \u2192 400 Bad Request</li> <li>Invalid file type \u2192 400 Bad Request</li> <li> <p>Disk space exceeded \u2192 507 Insufficient Storage</p> </li> <li> <p>Download Errors</p> </li> <li>File not found \u2192 404 Not Found</li> <li>Access denied \u2192 403 Forbidden</li> </ol>"},{"location":"reference/errors/#job-execution-errors","title":"Job Execution Errors","text":"<ol> <li>Job Creation</li> <li>Invalid job parameters \u2192 400 Bad Request</li> <li>Missing required fields \u2192 400 Bad Request</li> <li> <p>Invalid hashlist ID \u2192 404 Not Found</p> </li> <li> <p>Job Execution</p> </li> <li>No available agents \u2192 503 Service Unavailable</li> <li>Agent disconnected \u2192 Job marked as failed</li> <li>Hashcat execution error \u2192 Job error status</li> </ol>"},{"location":"reference/errors/#database-errors","title":"Database Errors","text":"<ol> <li>Connection Errors</li> <li>Database unreachable \u2192 500 Internal Server Error</li> <li> <p>Connection pool exhausted \u2192 500 Internal Server Error</p> </li> <li> <p>Query Errors</p> </li> <li>Record not found \u2192 404 Not Found</li> <li>Duplicate key violation \u2192 409 Conflict</li> <li>Foreign key constraint \u2192 400 Bad Request</li> </ol>"},{"location":"reference/errors/#error-resolution-guide","title":"Error Resolution Guide","text":""},{"location":"reference/errors/#for-api-consumers","title":"For API Consumers","text":"<ol> <li>401 Unauthorized</li> <li>Check if access token is expired</li> <li>Refresh token if needed</li> <li> <p>Ensure proper authentication headers</p> </li> <li> <p>403 Forbidden</p> </li> <li>Verify user has required permissions</li> <li> <p>Check role-based access requirements</p> </li> <li> <p>404 Not Found</p> </li> <li>Verify resource ID is correct</li> <li> <p>Check if resource was deleted</p> </li> <li> <p>500 Internal Server Error</p> </li> <li>Retry with exponential backoff</li> <li>Check server logs for details</li> <li>Contact support if persistent</li> </ol>"},{"location":"reference/errors/#for-agent-developers","title":"For Agent Developers","text":"<ol> <li>WebSocket Connection Issues</li> <li>Ensure TLS is enabled</li> <li>Verify API key is valid</li> <li> <p>Check network connectivity</p> </li> <li> <p>File Sync Errors</p> </li> <li>Verify file permissions</li> <li>Check disk space</li> <li> <p>Ensure file hashes match</p> </li> <li> <p>Job Execution Errors</p> </li> <li>Check hashcat installation</li> <li>Verify GPU drivers</li> <li>Monitor system resources</li> </ol>"},{"location":"reference/errors/#error-logging","title":"Error Logging","text":"<p>All errors are logged with appropriate severity levels:</p> <ul> <li>DEBUG: Detailed debugging information</li> <li>INFO: General informational messages</li> <li>WARNING: Warning messages for potential issues</li> <li>ERROR: Error messages for failures</li> <li>CRITICAL: Critical system failures</li> </ul> <p>Error logs include: - Timestamp - Error message - Stack trace (when available) - Request context - User/Agent information</p>"},{"location":"reference/errors/#best-practices","title":"Best Practices","text":"<ol> <li>Client-Side Error Handling</li> <li>Always check HTTP status codes</li> <li>Parse error response bodies</li> <li>Implement retry logic for transient errors</li> <li> <p>Display user-friendly error messages</p> </li> <li> <p>Server-Side Error Handling</p> </li> <li>Use consistent error formats</li> <li>Include helpful error details</li> <li>Log errors with appropriate context</li> <li> <p>Monitor error rates and patterns</p> </li> <li> <p>Agent Error Handling</p> </li> <li>Report errors via WebSocket</li> <li>Implement local error recovery</li> <li>Maintain error history</li> <li>Include system state in error reports</li> </ol>"},{"location":"reference/glossary/","title":"KrakenHashes Glossary","text":"<p>This glossary provides definitions for terms used throughout the KrakenHashes system, organized by category.</p>"},{"location":"reference/glossary/#password-cracking-terminology","title":"Password Cracking Terminology","text":""},{"location":"reference/glossary/#a-z","title":"A-Z","text":"<p>Attack Mode: The method used by hashcat to attempt password recovery. Common modes include dictionary attack (-a 0), combinator attack (-a 1), brute-force/mask attack (-a 3), and hybrid attacks (-a 6, -a 7).</p> <p>Benchmark: A test run to measure the cracking speed (hashes per second) of specific hardware against various hash algorithms.</p> <p>Brute Force Attack: An attack method that systematically tries all possible character combinations within a defined character set and length range.</p> <p>Candidate: A potential password generated during the cracking process that will be tested against the target hash.</p> <p>Charset: A defined set of characters used in mask or brute-force attacks (e.g., ?l = lowercase, ?u = uppercase, ?d = digits, ?s = special characters).</p> <p>Combinator Attack: An attack that combines words from two wordlists to create password candidates (e.g., \"password\" + \"123\" = \"password123\").</p> <p>Cracked Hash: A hash that has been successfully reversed to reveal its plaintext password.</p> <p>Dictionary Attack: An attack using a wordlist of common passwords and variations to attempt hash cracking.</p> <p>Hash: A one-way cryptographic function output that represents a password. Common types include MD5, SHA1, SHA256, bcrypt, and NTLM.</p> <p>Hash Algorithm: The specific cryptographic function used to create a hash (e.g., MD5, SHA-1, SHA-256, bcrypt, scrypt, Argon2).</p> <p>Hash Rate: The speed at which password candidates are tested, measured in hashes per second (H/s), kilohashes/s (KH/s), megahashes/s (MH/s), or gigahashes/s (GH/s).</p> <p>Hashcat: The underlying password recovery tool used by KrakenHashes for distributed cracking operations.</p> <p>Hashlist: A collection of password hashes to be cracked, typically organized by source, client, or campaign.</p> <p>Hybrid Attack: An attack combining wordlist entries with masks or rules to generate password candidates.</p> <p>Keyspace: The total number of possible password combinations for a given attack configuration.</p> <p>Mask: A pattern defining the structure of passwords to generate in a mask attack (e.g., ?u?l?l?l?d?d?d?d for Abcd1234 format).</p> <p>Mask Attack: A targeted brute-force approach using patterns to generate password candidates based on known password structures.</p> <p>Password Candidate: A potential password being tested against a hash during the cracking process.</p> <p>Plaintext: The original, unencrypted password that produces a given hash.</p> <p>Potfile: A file storing previously cracked hashes and their plaintext passwords to avoid redundant work.</p> <p>Rainbow Table: Pre-computed tables of hash-to-plaintext mappings (not used by hashcat/KrakenHashes).</p> <p>Rule: A transformation applied to wordlist entries to generate password variants (e.g., appending numbers, capitalizing letters, character substitution).</p> <p>Rule Splitting: KrakenHashes feature that divides large rule files into chunks for distributed processing across multiple agents.</p> <p>Salt: Random data added to passwords before hashing to prevent identical passwords from producing identical hashes.</p> <p>Wordlist: A file containing potential passwords, one per line, used as input for dictionary attacks.</p>"},{"location":"reference/glossary/#system-architecture-terms","title":"System Architecture Terms","text":""},{"location":"reference/glossary/#a-z_1","title":"A-Z","text":"<p>Agent: A compute node running the KrakenHashes agent software that executes hashcat jobs and reports results to the backend.</p> <p>Agent Pool: A group of agents that can be assigned to work together on jobs.</p> <p>API Key: Authentication credential used by agents to communicate with the backend server.</p> <p>Backend: The central KrakenHashes server that manages jobs, stores data, and coordinates agent activities.</p> <p>Claim Code: A one-time voucher code used to register new agents with the system.</p> <p>Client: In KrakenHashes context, a customer or engagement for which password cracking services are performed.</p> <p>Chunk: A portion of work (keyspace segment or rule subset) assigned to an individual agent for processing.</p> <p>Chunking: The process of dividing large cracking jobs into smaller segments for distributed processing.</p> <p>Data Retention: Policies and mechanisms for automatically removing old data based on configured time periods.</p> <p>Heartbeat: Regular status updates sent by agents to the backend to indicate they are alive and processing.</p> <p>Job: A single password cracking task with specific parameters, wordlists, rules, and target hashes.</p> <p>Job Execution: An instance of a job being run, which may involve multiple agents and chunks.</p> <p>Job Template: A reusable job configuration that can be applied to different hashlists.</p> <p>Job Workflow: A sequence of jobs designed to implement a comprehensive attack strategy.</p> <p>Preset: Pre-configured job templates or workflows for common attack scenarios.</p> <p>Repository Pattern: Software design pattern used in KrakenHashes for database access abstraction.</p> <p>Service Layer: Business logic layer in the backend that processes requests between handlers and repositories.</p> <p>WebSocket: Protocol used for real-time bidirectional communication between agents and the backend.</p> <p>Work Directory: Temporary directory where agents store files and data during job execution.</p>"},{"location":"reference/glossary/#security-and-authentication-terms","title":"Security and Authentication Terms","text":""},{"location":"reference/glossary/#a-z_2","title":"A-Z","text":"<p>2FA/MFA: Two-Factor/Multi-Factor Authentication requiring multiple verification methods for user login.</p> <p>Access Token: Short-lived JWT token used for API authentication.</p> <p>API Authentication: Token-based authentication system for programmatic access to KrakenHashes.</p> <p>Backup Codes: One-time use codes for account recovery when primary MFA method is unavailable.</p> <p>Certificate Authority (CA): Entity that issues digital certificates for TLS/SSL encryption.</p> <p>CORS: Cross-Origin Resource Sharing - security feature controlling which domains can access the API.</p> <p>JWT: JSON Web Token - standard for securely transmitting information between parties as a JSON object.</p> <p>LDAP: Lightweight Directory Access Protocol - external authentication system support.</p> <p>Rate Limiting: Security measure limiting the number of API requests per time period.</p> <p>RBAC: Role-Based Access Control - authorization system based on user roles (admin, user, agent, system).</p> <p>Refresh Token: Long-lived token used to obtain new access tokens without re-authentication.</p> <p>Self-Signed Certificate: TLS certificate signed by its creator rather than a trusted CA.</p> <p>Session Management: System for tracking and controlling user login sessions.</p> <p>TLS/SSL: Transport Layer Security/Secure Sockets Layer - encryption protocols for secure communication.</p> <p>TOTP: Time-based One-Time Password - MFA method using authenticator apps.</p> <p>Voucher: Authorization code for specific actions like agent registration or user invitation.</p>"},{"location":"reference/glossary/#performance-and-optimization-terms","title":"Performance and Optimization Terms","text":""},{"location":"reference/glossary/#a-z_3","title":"A-Z","text":"<p>Benchmark Score: Measured performance of hardware against specific hash algorithms.</p> <p>Cache: Temporary storage of frequently accessed data to improve performance.</p> <p>Concurrency: Number of simultaneous operations or connections the system can handle.</p> <p>GPU: Graphics Processing Unit - primary hardware for high-speed password cracking.</p> <p>GPU Utilization: Percentage of GPU resources being used during cracking operations.</p> <p>Hash Rate: Speed of password testing, measured in hashes per second (H/s).</p> <p>Keyspace Distribution: Method of dividing the total keyspace among multiple agents for parallel processing.</p> <p>Load Balancing: Distribution of work across multiple agents based on their capabilities.</p> <p>Memory Usage: RAM consumption by hashcat and the agent during operations.</p> <p>Optimization: Techniques to improve cracking speed or resource efficiency.</p> <p>Parallel Processing: Simultaneous execution of job chunks across multiple agents.</p> <p>Performance Metrics: Measurements of system efficiency including hash rate, completion time, and resource usage.</p> <p>Resource Allocation: Assignment of CPU, GPU, and memory resources to cracking operations.</p> <p>Thermal Throttling: Automatic reduction in GPU performance to prevent overheating.</p> <p>Workload Distribution: Strategy for assigning job chunks to agents based on their capabilities.</p>"},{"location":"reference/glossary/#common-abbreviations","title":"Common Abbreviations","text":""},{"location":"reference/glossary/#a-z_4","title":"A-Z","text":"<p>API: Application Programming Interface</p> <p>CA: Certificate Authority</p> <p>CLI: Command Line Interface</p> <p>CPU: Central Processing Unit</p> <p>CRUD: Create, Read, Update, Delete (database operations)</p> <p>CSV: Comma-Separated Values</p> <p>DB: Database</p> <p>DNS: Domain Name System</p> <p>DTO: Data Transfer Object</p> <p>GPU: Graphics Processing Unit</p> <p>H/s: Hashes per second</p> <p>HTTP/HTTPS: Hypertext Transfer Protocol (Secure)</p> <p>ID: Identifier</p> <p>IP: Internet Protocol</p> <p>JSON: JavaScript Object Notation</p> <p>JWT: JSON Web Token</p> <p>KH/s: Kilohashes per second (1,000 H/s)</p> <p>LDAP: Lightweight Directory Access Protocol</p> <p>MFA: Multi-Factor Authentication</p> <p>MH/s: Megahashes per second (1,000,000 H/s)</p> <p>NTLM: NT LAN Manager (Windows password hash format)</p> <p>ORM: Object-Relational Mapping</p> <p>OS: Operating System</p> <p>RAM: Random Access Memory</p> <p>RBAC: Role-Based Access Control</p> <p>REST: Representational State Transfer</p> <p>SHA: Secure Hash Algorithm</p> <p>SMTP: Simple Mail Transfer Protocol</p> <p>SQL: Structured Query Language</p> <p>SSL: Secure Sockets Layer</p> <p>TLS: Transport Layer Security</p> <p>TOTP: Time-based One-Time Password</p> <p>UI/UX: User Interface/User Experience</p> <p>URI/URL: Uniform Resource Identifier/Locator</p> <p>UUID: Universally Unique Identifier</p> <p>VRAM: Video Random Access Memory (GPU memory)</p> <p>WS: WebSocket</p> <p>XML: Extensible Markup Language</p> <p>This glossary is continuously updated as new features and terminology are introduced to the KrakenHashes system.</p>"},{"location":"reference/hash-types/","title":"Hash Types Reference","text":"<p>This document provides comprehensive information about the hash types supported by KrakenHashes for password cracking operations.</p>"},{"location":"reference/hash-types/#overview","title":"Overview","text":"<p>Hash types in KrakenHashes correspond to hashcat modes and define the algorithm and format used to hash passwords. Each hash type has a unique numerical identifier (mode number) that matches hashcat's mode system. KrakenHashes currently supports 504 different hash types covering a wide range of algorithms and applications.</p>"},{"location":"reference/hash-types/#hash-type-categories","title":"Hash Type Categories","text":""},{"location":"reference/hash-types/#fast-hash-types","title":"Fast Hash Types","text":"<p>Fast hash algorithms are computationally inexpensive and can be cracked at high speeds. These include basic cryptographic hashes and simple salted variants.</p> <p>Examples: - MD5 (mode 0) - SHA1 (mode 100) - SHA2-256 (mode 1400) - SHA2-512 (mode 1700) - NTLM (mode 1000)</p>"},{"location":"reference/hash-types/#slow-hash-types","title":"Slow Hash Types","text":"<p>Slow hash algorithms are intentionally computationally expensive to resist brute-force attacks. These require significantly more time and resources to crack but provide better security. KrakenHashes supports 132 slow hash types.</p> <p>Examples: - bcrypt (mode 3200) - PBKDF2 variants (modes 10900, 12000, 12100) - scrypt (mode 8900) - Argon2 variants - TrueCrypt/VeraCrypt containers</p>"},{"location":"reference/hash-types/#application-specific-hashes","title":"Application-Specific Hashes","text":"<p>Many hash types are specific to particular applications, operating systems, or protocols.</p> <p>Categories include: - Database Systems: MySQL, PostgreSQL, Oracle, MSSQL - Operating Systems: Windows (NTLM, NetNTLM), Unix/Linux (crypt variants), macOS - Web Applications: WordPress, Joomla, Django, Drupal - Network Protocols: WPA/WPA2, Kerberos, SNMP - Archive Formats: ZIP, RAR, 7-Zip - Cryptocurrency: Bitcoin wallets, Ethereum wallets</p>"},{"location":"reference/hash-types/#hash-type-structure","title":"Hash Type Structure","text":"<p>Each hash type in KrakenHashes has the following properties:</p> <ul> <li>ID: Unique numerical identifier (hashcat mode number)</li> <li>Name: Descriptive name of the hash algorithm</li> <li>Example: Sample hash format showing the expected input structure</li> <li>Needs Processing: Flag indicating if special preprocessing is required</li> <li>Is Enabled: Whether the hash type is currently supported</li> <li>Slow: Flag indicating if this is a computationally expensive algorithm</li> </ul>"},{"location":"reference/hash-types/#common-hash-types-by-use-case","title":"Common Hash Types by Use Case","text":""},{"location":"reference/hash-types/#web-application-security-testing","title":"Web Application Security Testing","text":"Mode Algorithm Common Applications 400 phpass WordPress, phpBB 500 md5crypt Traditional Unix systems 1600 Apache \\(apr1\\) Apache htpasswd 7900 Drupal7 Drupal CMS 124 Django (SHA-1) Django framework 10000 Django (PBKDF2-SHA256) Modern Django"},{"location":"reference/hash-types/#enterpriseactive-directory","title":"Enterprise/Active Directory","text":"Mode Algorithm Use Case 1000 NTLM Windows password hashes 5500 NetNTLMv1 Network authentication 5600 NetNTLMv2 Network authentication 7500 Kerberos 5 AS-REQ Domain authentication 13100 Kerberos 5 TGS-REP Ticket Granting Service"},{"location":"reference/hash-types/#database-security","title":"Database Security","text":"Mode Algorithm Database 12 PostgreSQL PostgreSQL MD5 300 MySQL4.1/MySQL5 MySQL SHA1 200 MySQL323 Legacy MySQL 131 MSSQL (2000) SQL Server 132 MSSQL (2005) SQL Server 1731 MSSQL (2012, 2014) SQL Server"},{"location":"reference/hash-types/#filearchive-security","title":"File/Archive Security","text":"Mode Algorithm Application 13000 RAR5 WinRAR archives 12500 RAR3-hp WinRAR archives 11600 7-Zip 7-Zip archives 17200 PKZIP (Compressed) ZIP archives 17210 PKZIP (Uncompressed) ZIP archives"},{"location":"reference/hash-types/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/hash-types/#speed-classifications","title":"Speed Classifications","text":"<p>Ultra-Fast (&gt;1 billion attempts/sec on modern GPUs): - MD4 (900), MD5 (0), SHA1 (100) - Simple salted variants</p> <p>Fast (100M-1B attempts/sec): - SHA2 variants, NTLM - Basic application-specific hashes</p> <p>Medium (1M-100M attempts/sec): - Multiple iteration hashes - Complex salted variants</p> <p>Slow (&lt;1M attempts/sec): - PBKDF2, bcrypt, scrypt - Full disk encryption - Cryptocurrency wallets</p>"},{"location":"reference/hash-types/#resource-requirements","title":"Resource Requirements","text":"<p>GPU Memory Considerations: - Large wordlists may require significant GPU memory - Rule-based attacks can multiply memory requirements - Some hash types have higher per-hash memory overhead</p> <p>CPU vs GPU Performance: - Most hash types benefit significantly from GPU acceleration - Some algorithms may perform better on CPU for small datasets - Hybrid attacks may utilize both CPU and GPU resources</p>"},{"location":"reference/hash-types/#hash-format-examples","title":"Hash Format Examples","text":""},{"location":"reference/hash-types/#basic-formats","title":"Basic Formats","text":"<pre><code>MD5:           8743b52063cd84097a65d1633f5c74f5\nSHA1:          b89eaac7e61417341b710b727768294d0e6a277b\nSHA256:        127e6fbfe24a750e72930c220a8e138275656b8e5d8f48a98c3c92df2caba935\nNTLM:          b4b9b02e6f09a9bd760f388b67351e2b\n</code></pre>"},{"location":"reference/hash-types/#salted-formats","title":"Salted Formats","text":"<pre><code>md5($pass.$salt):     01dfae6e5d4d90d9892622325959afbe:7050461\nsha1($salt.$pass):    cac35ec206d868b7d7cb0b55f31d9425b075082b:5363620024\nsha256($pass.$salt):  c73d08de890479518ed60cf670d17faa26a4a71f995c1dcc978165399401a6c4:53743528\n</code></pre>"},{"location":"reference/hash-types/#application-specific-formats","title":"Application-Specific Formats","text":"<pre><code>WordPress:      $P$984478476IagS59wHZvyQMArzfx58u.\nbcrypt:         $2a$05$LhayLxezLhK1LhWvKxCyLOj0j1u.Kj0jZ0pEmm134uzrQlFvQJLF6\nDjango:         pbkdf2_sha256$20000$H0dPx8NeajVu$GiC4k5kqbbR9qWBlsRgDywNqC2vd9kqfk7zdorEnNas=\nNetNTLMv2:      admin::N46iSNekpT:08ca45b7d7ea58ee:88dcbe4446168966a153a0064958dac6\n</code></pre>"},{"location":"reference/hash-types/#hash-type-identification","title":"Hash Type Identification","text":""},{"location":"reference/hash-types/#automatic-detection","title":"Automatic Detection","text":"<p>KrakenHashes can often identify hash types based on: - Length: Different algorithms produce different output lengths - Character set: Hex vs base64 vs custom encoding - Delimiters: Colons, dollar signs, or other separators - Prefixes: Algorithm identifiers like <code>$2a$</code>, <code>{SHA}</code>, etc.</p>"},{"location":"reference/hash-types/#manual-identification-guidelines","title":"Manual Identification Guidelines","text":"<p>By Length (hex-encoded): - 32 characters: MD5, NTLM, MD4 - 40 characters: SHA1, MySQL4.1 - 56 characters: SHA2-224 - 64 characters: SHA2-256, BLAKE2s-256 - 96 characters: SHA2-384 - 128 characters: SHA2-512, BLAKE2b-512</p> <p>By Format Patterns: - <code>$algorithm$</code>: crypt-style formats (bcrypt, sha512crypt, etc.) - <code>{ALGORITHM}</code>: LDAP-style formats - <code>hash:salt</code>: Simple salted hashes - Complex delimited formats for specific applications</p>"},{"location":"reference/hash-types/#common-identification-mistakes","title":"Common Identification Mistakes","text":"<ul> <li>Confusing MD5 with NTLM (both 32 hex characters)</li> <li>Misidentifying base64-encoded vs hex-encoded hashes</li> <li>Not recognizing application-specific wrapper formats</li> </ul>"},{"location":"reference/hash-types/#best-practices","title":"Best Practices","text":""},{"location":"reference/hash-types/#hash-type-selection","title":"Hash Type Selection","text":"<ol> <li>Verify the source: Confirm the application/system that generated the hashes</li> <li>Check format carefully: Pay attention to delimiters, prefixes, and encoding</li> <li>Test with known samples: Use test hashes to verify correct identification</li> <li>Consider variations: Many applications have multiple hash format variants</li> </ol>"},{"location":"reference/hash-types/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Start with fast hashes: Test identification with quick attacks first</li> <li>Use appropriate wordlists: Match wordlist complexity to hash strength</li> <li>Consider slow hash implications: Budget time appropriately for PBKDF2, bcrypt, etc.</li> <li>Monitor resource usage: Slow hashes can consume significant GPU memory</li> </ol>"},{"location":"reference/hash-types/#security-considerations","title":"Security Considerations","text":"<ol> <li>Handle sensitive data properly: Ensure secure storage and transmission</li> <li>Use appropriate attack methods: Don't waste resources on over-engineered attacks</li> <li>Respect rate limits: Some hash types may benefit from attack rate limiting</li> <li>Document findings: Keep records of successful techniques for similar engagements</li> </ol>"},{"location":"reference/hash-types/#advanced-features","title":"Advanced Features","text":""},{"location":"reference/hash-types/#processing-requirements","title":"Processing Requirements","text":"<p>Some hash types require special preprocessing before cracking: - NTLM (mode 1000): Requires UTF-16LE encoding conversion - Character set normalization for international passwords - Case conversion requirements for specific applications</p>"},{"location":"reference/hash-types/#multi-hash-support","title":"Multi-Hash Support","text":"<p>KrakenHashes supports attacking multiple hashes of the same type simultaneously: - Efficient memory usage for large hashlist processing - Optimized GPU kernels for batch operations - Progress tracking per individual hash</p>"},{"location":"reference/hash-types/#custom-hash-types","title":"Custom Hash Types","text":"<p>For specialized requirements: - Contact development team for custom hash type implementation - Provide detailed specification and test vectors - Consider performance implications for custom algorithms</p>"},{"location":"reference/hash-types/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/hash-types/#common-issues","title":"Common Issues","text":"<ol> <li>Hash not cracking: Verify hash type identification</li> <li>Slow performance: Check if hash type is marked as \"slow\"</li> <li>GPU errors: Some hash types require specific GPU capabilities</li> <li>Memory errors: Large hashlists may exceed available GPU memory</li> </ol>"},{"location":"reference/hash-types/#getting-help","title":"Getting Help","text":"<ul> <li>Check the hash format against provided examples</li> <li>Verify the source application and version</li> <li>Test with known password/hash pairs</li> <li>Consult the troubleshooting guide for hardware-specific issues</li> </ul>"},{"location":"reference/hash-types/#complete-hash-type-list","title":"Complete Hash Type List","text":"<p>For a complete list of all supported hash types with examples, consult the database directly or use the admin interface. The list includes detailed information about: - Algorithm specifications - Example hash formats - Performance characteristics - Special requirements or limitations</p> <p>Last updated: Based on KrakenHashes database with 504 supported hash types For the most current information, always check the application's hash type management interface</p>"},{"location":"reference/architecture/benchmark-workflow/","title":"Benchmark-Based Job Assignment Workflow","text":""},{"location":"reference/architecture/benchmark-workflow/#overview","title":"Overview","text":"<p>The job scheduling service now implements a benchmark-first approach for job assignment. Before assigning work to an agent, the system verifies that the agent has a valid benchmark for the specific attack mode and hash type combination.</p>"},{"location":"reference/architecture/benchmark-workflow/#workflow","title":"Workflow","text":"<ol> <li>Job Assignment Request</li> <li>Scheduler identifies an available agent and a pending job</li> <li> <p>Job execution details are retrieved, including the hashlist</p> </li> <li> <p>Benchmark Check</p> </li> <li>System checks if agent has a benchmark for the attack mode and hash type</li> <li>If benchmark exists, checks if it's still valid (default: 7 days cache)</li> <li> <p>Cache duration can be configured via <code>benchmark_cache_duration_hours</code> setting</p> </li> <li> <p>Benchmark Request (if needed)</p> </li> <li>If no valid benchmark exists, system sends enhanced benchmark request</li> <li>Request includes actual job configuration:<ul> <li>Binary version</li> <li>Wordlists and rules (if applicable)</li> <li>Mask (for brute force attacks)</li> <li>Hash type and attack mode</li> <li>Test duration (30 seconds)</li> </ul> </li> <li> <p>Job assignment is deferred until benchmark completes</p> </li> <li> <p>Benchmark Execution (Agent side)</p> </li> <li>Agent receives benchmark request with full job configuration</li> <li>Runs actual hashcat benchmark with the specific parameters</li> <li> <p>Reports back real-world performance metrics</p> </li> <li> <p>Job Assignment (after benchmark)</p> </li> <li>Once benchmark is received and stored, agent becomes available again</li> <li>Next scheduling cycle will find the valid benchmark</li> <li>Chunk calculation uses accurate performance data</li> <li>Job task is assigned with properly sized chunks</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#benefits","title":"Benefits","text":"<ul> <li>Accurate Performance Estimation: Benchmarks use actual job configuration</li> <li>Optimal Chunk Sizing: Prevents under/over-utilization of agents</li> <li>Reduced Job Failures: Avoids assigning work that agents can't handle</li> <li>Better Resource Utilization: Chunks are sized based on real performance</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#configuration","title":"Configuration","text":"<ul> <li><code>benchmark_cache_duration_hours</code>: How long benchmarks remain valid (default: 168 hours / 7 days)</li> <li><code>chunk_fluctuation_percentage</code>: Tolerance for final chunk size variations (default: 20%)</li> <li><code>default_chunk_duration</code>: Target duration for each chunk in seconds (default: 1200 / 20 minutes)</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/architecture/benchmark-workflow/#modified-components","title":"Modified Components","text":"<ol> <li>JobSchedulingService (<code>assignWorkToAgent</code>)</li> <li>Added benchmark validation before chunk calculation</li> <li>Defers assignment if benchmark is needed</li> <li> <p>Retrieves hashlist to get hash type</p> </li> <li> <p>JobWebSocketIntegration (<code>RequestAgentBenchmark</code>)</p> </li> <li>New method implementing the interface</li> <li>Sends enhanced benchmark request with full job configuration</li> <li> <p>Includes wordlists, rules, mask, and binary information</p> </li> <li> <p>WebSocket Types</p> </li> <li><code>BenchmarkRequestPayload</code> enhanced with job-specific fields</li> <li>Supports real-world speed testing with actual attack parameters</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#error-handling","title":"Error Handling","text":"<ul> <li>Missing benchmarks trigger requests instead of failures</li> <li>Invalid benchmarks are detected and refreshed</li> <li>WebSocket unavailability is properly handled</li> <li>Graceful degradation if benchmark request fails</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#accurate-keyspace-tracking","title":"Accurate Keyspace Tracking","text":"<p>In addition to benchmarking for performance estimation, the system captures accurate keyspace values from hashcat to ensure precise progress tracking.</p>"},{"location":"reference/architecture/benchmark-workflow/#why-accurate-keyspace-tracking","title":"Why Accurate Keyspace Tracking?","text":"<p>When using rules or combination attacks, estimating the total keyspace can be inaccurate. For example: - Rule-based attacks: Estimated keyspace = wordlist_size \u00d7 rule_count, but actual keyspace varies based on rule effectiveness - Combination attacks: Certain combinations may be invalid or duplicates</p> <p>Hashcat provides the actual keyspace through <code>progress[1]</code> values, which the system captures to ensure accurate progress reporting.</p>"},{"location":"reference/architecture/benchmark-workflow/#keyspace-capture-workflow","title":"Keyspace Capture Workflow","text":"<ol> <li>Initial Job Creation</li> <li>Job created with estimated <code>effective_keyspace</code> based on wordlists/rules</li> <li>Flag <code>is_accurate_keyspace</code> set to <code>false</code></li> <li> <p>Estimation needed for rule splitting decisions</p> </li> <li> <p>Forced Benchmark for First Agent</p> </li> <li>When first agent connects (taskCount = 0), system requests benchmark</li> <li>Benchmark includes actual job configuration (wordlists, rules, mask, hash type)</li> <li> <p>Agent runs hashcat benchmark and captures <code>progress[1]</code> value</p> </li> <li> <p>Accurate Keyspace Capture</p> </li> <li>Backend receives benchmark result with <code>TotalEffectiveKeyspace</code> from <code>progress[1]</code></li> <li>Updates job execution:<ul> <li>Sets <code>effective_keyspace</code> to actual value from hashcat</li> <li>Sets <code>is_accurate_keyspace</code> to <code>true</code></li> <li>Calculates <code>avg_rule_multiplier</code> = actual / estimated</li> </ul> </li> <li> <p>Subsequent agents skip benchmark and use cached job-level keyspace</p> </li> <li> <p>Fallback: First Progress Update</p> </li> <li>If benchmark doesn't provide keyspace, first task progress update does</li> <li>Agent sends <code>progress[1]</code> value in first progress message with <code>IsFirstUpdate</code> flag</li> <li>Backend updates both job-level and task-level keyspace</li> <li> <p>Sets <code>is_actual_keyspace</code> to <code>true</code> for the task</p> </li> <li> <p>Future Task Improvements</p> </li> <li>New tasks use <code>avg_rule_multiplier</code> to improve estimated keyspace</li> <li>Provides better estimates for chunks not yet processed</li> <li>Helps with more accurate progress reporting across the job</li> </ol>"},{"location":"reference/architecture/benchmark-workflow/#benefits-of-accurate-keyspace-tracking","title":"Benefits of Accurate Keyspace Tracking","text":"<ul> <li>Precise Progress: Progress percentages reflect actual hashcat progress, not estimates</li> <li>Better Task Distribution: Chunk sizes calculated based on real keyspace</li> <li>Improved Estimates: Future tasks benefit from multiplier derived from actual values</li> <li>Consistency: All agents working on same job use same accurate keyspace</li> </ul>"},{"location":"reference/architecture/benchmark-workflow/#database-columns","title":"Database Columns","text":"<p>job_executions table: - <code>is_accurate_keyspace</code> (boolean): True when keyspace is from hashcat <code>progress[1]</code> - <code>avg_rule_multiplier</code> (float): Ratio of actual/estimated keyspace for improving future estimates</p> <p>job_tasks table: - <code>is_actual_keyspace</code> (boolean): True when task has actual keyspace from progress update</p>"},{"location":"reference/architecture/benchmark-workflow/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Benchmark History: Track benchmark trends over time</li> <li>Performance Prediction: Use ML to predict performance for new combinations</li> <li>Dynamic Re-benchmarking: Trigger new benchmarks on performance anomalies</li> <li>Multi-GPU Optimization: Per-device benchmark tracking</li> <li>Keyspace Prediction: Use historical multipliers to improve initial estimates</li> </ol>"},{"location":"reference/architecture/chunking/","title":"KrakenHashes Chunking System","text":""},{"location":"reference/architecture/chunking/#overview","title":"Overview","text":"<p>KrakenHashes uses an intelligent chunking system to distribute password cracking workloads across multiple agents. This document explains how chunks are created, distributed, and tracked for different attack types.</p>"},{"location":"reference/architecture/chunking/#what-is-chunking","title":"What is Chunking?","text":"<p>Chunking divides large password cracking jobs into smaller, manageable pieces that can be: - Distributed across multiple agents for parallel processing - Completed within a reasonable time frame (default: 20 minutes) - Resumed if interrupted or failed - Tracked for accurate progress reporting</p>"},{"location":"reference/architecture/chunking/#how-chunking-works","title":"How Chunking Works","text":""},{"location":"reference/architecture/chunking/#basic-chunking-no-rules","title":"Basic Chunking (No Rules)","text":"<p>For simple dictionary attacks without rules: 1. The system calculates the total keyspace (number of password candidates) 2. Based on agent benchmark speeds, it determines optimal chunk sizes 3. Each chunk processes a portion of the wordlist using hashcat's <code>--skip</code> and <code>--limit</code> parameters</p> <p>Example:  - Wordlist: 1,000,000 passwords - Agent speed: 1,000,000 H/s - Target chunk time: 1,200 seconds (20 minutes) - Chunk size: 1,200,000,000 candidates - Result: Single chunk processes entire wordlist</p>"},{"location":"reference/architecture/chunking/#enhanced-chunking-with-rules","title":"Enhanced Chunking with Rules","text":"<p>When rules are applied, the effective keyspace multiplies:</p> <p>Effective Keyspace = Wordlist Size \u00d7 Number of Rules</p> <p>For example: - Wordlist: 1,000,000 passwords - Rules: 1,000 rules - Effective keyspace: 1,000,000,000 candidates</p>"},{"location":"reference/architecture/chunking/#rule-splitting","title":"Rule Splitting","text":"<p>When a job with rules would take significantly longer than the target chunk time, KrakenHashes can split the rules:</p> <ol> <li>Detection: If estimated time &gt; 2\u00d7 target chunk time</li> <li>Splitting: Divides rules into smaller files</li> <li>Distribution: Each agent receives full wordlist + partial rules</li> <li>Progress: Tracks completion across all rule chunks</li> </ol> <p>Example: - Wordlist: 1,000,000 passwords - Rules: 10,000 rules - Agent speed: 1,000,000 H/s - Without splitting: 10,000 seconds (2.8 hours) per chunk - With splitting into 10 chunks: 1,000 rules each, ~1,000 seconds per chunk</p>"},{"location":"reference/architecture/chunking/#combination-attacks","title":"Combination Attacks","text":"<p>For combination attacks (-a 1), the effective keyspace is:</p> <p>Effective Keyspace = Wordlist1 Size \u00d7 Wordlist2 Size</p> <p>The system tracks progress through the virtual keyspace while hashcat processes the first wordlist sequentially.</p>"},{"location":"reference/architecture/chunking/#attack-mode-support","title":"Attack Mode Support","text":"Attack Mode Description Chunking Method 0 (Straight) Dictionary Wordlist position + optional rule splitting 1 (Combination) Two wordlists Virtual keyspace tracking 3 (Brute-force) Mask attack Mask position chunking 6 (Hybrid W+M) Wordlist + Mask Wordlist position chunking 7 (Hybrid M+W) Mask + Wordlist Mask position chunking 9 (Association) Per-hash rules Rule splitting when applicable"},{"location":"reference/architecture/chunking/#progress-tracking","title":"Progress Tracking","text":""},{"location":"reference/architecture/chunking/#standard-progress","title":"Standard Progress","text":"<ul> <li>Shows candidates tested vs total keyspace</li> <li>Updates in real-time via WebSocket</li> <li>Accurate percentage completion</li> </ul>"},{"location":"reference/architecture/chunking/#with-rule-multiplication","title":"With Rule Multiplication","text":"<ul> <li>Display format: \"X / Y (\u00d7Z)\" where Z is the multiplication factor</li> <li>Accounts for all rules across all chunks</li> <li>Aggregates progress from distributed rule chunks</li> </ul>"},{"location":"reference/architecture/chunking/#progress-bar-visualization","title":"Progress Bar Visualization","text":"<p>The progress bar always shows: - Green: Completed keyspace - Gray: Remaining keyspace - Percentage: Based on effective keyspace</p>"},{"location":"reference/architecture/chunking/#accurate-keyspace-tracking-from-hashcat","title":"Accurate Keyspace Tracking from Hashcat","text":""},{"location":"reference/architecture/chunking/#overview_1","title":"Overview","text":"<p>KrakenHashes captures actual keyspace values directly from hashcat's <code>progress[1]</code> field to ensure precise progress reporting, especially for jobs with rules or combination attacks where estimation can be inaccurate.</p>"},{"location":"reference/architecture/chunking/#how-it-works","title":"How It Works","text":"<ol> <li>Initial Job Creation: Job created with estimated keyspace based on wordlists/rules, <code>is_accurate_keyspace = false</code></li> <li>First Progress Update: Agent sends <code>progress[1]</code> value from hashcat, backend captures it as <code>chunk_actual_keyspace</code></li> <li>Cascade Recalculation: All subsequent chunks' start/end positions are recalculated using cumulative actual keyspace from completed chunks</li> <li>Self-Correcting: The system handles out-of-order chunk completion correctly by recalculating all subsequent chunks whenever a chunk receives its actual keyspace</li> </ol>"},{"location":"reference/architecture/chunking/#technical-details","title":"Technical Details","text":"<p>Chunk Actual Keyspace: - Each chunk's <code>chunk_actual_keyspace</code> field stores the immutable size from hashcat's <code>progress[1]</code> value - This represents the actual keyspace processed by hashcat for that specific chunk - Used to calculate exact start/end positions for subsequent chunks</p> <p>Cascade Updates: When a chunk completes and provides its actual keyspace: 1. Backend updates the chunk's <code>chunk_actual_keyspace</code> and <code>effective_keyspace_end</code> 2. All subsequent chunks (higher chunk numbers) have their positions recalculated 3. New chunk start position = sum of all previous chunks' actual keyspace 4. Works correctly even if chunks complete out of order</p> <p>Dispatched Keyspace Adjustments: - When actual keyspace differs from estimate, <code>dispatched_keyspace</code> is adjusted - Ensures job progress accurately reflects work distribution</p>"},{"location":"reference/architecture/chunking/#database-schema","title":"Database Schema","text":"<p>job_tasks table: - <code>chunk_actual_keyspace</code> (BIGINT): Immutable chunk size from hashcat <code>progress[1]</code> - <code>is_actual_keyspace</code> (BOOLEAN): True when task has actual keyspace from hashcat</p> <p>job_executions table: - <code>is_accurate_keyspace</code> (BOOLEAN): True when keyspace is from hashcat <code>progress[1]</code> - <code>avg_rule_multiplier</code> (FLOAT): Actual/estimated ratio for improving future estimates</p>"},{"location":"reference/architecture/chunking/#benefits","title":"Benefits","text":"<ol> <li>Precise Progress: Progress percentages match hashcat's actual progress, not estimates</li> <li>Better Task Distribution: Chunk sizes calculated based on real keyspace</li> <li>Improved Future Estimates: Multiplier derived from actual values helps estimate remaining work</li> <li>Self-Correcting Ranges: Chunks automatically adjust when actual differs from estimate</li> <li>No Backwards Ranges: Ensures ascending keyspace ranges (10.45T \u2192 20.14T, not 10.45T \u2192 9.60T)</li> </ol>"},{"location":"reference/architecture/chunking/#example","title":"Example","text":"<p>Before accurate tracking: - Estimated: 10.00T per chunk - Reality: 10.51T, 9.63T, 9.71T (varies by rule effectiveness) - Problem: Progress bars and ETAs inaccurate</p> <p>After accurate tracking: - Chunk 1: 0 \u2192 10.51T (actual from hashcat) - Chunk 2: 10.51T \u2192 20.14T (starts where chunk 1 ended) - Chunk 3: 20.14T \u2192 29.85T (continues from chunk 2) - Result: Perfect cumulative chain with accurate progress</p> <p>See Benchmark Workflow for more details on keyspace capture during benchmarking.</p>"},{"location":"reference/architecture/chunking/#dynamic-updates-during-execution","title":"Dynamic Updates During Execution","text":"<p>When wordlists, rules, or potfiles are modified while jobs are running, the chunking system adapts:</p>"},{"location":"reference/architecture/chunking/#automatic-recalculation","title":"Automatic Recalculation","text":"<ul> <li>Keyspace Updates: Effective keyspace recalculates based on current file state</li> <li>Forward-Only: Only undispatched chunks are affected by changes</li> <li>No Interruption: Active chunks continue with their original parameters</li> </ul>"},{"location":"reference/architecture/chunking/#update-scenarios","title":"Update Scenarios","text":"<ol> <li>Wordlist Growth: New words available for future chunks (if not using rule splitting)</li> <li>Rule Changes: Multiplication factor adjusts for remaining work</li> <li>Potfile Updates: Manual refresh triggers keyspace recalculation</li> </ol> <p>For detailed information about how file updates affect running jobs, see the Job Update System documentation.</p>"},{"location":"reference/architecture/chunking/#configuration","title":"Configuration","text":"<p>Administrators can tune chunking behavior via system settings:</p> Setting Default Description <code>default_chunk_duration</code> 1200s Target time per chunk (20 minutes) <code>chunk_fluctuation_percentage</code> 20% Threshold for merging final chunks <code>rule_split_enabled</code> true Enable automatic rule splitting <code>rule_split_threshold</code> 2.0 Time multiplier to trigger splitting <code>rule_split_min_rules</code> 100 Minimum rules before considering split"},{"location":"reference/architecture/chunking/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/chunking/#for-users","title":"For Users","text":"<ol> <li>Large Rule Files: Will automatically split for better distribution</li> <li>Multiple Rule Files: Multiplication is handled automatically</li> <li>Progress Monitoring: Check effective keyspace in job details</li> <li>Benchmarks: Ensure agents have current benchmarks for accurate chunking</li> </ol>"},{"location":"reference/architecture/chunking/#for-administrators","title":"For Administrators","text":"<ol> <li>Chunk Duration: Balance between progress granularity and overhead</li> <li>Rule Splitting: Monitor temp directory space for large rule files</li> <li>Benchmarks: Configure benchmark validity period appropriately</li> <li>Resource Usage: Rule splitting creates temporary files</li> </ol>"},{"location":"reference/architecture/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/chunking/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check if effective keyspace is much larger than expected</li> <li>Verify agent benchmarks are current</li> <li>Consider enabling rule splitting if disabled</li> </ul>"},{"location":"reference/architecture/chunking/#uneven-distribution","title":"Uneven Distribution","text":"<ul> <li>Some chunks may be larger due to:</li> <li>Fluctuation threshold preventing small final chunks</li> <li>Rule count not evenly divisible</li> <li>Different agent speeds</li> </ul>"},{"location":"reference/architecture/chunking/#rule-splitting-not-occurring","title":"Rule Splitting Not Occurring","text":"<p>Verify: - <code>rule_split_enabled</code> is true - Rule file has &gt; <code>rule_split_min_rules</code> rules - Estimated time exceeds threshold</p>"},{"location":"reference/architecture/chunking/#technical-details_1","title":"Technical Details","text":""},{"location":"reference/architecture/chunking/#keyspace-calculation","title":"Keyspace Calculation","text":"<pre><code>Attack Mode 0 (Dictionary):\n- Without rules: wordlist_size\n- With rules: wordlist_size \u00d7 total_rule_count\n\nAttack Mode 1 (Combination):\n- Always: wordlist1_size \u00d7 wordlist2_size\n\nAttack Mode 3 (Brute-force):\n- Calculated from mask: charset_size^length\n\nAttack Mode 6/7 (Hybrid):\n- Wordlist_size \u00d7 mask_keyspace\n</code></pre>"},{"location":"reference/architecture/chunking/#chunk-assignment","title":"Chunk Assignment","text":"<ol> <li>Agent requests work</li> <li>System calculates optimal chunk size based on:</li> <li>Agent's benchmark speed</li> <li>Target chunk duration</li> <li>Remaining keyspace</li> <li>Chunk boundaries determined:</li> <li>Start position (skip)</li> <li>Chunk size (limit)</li> <li>Agent receives chunk assignment</li> <li>Progress tracked and aggregated</li> </ol>"},{"location":"reference/architecture/chunking/#rule-chunk-files","title":"Rule Chunk Files","text":"<p>When rule splitting is active: - Temporary files created in configured directory - Named: <code>job_[ID]_chunk_[N].rule</code> - Automatically cleaned up after job completion - Synced to agents like normal rule files</p>"},{"location":"reference/architecture/chunking/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Pre-calculation of optimal chunk distribution</li> <li>Dynamic chunk resizing based on actual speed</li> <li>Rule deduplication before splitting</li> <li>Compression for rule chunk transfers</li> </ul>"},{"location":"reference/architecture/job-completion-system/","title":"Automatic Job Completion System","text":""},{"location":"reference/architecture/job-completion-system/#overview","title":"Overview","text":"<p>KrakenHashes automatically detects when all hashes in a hashlist have been cracked and manages the lifecycle of related jobs to prevent failures and wasted resources.</p>"},{"location":"reference/architecture/job-completion-system/#the-problem","title":"The Problem","text":"<p>Hashcat's <code>--remove</code> option removes cracked hashes from input files during execution. When all hashes are cracked: - The hashlist file becomes empty - Subsequent jobs targeting that hashlist fail immediately - Resources are wasted attempting to process empty files - Users receive confusing error messages</p>"},{"location":"reference/architecture/job-completion-system/#the-solution","title":"The Solution","text":""},{"location":"reference/architecture/job-completion-system/#status-code-6-detection","title":"Status Code 6 Detection","text":"<p>The agent monitors hashcat's JSON status output for status code 6, which indicates \"all hashes cracked.\" This code is sent by hashcat when: - The input file has no remaining uncracked hashes - All work is complete for the given hashlist</p>"},{"location":"reference/architecture/job-completion-system/#trust-model","title":"Trust Model","text":"<p>The system trusts status code 6 as authoritative without database verification because: - Hashcat knows definitively when all hashes are cracked - Database verification would create race conditions - Status code 6 is a reliable signal from hashcat - Prevents complex synchronization issues</p>"},{"location":"reference/architecture/job-completion-system/#job-cleanup-process","title":"Job Cleanup Process","text":"<p>When status code 6 is received:</p> <ol> <li>Identify All Affected Jobs: Query for ALL jobs (any status) targeting the same hashlist</li> <li>Running Jobs:</li> <li>Send WebSocket stop signals to active agents</li> <li>Mark jobs as \"completed\" at 100% progress</li> <li>Send completion email notifications</li> <li>Pending Jobs:</li> <li>Delete jobs that haven't started yet</li> <li>No email notifications (jobs never ran)</li> <li>Prevention: New tasks for this hashlist won't be created</li> </ol>"},{"location":"reference/architecture/job-completion-system/#technical-implementation","title":"Technical Implementation","text":"<p>Components: - <code>HashlistCompletionService</code>: Handles job cleanup logic - <code>AllHashesCracked</code> flag in WebSocket messages - Background processing with 5-minute timeout</p> <p>Flow: <pre><code>Agent detects status code 6 \u2192 Sets AllHashesCracked flag \u2192\nBackend handler triggered \u2192 HashlistCompletionService runs async \u2192\nStop running tasks + Delete pending jobs \u2192 Send notifications\n</code></pre></p> <p>Code Location: <code>backend/internal/services/hashlist_completion_service.go</code></p>"},{"location":"reference/architecture/job-completion-system/#agent-side-implementation","title":"Agent-Side Implementation","text":""},{"location":"reference/architecture/job-completion-system/#detection","title":"Detection","text":"<p>In <code>agent/internal/jobs/hashcat_executor.go</code>: - Parses hashcat JSON status output - Checks for <code>status</code> field equal to 6 - Sets <code>AllHashesCracked</code> flag in progress update message - Flag sent with regular progress updates (no special message needed)</p>"},{"location":"reference/architecture/job-completion-system/#timing","title":"Timing","text":"<ul> <li>Detection occurs during normal progress monitoring</li> <li>No additional API calls required</li> <li>Flag transmitted with existing WebSocket infrastructure</li> </ul>"},{"location":"reference/architecture/job-completion-system/#backend-side-implementation","title":"Backend-Side Implementation","text":""},{"location":"reference/architecture/job-completion-system/#message-handling","title":"Message Handling","text":"<p>In <code>backend/internal/routes/websocket_with_jobs.go</code>: - Checks <code>AllHashesCracked</code> flag in job progress messages - Triggers before status-specific processing - Runs HashlistCompletionService asynchronously</p>"},{"location":"reference/architecture/job-completion-system/#service-logic","title":"Service Logic","text":"<p><code>HashlistCompletionService.HandleHashlistCompletion()</code>:</p> <ol> <li> <p>Query Affected Jobs:    <pre><code>SELECT * FROM job_executions\nWHERE hashlist_id = ?\nAND status IN ('pending', 'running', 'paused')\n</code></pre></p> </li> <li> <p>Process Running Jobs:</p> </li> <li>Find active tasks for each running job</li> <li>Send stop signals via WebSocket</li> <li>Update job status to 'completed'</li> <li>Set progress to 100%</li> <li> <p>Trigger email notifications</p> </li> <li> <p>Process Pending Jobs:</p> </li> <li>Delete jobs that haven't started</li> <li>Clean up any associated data</li> <li> <p>No notifications needed</p> </li> <li> <p>Update Job Priority:</p> </li> <li>Comprehensive processing regardless of priority</li> <li>Handles all affected jobs in single operation</li> </ol>"},{"location":"reference/architecture/job-completion-system/#configuration","title":"Configuration","text":"<p>No configuration required - this feature is always active.</p>"},{"location":"reference/architecture/job-completion-system/#benefits","title":"Benefits","text":"<ol> <li>Prevents Failures: No more failed jobs due to empty hashlist files</li> <li>Resource Efficiency: Stops wasting resources on completed hashlists</li> <li>User Experience: Automatic cleanup without manual intervention</li> <li>Notifications: Users informed of successful completion</li> <li>Clean State: Queue automatically cleaned of obsolete jobs</li> </ol>"},{"location":"reference/architecture/job-completion-system/#error-handling","title":"Error Handling","text":""},{"location":"reference/architecture/job-completion-system/#timeout-protection","title":"Timeout Protection","text":"<ul> <li>5-minute timeout for cleanup operations</li> <li>Prevents hanging if service encounters issues</li> <li>Logged errors don't block agent progress reporting</li> </ul>"},{"location":"reference/architecture/job-completion-system/#transaction-safety","title":"Transaction Safety","text":"<ul> <li>Database operations use transactions</li> <li>Rollback on errors ensures consistency</li> <li>Agent continues normal operation regardless of cleanup success</li> </ul>"},{"location":"reference/architecture/job-completion-system/#websocket-errors","title":"WebSocket Errors","text":"<ul> <li>Gracefully handles disconnected agents</li> <li>Tasks marked for stop even if agent offline</li> <li>Agent reconnection triggers cleanup on next connection</li> </ul>"},{"location":"reference/architecture/job-completion-system/#limitations","title":"Limitations","text":"<ul> <li>Trusts hashcat status code 6 without verification</li> <li>Only handles jobs for the same hashlist (doesn't affect other hashlists)</li> <li>Requires agent to detect and report status code 6</li> <li>Depends on WebSocket connectivity for stop signals</li> </ul>"},{"location":"reference/architecture/job-completion-system/#testing","title":"Testing","text":"<p>Tested with hashlist 85: - 1 running job completed at 100% with stop signal sent - 2 pending jobs deleted (never started) - Email notifications triggered successfully - No errors in logs</p>"},{"location":"reference/architecture/job-completion-system/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"reference/architecture/job-completion-system/#log-messages","title":"Log Messages","text":"<p>Success: <pre><code>Successfully completed job [uuid] for hashlist [id]\nSuccessfully deleted pending job [uuid] for hashlist [id]\n</code></pre></p> <p>Errors: <pre><code>Failed to stop tasks for job [uuid]: [error]\nFailed to complete job [uuid]: [error]\n</code></pre></p>"},{"location":"reference/architecture/job-completion-system/#metrics","title":"Metrics","text":"<p>Track in monitoring: - Number of jobs auto-completed - Number of pending jobs cleaned up - Time taken for cleanup operations - Failed cleanup attempts</p>"},{"location":"reference/architecture/job-completion-system/#related-documentation","title":"Related Documentation","text":"<ul> <li>Chunking System - How jobs are divided into chunks</li> <li>Job Update System - How keyspace updates work</li> <li>Jobs &amp; Workflows - User perspective on automatic completion</li> <li>Core Concepts - Understanding job execution flow</li> </ul>"},{"location":"reference/architecture/job-completion-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements under consideration:</p> <ul> <li>Partial Completion Threshold: Complete jobs when X% of hashes cracked (configurable)</li> <li>Notification Customization: Per-client notification preferences</li> <li>Completion Hooks: Custom scripts triggered on hashlist completion</li> <li>Statistics Tracking: Historical data on completion rates and timing</li> <li>Manual Override: Allow users to force completion or prevent automatic cleanup</li> </ul>"},{"location":"reference/architecture/job-update-system/","title":"Job Update System","text":""},{"location":"reference/architecture/job-update-system/#overview","title":"Overview","text":"<p>The KrakenHashes Job Update System automatically recalculates job keyspaces when associated wordlists, rules, or potfiles change during execution. This is a \"going forward\" system - when files are updated, only undispatched work is affected. Already-assigned tasks continue with their original parameters, ensuring consistency while allowing jobs to benefit from updated resources.</p>"},{"location":"reference/architecture/job-update-system/#core-philosophy-forward-only-updates","title":"Core Philosophy: Forward-Only Updates","text":"<p>The system operates on these principles:</p> <ol> <li>No Deficit Tracking: The system doesn't track \"missed\" work from updates that occur after tasks are dispatched</li> <li>Current State Calculation: Keyspaces are recalculated based on the current file state and remaining work</li> <li>Non-Disruptive: Running tasks are never interrupted or restarted</li> <li>Automatic Adjustment: Jobs automatically adapt to file changes without user intervention</li> </ol>"},{"location":"reference/architecture/job-update-system/#how-it-works","title":"How It Works","text":""},{"location":"reference/architecture/job-update-system/#directory-monitoring","title":"Directory Monitoring","text":"<p>The system continuously monitors three key directories:</p> <ul> <li>Wordlists: <code>/data/krakenhashes/wordlists/</code></li> <li>Rules: <code>/data/krakenhashes/rules/</code></li> <li>Potfile: Special handling via staging mechanism</li> </ul> <p>Every 30 seconds (configurable), the directory monitor: 1. Calculates MD5 hashes of all monitored files 2. Compares with previous hashes to detect changes 3. Updates file metadata in the database 4. Triggers job updates for affected jobs</p>"},{"location":"reference/architecture/job-update-system/#change-detection-flow","title":"Change Detection Flow","text":"<pre><code>File Change \u2192 MD5 Hash Comparison \u2192 Metadata Update \u2192 Job Update Service \u2192 Keyspace Recalculation\n</code></pre>"},{"location":"reference/architecture/job-update-system/#wordlist-updates","title":"Wordlist Updates","text":"<p>When a wordlist file changes (words added or removed):</p>"},{"location":"reference/architecture/job-update-system/#for-jobs-without-rule-splitting","title":"For Jobs WITHOUT Rule Splitting","text":"<ol> <li>Base keyspace updates to new word count</li> <li>Effective keyspace recalculates:</li> <li>With rules: <code>new_wordlist_size \u00d7 multiplication_factor</code></li> <li>Without rules: <code>new_wordlist_size</code></li> </ol>"},{"location":"reference/architecture/job-update-system/#for-jobs-with-rule-splitting","title":"For Jobs WITH Rule Splitting","text":"<p>The system accounts for already-dispatched rule chunks:</p> <ol> <li>Calculates theoretical new effective keyspace</li> <li>Determines \"missed\" keyspace: <code>words_added \u00d7 rules_already_dispatched</code></li> <li>Actual effective keyspace: <code>theoretical - missed</code></li> </ol> <p>Example: <pre><code>Original: 1,000,000 words \u00d7 10,000 rules = 10 billion keyspace\nAfter 5,000 rules dispatched, add 100,000 words:\n- Theoretical: 1,100,000 \u00d7 10,000 = 11 billion\n- Missed: 100,000 \u00d7 5,000 = 500 million\n- Actual: 11 billion - 500 million = 10.5 billion\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#rule-updates","title":"Rule Updates","text":"<p>When a rule file changes (rules added or removed):</p>"},{"location":"reference/architecture/job-update-system/#jobs-without-tasks-yet","title":"Jobs Without Tasks Yet","text":"<ul> <li>Simple recalculation: <code>base_keyspace \u00d7 new_rule_count</code></li> <li>Multiplication factor updates to new rule count</li> </ul>"},{"location":"reference/architecture/job-update-system/#jobs-with-existing-tasks","title":"Jobs With Existing Tasks","text":"<p>For rule-splitting jobs: 1. Checks highest dispatched rule index 2. If new rule count \u2264 max dispatched: Job effectively complete 3. Otherwise: Updates multiplication factor and recalculates</p> <p>Example: <pre><code>Original: 10,000 rules, 5,000 dispatched\nRules reduced to 4,000: Job marked complete (all remaining rules gone)\nRules increased to 12,000: 7,000 rules remain to process\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#potfile-updates","title":"Potfile Updates","text":"<p>The potfile (collection of cracked passwords) has special handling:</p>"},{"location":"reference/architecture/job-update-system/#staging-mechanism","title":"Staging Mechanism","text":"<ol> <li>Cracked passwords accumulate in a staging table</li> <li>Periodic or manual refresh moves staged entries to potfile</li> <li>Potfile treated as a special wordlist for job purposes</li> </ol>"},{"location":"reference/architecture/job-update-system/#update-process","title":"Update Process","text":"<ol> <li>Manual Refresh: User triggers from frontend</li> <li>Staging Integration: Moves cracked passwords to main potfile</li> <li>Line Count Update: Updates wordlist metadata</li> <li>Job Updates: Triggers same update logic as regular wordlists</li> </ol>"},{"location":"reference/architecture/job-update-system/#key-differences","title":"Key Differences","text":"<ul> <li>Not monitored by directory monitor (excluded from scans)</li> <li>Updates via database staging, not file watching</li> <li>Requires explicit refresh action</li> <li>Always grows (passwords only added, never removed)</li> </ul>"},{"location":"reference/architecture/job-update-system/#keyspace-recalculation-logic","title":"Keyspace Recalculation Logic","text":""},{"location":"reference/architecture/job-update-system/#basic-formula","title":"Basic Formula","text":"<pre><code>Effective Keyspace = Base Keyspace \u00d7 Multiplication Factor\n</code></pre> <p>Where: - Base Keyspace: Current wordlist size - Multiplication Factor: Number of rules (or 1 if no rules)</p>"},{"location":"reference/architecture/job-update-system/#adjustments-for-dispatched-work","title":"Adjustments for Dispatched Work","text":"<p>For rule-splitting jobs with updates: <pre><code>Adjusted Keyspace = New Effective - (Change \u00d7 Dispatched Rules)\n</code></pre></p> <p>This ensures already-dispatched tasks aren't double-counted.</p>"},{"location":"reference/architecture/job-update-system/#real-world-examples","title":"Real-World Examples","text":""},{"location":"reference/architecture/job-update-system/#scenario-1-growing-wordlist","title":"Scenario 1: Growing Wordlist","text":"<p>Initial State: - Wordlist: 1 million words - Rules: 1,000 - No tasks dispatched yet</p> <p>After Adding 100,000 Words: - New base: 1.1 million - New effective: 1.1 billion - All future tasks use updated wordlist</p>"},{"location":"reference/architecture/job-update-system/#scenario-2-rule-file-expansion-during-execution","title":"Scenario 2: Rule File Expansion During Execution","text":"<p>Initial State: - Job using rule splitting - 10,000 rules, split into 100 chunks - 50 chunks already dispatched (5,000 rules)</p> <p>After Adding 2,000 Rules: - Total rules: 12,000 - Remaining: 7,000 rules (chunks 51-120) - Future chunks use expanded rule set</p>"},{"location":"reference/architecture/job-update-system/#scenario-3-potfile-growth","title":"Scenario 3: Potfile Growth","text":"<p>Initial State: - Potfile job with 1,000 existing passwords - Rules: 500 - Effective keyspace: 500,000</p> <p>After Cracking Campaign: - 200 new passwords cracked - Manual refresh triggered - New base: 1,200 passwords - New effective: 600,000</p>"},{"location":"reference/architecture/job-update-system/#configuration","title":"Configuration","text":""},{"location":"reference/architecture/job-update-system/#directory-monitor-settings","title":"Directory Monitor Settings","text":"<p>Located in backend configuration:</p> Setting Default Description Monitor Interval 30s How often to check for file changes MD5 Hash Check Enabled Method for detecting changes Concurrent Updates Enabled Allow parallel job updates"},{"location":"reference/architecture/job-update-system/#system-behavior-settings","title":"System Behavior Settings","text":"Setting Default Description Auto-update Jobs Enabled Automatically update affected jobs Update Lock Timeout 60s Maximum time to wait for job lock Staging Refresh Interval Manual Potfile staging refresh trigger"},{"location":"reference/architecture/job-update-system/#technical-implementation","title":"Technical Implementation","text":""},{"location":"reference/architecture/job-update-system/#components","title":"Components","text":"<ol> <li>DirectoryMonitorService: Detects file changes via MD5 hashing</li> <li>JobUpdateService: Handles keyspace recalculation logic</li> <li>PotfileService: Manages potfile staging and updates</li> <li>Repository Layer: Database operations for job updates</li> </ol>"},{"location":"reference/architecture/job-update-system/#database-tables-involved","title":"Database Tables Involved","text":"<ul> <li><code>job_executions</code>: Stores base_keyspace, effective_keyspace, multiplication_factor</li> <li><code>job_tasks</code>: Tracks dispatched work (rule_start_index, rule_end_index)</li> <li><code>wordlists</code>: Metadata including word_count, file_hash</li> <li><code>rules</code>: Metadata including rule_count, file_hash</li> <li><code>potfile_staging</code>: Temporary storage for cracked passwords</li> </ul>"},{"location":"reference/architecture/job-update-system/#locking-strategy","title":"Locking Strategy","text":"<p>The system uses per-job locks to prevent race conditions: <pre><code>// Lock specific job during updates\ns.lockJob(jobID)\ndefer s.unlockJob(jobID)\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#best-practices","title":"Best Practices","text":""},{"location":"reference/architecture/job-update-system/#for-users","title":"For Users","text":"<ol> <li>Expect Keyspace Changes: Don't be alarmed if keyspaces update during execution</li> <li>Manual Potfile Refresh: Remember to refresh potfile after cracking campaigns</li> <li>Monitor Progress: Check effective keyspace to understand total work</li> <li>Plan Updates: Large file changes can significantly affect running jobs</li> </ol>"},{"location":"reference/architecture/job-update-system/#for-administrators","title":"For Administrators","text":"<ol> <li>Monitor Disk Space: File updates may require temporary storage</li> <li>Adjust Check Intervals: Balance between responsiveness and system load</li> <li>Review Logs: Check for update failures or lock timeouts</li> <li>Database Maintenance: Ensure potfile staging table doesn't grow too large</li> </ol>"},{"location":"reference/architecture/job-update-system/#for-developers","title":"For Developers","text":"<ol> <li>Respect Forward-Only: Never try to retroactively update dispatched tasks</li> <li>Use Job Locks: Always lock jobs during updates to prevent races</li> <li>Handle Errors Gracefully: File update failures shouldn't crash jobs</li> <li>Test Edge Cases: Consider jobs with no tasks, completed tasks, etc.</li> </ol>"},{"location":"reference/architecture/job-update-system/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/architecture/job-update-system/#common-issues","title":"Common Issues","text":"<p>Keyspace Not Updating: - Verify file actually changed (MD5 hash different) - Check directory monitor is running - Ensure job is in eligible state (pending/running/paused)</p> <p>Incorrect Effective Keyspace: - Verify multiplication_factor is set correctly - Check if job uses rule splitting - Review calculation for \"missed\" keyspace</p> <p>Potfile Not Updating Jobs: - Ensure manual refresh was triggered - Check potfile staging has new entries - Verify job references potfile wordlist</p>"},{"location":"reference/architecture/job-update-system/#debug-logging","title":"Debug Logging","text":"<p>Enable debug logging to trace update flow: <pre><code>DEBUG: Directory monitor detected change\nDEBUG: Handling wordlist update, old: 1000000, new: 1100000\nDEBUG: Updated job keyspace, effective: 1100000000\n</code></pre></p>"},{"location":"reference/architecture/job-update-system/#limitations","title":"Limitations","text":"<ol> <li>No Retroactive Updates: Already-dispatched work won't get new words/rules</li> <li>Forward Progress Only: System doesn't track or compensate for missed combinations</li> <li>Manual Potfile Refresh: Requires user action to trigger potfile updates</li> <li>File Lock Conflicts: Rapid file changes might cause temporary update delays</li> </ol>"},{"location":"reference/architecture/job-update-system/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements under consideration:</p> <ul> <li>Deficit Tracking: Optional mode to track missed combinations</li> <li>Automatic Potfile Refresh: Configurable automatic refresh intervals</li> <li>Smart Chunking: Re-chunk remaining work when files change significantly</li> <li>Update History: Track all keyspace changes for job audit trail</li> <li>Predictive Updates: Estimate impact before applying changes</li> </ul>"},{"location":"reference/architecture/job-update-system/#summary","title":"Summary","text":"<p>The Job Update System ensures KrakenHashes jobs remain accurate and efficient as resources change. By following a forward-only philosophy, it provides a balance between consistency for running tasks and adaptability for future work. Understanding this system helps explain why job keyspaces may change during execution and how the system maintains integrity without disrupting active cracking operations.</p>"},{"location":"reference/architecture/rule-splitting/","title":"Rule Splitting Implementation Summary","text":""},{"location":"reference/architecture/rule-splitting/#current-status","title":"Current Status","text":"<p>The rule splitting feature has been fully implemented but requires testing with a fresh job. The implementation includes:</p>"},{"location":"reference/architecture/rule-splitting/#backend-components","title":"Backend Components","text":"<ol> <li>Database Schema (\u2713 Complete)</li> <li>Added columns to <code>job_executions</code>: <code>uses_rule_splitting</code>, <code>rule_split_count</code>, <code>base_keyspace</code>, <code>effective_keyspace</code>, <code>multiplication_factor</code></li> <li> <p>Added columns to <code>job_tasks</code>: <code>is_rule_split_task</code>, <code>rule_chunk_path</code>, <code>rule_start_index</code>, <code>rule_end_index</code></p> </li> <li> <p>Rule Split Manager (\u2713 Complete)</p> </li> <li><code>RuleSplitManager</code> service that handles splitting rule files into chunks</li> <li>Creates temporary chunk files in <code>/data/krakenhashes/temp/rule_chunks/</code></li> <li> <p>Supports counting rules and creating evenly distributed chunks</p> </li> <li> <p>Job Execution Service (\u2713 Complete)</p> </li> <li><code>calculateEffectiveKeyspace</code> - Calculates virtual keyspace for rules/combination attacks</li> <li><code>determineRuleSplitting</code> - Decides if a job should use rule splitting based on thresholds</li> <li><code>InitializeRuleSplitting</code> - Creates rule chunk tasks when job starts</li> <li> <p><code>GetNextRuleSplitTask</code> - Assigns rule chunks to agents</p> </li> <li> <p>Job Scheduling Service (\u2713 Complete)</p> </li> <li>Enhanced to detect rule-split jobs and initialize splitting on first assignment</li> <li>Routes rule-split jobs through special task assignment logic</li> <li> <p>Syncs rule chunk files to agents</p> </li> <li> <p>WebSocket Integration (\u2713 Complete)</p> </li> <li>Sends rule chunk paths to agents as <code>rules/chunks/&lt;filename&gt;</code></li> <li>Properly handles file sync for rule chunks</li> </ol>"},{"location":"reference/architecture/rule-splitting/#frontend-components","title":"Frontend Components","text":"<ol> <li>Keyspace Display (\u2713 Complete)</li> <li>Shows effective keyspace with multiplication factor badge</li> <li>Tooltips explain virtual keyspace calculation</li> <li> <p>Proper handling of snake_case field names from backend</p> </li> <li> <p>Admin Settings (\u2713 Complete)</p> </li> <li>Rule splitting configuration in Job Execution settings</li> <li>Controls for threshold, min rules, max chunks</li> </ol>"},{"location":"reference/architecture/rule-splitting/#known-issues","title":"Known Issues","text":"<ol> <li>Existing Jobs: Jobs created before the implementation don't have:</li> <li>Effective keyspace calculated</li> <li>Rule splitting decision made</li> <li> <p>This causes them to run with full rule files</p> </li> <li> <p>Rule File Path Resolution: The <code>calculateEffectiveKeyspace</code> method may fail to count rules if the file path resolution doesn't match the actual file location.</p> </li> </ol>"},{"location":"reference/architecture/rule-splitting/#how-rule-splitting-works","title":"How Rule Splitting Works","text":"<ol> <li>When a job is created with attack mode 0 (straight) and rules:</li> <li>Calculate effective keyspace (wordlist \u00d7 rules)</li> <li> <p>If effective keyspace &gt; threshold \u00d7 chunk_duration \u00d7 benchmark_speed AND rules &gt; min_rules:</p> <ul> <li>Mark job with <code>uses_rule_splitting = true</code></li> <li>Calculate optimal number of chunks</li> </ul> </li> <li> <p>When first agent picks up the job:</p> </li> <li><code>InitializeRuleSplitting</code> is called</li> <li>Rule file is split into N chunks</li> <li> <p>N tasks are created, each with a chunk file path</p> </li> <li> <p>Agents receive tasks with:</p> </li> <li>Full wordlist keyspace (no skip/limit)</li> <li>Rule chunk file instead of full rule file</li> <li> <p>Progress tracked per chunk</p> </li> <li> <p>Progress aggregation accounts for:</p> </li> <li>Each chunk processes full wordlist with subset of rules</li> <li>Total progress = sum of (chunk_progress \u00d7 rules_in_chunk)</li> </ol>"},{"location":"reference/architecture/rule-splitting/#testing-the-implementation","title":"Testing the Implementation","text":"<p>To test rule splitting with a new job:</p> <ol> <li>Create a preset job with:</li> <li>Attack mode 0 (straight)</li> <li>A wordlist (e.g., crackstation.txt with 1.2B words)</li> <li> <p>A large rule file (e.g., _nsakey.v2.dive.rule with 123K rules)</p> </li> <li> <p>Create a job execution from this preset</p> </li> <li>The job should be marked with <code>uses_rule_splitting = true</code></li> <li> <p><code>rule_split_count</code> should be calculated (e.g., 415 chunks)</p> </li> <li> <p>When agent picks up the job:</p> </li> <li>Check logs for \"Initializing rule splitting\"</li> <li>Verify chunk files created in temp directory</li> <li>Agent should receive <code>rules/chunks/job_*_chunk_*.rule</code> path</li> </ol>"},{"location":"reference/architecture/rule-splitting/#configuration","title":"Configuration","text":"<p>Current settings (in <code>system_settings</code> table): - <code>rule_split_enabled</code>: true - <code>rule_split_threshold</code>: 2.0 (split if job takes &gt; 2x chunk duration) - <code>rule_split_min_rules</code>: 100 (only split if &gt; 100 rules) - <code>rule_split_max_chunks</code>: 1000 (maximum chunks to create) - <code>rule_chunk_temp_dir</code>: /data/krakenhashes/temp/rule_chunks</p>"},{"location":"reference/architecture/rule-splitting/#agent-expectations","title":"Agent Expectations","text":"<p>Agents expect rule chunks to be available at: - <code>&lt;agent_data_dir&gt;/rules/chunks/&lt;chunk_filename&gt;</code></p> <p>The backend WebSocket integration correctly formats the path as <code>rules/chunks/&lt;filename&gt;</code> when sending task assignments for rule-split jobs.</p>"},{"location":"troubleshooting/potfile-issues/","title":"Troubleshooting Potfile Issues","text":""},{"location":"troubleshooting/potfile-issues/#potfile-preset-job-not-created","title":"Potfile Preset Job Not Created","text":""},{"location":"troubleshooting/potfile-issues/#symptoms","title":"Symptoms","text":"<ul> <li>Potfile wordlist exists in Resources \u2192 Wordlists</li> <li>No \"Potfile Run\" job in Admin \u2192 Preset Jobs</li> <li>Logs show \"No binary versions found\" messages</li> </ul>"},{"location":"troubleshooting/potfile-issues/#cause","title":"Cause","text":"<p>The potfile preset job requires a hashcat binary to be uploaded. On fresh installations, no binaries exist, so the preset job cannot be created due to database constraints (binary_version_id is NOT NULL).</p>"},{"location":"troubleshooting/potfile-issues/#solution","title":"Solution","text":"<ol> <li>Upload a hashcat binary via Admin \u2192 Binary Management</li> <li>Wait for verification to complete</li> <li>Check Admin \u2192 Preset Jobs - \"Potfile Run\" should appear within 5-10 seconds</li> <li>If not visible after 30 seconds, restart the backend service</li> </ol>"},{"location":"troubleshooting/potfile-issues/#prevention","title":"Prevention","text":"<p>Always upload a hashcat binary as the first step after installation. This is documented in the Quick Start and First Crack guides.</p>"},{"location":"troubleshooting/potfile-issues/#verification","title":"Verification","text":"<p>Check system logs for confirmation: <pre><code>docker logs krakenhashes 2&gt;&amp;1 | grep -i \"potfile preset job\"\n</code></pre></p> <p>You should see one of these messages: - \"Successfully created pot-file preset job with ID: [uuid]\" - Success - \"Waiting for binary versions to be added before creating pot-file preset job\" - Still waiting - \"Found existing pot-file preset job with ID\" - Already exists</p>"},{"location":"troubleshooting/potfile-issues/#technical-details","title":"Technical Details","text":"<p>The potfile system initializes in two stages:</p> <ol> <li>Potfile Wordlist Creation (always succeeds):</li> <li>Creates <code>/data/krakenhashes/wordlists/custom/potfile.txt</code></li> <li>Adds wordlist entry to database with <code>is_potfile = true</code></li> <li> <p>Sets system setting <code>potfile_wordlist_id</code></p> </li> <li> <p>Preset Job Creation (requires binary):</p> </li> <li>Attempts to create \"Potfile Run\" preset job</li> <li>Requires <code>binary_version_id</code> (NOT NULL constraint)</li> <li>If no binaries exist, starts background monitor</li> <li>Monitor checks every 5 seconds for binary availability</li> <li>Creates preset job once binary is uploaded</li> </ol>"},{"location":"troubleshooting/potfile-issues/#potfile-not-being-updated","title":"Potfile Not Being Updated","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_1","title":"Symptoms","text":"<ul> <li>Cracked passwords not appearing in potfile</li> <li>Potfile size not increasing</li> <li>Staging table has entries but potfile is unchanged</li> </ul>"},{"location":"troubleshooting/potfile-issues/#common-causes-and-solutions","title":"Common Causes and Solutions","text":"<ol> <li>Potfile Disabled</li> <li>Check: <code>SELECT value FROM system_settings WHERE key = 'potfile_enabled';</code></li> <li>Fix: <code>UPDATE system_settings SET value = 'true' WHERE key = 'potfile_enabled';</code></li> <li> <p>Restart backend service</p> </li> <li> <p>Background Worker Stopped</p> </li> <li>Check logs: <code>docker logs krakenhashes 2&gt;&amp;1 | grep \"pot-file service\"</code></li> <li>Look for: \"Pot-file service started\" vs error messages</li> <li> <p>Fix: Restart backend service</p> </li> <li> <p>Staging Table Processing Issues</p> </li> <li>Check staging count: <code>SELECT COUNT(*) FROM potfile_staging;</code></li> <li>Check for old entries: <code>SELECT COUNT(*) FROM potfile_staging WHERE created_at &lt; NOW() - INTERVAL '1 hour';</code></li> <li> <p>If stuck, manually clear: <code>DELETE FROM potfile_staging WHERE created_at &lt; NOW() - INTERVAL '1 hour';</code></p> </li> <li> <p>File Permission Issues</p> </li> <li>Check file exists: <code>ls -la /data/krakenhashes/wordlists/custom/potfile.txt</code></li> <li>Check permissions allow writing</li> <li>Fix: <code>chmod 644 /data/krakenhashes/wordlists/custom/potfile.txt</code></li> </ol>"},{"location":"troubleshooting/potfile-issues/#potfile-wordlist-shows-wrong-count","title":"Potfile Wordlist Shows Wrong Count","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_2","title":"Symptoms","text":"<ul> <li>Wordlist entry shows incorrect word count</li> <li>Keyspace calculations are wrong</li> <li>Jobs using potfile have incorrect progress</li> </ul>"},{"location":"troubleshooting/potfile-issues/#solution_1","title":"Solution","text":"<p>The system should automatically update counts during batch processing. If not:</p> <ol> <li> <p>Check recent batch processing:    <pre><code>SELECT * FROM wordlists WHERE is_potfile = true;\n</code></pre></p> </li> <li> <p>Manually trigger recount (requires backend restart):    <pre><code>UPDATE wordlists \nSET word_count = (SELECT COUNT(*) FROM potfile_lines),\n    updated_at = NOW()\nWHERE is_potfile = true;\n</code></pre></p> </li> <li> <p>Update preset job keyspace:    <pre><code>UPDATE preset_jobs \nSET keyspace = (SELECT word_count FROM wordlists WHERE is_potfile = true)\nWHERE name = 'Potfile Run';\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/potfile-issues/#duplicate-passwords-in-potfile","title":"Duplicate Passwords in Potfile","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_3","title":"Symptoms","text":"<ul> <li>Same password appears multiple times</li> <li>File size larger than expected</li> <li>Performance degradation</li> </ul>"},{"location":"troubleshooting/potfile-issues/#causes","title":"Causes","text":"<ul> <li>Manual editing of potfile while system is running</li> <li>Database/filesystem sync issues</li> <li>Processing errors</li> </ul>"},{"location":"troubleshooting/potfile-issues/#solution_2","title":"Solution","text":"<ol> <li>Stop the backend service</li> <li>Back up the current potfile</li> <li>Remove duplicates:    <pre><code>sort -u /data/krakenhashes/wordlists/custom/potfile.txt &gt; /tmp/potfile_clean.txt\nmv /tmp/potfile_clean.txt /data/krakenhashes/wordlists/custom/potfile.txt\n</code></pre></li> <li>Update database:    <pre><code>UPDATE wordlists \nSET word_count = (SELECT COUNT(*) FROM potfile_lines),\n    file_size = pg_stat_file('/data/krakenhashes/wordlists/custom/potfile.txt').size,\n    updated_at = NOW()\nWHERE is_potfile = true;\n</code></pre></li> <li>Restart backend service</li> </ol>"},{"location":"troubleshooting/potfile-issues/#monitor-not-creating-preset-job","title":"Monitor Not Creating Preset Job","text":""},{"location":"troubleshooting/potfile-issues/#symptoms_4","title":"Symptoms","text":"<ul> <li>Binary uploaded but preset job still missing</li> <li>Logs show monitor is running but not creating job</li> <li>System seems stuck waiting</li> </ul>"},{"location":"troubleshooting/potfile-issues/#debugging-steps","title":"Debugging Steps","text":"<ol> <li> <p>Check if monitor is running:    <pre><code>docker logs krakenhashes 2&gt;&amp;1 | grep \"monitor for binary versions\"\n</code></pre></p> </li> <li> <p>Check for any binaries:    <pre><code>SELECT id, binary_type, is_active, verification_status \nFROM binary_versions;\n</code></pre></p> </li> <li> <p>Check system settings:    <pre><code>SELECT * FROM system_settings \nWHERE key IN ('potfile_wordlist_id', 'potfile_preset_job_id');\n</code></pre></p> </li> <li> <p>Force retry by clearing preset job ID:    <pre><code>UPDATE system_settings \nSET value = NULL \nWHERE key = 'potfile_preset_job_id';\n</code></pre>    Then restart backend</p> </li> </ol>"},{"location":"troubleshooting/potfile-issues/#manual-creation-last-resort","title":"Manual Creation (Last Resort)","text":"<p>If automatic creation fails, manually create the preset job:</p> <pre><code>-- Get the wordlist ID\nSELECT id FROM wordlists WHERE is_potfile = true;\n\n-- Get a binary version ID\nSELECT id FROM binary_versions WHERE is_active = true LIMIT 1;\n\n-- Create the preset job (replace IDs)\nINSERT INTO preset_jobs (\n    id, name, wordlist_ids, rule_ids, attack_mode, \n    priority, chunk_size_seconds, status_updates_enabled,\n    allow_high_priority_override, binary_version_id, keyspace\n) VALUES (\n    gen_random_uuid(),\n    'Potfile Run',\n    '[\"WORDLIST_ID\"]'::jsonb,  -- Replace WORDLIST_ID\n    '[]'::jsonb,\n    0,  -- Dictionary attack\n    1000,  -- Max priority\n    1200,  -- 20 minute chunks\n    true,\n    true,\n    BINARY_ID,  -- Replace BINARY_ID\n    1  -- Initial keyspace\n);\n\n-- Update system settings with the new ID\nUPDATE system_settings \nSET value = (SELECT id::text FROM preset_jobs WHERE name = 'Potfile Run')\nWHERE key = 'potfile_preset_job_id';\n</code></pre>"},{"location":"troubleshooting/potfile-issues/#best-practices","title":"Best Practices","text":"<ol> <li>Always upload a binary first during initial setup</li> <li>Don't manually edit the potfile while the system is running</li> <li>Monitor staging table size - large backlogs indicate processing issues</li> <li>Check logs regularly for potfile-related errors</li> <li>Keep batch intervals reasonable (30-60 seconds recommended)</li> <li>Archive old potfiles if they grow beyond 1GB</li> </ol>"},{"location":"troubleshooting/potfile-issues/#related-documentation","title":"Related Documentation","text":"<ul> <li>Potfile Management Guide</li> <li>Binary Management</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Learn how to effectively use KrakenHashes for your password auditing needs.</p>"},{"location":"user-guide/#in-this-section","title":"In This Section","text":"<ul> <li> <p> Core Concepts</p> <p>Understand the fundamental concepts and terminology</p> </li> <li> <p> Managing Hashlists</p> <p>Import, organize, and manage your password hashes</p> </li> <li> <p> Jobs &amp; Workflows</p> <p>Create and manage cracking jobs with preset workflows</p> </li> <li> <p> Wordlists &amp; Rules</p> <p>Work with wordlists and transformation rules</p> </li> <li> <p> Analyzing Results</p> <p>Understanding POT files and analyzing cracking results</p> </li> <li> <p> Troubleshooting</p> <p>Common issues and how to resolve them</p> </li> </ul> <p> KrakenHashes dashboard overview featuring the left navigation sidebar, hashlist overview table showing crack progress, and jobs management interface in the dark theme</p>"},{"location":"user-guide/#quick-navigation","title":"Quick Navigation","text":""},{"location":"user-guide/#new-users","title":"New Users","text":"<ol> <li>Start with Core Concepts to understand the system</li> <li>Learn about Managing Hashlists</li> <li>Create your first job with Jobs &amp; Workflows</li> </ol>"},{"location":"user-guide/#common-tasks","title":"Common Tasks","text":"<ul> <li>Upload a hashlist</li> <li>Create a cracking job</li> <li>View cracked passwords</li> <li>Export results</li> </ul>"},{"location":"user-guide/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Custom attack workflows</li> <li>Rule chaining strategies</li> <li>Performance optimization</li> </ul>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<p>Pro Tips</p> <ul> <li>Always start with fast hash types to quickly identify weak passwords</li> <li>Use preset workflows for common scenarios</li> <li>Monitor job progress and adjust priorities as needed</li> <li>Regularly clean up old hashlists to maintain performance</li> </ul>"},{"location":"user-guide/#getting-support","title":"Getting Support","text":"<ul> <li> Review the Troubleshooting Guide</li> <li> Ask questions on Discord</li> <li> Report bugs on GitHub</li> </ul>"},{"location":"user-guide/analyzing-results/","title":"Analyzing Password Cracking Results","text":"<p>This guide covers how to view, analyze, and export password cracking results in KrakenHashes. The system provides comprehensive tools for examining cracked passwords at different levels and exporting data for reporting purposes.</p>"},{"location":"user-guide/analyzing-results/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding POT Files and Results</li> <li>Viewing Results at Different Levels</li> <li>Filtering and Searching Results</li> <li>Export Options and Formats</li> <li>Analyzing Patterns and Statistics</li> <li>Using Results for Reporting</li> <li>Best Practices for Handling Sensitive Data</li> </ul>"},{"location":"user-guide/analyzing-results/#understanding-pot-files-and-results","title":"Understanding POT Files and Results","text":""},{"location":"user-guide/analyzing-results/#what-is-a-pot-file","title":"What is a POT File?","text":"<p>A POT file (short for \"potfile\") is a standard format used by Hashcat and other password cracking tools to store successfully cracked hashes. KrakenHashes maintains a centralized POT database that tracks:</p> <ul> <li>Original Hash: The hash value as it appeared in the source data</li> <li>Password: The plaintext password that was recovered</li> <li>Username: Associated username (if available from the hash format)</li> <li>Hash Type: The algorithm used (MD5, SHA1, NTLM, etc.)</li> <li>Metadata: When the hash was cracked, which job cracked it, etc.</li> </ul>"},{"location":"user-guide/analyzing-results/#how-results-are-stored","title":"How Results are Stored","text":"<p>KrakenHashes stores cracked passwords in a PostgreSQL database rather than traditional POT files. This provides:</p> <ul> <li>Deduplication: Each unique hash is stored only once</li> <li>Performance: Fast queries across millions of cracked hashes</li> <li>Security: Database-level access controls and encryption</li> <li>Rich Metadata: Track which client, hashlist, and job produced each crack</li> </ul>"},{"location":"user-guide/analyzing-results/#viewing-results-at-different-levels","title":"Viewing Results at Different Levels","text":"<p>KrakenHashes provides three levels of result viewing, each offering different perspectives on your cracked passwords:</p>"},{"location":"user-guide/analyzing-results/#1-master-pot-view","title":"1. Master POT View","text":"<p>The Master POT view shows all cracked hashes across your entire KrakenHashes instance.</p> <p>Access: Navigate to POT in the main menu</p> <p> Master POT view showing all cracked hashes with filter options for different export formats</p> <p>Use Cases: - Overall password analysis across all engagements - Building universal password dictionaries - Identifying common passwords across different clients</p>"},{"location":"user-guide/analyzing-results/#2-client-level-results","title":"2. Client-Level Results","text":"<p>View all cracked passwords for a specific client/engagement.</p> <p>Access:  1. Go to Clients 2. Select a client 3. Click View POT or the POT icon</p> <p> Filter results to show only cracked passwords for a specific client or engagement</p> <p>Use Cases: - Client-specific reporting - Understanding password patterns within an organization - Compliance reporting for specific engagements</p>"},{"location":"user-guide/analyzing-results/#3-hashlist-level-results","title":"3. Hashlist-Level Results","text":"<p>View cracked passwords for a specific hashlist.</p> <p>Access: 1. Go to Hashlists 2. Select a hashlist 3. Click View POT in the actions menu</p> <p> View cracked passwords from a particular hashlist with detailed crack information</p> <p>Use Cases: - Analyzing results from specific sources (AD dump, database, etc.) - Comparing crack rates between different hash types - Focused analysis on specific user groups</p>"},{"location":"user-guide/analyzing-results/#4-job-level-results-coming-soon","title":"4. Job-Level Results (Coming Soon)","text":"<p>Future releases will add job-specific result viewing to track which attack strategies were most effective.</p>"},{"location":"user-guide/analyzing-results/#filtering-and-searching-results","title":"Filtering and Searching Results","text":""},{"location":"user-guide/analyzing-results/#search-functionality","title":"Search Functionality","text":"<p>Each POT view includes a search bar that filters results in real-time:</p> <ul> <li>Hash Search: Find specific hash values</li> <li>Password Search: Look for passwords containing specific strings</li> <li>Username Search: Filter by username patterns</li> </ul> <p>Example Searches: <pre><code>admin          # Find all results with \"admin\" in hash, password, or username\npassword123    # Find all instances of this specific password\n@company.com   # Find all email-based usernames from a domain\n</code></pre></p>"},{"location":"user-guide/analyzing-results/#pagination-options","title":"Pagination Options","text":"<p>Control how many results are displayed:</p> <ul> <li>500 (Default): Good balance of performance and visibility</li> <li>1000, 1500, 2000: Larger result sets for analysis</li> <li>All: Load entire dataset (use with caution on large sets)</li> </ul> <p>Warning: Loading all results may impact browser performance with datasets over 50,000 entries.</p>"},{"location":"user-guide/analyzing-results/#export-options-and-formats","title":"Export Options and Formats","text":"<p>KrakenHashes supports multiple export formats optimized for different use cases:</p>"},{"location":"user-guide/analyzing-results/#available-export-formats","title":"Available Export Formats","text":""},{"location":"user-guide/analyzing-results/#1-hashpass-format-filename-h-plst","title":"1. Hash:Pass Format (<code>filename-h-p.lst</code>)","text":"<p><pre><code>5f4dcc3b5aa765d61d8327deb882cf99:password\ne10adc3949ba59abbe56e057f20f883e:123456\n</code></pre> Use Cases:  - Re-importing into other tools - Hash verification - Building POT files</p>"},{"location":"user-guide/analyzing-results/#2-userpass-format-filename-u-plst","title":"2. User:Pass Format (<code>filename-u-p.lst</code>)","text":"<p><pre><code>admin:password123\njohn.doe:Summer2024!\nmary.smith:Welcome123\n</code></pre> Use Cases: - Password spraying lists - User-specific analysis - Account takeover testing</p>"},{"location":"user-guide/analyzing-results/#3-username-only-filename-ulst","title":"3. Username Only (<code>filename-u.lst</code>)","text":"<p><pre><code>admin\njohn.doe\nmary.smith\n</code></pre> Use Cases: - Username enumeration - Account existence validation - User list generation</p>"},{"location":"user-guide/analyzing-results/#4-password-only-filename-plst","title":"4. Password Only (<code>filename-p.lst</code>)","text":"<p><pre><code>password123\nSummer2024!\nWelcome123\n</code></pre> Use Cases: - Wordlist generation - Password frequency analysis - Rule generation input</p>"},{"location":"user-guide/analyzing-results/#export-scope","title":"Export Scope","text":"<p>Exports can be performed at different levels: - Current View: Export only visible/filtered results - Full Dataset: Export all results for the current context (client/hashlist/master)</p>"},{"location":"user-guide/analyzing-results/#practical-export-examples","title":"Practical Export Examples","text":""},{"location":"user-guide/analyzing-results/#building-a-client-specific-wordlist","title":"Building a Client-Specific Wordlist","text":"<ol> <li>Navigate to client POT view</li> <li>Click Download \u2192 Password</li> <li>Use the exported list as input for future attacks</li> </ol>"},{"location":"user-guide/analyzing-results/#creating-credential-lists-for-testing","title":"Creating Credential Lists for Testing","text":"<ol> <li>Go to hashlist POT view</li> <li>Ensure hashlist contains usernames</li> <li>Click Download \u2192 User:Pass</li> <li>Import into your testing tools</li> </ol>"},{"location":"user-guide/analyzing-results/#analyzing-patterns-and-statistics","title":"Analyzing Patterns and Statistics","text":""},{"location":"user-guide/analyzing-results/#current-statistics","title":"Current Statistics","text":"<p>Each view displays: - Total Cracked: Number of successfully cracked hashes - Crack Rate: Percentage of hashes cracked (shown in hashlist view) - Result Distribution: Visual breakdown by hash type</p>"},{"location":"user-guide/analyzing-results/#pattern-analysis-techniques","title":"Pattern Analysis Techniques","text":""},{"location":"user-guide/analyzing-results/#1-password-complexity-analysis","title":"1. Password Complexity Analysis","text":"<p>Export passwords and analyze with external tools: <pre><code># Length distribution\ncat client-p.lst | awk '{print length}' | sort -n | uniq -c\n\n# Character set analysis\ncat client-p.lst | grep -E '^[a-z]+$' | wc -l  # lowercase only\ncat client-p.lst | grep -E '^[A-Za-z0-9]+$' | wc -l  # alphanumeric\n</code></pre></p>"},{"location":"user-guide/analyzing-results/#2-common-password-identification","title":"2. Common Password Identification","text":"<pre><code># Top 20 most common passwords\ncat client-p.lst | sort | uniq -c | sort -rn | head -20\n</code></pre>"},{"location":"user-guide/analyzing-results/#3-username-password-relationship","title":"3. Username-Password Relationship","text":"<pre><code># Find passwords containing username\nwhile IFS=: read user pass; do\n  echo \"$pass\" | grep -i \"$user\" &amp;&amp; echo \"$user:$pass\"\ndone &lt; client-u-p.lst\n</code></pre>"},{"location":"user-guide/analyzing-results/#advanced-analysis-with-sql","title":"Advanced Analysis with SQL","text":"<p>For advanced users with database access:</p> <pre><code>-- Password length distribution\nSELECT LENGTH(password) as pwd_length, COUNT(*) as count\nFROM hashes\nWHERE is_cracked = true\nGROUP BY pwd_length\nORDER BY pwd_length;\n\n-- Most common passwords by client\nSELECT h.password, COUNT(*) as usage_count\nFROM hashes h\nJOIN hashlist_hashes hh ON h.id = hh.hash_id\nJOIN hashlists hl ON hh.hashlist_id = hl.id\nWHERE hl.client_id = 'CLIENT_UUID_HERE'\n  AND h.is_cracked = true\nGROUP BY h.password\nORDER BY usage_count DESC\nLIMIT 50;\n</code></pre>"},{"location":"user-guide/analyzing-results/#using-results-for-reporting","title":"Using Results for Reporting","text":""},{"location":"user-guide/analyzing-results/#executive-summary-data","title":"Executive Summary Data","text":"<p>Extract key metrics for reports:</p> <ol> <li>Overall Statistics</li> <li>Total unique passwords cracked</li> <li>Crack rate percentage</li> <li> <p>Time to first/last crack</p> </li> <li> <p>Risk Indicators</p> </li> <li>Passwords meeting complexity requirements</li> <li>Default/common passwords found</li> <li> <p>Password reuse across accounts</p> </li> <li> <p>Compliance Metrics</p> </li> <li>Accounts with weak passwords</li> <li>Policy compliance rates</li> <li>High-privilege accounts with poor passwords</li> </ol>"},{"location":"user-guide/analyzing-results/#generating-report-data","title":"Generating Report Data","text":""},{"location":"user-guide/analyzing-results/#powershell-script-example","title":"PowerShell Script Example","text":"<pre><code># Import cracked credentials\n$creds = Import-Csv \"client-u-p.lst\" -Delimiter \":\" -Header \"Username\",\"Password\"\n\n# Analyze password policies\n$results = $creds | ForEach-Object {\n    [PSCustomObject]@{\n        Username = $_.Username\n        Length = $_.Password.Length\n        HasUppercase = $_.Password -cmatch '[A-Z]'\n        HasLowercase = $_.Password -cmatch '[a-z]'\n        HasNumbers = $_.Password -match '\\d'\n        HasSpecial = $_.Password -match '[^a-zA-Z0-9]'\n        MeetsPolicy = (\n            $_.Password.Length -ge 8 -and\n            $_.Password -cmatch '[A-Z]' -and\n            $_.Password -cmatch '[a-z]' -and\n            $_.Password -match '\\d'\n        )\n    }\n}\n\n# Generate statistics\n$stats = @{\n    TotalAccounts = $results.Count\n    MeetsPolicy = ($results | Where-Object MeetsPolicy).Count\n    AverageLength = ($results | Measure-Object -Property Length -Average).Average\n}\n</code></pre>"},{"location":"user-guide/analyzing-results/#visualization-recommendations","title":"Visualization Recommendations","text":"<p>Create impactful visualizations using exported data:</p> <ol> <li>Password Length Distribution - Bar chart</li> <li>Crack Timeline - Line graph showing cracks over time</li> <li>Complexity Heatmap - Visual grid of password characteristics</li> <li>Top 10 Passwords - Horizontal bar chart</li> </ol>"},{"location":"user-guide/analyzing-results/#best-practices-for-handling-sensitive-data","title":"Best Practices for Handling Sensitive Data","text":""},{"location":"user-guide/analyzing-results/#security-considerations","title":"Security Considerations","text":"<ol> <li>Access Control</li> <li>Limit POT access to authorized personnel only</li> <li>Use role-based permissions for different result levels</li> <li> <p>Audit all exports and downloads</p> </li> <li> <p>Data Handling</p> </li> <li>Never store exported credentials in unencrypted formats</li> <li>Use secure deletion for temporary export files</li> <li> <p>Implement retention policies for cracked passwords</p> </li> <li> <p>Client Confidentiality</p> </li> <li>Keep client results strictly separated</li> <li>Never mix credentials between engagements</li> <li>Sanitize data before sharing externally</li> </ol>"},{"location":"user-guide/analyzing-results/#operational-security","title":"Operational Security","text":""},{"location":"user-guide/analyzing-results/#secure-export-workflow","title":"Secure Export Workflow","text":"<pre><code># 1. Export to encrypted container\nkrakenhashes-export | gpg -c &gt; results.gpg\n\n# 2. Verify encryption\ngpg --list-packets results.gpg\n\n# 3. Secure transfer\nscp results.gpg user@secure-host:/encrypted/\n\n# 4. Secure deletion\nshred -vfz original-export.lst\n</code></pre>"},{"location":"user-guide/analyzing-results/#audit-trail-maintenance","title":"Audit Trail Maintenance","text":"<ul> <li>Log all POT access with timestamps</li> <li>Track export operations with user attribution</li> <li>Monitor for unusual access patterns</li> </ul>"},{"location":"user-guide/analyzing-results/#compliance-considerations","title":"Compliance Considerations","text":"<ol> <li>Data Retention</li> <li>Define retention periods per client agreement</li> <li>Implement automatic purging of old results</li> <li> <p>Document retention policies</p> </li> <li> <p>Reporting Requirements</p> </li> <li>Redact sensitive information in reports</li> <li>Use password hashes in documentation, not plaintexts</li> <li> <p>Implement need-to-know access controls</p> </li> <li> <p>Cross-Border Considerations</p> </li> <li>Be aware of data residency requirements</li> <li>Implement geographic access controls if needed</li> <li>Consider encryption-at-rest requirements</li> </ol>"},{"location":"user-guide/analyzing-results/#incident-response","title":"Incident Response","text":"<p>If credentials are accidentally exposed:</p> <ol> <li>Immediate Actions</li> <li>Revoke access to affected systems</li> <li>Force password resets for exposed accounts</li> <li> <p>Document the incident</p> </li> <li> <p>Investigation</p> </li> <li>Review access logs</li> <li>Identify scope of exposure</li> <li> <p>Determine root cause</p> </li> <li> <p>Remediation</p> </li> <li>Update access controls</li> <li>Implement additional monitoring</li> <li>Review and update procedures</li> </ol>"},{"location":"user-guide/analyzing-results/#summary","title":"Summary","text":"<p>Effective analysis of password cracking results requires understanding the available tools, choosing appropriate export formats, and maintaining strict security controls. KrakenHashes provides flexible viewing and export options while maintaining the security and segregation necessary for professional password auditing services.</p> <p>Remember that cracked passwords represent sensitive security information that must be handled with appropriate care and professionalism. Always follow your organization's data handling policies and client agreements when working with password cracking results.</p>"},{"location":"user-guide/core-concepts/","title":"KrakenHashes Core Concepts Guide","text":""},{"location":"user-guide/core-concepts/#overview","title":"Overview","text":"<p>KrakenHashes is a distributed password cracking management system that orchestrates multiple agents running hashcat to efficiently crack password hashes. This guide explains the fundamental concepts and terminology you need to understand to effectively use the system.</p>"},{"location":"user-guide/core-concepts/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Key Terminology</li> <li>System Architecture</li> <li>Job Execution Flow</li> <li>Priority System</li> <li>Chunking and Distribution</li> <li>File Management</li> <li>Result Handling</li> </ul>"},{"location":"user-guide/core-concepts/#key-terminology","title":"Key Terminology","text":""},{"location":"user-guide/core-concepts/#hashlist","title":"Hashlist","text":"<p>A hashlist is a collection of password hashes uploaded to the system for cracking. Each hashlist: - Contains one or more individual hashes of the same type (MD5, NTLM, SHA1, etc.) - Has a lifecycle: <code>uploading</code> \u2192 <code>processing</code> \u2192 <code>ready</code> (or <code>error</code>) - Can be associated with a client for engagement tracking - Tracks total hashes and cracked count - May include usernames in formats like <code>username:hash</code></p>"},{"location":"user-guide/core-concepts/#hash","title":"Hash","text":"<p>A hash is an individual password hash within a hashlist. Each hash: - Contains the encrypted password value to be cracked - May include an associated username (preserved from original line format) - Tracks whether it has been cracked and its plaintext value - Can appear in multiple hashlists (deduplication handled by full line, not just hash value) - Cross-hashlist updates: When a hash value is cracked, ALL hashes with that same value are updated across all hashlists</p>"},{"location":"user-guide/core-concepts/#preset-job","title":"Preset Job","text":"<p>A preset job is a pre-configured attack strategy that defines: - Which wordlists to use - Which rule files to apply - The hashcat attack mode (dictionary, brute-force, hybrid, etc.) - Priority level (0-1000, higher = more important) - Chunk duration (how long each task should run) - Maximum number of agents allowed to work on it - The specific hashcat binary version to use</p>"},{"location":"user-guide/core-concepts/#job-workflow","title":"Job Workflow","text":"<p>A job workflow is a named sequence of preset jobs executed in order. Workflows enable: - Systematic attack progression (e.g., common passwords \u2192 rules \u2192 brute force) - Reusable attack strategies across different hashlists - Automated execution of multiple attack phases</p>"},{"location":"user-guide/core-concepts/#job-execution","title":"Job Execution","text":"<p>A job execution is an actual running instance of a preset job against a specific hashlist. It: - Tracks the overall status: <code>pending</code>, <code>running</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code>, <code>interrupted</code> - Manages keyspace progress (how much of the search space has been processed) - Supports dynamic rule splitting for large rule files - Can be interrupted by higher priority jobs and automatically resumed - Tracks which user created it and when</p>"},{"location":"user-guide/core-concepts/#job-interruption-behavior","title":"Job Interruption Behavior","text":"<p>When a job is interrupted by a higher priority job: - Status changes from <code>running</code> to <code>pending</code> (not <code>paused</code>) - All progress is preserved and saved - Job automatically returns to the queue with the same priority - When agents become available, the job resumes from where it stopped - No manual intervention required - the system handles everything automatically</p>"},{"location":"user-guide/core-concepts/#job-task-chunk","title":"Job Task (Chunk)","text":"<p>A job task or chunk is an individual unit of work assigned to an agent. Tasks are: - Time-based chunks (e.g., 10-minute segments of work) - Defined by keyspace ranges (start/end positions in the search space) - Self-contained with the complete hashcat command - Tracked for progress and can be retried on failure - For rule-based attacks, may represent a subset of rules</p>"},{"location":"user-guide/core-concepts/#agent","title":"Agent","text":"<p>An agent is a compute node that executes hashcat commands. Agents: - Run on systems with GPUs or CPUs capable of password cracking - Connect via WebSocket for real-time communication - Report hardware capabilities (GPU types, benchmark speeds) - Can be scheduled for specific time windows - Are authenticated via API keys and claim codes - Can be owned by specific users or shared within teams</p>"},{"location":"user-guide/core-concepts/#pot-file","title":"Pot File","text":"<p>A pot file (short for \"potfile\") is hashcat's database of cracked hashes. In KrakenHashes: - Each successful crack is immediately synchronized to the backend - Results are shared across all agents and jobs - The system maintains a centralized view of all cracked hashes - Previously cracked hashes are automatically filtered from new jobs</p>"},{"location":"user-guide/core-concepts/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Web Frontend  \u2502     \u2502  Backend API    \u2502     \u2502   PostgreSQL    \u2502\n\u2502   (React/TS)    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   (Go REST)     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Database      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    WebSocket    \u2502\n                    Connection   \u2502\n                                 \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Distributed Agents \u2502\n                    \u2502    (Go + Hashcat)   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/core-concepts/#component-responsibilities","title":"Component Responsibilities","text":"<ol> <li>Frontend: User interface for uploading hashlists, creating jobs, monitoring progress</li> <li>Backend: Orchestrates job scheduling, manages data, handles agent communication</li> <li>Database: Stores all persistent data (users, hashlists, jobs, results)</li> <li>Agents: Execute hashcat commands and report results back to the backend</li> </ol>"},{"location":"user-guide/core-concepts/#job-execution-flow","title":"Job Execution Flow","text":""},{"location":"user-guide/core-concepts/#1-job-creation-phase","title":"1. Job Creation Phase","text":"<pre><code>User uploads hashlist \u2192 System processes hashes \u2192 Hashlist marked as \"ready\"\n                                    \u2193\nUser creates job execution \u2192 Selects preset job \u2192 Sets priority\n                                    \u2193\nSystem calculates keyspace \u2192 Creates job execution record\n</code></pre>"},{"location":"user-guide/core-concepts/#2-task-generation-phase","title":"2. Task Generation Phase","text":"<pre><code>Job execution created \u2192 System analyzes attack parameters\n                               \u2193\n        Calculate total keyspace (wordlist \u00d7 rules)\n                               \u2193\n    Generate time-based chunks using agent benchmarks\n                               \u2193\n          Create job tasks in \"pending\" status\n</code></pre>"},{"location":"user-guide/core-concepts/#3-assignment-phase","title":"3. Assignment Phase","text":"<pre><code>Agent requests work \u2192 Scheduler checks available jobs (priority order)\n                                    \u2193\n              Find highest priority job with pending tasks\n                                    \u2193\n          Check agent capabilities and assign compatible task\n                                    \u2193\n               Agent downloads required files if needed\n</code></pre>"},{"location":"user-guide/core-concepts/#4-execution-phase","title":"4. Execution Phase","text":"<pre><code>Agent receives task \u2192 Builds hashcat command \u2192 Starts execution\n                                \u2193\n                    Reports progress every 5 seconds\n                                \u2193\n            Backend updates task and job execution progress\n                                \u2193\n                  Cracked hashes synchronized in real-time\n</code></pre>"},{"location":"user-guide/core-concepts/#5-completion-phase","title":"5. Completion Phase","text":"<pre><code>Task completes \u2192 Agent reports final status \u2192 Backend updates records\n                                \u2193\n                Check if more tasks remain for job\n                                \u2193\n    If no tasks remain \u2192 Mark job execution as completed\n                                \u2193\n            Update hashlist cracked count\n</code></pre>"},{"location":"user-guide/core-concepts/#priority-system","title":"Priority System","text":"<p>KrakenHashes uses a priority scale from 0 to 1000, where: - 1000 = Highest priority (urgent/critical jobs) - 500 = Normal priority (default for most jobs) - 0 = Lowest priority (background/research jobs)</p>"},{"location":"user-guide/core-concepts/#priority-behavior","title":"Priority Behavior","text":"<ol> <li>Job Selection: When agents request work, jobs are assigned in priority order</li> <li>FIFO Within Priority: Jobs with the same priority follow First-In-First-Out</li> <li>Job Interruption: Higher priority jobs with override enabled can interrupt lower priority running jobs</li> <li>Resource Allocation: High priority jobs can use more agents simultaneously</li> <li>Automatic Resumption: Interrupted jobs automatically resume when resources are available</li> </ol>"},{"location":"user-guide/core-concepts/#priority-guidelines","title":"Priority Guidelines","text":"<ul> <li>900-1000: Time-critical engagements, incident response</li> <li>600-899: Active client engagements with deadlines</li> <li>400-599: Standard testing and assessments</li> <li>100-399: Research and development tasks</li> <li>0-99: Background processing, long-running attacks</li> </ul>"},{"location":"user-guide/core-concepts/#chunking-and-distribution","title":"Chunking and Distribution","text":""},{"location":"user-guide/core-concepts/#time-based-chunking","title":"Time-Based Chunking","text":"<p>KrakenHashes uses time-based chunks rather than fixed keyspace divisions:</p> <pre><code>Total Keyspace: 1,000,000,000 candidates\nAgent Benchmark: 10,000,000 hashes/second\nChunk Duration: 600 seconds (10 minutes)\n\nChunk Size = Benchmark \u00d7 Duration = 6,000,000,000 candidates\nNumber of Chunks = Total Keyspace \u00f7 Chunk Size = 167 chunks\n</code></pre>"},{"location":"user-guide/core-concepts/#benefits-of-time-based-chunks","title":"Benefits of Time-Based Chunks","text":"<ol> <li>Predictable Duration: Each chunk runs for approximately the same time</li> <li>Fair Distribution: Fast and slow agents get appropriately sized work</li> <li>Better Scheduling: Easier to estimate completion times</li> <li>Checkpoint Recovery: Regular checkpoints minimize lost work</li> </ol>"},{"location":"user-guide/core-concepts/#rule-splitting","title":"Rule Splitting","text":"<p>For attacks using large rule files:</p> <pre><code>Wordlist: 10,000 words\nRules: 100,000 rules\nEffective Keyspace: 1,000,000,000 (words \u00d7 rules)\n\nInstead of processing all rules at once:\n- Split into chunks of 1,000 rules each\n- Create 100 separate tasks\n- Each task processes: 10,000 words \u00d7 1,000 rules\n</code></pre>"},{"location":"user-guide/core-concepts/#file-management","title":"File Management","text":""},{"location":"user-guide/core-concepts/#storage-hierarchy","title":"Storage Hierarchy","text":"<pre><code>/data/krakenhashes/\n\u251c\u2500\u2500 binaries/          # Hashcat executables (multiple versions)\n\u251c\u2500\u2500 wordlists/         # Dictionary files\n\u2502   \u251c\u2500\u2500 general/       # Common password lists\n\u2502   \u251c\u2500\u2500 specialized/   # Domain-specific lists\n\u2502   \u2514\u2500\u2500 custom/        # User-uploaded lists\n\u251c\u2500\u2500 rules/             # Rule files for mutations\n\u2502   \u251c\u2500\u2500 hashcat/       # Hashcat-format rules\n\u2502   \u2514\u2500\u2500 custom/        # User-created rules\n\u251c\u2500\u2500 hashlists/         # Uploaded hash files\n\u2502   \u2514\u2500\u2500 {hashlist_id}/ # Organized by hashlist ID\n\u2514\u2500\u2500 temp/              # Temporary files (rule chunks, etc.)\n</code></pre>"},{"location":"user-guide/core-concepts/#file-synchronization","title":"File Synchronization","text":"<ol> <li>Lazy Sync: Agents download files only when needed</li> <li>Hash Verification: MD5 checksums ensure file integrity</li> <li>Local Caching: Agents cache files to avoid re-downloading</li> <li>Automatic Cleanup: Temporary files removed after job completion</li> </ol>"},{"location":"user-guide/core-concepts/#result-handling","title":"Result Handling","text":""},{"location":"user-guide/core-concepts/#real-time-crack-synchronization","title":"Real-Time Crack Synchronization","text":"<pre><code>Agent cracks hash \u2192 Sends result to backend \u2192 Backend updates database\n                            \u2193\n        Broadcast to other agents working on same hashlist\n                            \u2193\n            Update hashlist statistics\n                            \u2193\n        Notify connected web clients\n</code></pre>"},{"location":"user-guide/core-concepts/#result-storage","title":"Result Storage","text":"<p>Each cracked hash stores: - Original hash value - Plaintext password - Username (if available) - Crack timestamp - Which job/task found it - Position in keyspace where found</p>"},{"location":"user-guide/core-concepts/#pot-file-management","title":"Pot File Management","text":"<ul> <li>Centralized Pot: Backend maintains master record of all cracks</li> <li>Agent Sync: Agents receive relevant cracks for their current hashlist</li> <li>Deduplication by Line: Each unique input line preserved; duplicates by hash value automatically updated when cracked</li> <li>Cross-Hashlist: Cracks are automatically applied to all hashlists containing that hash value</li> <li>Username Preservation: Multiple users with same password (e.g., \"Administrator\", \"Administrator1\") tracked separately</li> </ul>"},{"location":"user-guide/core-concepts/#result-access","title":"Result Access","text":"<p>Users can access results through: 1. Hashlist View: See all cracked hashes for a specific hashlist 2. Pot File Export: Download results in hashcat pot format 3. Client Reports: Generate reports filtered by client/engagement 4. Real-Time Updates: Live view of cracks as they happen</p>"},{"location":"user-guide/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/core-concepts/#job-design","title":"Job Design","text":"<ol> <li>Start with fast attacks (common passwords, small rules)</li> <li>Progress to more intensive attacks (large wordlists, complex rules)</li> <li>Use workflows to automate multi-stage attacks</li> <li>Set appropriate priorities based on urgency</li> </ol>"},{"location":"user-guide/core-concepts/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Use appropriate chunk durations (5-15 minutes typically)</li> <li>Limit max agents for jobs that don't scale well</li> <li>Schedule large jobs during off-peak hours</li> <li>Monitor agent efficiency and adjust benchmarks</li> </ol>"},{"location":"user-guide/core-concepts/#resource-management","title":"Resource Management","text":"<ol> <li>Organize wordlists by effectiveness</li> <li>Test and optimize custom rules</li> <li>Regular cleanup of old hashlists (retention policies)</li> <li>Monitor storage usage</li> </ol>"},{"location":"user-guide/core-concepts/#conclusion","title":"Conclusion","text":"<p>Understanding these core concepts enables you to effectively use KrakenHashes for distributed password cracking. The system handles the complexity of distributing work, managing results, and coordinating agents, allowing you to focus on designing effective attack strategies and analyzing results.</p> <p>For more detailed information on specific features, refer to the appropriate sections of the user guide.</p>"},{"location":"user-guide/hashlists/","title":"Hashlists","text":"<p>Hashlists are fundamental to KrakenHashes, representing collections of hashes uploaded for cracking jobs or analysis. This document outlines how hashlists are uploaded, processed, and managed within the system.</p>"},{"location":"user-guide/hashlists/#overview","title":"Overview","text":"<ul> <li>Definition: A hashlist is a file containing multiple lines, where each line typically represents a single hash (and potentially its cracked password).</li> <li>Association: Each hashlist is associated with a specific Hash Type (e.g., NTLM, SHA1) and can optionally be linked to a Client/Engagement.</li> <li>Processing: Uploaded hashlists undergo an asynchronous background processing workflow to ingest the hashes into the central database.</li> <li>Storage: Hashlist files are stored on the backend server in a configured directory.</li> </ul>"},{"location":"user-guide/hashlists/#uploading-hashlists","title":"Uploading Hashlists","text":"<p>Hashlists are typically uploaded through the frontend UI.</p> <ol> <li>Navigate: Go to the \"Hashlists\" section of the dashboard.</li> </ol> <p> Hashlist Management page showing the upload interface with UPLOAD HASHLIST button and data table displaying hashlist details including Name, Client, Status, Total Hashes, Cracked percentages, and Created dates</p> <ol> <li>Initiate Upload: Click the \"Upload Hashlist\" button.</li> <li>Fill Details: In the dialog, provide:<ul> <li>Name: A descriptive name for the hashlist.</li> <li>Hash Type: Select the correct hash type from the dropdown. This list is populated from the <code>hash_types</code> table in the database (see Hash Types below). The format displayed is <code>ID - Name</code> (e.g., <code>1000 - NTLM</code>).</li> <li>Client: (Optional) Select an existing client to associate this hashlist with, or create a new one on the fly.</li> <li>File: Choose the hashlist file from your local machine.</li> </ul> </li> <li>Submit: Click the upload button in the dialog.</li> </ol>"},{"location":"user-guide/hashlists/#api-endpoint","title":"API Endpoint","text":"<p>The frontend interacts with the <code>POST /api/hashlists</code> endpoint. This endpoint expects a <code>multipart/form-data</code> request containing the fields mentioned above (name, hash_type_id, client_id) and the hashlist file itself.</p>"},{"location":"user-guide/hashlists/#file-storage","title":"File Storage","text":"<ul> <li>Uploaded hashlist files are stored on the backend server.</li> <li>The base directory for uploads is configured via the <code>KH_DATA_DIR</code> environment variable.</li> <li>Within the data directory, hashlists are stored in a specific subdirectory, typically <code>hashlist_uploads</code>, but configurable via <code>KH_HASH_UPLOAD_DIR</code>.</li> <li>The maximum allowed upload size is determined by the <code>KH_MAX_UPLOAD_SIZE_MB</code> environment variable (default: 32 MiB).</li> </ul>"},{"location":"user-guide/hashlists/#hashlist-processing","title":"Hashlist Processing","text":"<p>Once a hashlist file is uploaded and initial metadata is saved, it enters an asynchronous processing queue.</p>"},{"location":"user-guide/hashlists/#status-workflow","title":"Status Workflow","text":"<p>A hashlist progresses through the following statuses:</p> <ol> <li><code>uploading</code>: Initial state when the upload request is received.</li> <li><code>processing</code>: The backend worker has picked up the hashlist and is actively reading the file and ingesting hashes.</li> <li><code>ready</code>: Processing completed successfully. All valid lines have been processed and stored. The hashlist is now available for use in cracking jobs.</li> <li><code>ready_with_errors</code>: Processing finished, but one or more lines in the file could not be processed correctly (e.g., invalid format for the selected hash type). Valid lines were still ingested. Check backend logs for details on specific line errors. (Not fully implemented)    `</li> <li><code>error</code>: A fatal error occurred during processing (e.g., file unreadable, database error during batch insert). The <code>error_message</code> field on the hashlist provides a general reason. Check backend logs for more details.</li> </ol>"},{"location":"user-guide/hashlists/#processing-steps","title":"Processing Steps","text":"<p>The backend processor performs the following steps:</p> <ol> <li>Fetch Details: Retrieves the hashlist metadata (ID, file path, hash type ID) from the database.</li> <li>Open File: Opens the stored hashlist file.</li> <li>Scan Line by Line: Reads the file line by line.<ul> <li>Empty lines are skipped.</li> <li>Lines starting with <code>#</code> are treated as comments and skipped.</li> </ul> </li> <li>Extract Hash/Password:<ul> <li>Default: Checks for a colon (<code>:</code>) separator. If found, the part before the colon is treated as the hash, and the part after is treated as the pre-cracked password (<code>is_cracked</code> = true). If no colon is found, the entire line is treated as the hash (<code>is_cracked</code> = false).</li> <li>Type-Specific Processing: For certain hash types (e.g., <code>1000 - NTLM</code>), specific processing logic might be applied to extract the canonical hash format from more complex lines (like <code>user:sid:LM:NT:::</code>). This logic is determined by the <code>needs_processing</code> flag and potentially the <code>processing_logic</code> field in the <code>hash_types</code> table.</li> </ul> </li> <li>Batching: Hashes are collected into batches (size configured by <code>KH_HASHLIST_BATCH_SIZE</code>, default: 1000).</li> <li>Database Insertion with Deduplication: Each batch is processed:<ul> <li>Deduplication Strategy: The system deduplicates by <code>original_hash</code> (the complete input line), not just by <code>hash_value</code>. This ensures that different users with the same password hash are preserved as separate entries.<ul> <li>Example: Lines like <code>Administrator:...:hash123</code>, <code>Administrator1:...:hash123</code>, and <code>Administrator2:...:hash123</code> are all stored as distinct hash records.</li> </ul> </li> <li>The system checks if any hashes in the batch already exist in the central <code>hashes</code> table (based on <code>original_hash</code> and hash type ID).</li> <li>New, unique hashes are inserted into the <code>hashes</code> table with both <code>hash_value</code> and <code>original_hash</code>.</li> <li>Entries are created in the <code>hashlist_hashes</code> join table to link both new and existing hashes from the batch to the current hashlist.</li> <li>Cross-Hashlist Crack Propagation: When a hash is cracked, ALL hashes with the same <code>hash_value</code> (across all hashlists) are automatically marked as cracked. This means if \"Administrator\", \"Administrator1\", and \"Administrator2\" share the same password, cracking one updates all three.</li> <li>If a hash being added includes a pre-cracked password, the corresponding record in the <code>hashes</code> table is updated (<code>is_cracked</code>=true, <code>password</code>=...).</li> </ul> </li> <li>Update Status: Once the entire file is processed, the hashlist status is updated to <code>ready</code>, <code>ready_with_errors</code>, or <code>error</code>, along with the final <code>total_hashes</code> and <code>cracked_hashes</code> counts.</li> </ol>"},{"location":"user-guide/hashlists/#efficient-hashcat-processing","title":"Efficient Hashcat Processing","text":"<p>When generating hashlist files for hashcat: *   DISTINCT Query: The system uses a DISTINCT query on <code>hash_value</code> to prevent sending duplicate password hashes to hashcat. Even if multiple users share the same password, hashcat only needs to crack it once. *   Ordering: Results are ordered by <code>hash_value</code> for stable, consistent output.</p>"},{"location":"user-guide/hashlists/#supported-input-formats","title":"Supported Input Formats","text":"<p>The processor primarily expects:</p> <ul> <li>One hash per line.</li> <li>Optional: <code>hash:password</code> format for lines containing already cracked hashes.</li> <li>Lines starting with <code>#</code> are ignored.</li> <li>Empty lines are ignored.</li> <li>Specific formats handled by type-specific processors (e.g., NTLM).</li> </ul>"},{"location":"user-guide/hashlists/#hash-types","title":"Hash Types","text":"<ul> <li>Supported hash types are defined in the <code>hash_types</code> database table.</li> <li>This table is populated by a database migration (<code>000016_add_hashcat_hash_types.up.sql</code>) which includes common types and examples sourced from the Hashcat wiki.</li> <li>Each type has an ID (corresponding to the Hashcat mode), Name, Description (optional), Example (optional), and flags indicating if it needs special processing (<code>needs_processing</code>, <code>processing_logic</code>) or is enabled (<code>is_enabled</code>).</li> <li>The frontend uses the <code>GET /api/hashtypes</code> endpoint (filtered by <code>is_enabled=true</code> by default) to populate the selection dropdown during hashlist upload.</li> </ul>"},{"location":"user-guide/hashlists/#managing-hashlists","title":"Managing Hashlists","text":"<ul> <li>Viewing: The \"Hashlists\" dashboard provides a sortable and filterable view of all accessible hashlists, showing Name, Client, Status, Progress (% Cracked), and Creation Date.</li> </ul> <p> Detailed view of a hashlist named 'Test' showing ready status, crack progress indicator, and sample hashes section with individual hash entries and their crack status -   Downloading: Use the download icon on the dashboard or the <code>GET /api/hashlists/{id}/download</code> endpoint to retrieve the original uploaded hashlist file. -   Deleting:     *   Use the delete button in the hashlist detail view (with confirmation dialog) or the <code>DELETE /api/hashlists/{id}</code> endpoint.     *   Deleting a hashlist removes its entry from the <code>hashlists</code> table and removes associated entries from the <code>hashlist_hashes</code> table.     *   The original hashlist file is securely deleted from backend storage (overwritten with random data before removal).     *   Individual hashes in the central <code>hashes</code> table are not deleted if they are referenced by other hashlists.     *   Orphaned hashes (not linked to any hashlist) are automatically cleaned up.</p>"},{"location":"user-guide/hashlists/#data-retention","title":"Data Retention","text":"<p>Uploaded hashlists and their associated data are subject to the system's data retention policies. Old hashlists may be automatically purged based on client-specific or default retention settings configured by an administrator. See Admin Settings documentation for details. </p>"},{"location":"user-guide/jobs-workflows/","title":"Understanding Jobs and Workflows","text":""},{"location":"user-guide/jobs-workflows/#quick-overview","title":"Quick Overview","text":"<p>KrakenHashes uses a two-tier system to organize password cracking attacks:</p> <ol> <li>Preset Jobs: Individual attack configurations (like a single recipe)</li> <li>Job Workflows: Collections of preset jobs that run in sequence (like a cookbook)</li> </ol> <p>This system ensures consistent, efficient password auditing across your organization.</p>"},{"location":"user-guide/jobs-workflows/#how-it-works","title":"How It Works","text":""},{"location":"user-guide/jobs-workflows/#the-password-cracking-process","title":"The Password Cracking Process","text":"<p>When you submit a hashlist for cracking, KrakenHashes can apply a workflow that: 1. Tries the most likely passwords first (common passwords) 2. Progressively tries more complex attacks 3. Ensures no time is wasted on inefficient approaches</p>"},{"location":"user-guide/jobs-workflows/#example-workflow-in-action","title":"Example Workflow in Action","text":"<p>Imagine you're auditing passwords for a company. A typical workflow might:</p> <ol> <li>First: Check against known leaked passwords (fast, high success rate)</li> <li>Next: Try common passwords with variations (password1, Password123!)</li> <li>Then: Combine company terms with numbers (CompanyName2024)</li> <li>Finally: Attempt more exhaustive searches if needed</li> </ol> <p>"},{"location":"user-guide/jobs-workflows/#benefits","title":"Benefits","text":""},{"location":"user-guide/jobs-workflows/#consistency","title":"Consistency","text":"<ul> <li>Every password audit follows the same proven methodology</li> <li>No steps are accidentally skipped</li> <li>New team members can run expert-level audits immediately</li> </ul>"},{"location":"user-guide/jobs-workflows/#efficiency","title":"Efficiency","text":"<ul> <li>Fast attacks run first, finding easy passwords quickly</li> <li>Resource-intensive attacks only run when necessary</li> <li>Priority system ensures optimal resource usage</li> </ul>"},{"location":"user-guide/jobs-workflows/#customization","title":"Customization","text":"<ul> <li>Different workflows for different scenarios:</li> <li>Quick compliance checks</li> <li>Thorough security audits  </li> <li>Industry-specific patterns</li> <li>Post-breach assessments</li> </ul>"},{"location":"user-guide/jobs-workflows/#common-attack-types","title":"Common Attack Types","text":""},{"location":"user-guide/jobs-workflows/#dictionary-attacks","title":"Dictionary Attacks","text":"<p>Uses lists of known passwords: - Common passwords (password, 123456) - Previously leaked passwords - Industry-specific terms</p>"},{"location":"user-guide/jobs-workflows/#rule-based-attacks","title":"Rule-Based Attacks","text":"<p>Applies transformations to dictionary words: - Capitalize first letter - Add numbers at the end - Replace letters with symbols (@ for a, 3 for e)</p>"},{"location":"user-guide/jobs-workflows/#hybrid-attacks","title":"Hybrid Attacks","text":"<p>Combines dictionaries with patterns: - Dictionary word + 4 digits (password2024) - Year + dictionary word (2024password)</p>"},{"location":"user-guide/jobs-workflows/#brute-force","title":"Brute Force","text":"<p>Tries all possible combinations for a pattern: - All 4-digit PINs (0000-9999) - All 6-character lowercase (aaaaaa-zzzzzz)</p>"},{"location":"user-guide/jobs-workflows/#understanding-priorities","title":"Understanding Priorities","text":"<p>Jobs within workflows run in priority order: - Critical Priority (90-100): Emergency response, security incidents - High Priority (70-89): Time-sensitive audits, compliance deadlines - Normal Priority (40-69): Standard security assessments - Low Priority (10-39): Background processing, research tasks - Minimal Priority (0-9): Non-urgent, opportunistic processing</p>"},{"location":"user-guide/jobs-workflows/#how-priority-affects-your-jobs","title":"How Priority Affects Your Jobs","text":"<ol> <li>Execution Order: Higher priority jobs start first when agents are available</li> <li>Resource Allocation: Critical jobs can use more agents simultaneously</li> <li>Queue Management: Jobs with the same priority run in the order they were submitted</li> <li>Smart Scheduling: The system optimizes agent assignment based on priorities</li> </ol> <p>"},{"location":"user-guide/jobs-workflows/#job-interruption-and-resumption","title":"Job Interruption and Resumption","text":"<p>KrakenHashes includes an intelligent job interruption system that ensures critical tasks get the resources they need without losing work on other jobs.</p>"},{"location":"user-guide/jobs-workflows/#how-job-interruption-works","title":"How Job Interruption Works","text":"<p>When a high-priority job needs immediate attention:</p> <ol> <li>Automatic Detection: The system identifies when critical jobs are waiting</li> <li>Smart Interruption: Only interrupts lower priority jobs when necessary</li> <li>Progress Preservation: All completed work is saved before interruption</li> <li>Seamless Resumption: Interrupted jobs automatically continue when resources are available</li> </ol>"},{"location":"user-guide/jobs-workflows/#what-this-means-for-your-jobs","title":"What This Means for Your Jobs","text":"<ul> <li>No Lost Work: If your job is interrupted, it will resume from where it stopped</li> <li>Transparent Process: You'll see the job status change from \"running\" to \"pending\" and back</li> <li>Fair Resource Sharing: The system balances urgent needs with ongoing work</li> <li>Automatic Management: No manual intervention required for resumption</li> </ul>"},{"location":"user-guide/jobs-workflows/#understanding-job-statuses","title":"Understanding Job Statuses","text":"<p> The Jobs Management interface showing active password cracking jobs with status filtering (ALL, PENDING, RUNNING, COMPLETED, FAILED). The table displays job details including name, hashlist, progress, keyspace, cracked count, agents assigned, priority level, and available actions.</p> <ul> <li>Pending: Job is waiting for available agents</li> <li>Running: Job is actively being processed</li> <li>Completed: Job finished successfully</li> <li>Failed: Job encountered an error</li> <li>Interrupted: Job was paused for a higher priority task (automatically resumes)</li> </ul>"},{"location":"user-guide/jobs-workflows/#priority-best-practices","title":"Priority Best Practices","text":"<p>To ensure optimal performance:</p> <ol> <li>Use Appropriate Priorities: Don't mark everything as high priority</li> <li>Plan for Interruptions: Expect that low-priority jobs may pause for critical work</li> <li>Monitor Progress: Check job status regularly for time-sensitive tasks</li> <li>Communicate Urgency: Work with administrators to set correct priorities for critical audits</li> </ol>"},{"location":"user-guide/jobs-workflows/#automatic-job-completion","title":"Automatic Job Completion","text":"<p>KrakenHashes automatically detects when all hashes in a hashlist have been cracked and manages the lifecycle of related jobs to prevent failures and wasted resources.</p>"},{"location":"user-guide/jobs-workflows/#how-it-works_1","title":"How It Works","text":"<p>When an agent reports hashcat status code 6 (all hashes cracked):</p> <ol> <li>Detection: Backend receives status code 6 from hashcat's JSON status output</li> <li>Trust Model: Status code 6 is trusted as authoritative (no database verification needed)</li> <li>Running Jobs: Currently executing jobs are stopped and marked as completed at 100%</li> <li>Pending Jobs: Jobs that haven't started yet are automatically deleted</li> <li>Notifications: Email notifications sent for completed jobs (if configured)</li> </ol>"},{"location":"user-guide/jobs-workflows/#why-this-matters","title":"Why This Matters","text":"<p>Hashcat's <code>--remove</code> option removes cracked hashes from the input file during execution. If all hashes are cracked, the file becomes empty, causing subsequent jobs to fail. This automatic detection prevents those failures.</p>"},{"location":"user-guide/jobs-workflows/#what-youll-see","title":"What You'll See","text":"<ul> <li>Job status changes to \"completed\" even if not all keyspace was processed</li> <li>Progress shows 100% when all target hashes are cracked</li> <li>Related pending jobs for the same hashlist disappear from the queue</li> <li>Email notification of job completion (if email is configured)</li> </ul> <p>This ensures your workflow doesn't encounter errors when your cracking campaign is successful!</p>"},{"location":"user-guide/jobs-workflows/#technical-details","title":"Technical Details","text":"<p>For administrators and developers interested in the implementation: - Status code 6 detection occurs in the agent's hashcat output parser - The <code>AllHashesCracked</code> flag is transmitted via WebSocket to the backend - <code>HashlistCompletionService</code> handles the cleanup asynchronously - See the Job Completion System architecture documentation for full details</p>"},{"location":"user-guide/jobs-workflows/#monitoring-job-execution","title":"Monitoring Job Execution","text":""},{"location":"user-guide/jobs-workflows/#the-job-details-page","title":"The Job Details Page","text":"<p>Once you've submitted a job, you can monitor its progress in real-time through the Job Details page. Access it by clicking on any job in the Jobs list or navigating to <code>/jobs/{job-id}</code>.</p> <p>The Job Details page shows real-time progress, assigned agents, and crack results</p>"},{"location":"user-guide/jobs-workflows/#real-time-updates","title":"Real-Time Updates","text":"<p>The Job Details page provides live updates for active jobs:</p>"},{"location":"user-guide/jobs-workflows/#auto-refresh","title":"Auto-Refresh","text":"<ul> <li>Automatic Updates: The page refreshes every 5 seconds for running jobs (configurable by administrators)</li> <li>Smart Refresh: Auto-refresh pauses when you're editing settings to prevent data loss</li> <li>Manual Refresh: Click the refresh button for immediate updates</li> <li>Status-Based: Auto-refresh only active for pending, running, or paused jobs</li> </ul>"},{"location":"user-guide/jobs-workflows/#progress-visualization","title":"Progress Visualization","text":"<p>The new progress bar provides at-a-glance job status: - Visual Progress: Color-coded bar showing completion percentage - Keyspace Coverage: Shows processed vs. total keyspace - Time Estimates: Estimated time remaining based on current speed - Agent Distribution: See how work is distributed across agents</p>"},{"location":"user-guide/jobs-workflows/#information-available","title":"Information Available","text":""},{"location":"user-guide/jobs-workflows/#job-summary","title":"Job Summary","text":"<ul> <li>Status: Current job state (pending, running, completed, failed)</li> <li>Priority: Job priority level and queue position</li> <li>Hashlist: Target hashlist being processed</li> <li>Workflow: Associated workflow and current preset job</li> </ul>"},{"location":"user-guide/jobs-workflows/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Hash Rate: Combined speed across all agents (H/s, MH/s, GH/s)</li> <li>Keyspace Progress: Amount of keyspace processed</li> <li>Cracks Found: Real-time count of cracked passwords</li> <li>Efficiency: Cracks per billion attempts</li> </ul>"},{"location":"user-guide/jobs-workflows/#agent-assignment","title":"Agent Assignment","text":"<ul> <li>Active Agents: List of agents currently working on the job</li> <li>Agent Performance: Individual agent hash rates and progress</li> <li>Task Distribution: How chunks are distributed</li> <li>Agent Status: Online/offline status of assigned agents</li> </ul>"},{"location":"user-guide/jobs-workflows/#crack-results","title":"Crack Results","text":"<ul> <li>Real-Time Cracks: See passwords as they're cracked</li> <li>Crack Positions: Where in the attack the crack occurred</li> <li>Plain Text: Recovered passwords (if permissions allow)</li> <li>Export Options: Download results in various formats</li> </ul>"},{"location":"user-guide/jobs-workflows/#completed-tasks-history","title":"Completed Tasks History","text":"<p>The Job Details page maintains a comprehensive history of all completed tasks, providing valuable insights into job execution:</p>"},{"location":"user-guide/jobs-workflows/#information-displayed","title":"Information Displayed","text":"<p>For each completed task, you can view: - Agent ID: The specific agent that processed the task - Task ID: Unique identifier for reference and troubleshooting - Completion Time: When the task finished processing - Keyspace Range: The exact portion of keyspace that was processed - Final Progress: The percentage of the task that was completed - Average Speed: Hash rate achieved during task execution - Cracks Found: Number of passwords cracked (click to view details in the POT)</p>"},{"location":"user-guide/jobs-workflows/#organization-and-navigation","title":"Organization and Navigation","text":"<ul> <li>Automatic Sorting: Tasks are sorted by completion time, with most recent first</li> <li>Pagination Controls: Navigate through large task lists with configurable page sizes:</li> <li>25 items per page (default)</li> <li>50, 100, or 200 items for larger views</li> <li>Persistent History: Completed tasks remain visible even after job completion</li> </ul>"},{"location":"user-guide/jobs-workflows/#use-cases","title":"Use Cases","text":"<p>The completed tasks history helps with: - Performance Analysis: Compare hash rates across different agents to identify performance variations - Crack Distribution: See which keyspace ranges yielded the most cracks - Troubleshooting: Identify if specific agents or keyspace ranges had issues - Audit Trail: Maintain a complete record of job execution for compliance or review - Resource Planning: Analyze task completion patterns to optimize future job configurations</p> <p>Performance Insights</p> <p>Use the completed tasks table to identify your most efficient agents and optimize task distribution in future jobs.</p>"},{"location":"user-guide/jobs-workflows/#interactive-controls","title":"Interactive Controls","text":"<p>While monitoring your job, you can:</p>"},{"location":"user-guide/jobs-workflows/#adjust-settings","title":"Adjust Settings","text":"<ul> <li>Change Priority: Modify job priority to speed up or slow down execution</li> <li>Agent Limits: Adjust maximum agents assigned to the job</li> <li>Pause/Resume: Temporarily halt job execution</li> </ul>"},{"location":"user-guide/jobs-workflows/#job-actions","title":"Job Actions","text":"<ul> <li>Stop Job: Terminate the job (progress is saved)</li> <li>Clone Job: Create a new job with the same settings</li> <li>View Logs: Access detailed execution logs</li> <li>Export Results: Download crack results and reports</li> </ul>"},{"location":"user-guide/jobs-workflows/#understanding-progress-indicators","title":"Understanding Progress Indicators","text":""},{"location":"user-guide/jobs-workflows/#keyspace-progress","title":"Keyspace Progress","text":"<p>The keyspace represents the total search space: - Linear Progress: Steady advancement through wordlist attacks - Chunk-Based: Progress jumps as chunks complete - Rule Multiplication: Progress may seem slow with large rule sets - Accurate Tracking: Progress values are captured directly from hashcat for precision</p> <p>Accurate Progress Tracking</p> <p>KrakenHashes captures actual keyspace values directly from hashcat (<code>progress[1]</code>), ensuring that progress percentages accurately reflect the real search progress, especially for jobs with rules or combination attacks where estimation can be inaccurate.</p>"},{"location":"user-guide/jobs-workflows/#time-estimates","title":"Time Estimates","text":"<p>Estimated completion times are based on: - Current hash rate - Remaining keyspace - Historical performance - Agent availability - Actual keyspace values from hashcat</p> <p>Estimate Accuracy</p> <p>Time estimates become more accurate as the job progresses and the system learns the actual performance characteristics. After the first benchmark or progress update, the system uses actual keyspace values from hashcat instead of estimates.</p>"},{"location":"user-guide/jobs-workflows/#monitoring-best-practices","title":"Monitoring Best Practices","text":"<ol> <li>Check Early Progress: Verify the job started correctly in the first few minutes</li> <li>Monitor Agent Assignment: Ensure sufficient agents are assigned</li> <li>Watch for Stalls: If progress stops, check agent status</li> <li>Review Partial Results: Examine cracked passwords as they appear</li> <li>Adjust Priority if Needed: Increase priority for time-sensitive jobs</li> </ol>"},{"location":"user-guide/jobs-workflows/#troubleshooting-job-issues","title":"Troubleshooting Job Issues","text":""},{"location":"user-guide/jobs-workflows/#job-stuck-in-pending","title":"Job Stuck in Pending","text":"<ul> <li>Check if agents are available</li> <li>Verify agent scheduling settings</li> <li>Review job priority relative to other jobs</li> </ul>"},{"location":"user-guide/jobs-workflows/#slow-progress","title":"Slow Progress","text":"<ul> <li>Check assigned agent count</li> <li>Review chunk size settings</li> <li>Verify network connectivity</li> <li>Consider increasing job priority</li> </ul>"},{"location":"user-guide/jobs-workflows/#no-cracks-found","title":"No Cracks Found","text":"<ul> <li>Normal for strong passwords</li> <li>Review attack methodology</li> <li>Consider different workflows</li> <li>Check hashlist format</li> </ul>"},{"location":"user-guide/jobs-workflows/#real-world-applications","title":"Real-World Applications","text":""},{"location":"user-guide/jobs-workflows/#compliance-auditing","title":"Compliance Auditing","text":"<p>Verify passwords meet policy requirements: - Minimum length checks - Complexity requirements - Banned password lists</p>"},{"location":"user-guide/jobs-workflows/#security-assessments","title":"Security Assessments","text":"<p>Identify weak passwords before attackers do: - Test against current attack techniques - Benchmark password strength - Provide actionable reports</p>"},{"location":"user-guide/jobs-workflows/#incident-response","title":"Incident Response","text":"<p>Quickly assess breach impact: - Check if compromised passwords are reused - Identify affected accounts - Prioritize password resets</p>"},{"location":"user-guide/jobs-workflows/#what-this-means-for-you","title":"What This Means for You","text":"<p>As a user, the preset jobs and workflows system: - Ensures thorough password testing - Provides consistent results - Optimizes cracking time - Delivers actionable insights</p> <p>Your administrators have configured these workflows based on: - Industry best practices - Your organization's specific needs - Current threat landscape - Compliance requirements</p>"},{"location":"user-guide/jobs-workflows/#next-steps","title":"Next Steps","text":"<ul> <li>Review your hashlist results to understand which attacks succeeded</li> <li>Work with your security team to address found passwords</li> <li>Consider implementing stronger password policies</li> <li>Schedule regular password audits using these workflows</li> </ul> <p>For more detailed information about creating and managing workflows, see the administrator documentation.</p>"},{"location":"user-guide/troubleshooting/","title":"KrakenHashes Troubleshooting Guide","text":"<p>This guide helps you resolve common issues when using KrakenHashes. If your issue isn't covered here, please contact support with relevant error messages and logs.</p>"},{"location":"user-guide/troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation Issues</li> <li>Login and Authentication Problems</li> <li>Job Creation and Execution Issues</li> <li>Agent Connection Problems</li> <li>Performance Issues</li> <li>File Upload Errors</li> <li>Results Not Appearing</li> <li>How to Check Logs and Get Help</li> </ol>"},{"location":"user-guide/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"user-guide/troubleshooting/#docker-compose-fails-to-start","title":"Docker Compose Fails to Start","text":"<p>Error: <code>docker-compose: command not found</code> - Solution: Install Docker Compose following the official documentation for your OS</p> <p>Error: <code>Cannot connect to the Docker daemon</code> - Solution:    - Ensure Docker is running: <code>sudo systemctl start docker</code>   - Add your user to the docker group: <code>sudo usermod -aG docker $USER</code>   - Log out and back in for group changes to take effect</p> <p>Error: <code>Port 8080 is already in use</code> - Solution:   - Check what's using the port: <code>sudo lsof -i :8080</code>   - Stop the conflicting service or change KrakenHashes ports in <code>docker-compose.yml</code></p>"},{"location":"user-guide/troubleshooting/#database-migration-failures","title":"Database Migration Failures","text":"<p>Error: <code>pq: password authentication failed for user</code> - Solution:    - Check your <code>DATABASE_URL</code> environment variable   - Ensure PostgreSQL credentials match in <code>docker-compose.yml</code>   - Try resetting with: <code>docker-compose down -v</code> then <code>docker-compose up -d</code></p> <p>Error: <code>migration failed: table already exists</code> - Solution:   - Reset migrations: <code>docker-compose exec backend make migrate-down</code>   - Clean database: <code>docker-compose down -v</code>   - Restart: <code>docker-compose up -d</code></p>"},{"location":"user-guide/troubleshooting/#ssltls-certificate-issues","title":"SSL/TLS Certificate Issues","text":"<p>Error: <code>NET::ERR_CERT_AUTHORITY_INVALID</code> - Solution:   - For self-signed certificates, follow the installation guide in <code>docs/SSL_TLS_SETUP.md</code>   - Import the CA certificate to your browser/OS trust store   - For production, use proper certificates with <code>KH_TLS_MODE=provided</code></p>"},{"location":"user-guide/troubleshooting/#login-and-authentication-problems","title":"Login and Authentication Problems","text":""},{"location":"user-guide/troubleshooting/#cannot-log-in","title":"Cannot Log In","text":"<p>Error: <code>Invalid credentials</code> - Solution:   - Verify username and password are correct   - Check if account is active (admin can verify)   - Try resetting password through admin</p> <p>Error: <code>Token expired</code> - Solution:   - Clear browser cookies/localStorage   - Log in again   - If persistent, check system time synchronization</p>"},{"location":"user-guide/troubleshooting/#multi-factor-authentication-issues","title":"Multi-Factor Authentication Issues","text":"<p>Error: <code>Invalid TOTP code</code> - Solution:   - Ensure device time is synchronized   - Verify you're using the correct authenticator app   - Try backup codes if available   - Contact admin to reset MFA</p> <p>Error: <code>Email verification code not received</code> - Solution:   - Check spam/junk folder   - Verify email address is correct in profile   - Check backend logs for SMTP errors   - Contact admin to check email configuration</p>"},{"location":"user-guide/troubleshooting/#session-timeout","title":"Session Timeout","text":"<p>Issue: Logged out unexpectedly - Solution:   - Check JWT token expiration settings   - Enable \"Remember Me\" during login   - Check for network connectivity issues</p>"},{"location":"user-guide/troubleshooting/#job-creation-and-execution-issues","title":"Job Creation and Execution Issues","text":""},{"location":"user-guide/troubleshooting/#cannot-create-job","title":"Cannot Create Job","text":"<p>Error: <code>No hashlist selected</code> - Solution:   - Upload a hashlist first via Hashlists page   - Ensure hashlist contains valid hashes   - Check hashlist format matches selected hash type</p> <p>Error: <code>No available agents</code> - Solution:   - Verify at least one agent is connected   - Check agent status on Agents page   - Ensure agents have required capabilities</p> <p>Error: <code>Invalid workflow configuration</code> - Solution:   - Verify all required fields are filled   - Check attack mode parameters are valid   - Ensure selected wordlists/rules exist</p>"},{"location":"user-guide/troubleshooting/#job-stuck-in-pending","title":"Job Stuck in Pending","text":"<p>Issue: Job never starts - Causes:   - No agents available with required capabilities   - Agent offline or disconnected   - Resource constraints on agent - Solution:   - Check agent status and capabilities   - Verify agent GPU requirements match job   - Check agent logs for errors</p>"},{"location":"user-guide/troubleshooting/#job-failed-immediately","title":"Job Failed Immediately","text":"<p>Error: <code>Hashcat execution failed</code> - Solution:   - Check job logs for specific hashcat errors   - Verify hash format matches selected type   - Ensure wordlists/rules are accessible   - Check agent has sufficient disk space</p>"},{"location":"user-guide/troubleshooting/#agent-connection-problems","title":"Agent Connection Problems","text":""},{"location":"user-guide/troubleshooting/#agent-wont-connect","title":"Agent Won't Connect","text":"<p>Error: <code>websocket: bad handshake</code> - Solution:   - Verify backend URL in agent config   - Check firewall allows WebSocket connections   - Ensure SSL certificates are trusted by agent</p> <p>Error: <code>Invalid API key</code> - Solution:   - Regenerate API key from agent settings   - Update agent configuration with new key   - Restart agent after configuration change</p> <p>Error: <code>Claim code invalid or expired</code> - Solution:   - Generate new claim code from UI   - Use claim code within 15 minutes   - Ensure claim code hasn't been used already</p>"},{"location":"user-guide/troubleshooting/#agent-keeps-disconnecting","title":"Agent Keeps Disconnecting","text":"<p>Issue: Frequent reconnections - Causes:   - Network instability   - Firewall/proxy timeout settings   - Backend overload - Solution:   - Check network connectivity   - Increase WebSocket timeout settings   - Monitor backend resource usage</p>"},{"location":"user-guide/troubleshooting/#agent-not-detecting-gpus","title":"Agent Not Detecting GPUs","text":"<p>Error: <code>No GPUs detected</code> - Solution:   - Verify GPU drivers are installed:     - NVIDIA: <code>nvidia-smi</code>     - AMD: <code>rocm-smi</code>     - Intel: Check oneAPI installation   - Run agent with sudo if needed   - Check GPU is not in exclusive mode</p>"},{"location":"user-guide/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"user-guide/troubleshooting/#slow-hash-cracking","title":"Slow Hash Cracking","text":"<p>Issue: Lower than expected hash rates - Causes:   - Thermal throttling   - Incorrect workload tuning   - CPU bottleneck - Solution:   - Monitor GPU temperature   - Adjust workload profile in job settings   - Ensure adequate cooling   - Check hashcat benchmark mode</p>"},{"location":"user-guide/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Issue: System running out of memory - Solution:   - Reduce wordlist buffer size   - Split large hashlists   - Use rule-based attacks instead of large wordlists   - Monitor with <code>docker stats</code></p>"},{"location":"user-guide/troubleshooting/#database-performance","title":"Database Performance","text":"<p>Issue: Slow query responses - Solution:   - Check database indexes are created   - Monitor with <code>docker-compose logs postgres</code>   - Consider increasing PostgreSQL resources   - Clean up old completed jobs</p>"},{"location":"user-guide/troubleshooting/#file-upload-errors","title":"File Upload Errors","text":""},{"location":"user-guide/troubleshooting/#upload-fails","title":"Upload Fails","text":"<p>Error: <code>Request entity too large</code> - Solution:   - Check file size limits in nginx config   - Split large files into smaller chunks   - Use compression for text files</p> <p>Error: <code>Invalid file format</code> - Solution:   - Verify file format matches expected type:     - Hashlists: One hash per line     - Wordlists: Plain text, one word per line     - Rules: Hashcat rule format   - Remove any special characters or BOM</p> <p>Error: <code>Permission denied</code> - Solution:   - Check backend data directory permissions   - Ensure Docker volumes are writable   - Verify disk space available</p>"},{"location":"user-guide/troubleshooting/#files-not-appearing","title":"Files Not Appearing","text":"<p>Issue: Uploaded files not visible - Solution:   - Refresh the page   - Check upload completed successfully   - Verify file processing logs   - Check file storage directory</p>"},{"location":"user-guide/troubleshooting/#results-not-appearing","title":"Results Not Appearing","text":""},{"location":"user-guide/troubleshooting/#cracked-passwords-not-showing","title":"Cracked Passwords Not Showing","text":"<p>Issue: Job shows progress but no results - Causes:   - Results not yet synced   - Database write issues   - Display filtering - Solution:   - Wait for sync interval (usually 30 seconds)   - Check job logs for errors   - Verify database connectivity   - Check results filter settings</p>"},{"location":"user-guide/troubleshooting/#export-not-working","title":"Export Not Working","text":"<p>Error: <code>Export failed</code> - Solution:   - Check browser download permissions   - Try different export format   - Verify results exist to export   - Check browser console for errors</p>"},{"location":"user-guide/troubleshooting/#statistics-incorrect","title":"Statistics Incorrect","text":"<p>Issue: Progress/statistics don't match - Solution:   - Force refresh the page   - Check for duplicate hashes   - Verify job status is updated   - Review calculation logic in logs</p>"},{"location":"user-guide/troubleshooting/#how-to-check-logs-and-get-help","title":"How to Check Logs and Get Help","text":""},{"location":"user-guide/troubleshooting/#accessing-logs","title":"Accessing Logs","text":""},{"location":"user-guide/troubleshooting/#docker-logs","title":"Docker Logs","text":"<pre><code># View all logs\ndocker-compose logs\n\n# View specific service logs\ndocker-compose logs -f backend    # Backend logs\ndocker-compose logs -f postgres   # Database logs\ndocker-compose logs -f app        # Frontend/nginx logs\n\n# Save logs to file\ndocker-compose logs backend &gt; backend.log\n</code></pre>"},{"location":"user-guide/troubleshooting/#log-file-locations","title":"Log File Locations","text":"<ul> <li>Backend: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/backend/</code></li> <li>PostgreSQL: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/postgres/</code></li> <li>Nginx: <code>/home/zerkereod/Programming/passwordCracking/kh-backend/logs/krakenhashes/nginx/</code></li> </ul>"},{"location":"user-guide/troubleshooting/#agent-logs","title":"Agent Logs","text":"<pre><code># On agent machine\ntail -f /var/log/krakenhashes-agent.log\n\n# Or check systemd\njournalctl -u krakenhashes-agent -f\n</code></pre>"},{"location":"user-guide/troubleshooting/#what-to-include-when-reporting-issues","title":"What to Include When Reporting Issues","text":"<ol> <li>Error Messages</li> <li>Exact error text</li> <li>Screenshot if UI issue</li> <li> <p>Time when error occurred</p> </li> <li> <p>Environment Information</p> </li> <li>KrakenHashes version</li> <li>Browser type and version</li> <li>Operating system</li> <li> <p>Docker version</p> </li> <li> <p>Steps to Reproduce</p> </li> <li>What you were trying to do</li> <li>Exact steps taken</li> <li> <p>Expected vs actual behavior</p> </li> <li> <p>Relevant Logs</p> </li> <li>Error entries from logs</li> <li>Stack traces if available</li> <li>Related warning messages</li> </ol>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":"<ol> <li>Check Documentation</li> <li>Review relevant guides in <code>/docs</code></li> <li>Check CLAUDE.md for development info</li> <li> <p>Review API documentation</p> </li> <li> <p>Search Known Issues</p> </li> <li>Check GitHub issues</li> <li>Review release notes</li> <li> <p>Search error messages</p> </li> <li> <p>Contact Support</p> </li> <li>Email: support@krakenhashes.com</li> <li>Include issue report with details above</li> <li>Provide job IDs if relevant</li> </ol>"},{"location":"user-guide/troubleshooting/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for more details: <pre><code># Backend\nexport LOG_LEVEL=debug\n\n# Agent\nkrakenhashes-agent --debug\n\n# Frontend\n# Open browser developer console\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#common-quick-fixes","title":"Common Quick Fixes","text":"<ol> <li> <p>Restart Services <pre><code>docker-compose restart backend\n</code></pre></p> </li> <li> <p>Clear Browser Cache</p> </li> <li>Hard refresh: Ctrl+Shift+R (Cmd+Shift+R on Mac)</li> <li> <p>Clear site data in browser settings</p> </li> <li> <p>Reset Database Connection <pre><code>docker-compose restart postgres backend\n</code></pre></p> </li> <li> <p>Verify Connectivity <pre><code># Test backend\ncurl -k https://localhost:8080/api/v1/health\n\n# Test database\ndocker-compose exec postgres pg_isready\n</code></pre></p> </li> </ol>"},{"location":"user-guide/troubleshooting/#prevention-tips","title":"Prevention Tips","text":"<ol> <li>Regular Maintenance</li> <li>Monitor disk space</li> <li>Clean old job data</li> <li>Update regularly</li> <li> <p>Backup database</p> </li> <li> <p>Performance Monitoring</p> </li> <li>Use <code>docker stats</code></li> <li>Monitor agent resources</li> <li> <p>Set up alerting</p> </li> <li> <p>Security Best Practices</p> </li> <li>Keep software updated</li> <li>Use strong passwords</li> <li>Enable MFA</li> <li>Regular security audits</li> </ol>"},{"location":"user-guide/wordlists-rules/","title":"Wordlists and Rules User Guide","text":"<p>This guide explains how to use the wordlist and rule management features in KrakenHashes.</p>"},{"location":"user-guide/wordlists-rules/#overview","title":"Overview","text":"<p>KrakenHashes provides comprehensive management of wordlists and rules used for password cracking operations. These resources are essential for effective password cracking jobs.</p>"},{"location":"user-guide/wordlists-rules/#accessing-the-management-interface","title":"Accessing the Management Interface","text":"<ol> <li>Log in to the KrakenHashes web interface</li> <li>Navigate to the \"Resources\" section in the main menu</li> <li>Select either \"Wordlists\" or \"Rules\" to manage the respective resources</li> </ol>"},{"location":"user-guide/wordlists-rules/#wordlists-management","title":"Wordlists Management","text":""},{"location":"user-guide/wordlists-rules/#viewing-wordlists","title":"Viewing Wordlists","text":"<p>The wordlists page displays all available wordlists with the following information: - Name - Description - Type (General, Specialized, Targeted, Custom) - Format (Plaintext, Compressed) - Size - Word count - Tags - Status</p> <p> Wordlist Management interface showing category filtering, wordlist details table, and upload functionality</p> <p>You can sort and filter the list by any of these attributes.</p>"},{"location":"user-guide/wordlists-rules/#uploading-a-wordlist","title":"Uploading a Wordlist","text":"<p>To upload a new wordlist:</p> <ol> <li>Click the \"Upload Wordlist\" button</li> <li>Fill in the following information:</li> <li>Name (optional - defaults to filename)</li> <li>Description (optional)</li> <li>Type (General, Specialized, Targeted, Custom)</li> <li>Tags (optional)</li> <li>Select the file to upload</li> <li>Click \"Upload\"</li> </ol> <p>The system will: - Check if a file with the same name already exists - Calculate the MD5 hash of the file - Count the words in the file - Store the file in the appropriate directory based on the selected type</p> <p>For large files, the word counting process may take some time. The wordlist will be available with a \"pending\" status until counting completes.</p>"},{"location":"user-guide/wordlists-rules/#downloading-a-wordlist","title":"Downloading a Wordlist","text":"<p>To download a wordlist:</p> <ol> <li>Find the wordlist in the list</li> <li>Click the \"Download\" button</li> <li>The file will be downloaded to your computer</li> </ol>"},{"location":"user-guide/wordlists-rules/#deleting-a-wordlist","title":"Deleting a Wordlist","text":"<p>To delete a wordlist:</p> <ol> <li>Find the wordlist in the list</li> <li>Click the \"Delete\" button</li> <li>Confirm the deletion</li> </ol> <p>Note that this only removes the wordlist from the database, not from the filesystem.</p>"},{"location":"user-guide/wordlists-rules/#rules-management","title":"Rules Management","text":""},{"location":"user-guide/wordlists-rules/#viewing-rules","title":"Viewing Rules","text":"<p>The rules page displays all available rules with the following information: - Name - Description - Type (Hashcat, John, Custom) - Size - Rule count - Tags - Status</p> <p> Rule Management page showing the uploaded _nsakey.v2.dive rule file with verification status, type (Hashcat), file size (1.19 MB), and rule count (123,289 rules). Filter tabs allow sorting by rule type.</p> <p>You can sort and filter the list by any of these attributes.</p>"},{"location":"user-guide/wordlists-rules/#uploading-a-rule","title":"Uploading a Rule","text":"<p>To upload a new rule:</p> <ol> <li>Click the \"Upload Rule\" button</li> <li>Fill in the following information:</li> <li>Name (optional - defaults to filename)</li> <li>Description (optional)</li> <li>Type (Hashcat, John, Custom)</li> <li>Tags (optional)</li> <li>Select the file to upload</li> <li>Click \"Upload\"</li> </ol> <p>The system will: - Check if a file with the same name already exists - Calculate the MD5 hash of the file - Count the rules in the file - Store the file in the appropriate directory based on the selected type</p>"},{"location":"user-guide/wordlists-rules/#downloading-a-rule","title":"Downloading a Rule","text":"<p>To download a rule:</p> <ol> <li>Find the rule in the list</li> <li>Click the \"Download\" button</li> <li>The file will be downloaded to your computer</li> </ol>"},{"location":"user-guide/wordlists-rules/#deleting-a-rule","title":"Deleting a Rule","text":"<p>To delete a rule:</p> <ol> <li>Find the rule in the list</li> <li>Click the \"Delete\" button</li> <li>Confirm the deletion</li> </ol> <p>Note that this only removes the rule from the database, not from the filesystem.</p>"},{"location":"user-guide/wordlists-rules/#managing-tags","title":"Managing Tags","text":"<p>Tags help organize and categorize wordlists and rules.</p>"},{"location":"user-guide/wordlists-rules/#adding-tags","title":"Adding Tags","text":"<p>To add a tag to a wordlist or rule:</p> <ol> <li>Find the item in the list</li> <li>Click the \"Edit\" button</li> <li>Add the tag in the tags field</li> <li>Click \"Save\"</li> </ol>"},{"location":"user-guide/wordlists-rules/#removing-tags","title":"Removing Tags","text":"<p>To remove a tag:</p> <ol> <li>Find the item in the list</li> <li>Click the \"Edit\" button</li> <li>Remove the tag from the tags field</li> <li>Click \"Save\"</li> </ol>"},{"location":"user-guide/wordlists-rules/#duplicate-handling","title":"Duplicate Handling","text":"<p>The system handles duplicate files intelligently:</p> <ul> <li>If you upload a file with the same name as an existing file and the content is identical (same MD5 hash), the system will recognize it as a duplicate and return the existing entry.</li> <li>If you upload a file with the same name but different content, the system will update the existing file with the new content.</li> <li>If you upload a file with different name but identical content to an existing file, the system will store both files separately.</li> </ul> <p>This approach ensures that: - Files are not unnecessarily duplicated - Updates to existing files are properly tracked - You can maintain multiple versions of similar files with different names</p>"},{"location":"user-guide/wordlists-rules/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive filenames: Choose clear, descriptive filenames that indicate the content and purpose of the file.</li> <li>Add meaningful descriptions: Include detailed descriptions to help other users understand the purpose and content of the wordlist or rule.</li> <li>Use appropriate types: Select the correct type for each wordlist or rule to ensure proper organization.</li> <li>Apply relevant tags: Use tags to categorize and make resources easier to find.</li> <li>Organize by purpose: Use the specialized and targeted categories for wordlists with specific purposes. </li> </ol>"}]}